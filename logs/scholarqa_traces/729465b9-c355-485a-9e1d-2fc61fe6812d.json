{
    "query": "How do causal (unidirectional) and non-causal (bidirectional) attention mechanisms differ in encoder-only, decoder-only (including prefix/hybrid LMs), and encoder-decoder language models, particularly in terms of information flow and their suitability for common NLP tasks like text generation, understanding, and sequence-to-sequence applications?",
    "user_id": "lib_user",
    "task_id": "729465b9-c355-485a-9e1d-2fc61fe6812d",
    "timestamp": "2025-06-24T01:04:59.750189",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 38,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.613077,
    "decomposed_query": {
        "rewritten_query": "Differences between causal (unidirectional) and non-causal (bidirectional) attention mechanisms in encoder-only, decoder-only (including prefix/hybrid LMs), and encoder-decoder language models, particularly in terms of information flow and their suitability for common NLP tasks like text generation, understanding, and sequence-to-sequence applications.",
        "keyword_query": "causal unidirectional non-causal bidirectional attention mechanisms encoder-only decoder-only prefix hybrid encoder-decoder language models information flow NLP text generation understanding sequence-to-sequence",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Linguistics"
        },
        "cost": 0.011154,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "venue": "Applied Sciences",
            "year": 2024,
            "reference_count": 108,
            "citation_count": 80,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2076-3417/14/5/2074/pdf?version=1709291698",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app14052074?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app14052074, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289385425",
                    "name": "Rajvardhan Patil"
                },
                {
                    "authorId": "117730513",
                    "name": "Venkat Gudivada"
                }
            ],
            "abstract": "Natural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.",
            "corpus_id": 268157336,
            "sentences": [
                {
                    "corpus_id": "268157336",
                    "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                    "text": "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
                    "score": 0.79231747357041,
                    "section_title": "Prefix (Non-Causal) Language Model",
                    "char_start_offset": 19013,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 243
                        },
                        {
                            "start": 246,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1329
                        },
                        {
                            "start": 1330,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1798
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.853515625
                },
                {
                    "corpus_id": "268157336",
                    "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                    "text": "An example of encoder-decoder architecture is the transformer model proposed in [24]. Its encoder and decoder blocks are stacked with multiple layers. As shown in Figure 3, the transformer encoder layer consists of a self-attention layer and a position-wise feed-forward layer. In addition to these two layers, decoder consists of a third cross-attention layer, which is responsible for attending to encoder output. \n\nEncoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks. T5 [25] uses encoder-decoder architecture. As stated in T5, using encoder-decoder structure helped to achieve good performance regarding classification as well as for generative tasks. \n\nAlthough encoder-decoder models end up having twice as many parameters as their decoder-only or encoder-only counterparts, they still have similar computational cost. Compared to PrefixLM models where the parameters are shared, here, the input and target are independently processed and use separate sets of parameters. Unlike decoder-only language models that are trained to generate the input, encoder-decoder models output target tokens. \n\nThe original transformer consisted of encoder-decoder blocks and was initially used for sequence-to-sequence tasks, such as NMT. However, it was discovered that, with the change in how the input is fed to the model, the single-stack (decoder or encoder) could also complete sequence-sequence model tasks. As a result, the subsequent models started containing either an encoder or decoder architecture. Below, we discuss these architectural variants of the original transformer model.",
                    "score": 0.7109203804130689,
                    "section_title": "Encoder-Decoder-Based Model",
                    "char_start_offset": 15589,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 85
                        },
                        {
                            "start": 86,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1169
                        },
                        {
                            "start": 1172,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1612
                        },
                        {
                            "start": 1615,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 2016
                        },
                        {
                            "start": 2017,
                            "end": 2098
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 988,
                            "end": 992,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5947265625
                }
            ],
            "relevance_judgement": 0.853515625,
            "relevance_judgment_input_expanded": "# Title: A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)\n# Venue: Applied Sciences\n# Authors: Rajvardhan Patil, Venkat Gudivada\n## Abstract\nNatural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.\n## Encoder-Decoder-Based Model\nAn example of encoder-decoder architecture is the transformer model proposed in [24]. Its encoder and decoder blocks are stacked with multiple layers. As shown in Figure 3, the transformer encoder layer consists of a self-attention layer and a position-wise feed-forward layer. In addition to these two layers, decoder consists of a third cross-attention layer, which is responsible for attending to encoder output. \n\nEncoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks. T5 [25] uses encoder-decoder architecture. As stated in T5, using encoder-decoder structure helped to achieve good performance regarding classification as well as for generative tasks. \n\nAlthough encoder-decoder models end up having twice as many parameters as their decoder-only or encoder-only counterparts, they still have similar computational cost. Compared to PrefixLM models where the parameters are shared, here, the input and target are independently processed and use separate sets of parameters. Unlike decoder-only language models that are trained to generate the input, encoder-decoder models output target tokens. \n\nThe original transformer consisted of encoder-decoder blocks and was initially used for sequence-to-sequence tasks, such as NMT. However, it was discovered that, with the change in how the input is fed to the model, the single-stack (decoder or encoder) could also complete sequence-sequence model tasks. As a result, the subsequent models started containing either an encoder or decoder architecture. Below, we discuss these architectural variants of the original transformer model.\n\n## Prefix (Non-Causal) Language Model\nPrefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
            "reference_string": "[268157336 | Patil et al. | 2024 | Citations: 80]"
        },
        {
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 14,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1658871094",
                    "name": "P. Suganthan"
                },
                {
                    "authorId": "2165469946",
                    "name": "Fedor Moiseev"
                },
                {
                    "authorId": "2348489099",
                    "name": "Le Yan"
                },
                {
                    "authorId": "2261361394",
                    "name": "Junru Wu"
                },
                {
                    "authorId": "2348507846",
                    "name": "Jianmo Ni"
                },
                {
                    "authorId": "2348488953",
                    "name": "Jay Han"
                },
                {
                    "authorId": "1954563",
                    "name": "I. Zitouni"
                },
                {
                    "authorId": "1727837",
                    "name": "Enrique Alfonseca"
                },
                {
                    "authorId": "2348422460",
                    "name": "Xuanhui Wang"
                },
                {
                    "authorId": "2349772191",
                    "name": "Zhe Dong"
                }
            ],
            "abstract": "Decoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in tasks like classification, regression, and ranking. This is primarily due to the inherent structure of decoder-based models, which limits their direct applicability to these tasks. In this paper, we introduce Gemma Encoder, adapting the powerful Gemma decoder model to an encoder architecture, thereby unlocking its potential for a wider range of non-generative applications. To optimize the adaptation from decoder to encoder, we systematically analyze various pooling strategies, attention mechanisms, and hyperparameters (e.g., dropout rate). Furthermore, we benchmark Gemma Encoder against established approaches on the GLUE benchmarks, and MS MARCO ranking benchmark, demonstrating its effectiveness and versatility.",
            "corpus_id": 276771845,
            "sentences": [
                {
                    "corpus_id": "276771845",
                    "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                    "text": "Pre-trained generative models often employ three types of attention mask patterns: bidirectional, causal, and prefix masking (Figures 3 and 4 in Raffel et al. (2020)). For our focus on non-generative tasks, we limited the explorations to causal attention and bidirectional attention. \n\nBidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence. \n\nGiven that Gemma models are pre-trained with causal attention, we investigated the impact of both bidirectional and causal attention masks during fine-tuning to maximize the performance of Gemma Encoder models.",
                    "score": 0.7818135545632132,
                    "section_title": "Attention Masking",
                    "char_start_offset": 4030,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 283
                        },
                        {
                            "start": 286,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 592
                        },
                        {
                            "start": 595,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1028
                        },
                        {
                            "start": 1031,
                            "end": 1137
                        },
                        {
                            "start": 1138,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1538
                        },
                        {
                            "start": 1541,
                            "end": 1751
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 145,
                            "end": 165,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 347,
                            "end": 368,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82861328125
                },
                {
                    "corpus_id": "276771845",
                    "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                    "text": "When processing sequences of varying lengths in batched inputs, padding is essential to create uniform input tensors. However, the choice between left padding (prepending padding tokens) and right padding (appending padding tokens) may impacts model behavior, especially when adapting encoder model from decoder-only models. \n\nIn decoder-only transformer models, such as those used for language generation (e.g., GPT (OpenAI, 2023), Gemma (Gemma Team, 2024a,b) models), left padding is typically used to align the inputs during training or inference, due to the auto-regressive training objective and efficient positional embedding. Decoder-only models are trained in an auto-regressive manner, where the task is to predict the next token based on all previous tokens. Decoder-only transformer models use positional embeddings to encode the order of tokens. Left-padding ensures that the relative positions of the actual tokens remain consistent regardless of the sequence length. \n\nHowever, right padding is acceptable for encoder models because of the way these models process input sequences. Unlike decoder-only models, encoders like those in BERT (Devlin et al., 2019), T5 (Raffel et al., 2020) or other bidirectional transformer models handle the entire input sequence at once, and their attention mechanisms allow tokens to attend to any other token in the input through bidirectional attention. In encoder models, positional embeddings are applied to the entire sequence, including padding tokens. Since the padding tokens are ignored during attention, their positional embeddings don't interfere with the actual computation. Whether the padding is on the right or left does not affect the functionality. \n\nThe choice of padding strategy has implications for the pooling layer, especially in conjunction with causal attention and First-K/Last-K token pooling.",
                    "score": 0.5634673674060909,
                    "section_title": "Padding Strategies",
                    "char_start_offset": 10417,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 324
                        },
                        {
                            "start": 327,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 980
                        },
                        {
                            "start": 983,
                            "end": 1095
                        },
                        {
                            "start": 1096,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1712
                        },
                        {
                            "start": 1715,
                            "end": 1867
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1152,
                            "end": 1173,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1178,
                            "end": 1199,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5224609375
                }
            ],
            "relevance_judgement": 0.82861328125,
            "relevance_judgment_input_expanded": "# Title: Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks\n# Venue: arXiv.org\n# Authors: P. Suganthan, Fedor Moiseev, Le Yan, Junru Wu, Jianmo Ni, Jay Han, I. Zitouni, Enrique Alfonseca, Xuanhui Wang, Zhe Dong\n## Abstract\nDecoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in tasks like classification, regression, and ranking. This is primarily due to the inherent structure of decoder-based models, which limits their direct applicability to these tasks. In this paper, we introduce Gemma Encoder, adapting the powerful Gemma decoder model to an encoder architecture, thereby unlocking its potential for a wider range of non-generative applications. To optimize the adaptation from decoder to encoder, we systematically analyze various pooling strategies, attention mechanisms, and hyperparameters (e.g., dropout rate). Furthermore, we benchmark Gemma Encoder against established approaches on the GLUE benchmarks, and MS MARCO ranking benchmark, demonstrating its effectiveness and versatility.\n## Attention Masking\nPre-trained generative models often employ three types of attention mask patterns: bidirectional, causal, and prefix masking (Figures 3 and 4 in Raffel et al. (2020)). For our focus on non-generative tasks, we limited the explorations to causal attention and bidirectional attention. \n\nBidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence. \n\nGiven that Gemma models are pre-trained with causal attention, we investigated the impact of both bidirectional and causal attention masks during fine-tuning to maximize the performance of Gemma Encoder models.\n\n## Padding Strategies\nWhen processing sequences of varying lengths in batched inputs, padding is essential to create uniform input tensors. However, the choice between left padding (prepending padding tokens) and right padding (appending padding tokens) may impacts model behavior, especially when adapting encoder model from decoder-only models. \n\nIn decoder-only transformer models, such as those used for language generation (e.g., GPT (OpenAI, 2023), Gemma (Gemma Team, 2024a,b) models), left padding is typically used to align the inputs during training or inference, due to the auto-regressive training objective and efficient positional embedding. Decoder-only models are trained in an auto-regressive manner, where the task is to predict the next token based on all previous tokens. Decoder-only transformer models use positional embeddings to encode the order of tokens. Left-padding ensures that the relative positions of the actual tokens remain consistent regardless of the sequence length. \n\nHowever, right padding is acceptable for encoder models because of the way these models process input sequences. Unlike decoder-only models, encoders like those in BERT (Devlin et al., 2019), T5 (Raffel et al., 2020) or other bidirectional transformer models handle the entire input sequence at once, and their attention mechanisms allow tokens to attend to any other token in the input through bidirectional attention. In encoder models, positional embeddings are applied to the entire sequence, including padding tokens. Since the padding tokens are ignored during attention, their positional embeddings don't interfere with the actual computation. Whether the padding is on the right or left does not affect the functionality. \n\nThe choice of padding strategy has implications for the pooling layer, especially in conjunction with causal attention and First-K/Last-K token pooling.",
            "reference_string": "[276771845 | Suganthan et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Bitune: Bidirectional Instruction-Tuning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258964424",
                    "name": "D. J. Kopiczko"
                },
                {
                    "authorId": "83133279",
                    "name": "Tijmen Blankevoort"
                },
                {
                    "authorId": "2258963625",
                    "name": "Yuki Markus Asano"
                }
            ],
            "abstract": "We introduce Bitune, a method that improves instruction-tuning of pretrained decoder-only large language models, leading to consistent gains on downstream tasks. Bitune applies both causal and bidirectional attention to the prompt, to obtain a better representation of the query or instruction. We realize this by introducing two sets of parameters, for which we apply parameter-efficient finetuning techniques. These causal and bidirectional features are then combined into a weighted average with trainable coefficients, which is subsequently used to generate new tokens. We demonstrate significant improvements in zero-shot performance on commonsense reasoning, arithmetic, and language understanding tasks, while extensive ablation studies validate the role of each component and demonstrate the method's agnosticism to different PEFT techniques.",
            "corpus_id": 269981935,
            "sentences": [
                {
                    "corpus_id": "269981935",
                    "title": "Bitune: Bidirectional Instruction-Tuning",
                    "text": "But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2020) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020;Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling.As there is simply more unlabeled data available for pretraining, training a decoder-only architecture on unlabeled data, and then finetuning it for tasks with instruction-tuning, is the best modus operandi of today (Wang et al., 2022).However, with this switch to decoder-only architectures, we lost bidirectional attention in the process.As we know this can improve feature representations for instructions, we set out to re-introduce bidirectional attention, such that it can be integrated into pretrained decoder-only LLMs.\n\nOur new method Bitune adds bidirectional attention to decoder-only architectures and combines it with causal attention to generate two sets of instruction features, using two different sets of weights.These features are then integrated, utilizing learnable mixing coefficients, and later used as the KV-cache for response generation.Notably, the autoregressive response generation process remains unaffected by the bidirectional attention and continues to be causal.",
                    "score": 0.7167230908070252,
                    "section_title": "Introduction",
                    "char_start_offset": 1637,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 172,
                            "end": 560
                        },
                        {
                            "start": 560,
                            "end": 826
                        },
                        {
                            "start": 826,
                            "end": 993
                        },
                        {
                            "start": 993,
                            "end": 1229
                        },
                        {
                            "start": 1229,
                            "end": 1333
                        },
                        {
                            "start": 1333,
                            "end": 1520
                        },
                        {
                            "start": 1522,
                            "end": 1723
                        },
                        {
                            "start": 1723,
                            "end": 1855
                        },
                        {
                            "start": 1855,
                            "end": 1988
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 248,
                            "end": 269,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 293,
                            "end": 314,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 419,
                            "end": 439,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1209,
                            "end": 1228,
                            "matchedPaperCorpusId": "248118752"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76611328125
                }
            ],
            "relevance_judgement": 0.76611328125,
            "relevance_judgment_input_expanded": "# Title: Bitune: Bidirectional Instruction-Tuning\n# Venue: arXiv.org\n# Authors: D. J. Kopiczko, Tijmen Blankevoort, Yuki Markus Asano\n## Abstract\nWe introduce Bitune, a method that improves instruction-tuning of pretrained decoder-only large language models, leading to consistent gains on downstream tasks. Bitune applies both causal and bidirectional attention to the prompt, to obtain a better representation of the query or instruction. We realize this by introducing two sets of parameters, for which we apply parameter-efficient finetuning techniques. These causal and bidirectional features are then combined into a weighted average with trainable coefficients, which is subsequently used to generate new tokens. We demonstrate significant improvements in zero-shot performance on commonsense reasoning, arithmetic, and language understanding tasks, while extensive ablation studies validate the role of each component and demonstrate the method's agnosticism to different PEFT techniques.\n## Introduction\nBut with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2020) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020;Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling.As there is simply more unlabeled data available for pretraining, training a decoder-only architecture on unlabeled data, and then finetuning it for tasks with instruction-tuning, is the best modus operandi of today (Wang et al., 2022).However, with this switch to decoder-only architectures, we lost bidirectional attention in the process.As we know this can improve feature representations for instructions, we set out to re-introduce bidirectional attention, such that it can be integrated into pretrained decoder-only LLMs.\n\nOur new method Bitune adds bidirectional attention to decoder-only architectures and combines it with causal attention to generate two sets of instruction features, using two different sets of weights.These features are then integrated, utilizing learnable mixing coefficients, and later used as the KV-cache for response generation.Notably, the autoregressive response generation process remains unaffected by the bidirectional attention and continues to be causal.",
            "reference_string": "[269981935 | Kopiczko et al. | 2024 | Citations: 3]"
        },
        {
            "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 56,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.08648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2056070459",
                    "name": "Savya Khosla"
                },
                {
                    "authorId": "2218199485",
                    "name": "Aditi Tiwari"
                },
                {
                    "authorId": "33315685",
                    "name": "Kushal Kafle"
                },
                {
                    "authorId": "2297849207",
                    "name": "Simon Jenni"
                },
                {
                    "authorId": "2341050386",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "2288642908",
                    "name": "John P. Collomosse"
                },
                {
                    "authorId": "2288445861",
                    "name": "Jing Shi"
                }
            ],
            "abstract": "While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.",
            "corpus_id": 275544523,
            "sentences": [
                {
                    "corpus_id": "275544523",
                    "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
                    "text": "We categorize the input tokens as either context tokens or span tokens and use the attention mask shown in Figure 2. Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details). \n\nDuring training, an input sequence includes one or more spans of span tokens surrounded by context tokens. During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling.",
                    "score": 0.6002155759966012,
                    "section_title": "Modifying Attention",
                    "char_start_offset": 8300,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 781
                        },
                        {
                            "start": 782,
                            "end": 975
                        },
                        {
                            "start": 978,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1348
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76513671875
                },
                {
                    "corpus_id": "275544523",
                    "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
                    "text": "While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.",
                    "score": 0.6085607077984898,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71435546875
                },
                {
                    "corpus_id": "275544523",
                    "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
                    "text": "Representation Learning. Text representation learning focuses on understanding contextual relationships within sentences. Traditionally, encoder models dominated this field due to their bidirectional context modeling, using masked language modeling for token-level representations (Devlin et al., 2019;Liu et al., 2019;He et al., 2020;Clark et al., 2020;He et al., 2021) and special tokens with similarity-based optimization for sentencelevel understanding (Gunel et al., 2020;Reimers and Gurevych, 2019;Wu et al., 2020;Carlsson et al., 2021;Gao et al., 2021;Wei et al., 2020). Recent work has explored adapting decoder-only LLMs for text encoding through various methods, including introducing special tokens to the model's vocabulary (Zhang et al., 2024), using last-token or mean-pooled representations (Neelakantan et al., 2022;Wang et al., 2023), or fine-tuning with masked modeling (BehnamGhader et al., 2024) or label supervision (Li et al., 2023;Duki'c and vSnajder, 2024). While some approaches modify the decoder's causal attention to be bidirectional (BehnamGhader et al., 2024;Muennighoff et al., 2024;Li and Li, 2023;Duki'c and vSnajder, 2024;Man et al., 2024), this often compromises the model's text generation abilities. In contrast, MAGNET employs a hybrid attention mechanism that combines causal and bidirectional attention, enabling both robust representation learning and preserved generation capabilities. Text Infilling. Text infilling requires considering both left and right context when generating text in the middle of a sequence. Encoder-decoder models (Raffel et al., 2019;Lewis et al., 2019;Kalinsky et al., 2023) can handle this task by encoding available context and decoding infilled text.",
                    "score": 0.5664913255135076,
                    "section_title": "Related Works",
                    "char_start_offset": 3793,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 24
                        },
                        {
                            "start": 25,
                            "end": 121
                        },
                        {
                            "start": 122,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1427
                        },
                        {
                            "start": 1428,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1722
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 281,
                            "end": 302,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 335,
                            "end": 354,
                            "matchedPaperCorpusId": "208229926"
                        },
                        {
                            "start": 477,
                            "end": 504,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 520,
                            "end": 542,
                            "matchedPaperCorpusId": "235613354"
                        },
                        {
                            "start": 542,
                            "end": 559,
                            "matchedPaperCorpusId": "233296292"
                        },
                        {
                            "start": 736,
                            "end": 756,
                            "matchedPaperCorpusId": "272524678"
                        },
                        {
                            "start": 954,
                            "end": 980,
                            "matchedPaperCorpusId": "267301268"
                        },
                        {
                            "start": 1130,
                            "end": 1156,
                            "matchedPaperCorpusId": "267301268"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.494140625
                }
            ],
            "relevance_judgement": 0.76513671875,
            "relevance_judgment_input_expanded": "# Title: MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities\n# Venue: arXiv.org\n# Authors: Savya Khosla, Aditi Tiwari, Kushal Kafle, Simon Jenni, Handong Zhao, John P. Collomosse, Jing Shi\n## Abstract\nWhile originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.\n## Related Works\nRepresentation Learning. Text representation learning focuses on understanding contextual relationships within sentences. Traditionally, encoder models dominated this field due to their bidirectional context modeling, using masked language modeling for token-level representations (Devlin et al., 2019;Liu et al., 2019;He et al., 2020;Clark et al., 2020;He et al., 2021) and special tokens with similarity-based optimization for sentencelevel understanding (Gunel et al., 2020;Reimers and Gurevych, 2019;Wu et al., 2020;Carlsson et al., 2021;Gao et al., 2021;Wei et al., 2020). Recent work has explored adapting decoder-only LLMs for text encoding through various methods, including introducing special tokens to the model's vocabulary (Zhang et al., 2024), using last-token or mean-pooled representations (Neelakantan et al., 2022;Wang et al., 2023), or fine-tuning with masked modeling (BehnamGhader et al., 2024) or label supervision (Li et al., 2023;Duki'c and vSnajder, 2024). While some approaches modify the decoder's causal attention to be bidirectional (BehnamGhader et al., 2024;Muennighoff et al., 2024;Li and Li, 2023;Duki'c and vSnajder, 2024;Man et al., 2024), this often compromises the model's text generation abilities. In contrast, MAGNET employs a hybrid attention mechanism that combines causal and bidirectional attention, enabling both robust representation learning and preserved generation capabilities. Text Infilling. Text infilling requires considering both left and right context when generating text in the middle of a sequence. Encoder-decoder models (Raffel et al., 2019;Lewis et al., 2019;Kalinsky et al., 2023) can handle this task by encoding available context and decoding infilled text.\n\n## Modifying Attention\nWe categorize the input tokens as either context tokens or span tokens and use the attention mask shown in Figure 2. Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details). \n\nDuring training, an input sequence includes one or more spans of span tokens surrounded by context tokens. During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling.",
            "reference_string": "[275544523 | Khosla et al. | 2025 | Citations: 1]"
        },
        {
            "title": "A Complete Survey on LLM-based AI Chatbots",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 207,
            "citation_count": 62,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1642047032",
                    "name": "Sumit Kumar Dam"
                },
                {
                    "authorId": "2159807650",
                    "name": "Choong-Seon Hong"
                },
                {
                    "authorId": "2209185953",
                    "name": "Yu Qiao"
                },
                {
                    "authorId": "2267861252",
                    "name": "Chaoning Zhang"
                }
            ],
            "abstract": "The past few decades have witnessed an upsurge in data, forming the foundation for data-hungry, learning-based AI technology. Conversational agents, often referred to as AI chatbots, rely heavily on such data to train large language models (LLMs) and generate new content (knowledge) in response to user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have set new standards in the AI community. This paper presents a complete survey of the evolution and deployment of LLM-based chatbots in various sectors. We first summarize the development of foundational chatbots, followed by the evolution of LLMs, and then provide an overview of LLM-based chatbots currently in use and those in the development phase. Recognizing AI chatbots as tools for generating new knowledge, we explore their diverse applications across various industries. We then discuss the open challenges, considering how the data used to train the LLMs and the misuse of the generated knowledge can cause several issues. Finally, we explore the future outlook to augment their efficiency and reliability in numerous applications. By addressing key milestones and the present-day context of LLM-based chatbots, our survey invites readers to delve deeper into this realm, reflecting on how their next generation will reshape conversational AI.",
            "corpus_id": 270711050,
            "sentences": [
                {
                    "corpus_id": "270711050",
                    "title": "A Complete Survey on LLM-based AI Chatbots",
                    "text": "The GPT-series models [3], [75] use an autoregressive or causal decoder architecture with a one-way attention mask that lets each input token think about only the elements that came before it and itself while it is being processed (see Fig. 8). This makes the processing more like how a conversation would naturally flow. Both input and output tokens undergo similar processing within this framework. The non-causal or prefix decoder architecture [76] performs bidirectional attention on prefix tokens, i.e., it considers both preceding and subsequent tokens. While autoregressively predicting the output tokens utilizing the same set of parameters used in encoding, it performs unidirectional attention [77], [78]. \n\nThe working principle of LLMs includes a series of steps. The process begins with word embedding, which involves representing words as vectors in a high-dimensional space. Here, similar words are clustered in specific groups or categories. This clustering of words enables the model to understand their meanings, which helps the LLM make accurate predictions. The model is trained on large text corpora like news articles or books, and during training, it learns to predict the likelihood of a word appearing in a specific context. Positional encoding further adds an understanding of word order within sequences, essential for tasks like translation, summarization, and question answering. Then comes the core of these models, the transformer architecture. It consists of the self-attention mechanism that helps understand textual dependencies by al-Fig. 7. Transformer model architecture [74]. locating distinct weights to individual words. This is achieved by computing: \n\nwhere matrices Q (query), K (key), and V (value) represent the current element, other elements, and information to be aggregated, respectively. The similarity between the query and key matrices is calculated through a dot product operation. It is then scaled by 1 \n\nto prevent gradient vanishing problems and subsequently normalized using the SoftMax activation function to produce the attention matrix A. The updated representations are obtained by performing matrix multiplication on A and V. This aggregation of weighted values forms a new representation that captures the inherent associations within the text. Algorithm 1 outlines the step-by-step procedure for this self-attention mechanism.",
                    "score": 0.5499243439326633,
                    "section_title": "B. Large Language Models (LLMs)",
                    "char_start_offset": 19825,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 715
                        },
                        {
                            "start": 718,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 889
                        },
                        {
                            "start": 890,
                            "end": 957
                        },
                        {
                            "start": 958,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1691
                        },
                        {
                            "start": 1694,
                            "end": 1837
                        },
                        {
                            "start": 1838,
                            "end": 1934
                        },
                        {
                            "start": 1935,
                            "end": 1957
                        },
                        {
                            "start": 1960,
                            "end": 2188
                        },
                        {
                            "start": 2189,
                            "end": 2308
                        },
                        {
                            "start": 2309,
                            "end": 2391
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 22,
                            "end": 25,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 27,
                            "end": 31,
                            "matchedPaperCorpusId": "269498086"
                        },
                        {
                            "start": 447,
                            "end": 451,
                            "matchedPaperCorpusId": "246441975"
                        },
                        {
                            "start": 704,
                            "end": 708,
                            "matchedPaperCorpusId": "236493269"
                        },
                        {
                            "start": 710,
                            "end": 714,
                            "matchedPaperCorpusId": "147704286"
                        },
                        {
                            "start": 1608,
                            "end": 1612,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75
                }
            ],
            "relevance_judgement": 0.75,
            "relevance_judgment_input_expanded": "# Title: A Complete Survey on LLM-based AI Chatbots\n# Venue: arXiv.org\n# Authors: Sumit Kumar Dam, Choong-Seon Hong, Yu Qiao, Chaoning Zhang\n## Abstract\nThe past few decades have witnessed an upsurge in data, forming the foundation for data-hungry, learning-based AI technology. Conversational agents, often referred to as AI chatbots, rely heavily on such data to train large language models (LLMs) and generate new content (knowledge) in response to user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have set new standards in the AI community. This paper presents a complete survey of the evolution and deployment of LLM-based chatbots in various sectors. We first summarize the development of foundational chatbots, followed by the evolution of LLMs, and then provide an overview of LLM-based chatbots currently in use and those in the development phase. Recognizing AI chatbots as tools for generating new knowledge, we explore their diverse applications across various industries. We then discuss the open challenges, considering how the data used to train the LLMs and the misuse of the generated knowledge can cause several issues. Finally, we explore the future outlook to augment their efficiency and reliability in numerous applications. By addressing key milestones and the present-day context of LLM-based chatbots, our survey invites readers to delve deeper into this realm, reflecting on how their next generation will reshape conversational AI.\n## B. Large Language Models (LLMs)\nThe GPT-series models [3], [75] use an autoregressive or causal decoder architecture with a one-way attention mask that lets each input token think about only the elements that came before it and itself while it is being processed (see Fig. 8). This makes the processing more like how a conversation would naturally flow. Both input and output tokens undergo similar processing within this framework. The non-causal or prefix decoder architecture [76] performs bidirectional attention on prefix tokens, i.e., it considers both preceding and subsequent tokens. While autoregressively predicting the output tokens utilizing the same set of parameters used in encoding, it performs unidirectional attention [77], [78]. \n\nThe working principle of LLMs includes a series of steps. The process begins with word embedding, which involves representing words as vectors in a high-dimensional space. Here, similar words are clustered in specific groups or categories. This clustering of words enables the model to understand their meanings, which helps the LLM make accurate predictions. The model is trained on large text corpora like news articles or books, and during training, it learns to predict the likelihood of a word appearing in a specific context. Positional encoding further adds an understanding of word order within sequences, essential for tasks like translation, summarization, and question answering. Then comes the core of these models, the transformer architecture. It consists of the self-attention mechanism that helps understand textual dependencies by al-Fig. 7. Transformer model architecture [74]. locating distinct weights to individual words. This is achieved by computing: \n\nwhere matrices Q (query), K (key), and V (value) represent the current element, other elements, and information to be aggregated, respectively. The similarity between the query and key matrices is calculated through a dot product operation. It is then scaled by 1 \n\nto prevent gradient vanishing problems and subsequently normalized using the SoftMax activation function to produce the attention matrix A. The updated representations are obtained by performing matrix multiplication on A and V. This aggregation of weighted values forms a new representation that captures the inherent associations within the text. Algorithm 1 outlines the step-by-step procedure for this self-attention mechanism.",
            "reference_string": "[270711050 | Dam et al. | 2024 | Citations: 62]"
        },
        {
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "venue": "Neurocomputing",
            "year": 2024,
            "reference_count": 203,
            "citation_count": 74,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.02038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116426849",
                    "name": "Yi-Hsueh Liu"
                },
                {
                    "authorId": "2155082967",
                    "name": "Haoyang He"
                },
                {
                    "authorId": "2184719751",
                    "name": "Tianle Han"
                },
                {
                    "authorId": "2273584640",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2210636248",
                    "name": "Mengyuan Liu"
                },
                {
                    "authorId": "2257433902",
                    "name": "Jiaming Tian"
                },
                {
                    "authorId": "2257095790",
                    "name": "Yutong Zhang"
                },
                {
                    "authorId": "2110238778",
                    "name": "Jiaqi Wang"
                },
                {
                    "authorId": "2277869261",
                    "name": "Xiaohui Gao"
                },
                {
                    "authorId": "2215167446",
                    "name": "Tianyang Zhong"
                },
                {
                    "authorId": "2221032216",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "2211904452",
                    "name": "Shaochen Xu"
                },
                {
                    "authorId": "2263593041",
                    "name": "Zihao Wu"
                },
                {
                    "authorId": "2145977326",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2257586495",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "2277750447",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "1742535",
                    "name": "Xintao Hu"
                },
                {
                    "authorId": "49104946",
                    "name": "Tuo Zhang"
                },
                {
                    "authorId": "2251076040",
                    "name": "Ning Qiang"
                },
                {
                    "authorId": "2254792886",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2257302793",
                    "name": "Bao Ge"
                }
            ],
            "abstract": "The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.",
            "corpus_id": 266755678,
            "sentences": [
                {
                    "corpus_id": "266755678",
                    "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                    "text": "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
                    "score": 0.7140742294667604,
                    "section_title": "Decoder-only Architecture",
                    "char_start_offset": 35165,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 725
                        },
                        {
                            "start": 728,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1591
                        },
                        {
                            "start": 1594,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1956
                        },
                        {
                            "start": 1957,
                            "end": 2102
                        },
                        {
                            "start": 2103,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1951,
                            "end": 1955,
                            "matchedPaperCorpusId": "147704286"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.732421875
                }
            ],
            "relevance_judgement": 0.732421875,
            "relevance_judgment_input_expanded": "# Title: Understanding LLMs: A Comprehensive Overview from Training to Inference\n# Venue: Neurocomputing\n# Authors: Yi-Hsueh Liu, Haoyang He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zheng Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge\n## Abstract\nThe introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.\n## Decoder-only Architecture\nLLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
            "reference_string": "[266755678 | Liu et al. | 2024 | Citations: 74]"
        },
        {
            "title": "Language Models are General-Purpose Interfaces",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 139,
            "citation_count": 99,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2206.06336",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.06336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34128716",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "2187079427",
                    "name": "Haoyu Song"
                },
                {
                    "authorId": "145307652",
                    "name": "Li Dong"
                },
                {
                    "authorId": "3110003",
                    "name": "Shaohan Huang"
                },
                {
                    "authorId": "46221722",
                    "name": "Zewen Chi"
                },
                {
                    "authorId": "51456429",
                    "name": "Wenhui Wang"
                },
                {
                    "authorId": "2118866998",
                    "name": "Shuming Ma"
                },
                {
                    "authorId": "49807919",
                    "name": "Furu Wei"
                }
            ],
            "abstract": "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",
            "corpus_id": 249626024,
            "sentences": [
                {
                    "corpus_id": "249626024",
                    "title": "Language Models are General-Purpose Interfaces",
                    "text": "As shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders. \n\nBackbone Network We use Transformer (Vaswani et al., 2017) to build the models. Given an input sequence, we first pack their vector representations together. Then we feed the vectors into a multilayer Transformer, which encodes the input to contextualized representations. In each Transformer block, there is a multi-head self-attention layer and a feed-forward network layer that are used to aggregate the hidden states of the previous layer. Moreover, attention masks are used to control the context access. We use a triangular matrix as the attention mask for the universal task layer, so that it processes the input from left to right. For the bidirectional encoder, we allow all the tokens to access each other. After obtaining the output vectors of the universal task layer, we use a softmax classifier to predict over the vocabulary. The weight matrix is shared with the input token embeddings. \n\nConnector As shown in Figure 2, there is a connector layer between the universal task layer and various bidirectional encoders. The connectors project vector representations of bidirectional encoders before feeding them into the general-purpose interface. Moreover, the connectors are used to match the output dimensions of foundation models with the universal task layer. We empirically find that both linear projection and feed-forward network work well in our experiments.",
                    "score": 0.7166540556959149,
                    "section_title": "Model Architecture",
                    "char_start_offset": 7682,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 238
                        },
                        {
                            "start": 239,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 792
                        },
                        {
                            "start": 795,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1067
                        },
                        {
                            "start": 1068,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1635
                        },
                        {
                            "start": 1636,
                            "end": 1696
                        },
                        {
                            "start": 1699,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 1954
                        },
                        {
                            "start": 1955,
                            "end": 2071
                        },
                        {
                            "start": 2072,
                            "end": 2174
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 180,
                            "end": 197,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 483,
                            "end": 504,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7265625
                },
                {
                    "corpus_id": "249626024",
                    "title": "Language Models are General-Purpose Interfaces",
                    "text": "For example, we can use task instructions (Ouyang et al., 2022) to repurpose the model, and use demonstrations of some examples to conduct few-shot learning. \n\nNon-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available. In addition, non-causal encoder pretrained by the masked language modeling objective achieves competitive performance on cross-lingual transfer (Conneau et al., 2020), which makes it effective to adapt models to the multilingual setting. \n\nSemi-causal language modeling as a meta-pretraining task. Semi-causal language modeling plays the role of linking together non-causal encoders and the causal language model. It is a meta task in the sense of universal interface pretraining of pretrained encoders. Specifically, non-causal encoders learn to represent various input data, and a causal language model serves as a universal task layer. Non-causal encoders dock with a causal language model, so that we can benefit from both modeling methods described as above. In comparison with previous encoder-decoder pretraining (such as prefix language modeling, and T5; Raffel et al. 2020), our task non-causally encodes random spans of the whole sequence, while generating the rest via causal language modeling. Moreover, in terms of architecture, we directly feed the outputs of bidirectional encoders into the causal decoder, rather than relying on cross attention (Vaswani et al., 2017).",
                    "score": 0.7470007912363181,
                    "section_title": "body",
                    "char_start_offset": 2111,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 160,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 1088
                        },
                        {
                            "start": 1091,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1489
                        },
                        {
                            "start": 1490,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2035
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 359,
                            "end": 380,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 388,
                            "end": 409,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 995,
                            "end": 1017,
                            "matchedPaperCorpusId": "207880568"
                        },
                        {
                            "start": 1714,
                            "end": 1733,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64306640625
                }
            ],
            "relevance_judgement": 0.7265625,
            "relevance_judgment_input_expanded": "# Title: Language Models are General-Purpose Interfaces\n# Venue: arXiv.org\n# Authors: Y. Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei\n## Abstract\nFoundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.\n## body\nFor example, we can use task instructions (Ouyang et al., 2022) to repurpose the model, and use demonstrations of some examples to conduct few-shot learning. \n\nNon-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available. In addition, non-causal encoder pretrained by the masked language modeling objective achieves competitive performance on cross-lingual transfer (Conneau et al., 2020), which makes it effective to adapt models to the multilingual setting. \n\nSemi-causal language modeling as a meta-pretraining task. Semi-causal language modeling plays the role of linking together non-causal encoders and the causal language model. It is a meta task in the sense of universal interface pretraining of pretrained encoders. Specifically, non-causal encoders learn to represent various input data, and a causal language model serves as a universal task layer. Non-causal encoders dock with a causal language model, so that we can benefit from both modeling methods described as above. In comparison with previous encoder-decoder pretraining (such as prefix language modeling, and T5; Raffel et al. 2020), our task non-causally encodes random spans of the whole sequence, while generating the rest via causal language modeling. Moreover, in terms of architecture, we directly feed the outputs of bidirectional encoders into the causal decoder, rather than relying on cross attention (Vaswani et al., 2017).\n\n## Model Architecture\nAs shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders. \n\nBackbone Network We use Transformer (Vaswani et al., 2017) to build the models. Given an input sequence, we first pack their vector representations together. Then we feed the vectors into a multilayer Transformer, which encodes the input to contextualized representations. In each Transformer block, there is a multi-head self-attention layer and a feed-forward network layer that are used to aggregate the hidden states of the previous layer. Moreover, attention masks are used to control the context access. We use a triangular matrix as the attention mask for the universal task layer, so that it processes the input from left to right. For the bidirectional encoder, we allow all the tokens to access each other. After obtaining the output vectors of the universal task layer, we use a softmax classifier to predict over the vocabulary. The weight matrix is shared with the input token embeddings. \n\nConnector As shown in Figure 2, there is a connector layer between the universal task layer and various bidirectional encoders. The connectors project vector representations of bidirectional encoders before feeding them into the general-purpose interface. Moreover, the connectors are used to match the output dimensions of foundation models with the universal task layer. We empirically find that both linear projection and feed-forward network work well in our experiments.",
            "reference_string": "[249626024 | Hao et al. | 2022 | Citations: 99]"
        },
        {
            "title": "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?",
            "venue": "2024 2nd International Conference on Foundation and Large Language Models (FLLM)",
            "year": 2024,
            "reference_count": 63,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2411.18021",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.18021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332727631",
                    "name": "Lewen Yang"
                },
                {
                    "authorId": "2332813122",
                    "name": "Xuanyu Zhou"
                },
                {
                    "authorId": "2332600501",
                    "name": "Juao Fan"
                },
                {
                    "authorId": "2332733783",
                    "name": "Xinyi Xie"
                },
                {
                    "authorId": "2332541087",
                    "name": "Shengxin Zhu"
                }
            ],
            "abstract": "Over the past few decades, Artificial Intelligence(AI) has progressed from the initial machine learning stage to the deep learning stage, and now to the stage of foundational models. Foundational models have the characteristics of pre-training, transfer learning, and self-supervised learning, and pre-trained models can be fine-tuned and applied to various downstream tasks. Under the framework of foundational models, models such as Bidirectional Encoder Representations from Transformers(BERT) and Generative Pre-trained Transformer(GPT) have greatly advanced the development of natural language processing(NLP), especially the emergence of many models based on BERT. BERT broke through the limitation of only using one-way methods for language modeling in pre-training by using a masked language model. It can capture bidirectional context information to predict the masked words in the sequence, this can improve the feature extraction ability of the model. This makes the model very useful for downstream tasks, especially for specialized applications. The model using the bidirectional encoder can better understand the domain knowledge and be better applied to these downstream tasks. So we hope to help understand how this technology has evolved and improved model performance in various natural language processing tasks under the background of foundational models and reveal its importance in capturing context information and improving the model\u2019s performance on downstream tasks.This article analyzes one-way and bidirectional models based on GPT and BERT and compares their differences based on the purpose of the model. It also briefly analyzes BERT and the improvements of some models based on BERT. The model\u2019s performance on the Stanford Question Answering Dataset(SQuAD) and General Language Understanding Evaluation(GLUE) was compared.",
            "corpus_id": 274305758,
            "sentences": [
                {
                    "corpus_id": "274305758",
                    "title": "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?",
                    "text": "Better results also make models with bidirectional encoders have a variety of applications and improvements. But as the models become larger, due to the inherent complexity of bidirectional models, they require more time and computing power than one-way models, and driven by interests, it is clear that pure one-way models have a higher cost performance at present. Of course, understanding and being able to do are certainly different. With the improvement of computing power, the situation may change in the future. GPT and BERT are among the earliest foundational models, representing two distinct approaches in natural language processing. GPT is a one-way or causal language model that predicts the next word in a sequence by considering only the preceding words. In contrast, BERT is a bidirectional language model, which processes both forward and backward context simultaneously to improve feature extraction and representation capacity. Both models are based on the Transformer [28] architecture, which consists of two main components: the encoder and decoder. Transformer introduces a self-attention mechanism. For each position in the sequence, the self-attention mechanism computes the query, the key and value vectors, and the attention weight, and uses the attention weight to weigh the sum of the value vectors to get the final output representation. In the Transformer encoder architecture, each encoding layer's multi-head self-attention sublayer will interact with each position in the input sequence with all other positions in the sequence, so that the selfattention layer when encoding a word will consider the entire sentence's words when encoding a word. However, in the decoder, the self-attention layer uses a mask matrix so that each position in the sequence can only see the sequence before it and the positions behind it will be hidden. GPT employs the Transformer decoder, which uses masked self-attention to only attend to past tokens in the sequence, making it suitable for tasks like text generation. On the other hand, BERT utilizes the Transformer encoder, which allows it to attend to all tokens in the sequence, both before and after each word, making it ideal for tasks that require deep understanding of context, such as question answering and text classification.",
                    "score": 0.6572551212028048,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 2403,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 108
                        },
                        {
                            "start": 109,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1366
                        },
                        {
                            "start": 1367,
                            "end": 1678
                        },
                        {
                            "start": 1679,
                            "end": 1865
                        },
                        {
                            "start": 1866,
                            "end": 2033
                        },
                        {
                            "start": 2034,
                            "end": 2303
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 988,
                            "end": 992,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.701171875
                }
            ],
            "relevance_judgement": 0.701171875,
            "relevance_judgment_input_expanded": "# Title: Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?\n# Venue: 2024 2nd International Conference on Foundation and Large Language Models (FLLM)\n# Authors: Lewen Yang, Xuanyu Zhou, Juao Fan, Xinyi Xie, Shengxin Zhu\n## Abstract\nOver the past few decades, Artificial Intelligence(AI) has progressed from the initial machine learning stage to the deep learning stage, and now to the stage of foundational models. Foundational models have the characteristics of pre-training, transfer learning, and self-supervised learning, and pre-trained models can be fine-tuned and applied to various downstream tasks. Under the framework of foundational models, models such as Bidirectional Encoder Representations from Transformers(BERT) and Generative Pre-trained Transformer(GPT) have greatly advanced the development of natural language processing(NLP), especially the emergence of many models based on BERT. BERT broke through the limitation of only using one-way methods for language modeling in pre-training by using a masked language model. It can capture bidirectional context information to predict the masked words in the sequence, this can improve the feature extraction ability of the model. This makes the model very useful for downstream tasks, especially for specialized applications. The model using the bidirectional encoder can better understand the domain knowledge and be better applied to these downstream tasks. So we hope to help understand how this technology has evolved and improved model performance in various natural language processing tasks under the background of foundational models and reveal its importance in capturing context information and improving the model\u2019s performance on downstream tasks.This article analyzes one-way and bidirectional models based on GPT and BERT and compares their differences based on the purpose of the model. It also briefly analyzes BERT and the improvements of some models based on BERT. The model\u2019s performance on the Stanford Question Answering Dataset(SQuAD) and General Language Understanding Evaluation(GLUE) was compared.\n## I. INTRODUCTION\nBetter results also make models with bidirectional encoders have a variety of applications and improvements. But as the models become larger, due to the inherent complexity of bidirectional models, they require more time and computing power than one-way models, and driven by interests, it is clear that pure one-way models have a higher cost performance at present. Of course, understanding and being able to do are certainly different. With the improvement of computing power, the situation may change in the future. GPT and BERT are among the earliest foundational models, representing two distinct approaches in natural language processing. GPT is a one-way or causal language model that predicts the next word in a sequence by considering only the preceding words. In contrast, BERT is a bidirectional language model, which processes both forward and backward context simultaneously to improve feature extraction and representation capacity. Both models are based on the Transformer [28] architecture, which consists of two main components: the encoder and decoder. Transformer introduces a self-attention mechanism. For each position in the sequence, the self-attention mechanism computes the query, the key and value vectors, and the attention weight, and uses the attention weight to weigh the sum of the value vectors to get the final output representation. In the Transformer encoder architecture, each encoding layer's multi-head self-attention sublayer will interact with each position in the input sequence with all other positions in the sequence, so that the selfattention layer when encoding a word will consider the entire sentence's words when encoding a word. However, in the decoder, the self-attention layer uses a mask matrix so that each position in the sequence can only see the sequence before it and the positions behind it will be hidden. GPT employs the Transformer decoder, which uses masked self-attention to only attend to past tokens in the sequence, making it suitable for tasks like text generation. On the other hand, BERT utilizes the Transformer encoder, which allows it to attend to all tokens in the sequence, both before and after each word, making it ideal for tasks that require deep understanding of context, such as question answering and text classification.",
            "reference_string": "[274305758 | Yang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Contrastive learning and mixture of experts enables precise vector embeddings in biological databases",
            "venue": "Scientific Reports",
            "year": 2025,
            "reference_count": 60,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s41598-025-98185-8",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12041245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143308913",
                    "name": "Logan Hallee"
                },
                {
                    "authorId": "2281747985",
                    "name": "Rohan Kapur"
                },
                {
                    "authorId": "2281850189",
                    "name": "Arjun Patel"
                },
                {
                    "authorId": "4561626",
                    "name": "Jason P. Gleghorn"
                },
                {
                    "authorId": "2281747963",
                    "name": "Bohdan B. Khomtchouk"
                }
            ],
            "abstract": "The advancement of transformer neural networks has significantly enhanced the performance of sentence similarity models. However, these models often struggle with highly discriminative tasks and generate sub-optimal representations of complex documents such as peer-reviewed scientific literature. With the increased reliance on retrieval augmentation and search, representing structurally and thematically-varied research documents as concise and descriptive vectors is crucial. This study improves upon the vector embeddings of scientific text by assembling domain-specific datasets using co-citations as a similarity metric, focusing on biomedical domains. We introduce a novel Mixture of Experts (MoE) extension pipeline applied to pretrained BERT models, where every multi-layer perceptron section is copied into distinct experts. Our MoE variants are trained to classify whether two publications are cited together (co-cited) in a third paper based on their scientific abstracts across multiple biological domains. Notably, because of our unique routing scheme based on special tokens, the throughput of our extended MoE system is exactly the same as regular transformers. This holds promise for versatile and efficient One-Size-Fits-All transformer networks for encoding heterogeneous biomedical inputs. Our methodology marks advancements in representation learning and holds promise for enhancing vector database search and compilation.",
            "corpus_id": 278202626,
            "sentences": [
                {
                    "corpus_id": "278202626",
                    "title": "Contrastive learning and mixture of experts enables precise vector embeddings in biological databases",
                    "text": "GPT (Generative Pretrained Transformer) models, such as OpenAI's GPT series (GPT-3, GPT-4, etc.), are designed for generative tasks and use transformer decoders [36][37][38] . They employ causal (unidirectional) attention, meaning each token attends only to previous tokens in the sequence, enabling autoregressive generation during inference. This allows them to predict the next word in a sequence without direct access to future words. \n\nIn contrast, BERT models utilize transformer encoders with bidirectional attention, meaning they can attend to all tokens within an input simultaneously. This structure enables them to capture additional contextual dependencies, making them well-suited for tasks like text classification and sentence similarity 39 . Unlike GPT models, BERT is trained using a masked language modeling (MLM) objective, where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context.",
                    "score": 0.5316062866523026,
                    "section_title": "Transformer neural networks",
                    "char_start_offset": 7341,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 438
                        },
                        {
                            "start": 441,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 757
                        },
                        {
                            "start": 758,
                            "end": 951
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70068359375
                }
            ],
            "relevance_judgement": 0.70068359375,
            "relevance_judgment_input_expanded": "# Title: Contrastive learning and mixture of experts enables precise vector embeddings in biological databases\n# Venue: Scientific Reports\n# Authors: Logan Hallee, Rohan Kapur, Arjun Patel, Jason P. Gleghorn, Bohdan B. Khomtchouk\n## Abstract\nThe advancement of transformer neural networks has significantly enhanced the performance of sentence similarity models. However, these models often struggle with highly discriminative tasks and generate sub-optimal representations of complex documents such as peer-reviewed scientific literature. With the increased reliance on retrieval augmentation and search, representing structurally and thematically-varied research documents as concise and descriptive vectors is crucial. This study improves upon the vector embeddings of scientific text by assembling domain-specific datasets using co-citations as a similarity metric, focusing on biomedical domains. We introduce a novel Mixture of Experts (MoE) extension pipeline applied to pretrained BERT models, where every multi-layer perceptron section is copied into distinct experts. Our MoE variants are trained to classify whether two publications are cited together (co-cited) in a third paper based on their scientific abstracts across multiple biological domains. Notably, because of our unique routing scheme based on special tokens, the throughput of our extended MoE system is exactly the same as regular transformers. This holds promise for versatile and efficient One-Size-Fits-All transformer networks for encoding heterogeneous biomedical inputs. Our methodology marks advancements in representation learning and holds promise for enhancing vector database search and compilation.\n## Transformer neural networks\nGPT (Generative Pretrained Transformer) models, such as OpenAI's GPT series (GPT-3, GPT-4, etc.), are designed for generative tasks and use transformer decoders [36][37][38] . They employ causal (unidirectional) attention, meaning each token attends only to previous tokens in the sequence, enabling autoregressive generation during inference. This allows them to predict the next word in a sequence without direct access to future words. \n\nIn contrast, BERT models utilize transformer encoders with bidirectional attention, meaning they can attend to all tokens within an input simultaneously. This structure enables them to capture additional contextual dependencies, making them well-suited for tasks like text classification and sentence similarity 39 . Unlike GPT models, BERT is trained using a masked language modeling (MLM) objective, where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context.",
            "reference_string": "[278202626 | Hallee et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
            "venue": "Pattern Recognition Letters",
            "year": 2025,
            "reference_count": 57,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2222734467",
                    "name": "Andrea Busto-Casti\u00f1eira"
                },
                {
                    "authorId": "1405165681",
                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                },
                {
                    "authorId": "2326130687",
                    "name": "Francisco de Arriba-P\u00e9rez"
                },
                {
                    "authorId": "1395988865",
                    "name": "F. Gonz\u00e1lez-Casta\u00f1o"
                }
            ],
            "abstract": null,
            "corpus_id": 276423946,
            "sentences": [
                {
                    "corpus_id": "276423946",
                    "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
                    "text": "Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems. \n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51]. \n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way. \n\nNon-causal nlg strategies perform much worse in English than their causal counterparts [49]. However, to our knowledge, no prior research has been conducted on non-causal nlg in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the mlm task could be successfully exploited in Spanish nlg.",
                    "score": 0.7985783516462949,
                    "section_title": "Causality in generative transformer language models",
                    "char_start_offset": 6569,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 458
                        },
                        {
                            "start": 461,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 864
                        },
                        {
                            "start": 867,
                            "end": 947
                        },
                        {
                            "start": 948,
                            "end": 1146
                        },
                        {
                            "start": 1149,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1548
                        },
                        {
                            "start": 1551,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1892
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 79,
                            "end": 83,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 568,
                            "end": 572,
                            "matchedPaperCorpusId": "231645376"
                        },
                        {
                            "start": 572,
                            "end": 575,
                            "matchedPaperCorpusId": "234785837"
                        },
                        {
                            "start": 1035,
                            "end": 1038,
                            "matchedPaperCorpusId": "231715684"
                        },
                        {
                            "start": 1038,
                            "end": 1041,
                            "matchedPaperCorpusId": "247126308"
                        },
                        {
                            "start": 1081,
                            "end": 1085,
                            "matchedPaperCorpusId": "218487230"
                        },
                        {
                            "start": 1113,
                            "end": 1117,
                            "matchedPaperCorpusId": "208224776"
                        },
                        {
                            "start": 1138,
                            "end": 1142,
                            "matchedPaperCorpusId": "212657570"
                        },
                        {
                            "start": 1142,
                            "end": 1145,
                            "matchedPaperCorpusId": "267201220"
                        },
                        {
                            "start": 1231,
                            "end": 1235,
                            "matchedPaperCorpusId": "265629619"
                        },
                        {
                            "start": 1436,
                            "end": 1440,
                            "matchedPaperCorpusId": "211069634"
                        },
                        {
                            "start": 1638,
                            "end": 1642,
                            "matchedPaperCorpusId": "60441316"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6884765625
                }
            ],
            "relevance_judgement": 0.6884765625,
            "relevance_judgment_input_expanded": "# Title: Optimal word order for non-causal text generation with Large Language Models: The Spanish case\n# Venue: Pattern Recognition Letters\n# Authors: Andrea Busto-Casti\u00f1eira, Silvia Garc\u00eda-M\u00e9ndez, Francisco de Arriba-P\u00e9rez, F. Gonz\u00e1lez-Casta\u00f1o\n## Abstract\nNone\n## Causality in generative transformer language models\nTransformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems. \n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51]. \n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way. \n\nNon-causal nlg strategies perform much worse in English than their causal counterparts [49]. However, to our knowledge, no prior research has been conducted on non-causal nlg in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the mlm task could be successfully exploited in Spanish nlg.",
            "reference_string": "[276423946 | Busto-Castineira et al. | 2025 | Citations: 0]"
        },
        {
            "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.03206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319805585",
                    "name": "Mingze Gao"
                },
                {
                    "authorId": "2302790279",
                    "name": "Jingyu Liu"
                },
                {
                    "authorId": "2302785092",
                    "name": "Mingda Li"
                },
                {
                    "authorId": "2319964588",
                    "name": "Jiangtao Xie"
                },
                {
                    "authorId": "2258682951",
                    "name": "Qingbin Liu"
                },
                {
                    "authorId": "2304448412",
                    "name": "Bo Zhao"
                },
                {
                    "authorId": "2302990371",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2319814814",
                    "name": "Hui Xiong"
                }
            ],
            "abstract": "Multimodal Large Language Models (MLLMs) have significantly improved performance across various image-language applications. Recently, there has been a growing interest in adapting image pre-trained MLLMs for video-related tasks. However, most efforts concentrate on enhancing the vision encoder and projector components, while the core part, Large Language Models (LLMs), remains comparatively under-explored. In this paper, we propose two strategies to enhance the model's capability in video understanding tasks by improving inter-layer attention computation in LLMs. Specifically, the first approach focuses on the enhancement of Rotary Position Embedding (RoPE) with Temporal-Aware Dual RoPE, which introduces temporal position information to strengthen the MLLM's temporal modeling capabilities while preserving the relative position relationships of both visual and text tokens. The second approach involves enhancing the Attention Mask with the Frame-wise Block Causal Attention Mask, a simple yet effective method that broadens visual token interactions within and across video frames while maintaining the causal inference mechanism. Based on these proposed methods, we adapt LLaVA for video understanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our TC-LLaVA achieves new state-of-the-art performance across various video understanding benchmarks with only supervised fine-tuning (SFT) on video-related datasets.",
            "corpus_id": 272423598,
            "sentences": [
                {
                    "corpus_id": "272423598",
                    "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
                    "text": "Another often overlooked key point is the design of attention masks within the transformer layers in large lan- guage models. In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text. Mathematically, the causal attention mask M \u2208 R T \u00d7T for a sequence of length T is defined as: \n\nThis ensures that each position i only attends to previous positions (including itself), thus implementing the causal attention mechanism. The final attention weights are computed as: \n\nwhere Q is the query vectors, K is the key vectors, V is the value vectors, d k is the dimension of the key vectors, and M is the causal attention mask. However, for multimodal information involving both visual and textual inputs, the visual modality is only used as a conditional input to the language model. During the unidirectional decoding process of the language model, this design weakens the bidirectional attention interactions obtained from the visual encoder, reducing them to unidirectional attention interactions. To explore the impact of different attention masks, we design three distinct attention masks to enhance and investigate better interactions within visual tokens and between visual and text tokens, as illustrated in Figure 3. \n\nFirstly, the Full Visual Mask modifies the causal attention mask to enable more extensive interactions among visual tokens across different frames. This mask can be represented as follows: \n\nSecondly is Frame-wise Block Mask, which limits the attention to adjacent visual tokens within the same frame. This is defined as follows: \n\nFinally, we proposed Frame-wise Block Causal Attention Mask (FwBC), which combines the characteristics of the previous causal and block visual attention masks by incorporating broader visual token interactions within the frame while maintaining causal inference mode across video frames. This can be presented as:",
                    "score": 0.5794494120616847,
                    "section_title": "Frame-wise Block Causal Attention Mask",
                    "char_start_offset": 12069,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 711
                        },
                        {
                            "start": 714,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 897
                        },
                        {
                            "start": 900,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1426
                        },
                        {
                            "start": 1427,
                            "end": 1651
                        },
                        {
                            "start": 1654,
                            "end": 1801
                        },
                        {
                            "start": 1802,
                            "end": 1842
                        },
                        {
                            "start": 1845,
                            "end": 1955
                        },
                        {
                            "start": 1956,
                            "end": 1983
                        },
                        {
                            "start": 1986,
                            "end": 2273
                        },
                        {
                            "start": 2274,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6767578125
                }
            ],
            "relevance_judgement": 0.6767578125,
            "relevance_judgment_input_expanded": "# Title: TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations\n# Venue: arXiv.org\n# Authors: Mingze Gao, Jingyu Liu, Mingda Li, Jiangtao Xie, Qingbin Liu, Bo Zhao, Xi Chen, Hui Xiong\n## Abstract\nMultimodal Large Language Models (MLLMs) have significantly improved performance across various image-language applications. Recently, there has been a growing interest in adapting image pre-trained MLLMs for video-related tasks. However, most efforts concentrate on enhancing the vision encoder and projector components, while the core part, Large Language Models (LLMs), remains comparatively under-explored. In this paper, we propose two strategies to enhance the model's capability in video understanding tasks by improving inter-layer attention computation in LLMs. Specifically, the first approach focuses on the enhancement of Rotary Position Embedding (RoPE) with Temporal-Aware Dual RoPE, which introduces temporal position information to strengthen the MLLM's temporal modeling capabilities while preserving the relative position relationships of both visual and text tokens. The second approach involves enhancing the Attention Mask with the Frame-wise Block Causal Attention Mask, a simple yet effective method that broadens visual token interactions within and across video frames while maintaining the causal inference mechanism. Based on these proposed methods, we adapt LLaVA for video understanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our TC-LLaVA achieves new state-of-the-art performance across various video understanding benchmarks with only supervised fine-tuning (SFT) on video-related datasets.\n## Frame-wise Block Causal Attention Mask\nAnother often overlooked key point is the design of attention masks within the transformer layers in large lan- guage models. In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text. Mathematically, the causal attention mask M \u2208 R T \u00d7T for a sequence of length T is defined as: \n\nThis ensures that each position i only attends to previous positions (including itself), thus implementing the causal attention mechanism. The final attention weights are computed as: \n\nwhere Q is the query vectors, K is the key vectors, V is the value vectors, d k is the dimension of the key vectors, and M is the causal attention mask. However, for multimodal information involving both visual and textual inputs, the visual modality is only used as a conditional input to the language model. During the unidirectional decoding process of the language model, this design weakens the bidirectional attention interactions obtained from the visual encoder, reducing them to unidirectional attention interactions. To explore the impact of different attention masks, we design three distinct attention masks to enhance and investigate better interactions within visual tokens and between visual and text tokens, as illustrated in Figure 3. \n\nFirstly, the Full Visual Mask modifies the causal attention mask to enable more extensive interactions among visual tokens across different frames. This mask can be represented as follows: \n\nSecondly is Frame-wise Block Mask, which limits the attention to adjacent visual tokens within the same frame. This is defined as follows: \n\nFinally, we proposed Frame-wise Block Causal Attention Mask (FwBC), which combines the characteristics of the previous causal and block visual attention masks by incorporating broader visual token interactions within the frame while maintaining causal inference mode across video frames. This can be presented as:",
            "reference_string": "[272423598 | Gao et al. | 2024 | Citations: 2]"
        },
        {
            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265383225",
                    "name": "Kai Yin"
                },
                {
                    "authorId": "2308073678",
                    "name": "Chengkai Liu"
                },
                {
                    "authorId": "2258714985",
                    "name": "Ali Mostafavi"
                },
                {
                    "authorId": "2308068627",
                    "name": "Xia Hu"
                }
            ],
            "abstract": "In the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.",
            "corpus_id": 270702559,
            "sentences": [
                {
                    "corpus_id": "270702559",
                    "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                    "text": "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
                    "score": 0.7587938703243283,
                    "section_title": "Architecture of LLMs",
                    "char_start_offset": 7065,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 148,
                            "end": 503
                        },
                        {
                            "start": 503,
                            "end": 654
                        },
                        {
                            "start": 654,
                            "end": 749
                        },
                        {
                            "start": 751,
                            "end": 898
                        },
                        {
                            "start": 898,
                            "end": 1055
                        },
                        {
                            "start": 1055,
                            "end": 1243
                        },
                        {
                            "start": 1243,
                            "end": 1318
                        },
                        {
                            "start": 1318,
                            "end": 1430
                        },
                        {
                            "start": 1432,
                            "end": 1633
                        },
                        {
                            "start": 1633,
                            "end": 1756
                        },
                        {
                            "start": 1756,
                            "end": 1889
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1513,
                            "end": 1535,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6611328125
                }
            ],
            "relevance_judgement": 0.6611328125,
            "relevance_judgment_input_expanded": "# Title: CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics\n# Venue: arXiv.org\n# Authors: Kai Yin, Chengkai Liu, Ali Mostafavi, Xia Hu\n## Abstract\nIn the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.\n## Architecture of LLMs\nTypical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
            "reference_string": "[270702559 | Yin et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization",
            "venue": "Computational Linguistics",
            "year": 2021,
            "reference_count": 95,
            "citation_count": 45,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/coli/article-pdf/48/2/279/2029099/coli_a_00434.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.11670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46437970",
                    "name": "Md Tahmid Rahman Laskar"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "1683391",
                    "name": "J. Huang"
                }
            ],
            "abstract": "Abstract The Query-Focused Text Summarization (QFTS) task aims at building systems that generate the summary of the text document(s) based on the given query. A key challenge in addressing this task is the lack of large labeled data for training the summarization model. In this article, we address this challenge by exploring a series of domain adaptation techniques. Given the recent success of pre-trained transformer models in a wide range of natural language processing tasks, we utilize such models to generate abstractive summaries for the QFTS task for both single-document and multi-document scenarios. For domain adaptation, we apply a variety of techniques using pre-trained transformer-based summarization models including transfer learning, weakly supervised learning, and distant supervision. Extensive experiments on six datasets show that our proposed approach is very effective in generating abstractive summaries for the QFTS task while setting a new state-of-the-art result in several datasets across a set of automatic and human evaluation metrics.",
            "corpus_id": 245385261,
            "sentences": [
                {
                    "corpus_id": "245385261",
                    "title": "Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization",
                    "text": "Below, we describe these two attention mechanisms. \n\n(i) The Bidirectional Self-Attention Mechanism: In the original BERTSUM architecture, the bidirectional self-attention mechanism (Devlin et al., 2019) is utilized by the BERT encoder to generate the encoded representation of the input text. In the bidirectional self-attention mechanism, when a pair of sentences are combined together and given as input to the BERT encoder, both sentences will give attention to each other. Thus, when we utilize the bidirectional self-attention mechanism (see Figure 2a) in the PreQFAS model, both the query and the document will not only give attention to themselves, but also they will give attention to each other to provide the encoded representation of the concatenated input. \n\n(ii) The Query-Document Attention Mechanism: Dong et al. (2019) proposed the sequence-tosequence language modeling objective for text sequences that are consisted of two segments. In such text sequences, each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment. Following this approach, we propose the Query-Document (QD) attention mechanism, where each token in the query can only attend to the tokens which are within the query while the tokens in the document can attend to all tokens in both the query and the document bidirectionally. The intuition here is that in the original PreQFAS model, the bidirectional self-attention allows the query to also attend to the document and thus the query segment might get influenced by the document segment. As a consequence, the final encoded representation of the concatenated input may lose some query related information and the decoder may produce summaries that may not be fully relevant to the query. \n\nTo avoid such scenarios, we allow the query segment to only attend to itself whereas the document segment is allowed to provide a query focused representation by attending to both the query and to itself.",
                    "score": 0.5973880084921255,
                    "section_title": "The PreQFAS model for the SD-QFAS task",
                    "char_start_offset": 24575,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 50
                        },
                        {
                            "start": 53,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 769
                        },
                        {
                            "start": 772,
                            "end": 951
                        },
                        {
                            "start": 952,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1554
                        },
                        {
                            "start": 1555,
                            "end": 1766
                        },
                        {
                            "start": 1767,
                            "end": 1966
                        },
                        {
                            "start": 1969,
                            "end": 2173
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 182,
                            "end": 203,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 817,
                            "end": 835,
                            "matchedPaperCorpusId": "147704286"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.642578125
                }
            ],
            "relevance_judgement": 0.642578125,
            "relevance_judgment_input_expanded": "# Title: Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization\n# Venue: Computational Linguistics\n# Authors: Md Tahmid Rahman Laskar, Enamul Hoque, J. Huang\n## Abstract\nAbstract The Query-Focused Text Summarization (QFTS) task aims at building systems that generate the summary of the text document(s) based on the given query. A key challenge in addressing this task is the lack of large labeled data for training the summarization model. In this article, we address this challenge by exploring a series of domain adaptation techniques. Given the recent success of pre-trained transformer models in a wide range of natural language processing tasks, we utilize such models to generate abstractive summaries for the QFTS task for both single-document and multi-document scenarios. For domain adaptation, we apply a variety of techniques using pre-trained transformer-based summarization models including transfer learning, weakly supervised learning, and distant supervision. Extensive experiments on six datasets show that our proposed approach is very effective in generating abstractive summaries for the QFTS task while setting a new state-of-the-art result in several datasets across a set of automatic and human evaluation metrics.\n## The PreQFAS model for the SD-QFAS task\nBelow, we describe these two attention mechanisms. \n\n(i) The Bidirectional Self-Attention Mechanism: In the original BERTSUM architecture, the bidirectional self-attention mechanism (Devlin et al., 2019) is utilized by the BERT encoder to generate the encoded representation of the input text. In the bidirectional self-attention mechanism, when a pair of sentences are combined together and given as input to the BERT encoder, both sentences will give attention to each other. Thus, when we utilize the bidirectional self-attention mechanism (see Figure 2a) in the PreQFAS model, both the query and the document will not only give attention to themselves, but also they will give attention to each other to provide the encoded representation of the concatenated input. \n\n(ii) The Query-Document Attention Mechanism: Dong et al. (2019) proposed the sequence-tosequence language modeling objective for text sequences that are consisted of two segments. In such text sequences, each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment. Following this approach, we propose the Query-Document (QD) attention mechanism, where each token in the query can only attend to the tokens which are within the query while the tokens in the document can attend to all tokens in both the query and the document bidirectionally. The intuition here is that in the original PreQFAS model, the bidirectional self-attention allows the query to also attend to the document and thus the query segment might get influenced by the document segment. As a consequence, the final encoded representation of the concatenated input may lose some query related information and the decoder may produce summaries that may not be fully relevant to the query. \n\nTo avoid such scenarios, we allow the query segment to only attend to itself whereas the document segment is allowed to provide a query focused representation by attending to both the query and to itself.",
            "reference_string": "[245385261 | Laskar et al. | 2021 | Citations: 45]"
        },
        {
            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 284,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.21411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "94168461",
                    "name": "Tong Nie"
                },
                {
                    "authorId": "2028643500",
                    "name": "Jiangming Sun"
                },
                {
                    "authorId": "2277421553",
                    "name": "Wei Ma"
                }
            ],
            "abstract": "Modern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: https://github.com/tongnie/awesome-llm4tr.",
            "corpus_id": 277349741,
            "sentences": [
                {
                    "corpus_id": "277349741",
                    "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
                    "text": "Models like T5 (Raffel et al., 2020) and BART (Lewis et al., 2019) utilize both encoding and decoding mechanisms, enabling them to perform a wide range of tasks, including translation and summarization. The encoder applies stacked self-attention layers to encode the input sequence, and the decoder performs cross-attention on these representations and autoregressively generates the output. \n\n\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022). \n\nTo scale the capacity of LLMs efficiently, the Mixture of Experts (MoE) technique can be exploited to combine the above architectures, such as in Swich Transformer (Fedus et al., 2022) and GLaM (Du et al., 2022). MoE involves sparsely activating a subset of model parameters (the \"experts\") for each input, allowing the model to handle a vast number of parameters without incurring prohibitive computational costs. This is achieved by employing a trainable gating mechanism to route each input token to the most relevant subset of experts. \n\nApart from the mainstream Transformer architecture, there are also emerging architectures proposed to alleviate the inherent issues of Transformers (e.g., the quadratic complexity) such as State-Space Models (SSMs) (Gu et al., 2021), Mamba (Gu and Dao, 2023), and RWKV (Peng et al., 2023).",
                    "score": 0.7908673976962467,
                    "section_title": "Architecture",
                    "char_start_offset": 34135,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 391
                        },
                        {
                            "start": 394,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 776
                        },
                        {
                            "start": 779,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1061
                        },
                        {
                            "start": 1064,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1603
                        },
                        {
                            "start": 1606,
                            "end": 1895
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 15,
                            "end": 36,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 731,
                            "end": 756,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 756,
                            "end": 775,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1228,
                            "end": 1248,
                            "matchedPaperCorpusId": "231573431"
                        },
                        {
                            "start": 1258,
                            "end": 1275,
                            "matchedPaperCorpusId": "245124124"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63818359375
                }
            ],
            "relevance_judgement": 0.63818359375,
            "relevance_judgment_input_expanded": "# Title: Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap\n# Venue: arXiv.org\n# Authors: Tong Nie, Jiangming Sun, Wei Ma\n## Abstract\nModern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: https://github.com/tongnie/awesome-llm4tr.\n## Architecture\nModels like T5 (Raffel et al., 2020) and BART (Lewis et al., 2019) utilize both encoding and decoding mechanisms, enabling them to perform a wide range of tasks, including translation and summarization. The encoder applies stacked self-attention layers to encode the input sequence, and the decoder performs cross-attention on these representations and autoregressively generates the output. \n\n\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022). \n\nTo scale the capacity of LLMs efficiently, the Mixture of Experts (MoE) technique can be exploited to combine the above architectures, such as in Swich Transformer (Fedus et al., 2022) and GLaM (Du et al., 2022). MoE involves sparsely activating a subset of model parameters (the \"experts\") for each input, allowing the model to handle a vast number of parameters without incurring prohibitive computational costs. This is achieved by employing a trainable gating mechanism to route each input token to the most relevant subset of experts. \n\nApart from the mainstream Transformer architecture, there are also emerging architectures proposed to alleviate the inherent issues of Transformers (e.g., the quadratic complexity) such as State-Space Models (SSMs) (Gu et al., 2021), Mamba (Gu and Dao, 2023), and RWKV (Peng et al., 2023).",
            "reference_string": "[277349741 | Nie et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3420710",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.14283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2222734467",
                    "name": "Andrea Busto-Casti\u00f1eira"
                },
                {
                    "authorId": "2323809078",
                    "name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o"
                },
                {
                    "authorId": "1405165681",
                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                },
                {
                    "authorId": "2034282614",
                    "name": "Francisco de Arriba-P\u00e9rez"
                }
            ],
            "abstract": "In recent years, the field of Natural Language Generation (NLG) has been boosted by the recent advances in deep learning technologies. Nonetheless, these new data-intensive methods introduce language-dependent disparities in NLG as the main training data sets are in English. Also, most neural NLG systems use decoder-only (causal) transformer language models, which work well for English, but were not designed with other languages in mind. In this work we depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. This paper first compares causal and non-causal language modeling for English and Spanish, two languages with different grammatical structures and over 1.5 billion and 0.5 billion speakers, respectively. For this purpose, we define a novel metric of average causal and non-causal context-conditioned entropy of the grammatical category distribution for both languages as an information-theoretic a priori approach. The evaluation of natural text sources (such as training data) in both languages reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English. According to this experiment, Spanish is more predictable than English given a non-causal context. Then, by applying a conditional relative entropy metric to text generation experiments, we obtain as insights that the best performance is respectively achieved with causal NLG in English, and with non-causal NLG in Spanish. These insights support further research in NLG in Spanish using bidirectional transformer language models.",
            "corpus_id": 270832367,
            "sentences": [
                {
                    "corpus_id": "270832367",
                    "title": "Predictability and Causality in Spanish and English Natural Language Generation",
                    "text": "The contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
                    "score": 0.7532888559138182,
                    "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
                    "char_start_offset": 5984,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 305
                        },
                        {
                            "start": 308,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 456
                        },
                        {
                            "start": 459,
                            "end": 507
                        },
                        {
                            "start": 510,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 1012
                        },
                        {
                            "start": 1015,
                            "end": 1129
                        },
                        {
                            "start": 1132,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1118,
                            "end": 1122,
                            "matchedPaperCorpusId": "219463386"
                        },
                        {
                            "start": 1124,
                            "end": 1128,
                            "matchedPaperCorpusId": "209832341"
                        },
                        {
                            "start": 1138,
                            "end": 1142,
                            "matchedPaperCorpusId": "234785837"
                        },
                        {
                            "start": 1695,
                            "end": 1699,
                            "matchedPaperCorpusId": "265629619"
                        },
                        {
                            "start": 1794,
                            "end": 1798,
                            "matchedPaperCorpusId": "231924507"
                        },
                        {
                            "start": 1800,
                            "end": 1804,
                            "matchedPaperCorpusId": "231715684"
                        },
                        {
                            "start": 1806,
                            "end": 1810,
                            "matchedPaperCorpusId": "247126308"
                        },
                        {
                            "start": 1850,
                            "end": 1854,
                            "matchedPaperCorpusId": "218487230"
                        },
                        {
                            "start": 1882,
                            "end": 1886,
                            "matchedPaperCorpusId": "208224776"
                        },
                        {
                            "start": 1921,
                            "end": 1925,
                            "matchedPaperCorpusId": "210839508"
                        },
                        {
                            "start": 1927,
                            "end": 1931,
                            "matchedPaperCorpusId": "212657570"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63671875
                }
            ],
            "relevance_judgement": 0.63671875,
            "relevance_judgment_input_expanded": "# Title: Predictability and Causality in Spanish and English Natural Language Generation\n# Venue: IEEE Access\n# Authors: Andrea Busto-Casti\u00f1eira, Francisco Javier Gonz\u00e1lez-Casta\u00f1o, Silvia Garc\u00eda-M\u00e9ndez, Francisco de Arriba-P\u00e9rez\n## Abstract\nIn recent years, the field of Natural Language Generation (NLG) has been boosted by the recent advances in deep learning technologies. Nonetheless, these new data-intensive methods introduce language-dependent disparities in NLG as the main training data sets are in English. Also, most neural NLG systems use decoder-only (causal) transformer language models, which work well for English, but were not designed with other languages in mind. In this work we depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. This paper first compares causal and non-causal language modeling for English and Spanish, two languages with different grammatical structures and over 1.5 billion and 0.5 billion speakers, respectively. For this purpose, we define a novel metric of average causal and non-causal context-conditioned entropy of the grammatical category distribution for both languages as an information-theoretic a priori approach. The evaluation of natural text sources (such as training data) in both languages reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English. According to this experiment, Spanish is more predictable than English given a non-causal context. Then, by applying a conditional relative entropy metric to text generation experiments, we obtain as insights that the best performance is respectively achieved with causal NLG in English, and with non-causal NLG in Spanish. These insights support further research in NLG in Spanish using bidirectional transformer language models.\n## A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS\nThe contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
            "reference_string": "[270832367 | Busto-Castineira et al. | 2024 | Citations: 1]"
        },
        {
            "title": "LLM for SoC Security: A Paradigm Shift",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 242,
            "citation_count": 54,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256992493",
                    "name": "Dipayan Saha"
                },
                {
                    "authorId": "2114625129",
                    "name": "Shams Tarek"
                },
                {
                    "authorId": "2256991081",
                    "name": "Katayoon Yahyaei"
                },
                {
                    "authorId": "2231854143",
                    "name": "Sujan Kumar Saha"
                },
                {
                    "authorId": "2257235852",
                    "name": "Jingbo Zhou"
                },
                {
                    "authorId": "145954982",
                    "name": "M. Tehranipoor"
                },
                {
                    "authorId": "1997019",
                    "name": "Farimah Farahmandi"
                }
            ],
            "abstract": "As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to effectively verify modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, large language models (LLMs) are celebrated for their remarkable success in language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research explores leveraging the emergent capabilities of generative pre-trained transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, presents practical case studies, and demonstrates comprehensive experiments. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.",
            "corpus_id": 263829839,
            "sentences": [
                {
                    "corpus_id": "263829839",
                    "title": "LLM for SoC Security: A Paradigm Shift",
                    "text": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
                    "score": 0.7325500628597399,
                    "section_title": "A. Preliminaries",
                    "char_start_offset": 37836,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 853
                        },
                        {
                            "start": 856,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2140
                        },
                        {
                            "start": 2141,
                            "end": 2270
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 776,
                            "end": 781,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 791,
                            "end": 796,
                            "matchedPaperCorpusId": "252780443"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62548828125
                }
            ],
            "relevance_judgement": 0.62548828125,
            "relevance_judgment_input_expanded": "# Title: LLM for SoC Security: A Paradigm Shift\n# Venue: IEEE Access\n# Authors: Dipayan Saha, Shams Tarek, Katayoon Yahyaei, Sujan Kumar Saha, Jingbo Zhou, M. Tehranipoor, Farimah Farahmandi\n## Abstract\nAs the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to effectively verify modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, large language models (LLMs) are celebrated for their remarkable success in language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research explores leveraging the emergent capabilities of generative pre-trained transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, presents practical case studies, and demonstrates comprehensive experiments. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.\n## A. Preliminaries\nThe encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
            "reference_string": "[263829839 | Saha et al. | 2023 | Citations: 54]"
        },
        {
            "title": "Segment-Based Attention Masking for GPTs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.18487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "121254633",
                    "name": "Shahar Katz"
                },
                {
                    "authorId": "2186740854",
                    "name": "Liran Ringel"
                },
                {
                    "authorId": "2335566528",
                    "name": "Yaniv Romano"
                },
                {
                    "authorId": "2284763723",
                    "name": "Lior Wolf"
                }
            ],
            "abstract": "Modern Language Models (LMs) owe much of their success to masked causal attention, the backbone of Generative Pre-Trained Transformer (GPT) models. Although GPTs can process the entire user prompt at once, the causal masking is applied to all input tokens step-by-step, mimicking the generation process. This imposes an unnecessary constraint during the initial\"prefill\"phase when the model processes the input prompt and generates the internal representations before producing any output tokens. In this work, attention is masked based on the known block structure at the prefill phase, followed by the conventional token-by-token autoregressive process after that. For example, in a typical chat prompt, the system prompt is treated as one block, and the user prompt as the next one. Each of these is treated as a unit for the purpose of masking, such that the first tokens in each block can access the subsequent tokens in a non-causal manner. Then, the model answer is generated in the conventional causal manner. This Segment-by-Segment scheme entails no additional computational overhead. When integrating it into models such as Llama and Qwen, state-of-the-art performance is consistently achieved.",
            "corpus_id": 274992300,
            "sentences": [
                {
                    "corpus_id": "274992300",
                    "title": "Segment-Based Attention Masking for GPTs",
                    "text": "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding. \n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design. \n\nThe most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2020). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder. However, PrefixLM requires training from scratch and is limited to single-turn inputs, overlooking scenarios with multiple prefill phases, as often encountered in chat-based systems. \n\nIn contrast, our approach enables the easy enhancement of SOTA decoder-only models by unlocking the potential of bidirectional attention in non-generated segments through lightweight finetuning. Trained on massive corpora with causal masking, these models can be enhanced with limited hardware and just a few hours of fine-tuning, enabling them to effectively use bidirectional attention during the prefill phase.",
                    "score": 0.7048680968509291,
                    "section_title": "Related Work",
                    "char_start_offset": 3399,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 575
                        },
                        {
                            "start": 578,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1185
                        },
                        {
                            "start": 1188,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1681
                        },
                        {
                            "start": 1684,
                            "end": 1878
                        },
                        {
                            "start": 1879,
                            "end": 2097
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 595,
                            "end": 616,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 950,
                            "end": 970,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1278,
                            "end": 1299,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61474609375
                },
                {
                    "corpus_id": "274992300",
                    "title": "Segment-Based Attention Masking for GPTs",
                    "text": "The introduction of the transformer architecture (Vaswani et al., 2017) has significantly advanced the field of natural language processing (NLP). Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford   * Equal contribution. \n\nFigure 1: Causal and MAS attention. The plot shows binary values, where the y-axis represents the index of the current token, and the x-axis represents the set of indices of tokens it can attend to. MAS is inspired by the observation that input prompts are provided to the model as a whole, so they can be masked together in blocks, allowing access to future tokens within the same block of the prompt. \n\nFigure 2: Model performance on the Commonsense Reasoning benchmark for seven LLMs. L,M,Q stand for Llama, Mistral and Qwen respectively. et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI. \n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approxi-mately twice the number of parameters compared to decoder-only models with equivalent capacity. Efforts such as those by Dong et al. (2019), Raffel et al. (2020), and Tay et al. (2023) explored unified architectures, where a model's parameters are trained from scratch to function as both encoder and decoder. Despite their potential efficiency, these approaches failed to gain widespread adoption. In contrast, the remarkable success of decoderonly models, exemplified by GPT-3 (Brown et al., 2020), has shifted the field's focus toward architectures almost exclusively based on causal attention. \n\nWhile the popularity of GPT models continues to grow, a key limitation is their inability to fully leverage information from future tokens.",
                    "score": 0.6573764721289808,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 453
                        },
                        {
                            "start": 454,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 820
                        },
                        {
                            "start": 823,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 959
                        },
                        {
                            "start": 960,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1166
                        },
                        {
                            "start": 1169,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1804
                        },
                        {
                            "start": 1805,
                            "end": 2003
                        },
                        {
                            "start": 2006,
                            "end": 2145
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 71,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1221,
                            "end": 1242,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1527,
                            "end": 1545,
                            "matchedPaperCorpusId": "147704286"
                        },
                        {
                            "start": 1547,
                            "end": 1567,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1573,
                            "end": 1590,
                            "matchedPaperCorpusId": "252780443"
                        },
                        {
                            "start": 1885,
                            "end": 1905,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50537109375
                }
            ],
            "relevance_judgement": 0.61474609375,
            "relevance_judgment_input_expanded": "# Title: Segment-Based Attention Masking for GPTs\n# Venue: arXiv.org\n# Authors: Shahar Katz, Liran Ringel, Yaniv Romano, Lior Wolf\n## Abstract\nModern Language Models (LMs) owe much of their success to masked causal attention, the backbone of Generative Pre-Trained Transformer (GPT) models. Although GPTs can process the entire user prompt at once, the causal masking is applied to all input tokens step-by-step, mimicking the generation process. This imposes an unnecessary constraint during the initial\"prefill\"phase when the model processes the input prompt and generates the internal representations before producing any output tokens. In this work, attention is masked based on the known block structure at the prefill phase, followed by the conventional token-by-token autoregressive process after that. For example, in a typical chat prompt, the system prompt is treated as one block, and the user prompt as the next one. Each of these is treated as a unit for the purpose of masking, such that the first tokens in each block can access the subsequent tokens in a non-causal manner. Then, the model answer is generated in the conventional causal manner. This Segment-by-Segment scheme entails no additional computational overhead. When integrating it into models such as Llama and Qwen, state-of-the-art performance is consistently achieved.\n## Introduction\nThe introduction of the transformer architecture (Vaswani et al., 2017) has significantly advanced the field of natural language processing (NLP). Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford   * Equal contribution. \n\nFigure 1: Causal and MAS attention. The plot shows binary values, where the y-axis represents the index of the current token, and the x-axis represents the set of indices of tokens it can attend to. MAS is inspired by the observation that input prompts are provided to the model as a whole, so they can be masked together in blocks, allowing access to future tokens within the same block of the prompt. \n\nFigure 2: Model performance on the Commonsense Reasoning benchmark for seven LLMs. L,M,Q stand for Llama, Mistral and Qwen respectively. et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI. \n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approxi-mately twice the number of parameters compared to decoder-only models with equivalent capacity. Efforts such as those by Dong et al. (2019), Raffel et al. (2020), and Tay et al. (2023) explored unified architectures, where a model's parameters are trained from scratch to function as both encoder and decoder. Despite their potential efficiency, these approaches failed to gain widespread adoption. In contrast, the remarkable success of decoderonly models, exemplified by GPT-3 (Brown et al., 2020), has shifted the field's focus toward architectures almost exclusively based on causal attention. \n\nWhile the popularity of GPT models continues to grow, a key limitation is their inability to fully leverage information from future tokens.\n\n## Related Work\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding. \n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design. \n\nThe most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2020). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder. However, PrefixLM requires training from scratch and is limited to single-turn inputs, overlooking scenarios with multiple prefill phases, as often encountered in chat-based systems. \n\nIn contrast, our approach enables the easy enhancement of SOTA decoder-only models by unlocking the potential of bidirectional attention in non-generated segments through lightweight finetuning. Trained on massive corpora with causal masking, these models can be enhanced with limited hardware and just a few hours of fine-tuning, enabling them to effectively use bidirectional attention during the prefill phase.",
            "reference_string": "[274992300 | Katz et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Attentive Multi-Layer Perceptron for Non-autoregressive Generation",
            "venue": "ECML/PKDD",
            "year": 2023,
            "reference_count": 63,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2244128869",
                    "name": "Shuyang Jiang"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2093485",
                    "name": "Jiangtao Feng"
                },
                {
                    "authorId": "1633166807",
                    "name": "Lin Zheng"
                },
                {
                    "authorId": "47648549",
                    "name": "Lingpeng Kong"
                }
            ],
            "abstract": "Autoregressive~(AR) generation almost dominates sequence generation for its efficacy. Recently, non-autoregressive~(NAR) generation gains increasing popularity for its efficiency and growing efficacy. However, its efficiency is still bottlenecked by quadratic complexity in sequence lengths, which is prohibitive for scaling to long sequence generation and few works have been done to mitigate this problem. In this paper, we propose a novel MLP variant, \\textbf{A}ttentive \\textbf{M}ulti-\\textbf{L}ayer \\textbf{P}erceptron~(AMLP), to produce a generation model with linear time and space complexity. Different from classic MLP with static and learnable projection matrices, AMLP leverages adaptive projections computed from inputs in an attentive mode. The sample-aware adaptive projections enable communications among tokens in a sequence, and model the measurement between the query and key space. Furthermore, we marry AMLP with popular NAR models, deriving a highly efficient NAR-AMLP architecture with linear time and space complexity. Empirical results show that such marriage architecture surpasses competitive efficient NAR models, by a significant margin on text-to-speech synthesis and machine translation. We also test AMLP's self- and cross-attention ability separately with extensive ablation experiments, and find them comparable or even superior to the other efficient models. The efficiency analysis further shows that AMLP extremely reduces the memory cost against vanilla non-autoregressive models for long sequences.",
            "corpus_id": 262088801,
            "sentences": [
                {
                    "corpus_id": "262088801",
                    "title": "Attentive Multi-Layer Perceptron for Non-autoregressive Generation",
                    "text": "Given a source sequence X 1:m , conditional sequence generation targets to predict a target sequence Y 1:n by modeling the conditional probability p(Y |X). \n\nAutoregressive generation decomposes the probability p(Y |X) as: \n\nwhich is implemented as a typical encoder-decoder architecture shown in Fig. 1a. Although such decomposition is proved effective, it suffers from two main drawbacks: efficiency and exposure bias. On the one hand, the autoregressive decoding process, where each token depends on the previous predicted ones, prevents the model from fast inference in usage. On the other hand, teacher-forcing exposes ground truth tokens in network inputs during the training process, where the exposed tokens are unable to observe in inference. Such exposure creates an inconsistency between the training and inference, and harms the prediction quality. \n\nRecently, non-autoregressive generation, depicted as Fig. 1b, shows its capability of sequence modeling in terms of both efficiency and efficacy, which decomposes the conditional probability p(Y |X) via a Na\u00efve Bayes assumption: \n\nThe NAR decomposition enables parallel decoding for each token, and speeds up the inference process substantially. Although NAR generation is much faster than AR generation, its speed is still limited by the O n 2 + nm + m 2 time complexity of the multi-head softmax attention module. This is especially problematic in modeling long sequences. \n\nAttention Types in AR & NAR Models Although autoregressive and nonautoregressive models differ from each other in sequence generation paradigms, their underlying attention mechanisms in their architectures are also different. \n\nThe token-by-token generation of AR models requires a causal decoder that forces tokens to attend to only previous features. A typical causal decoder utilizes causal softmax attentions both in self-attention and cross-attention. The attention causality entails that during the computation, it is important to ensure that the query token does not attend to the context on its right side, just as the shaded blocks in Fig. 1a. In contrast, the NAR model, which allows for parallel generation of the output sequence and global contextualization using attention, employs a noncausal decoder in Fig. 1b.",
                    "score": 0.5288751227174636,
                    "section_title": "Background: Autoregressive and Non-Autoregressive Generation",
                    "char_start_offset": 3179,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 158,
                            "end": 222
                        },
                        {
                            "start": 225,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 751
                        },
                        {
                            "start": 752,
                            "end": 860
                        },
                        {
                            "start": 863,
                            "end": 1091
                        },
                        {
                            "start": 1094,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1437
                        },
                        {
                            "start": 1440,
                            "end": 1665
                        },
                        {
                            "start": 1668,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 2092
                        },
                        {
                            "start": 2093,
                            "end": 2266
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61376953125
                }
            ],
            "relevance_judgement": 0.61376953125,
            "relevance_judgment_input_expanded": "# Title: Attentive Multi-Layer Perceptron for Non-autoregressive Generation\n# Venue: ECML/PKDD\n# Authors: Shuyang Jiang, Jinchao Zhang, Jiangtao Feng, Lin Zheng, Lingpeng Kong\n## Abstract\nAutoregressive~(AR) generation almost dominates sequence generation for its efficacy. Recently, non-autoregressive~(NAR) generation gains increasing popularity for its efficiency and growing efficacy. However, its efficiency is still bottlenecked by quadratic complexity in sequence lengths, which is prohibitive for scaling to long sequence generation and few works have been done to mitigate this problem. In this paper, we propose a novel MLP variant, \\textbf{A}ttentive \\textbf{M}ulti-\\textbf{L}ayer \\textbf{P}erceptron~(AMLP), to produce a generation model with linear time and space complexity. Different from classic MLP with static and learnable projection matrices, AMLP leverages adaptive projections computed from inputs in an attentive mode. The sample-aware adaptive projections enable communications among tokens in a sequence, and model the measurement between the query and key space. Furthermore, we marry AMLP with popular NAR models, deriving a highly efficient NAR-AMLP architecture with linear time and space complexity. Empirical results show that such marriage architecture surpasses competitive efficient NAR models, by a significant margin on text-to-speech synthesis and machine translation. We also test AMLP's self- and cross-attention ability separately with extensive ablation experiments, and find them comparable or even superior to the other efficient models. The efficiency analysis further shows that AMLP extremely reduces the memory cost against vanilla non-autoregressive models for long sequences.\n## Background: Autoregressive and Non-Autoregressive Generation\nGiven a source sequence X 1:m , conditional sequence generation targets to predict a target sequence Y 1:n by modeling the conditional probability p(Y |X). \n\nAutoregressive generation decomposes the probability p(Y |X) as: \n\nwhich is implemented as a typical encoder-decoder architecture shown in Fig. 1a. Although such decomposition is proved effective, it suffers from two main drawbacks: efficiency and exposure bias. On the one hand, the autoregressive decoding process, where each token depends on the previous predicted ones, prevents the model from fast inference in usage. On the other hand, teacher-forcing exposes ground truth tokens in network inputs during the training process, where the exposed tokens are unable to observe in inference. Such exposure creates an inconsistency between the training and inference, and harms the prediction quality. \n\nRecently, non-autoregressive generation, depicted as Fig. 1b, shows its capability of sequence modeling in terms of both efficiency and efficacy, which decomposes the conditional probability p(Y |X) via a Na\u00efve Bayes assumption: \n\nThe NAR decomposition enables parallel decoding for each token, and speeds up the inference process substantially. Although NAR generation is much faster than AR generation, its speed is still limited by the O n 2 + nm + m 2 time complexity of the multi-head softmax attention module. This is especially problematic in modeling long sequences. \n\nAttention Types in AR & NAR Models Although autoregressive and nonautoregressive models differ from each other in sequence generation paradigms, their underlying attention mechanisms in their architectures are also different. \n\nThe token-by-token generation of AR models requires a causal decoder that forces tokens to attend to only previous features. A typical causal decoder utilizes causal softmax attentions both in self-attention and cross-attention. The attention causality entails that during the computation, it is important to ensure that the query token does not attend to the context on its right side, just as the shaded blocks in Fig. 1a. In contrast, the NAR model, which allows for parallel generation of the output sequence and global contextualization using attention, employs a noncausal decoder in Fig. 1b.",
            "reference_string": "[262088801 | Jiang et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Clustering in Causal Attention Masking",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.04990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329559804",
                    "name": "Nikita Karagodin"
                },
                {
                    "authorId": "2300093577",
                    "name": "Yury Polyanskiy"
                },
                {
                    "authorId": "2275197338",
                    "name": "Philippe Rigollet"
                }
            ],
            "abstract": "This work presents a modification of the self-attention dynamics proposed by Geshkovski et al. (arXiv:2312.10794) to better reflect the practically relevant, causally masked attention used in transformer architectures for generative AI. This modification translates into an interacting particle system that cannot be interpreted as a mean-field gradient flow. Despite this loss of structure, we significantly strengthen the results of Geshkovski et al. (arXiv:2312.10794) in this context: While previous rigorous results focused on cases where all three matrices (Key, Query, and Value) were scaled identities, we prove asymptotic convergence to a single cluster for arbitrary key-query matrices and a value matrix equal to the identity. Additionally, we establish a connection to the classical R\\'enyi parking problem from combinatorial geometry to make initial theoretical steps towards demonstrating the existence of meta-stable states.",
            "corpus_id": 273877976,
            "sentences": [
                {
                    "corpus_id": "273877976",
                    "title": "Clustering in Causal Attention Masking",
                    "text": "The introduction of the Transformer architecture Vaswani et al. (2017) has markedly impacted the landscape of natural language processing (NLP), signaling the advent of large language models. Central to the Transformer architecture is the self-attention mechanism, a special kind of layer that distinguishes it from preceding models such as ResNets. This innovation has yielded unprecedented performance not only in machine translation and text summarization but also in areas beyond NLP, including computer vision, speech recognition, and robotics. The flexibility and efficiency of Transformers underscore their integral role in the progression of artificial intelligence. Despite their widespread use, the theoretical foundations underlying their success remain underexplored. \n\nFollowing Sander et al. (2022), recent studies by Geshkovski et al. (2024b) and Geshkovski et al. (2023) have proposed a mathematical framework to analyze Transformers as interacting particle systems, demonstrating that tokens, when modeled as particles, exhibit clustering under certain conditions on the Key, Query, and Value matrices. These works primarily focus on full (mean-field) attention mechanisms, where each token can interact with every other token. Building upon this foundation, our research extends the analysis to causal attention mechanisms, wherein each token is restricted to interact only with preceding tokens. This distinction is crucial, as causal attention is prevalent in Transformer models employed in generative AI and known as decoder architectures. \n\nCausal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output.",
                    "score": 0.576454061388143,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 779
                        },
                        {
                            "start": 782,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1414
                        },
                        {
                            "start": 1415,
                            "end": 1560
                        },
                        {
                            "start": 1563,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1924
                        },
                        {
                            "start": 1925,
                            "end": 2124
                        },
                        {
                            "start": 2125,
                            "end": 2278
                        },
                        {
                            "start": 2279,
                            "end": 2518
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 70,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 792,
                            "end": 812,
                            "matchedPaperCorpusId": "239616044"
                        },
                        {
                            "start": 832,
                            "end": 857,
                            "matchedPaperCorpusId": "258564860"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60400390625
                }
            ],
            "relevance_judgement": 0.60400390625,
            "relevance_judgment_input_expanded": "# Title: Clustering in Causal Attention Masking\n# Venue: Neural Information Processing Systems\n# Authors: Nikita Karagodin, Yury Polyanskiy, Philippe Rigollet\n## Abstract\nThis work presents a modification of the self-attention dynamics proposed by Geshkovski et al. (arXiv:2312.10794) to better reflect the practically relevant, causally masked attention used in transformer architectures for generative AI. This modification translates into an interacting particle system that cannot be interpreted as a mean-field gradient flow. Despite this loss of structure, we significantly strengthen the results of Geshkovski et al. (arXiv:2312.10794) in this context: While previous rigorous results focused on cases where all three matrices (Key, Query, and Value) were scaled identities, we prove asymptotic convergence to a single cluster for arbitrary key-query matrices and a value matrix equal to the identity. Additionally, we establish a connection to the classical R\\'enyi parking problem from combinatorial geometry to make initial theoretical steps towards demonstrating the existence of meta-stable states.\n## Introduction\nThe introduction of the Transformer architecture Vaswani et al. (2017) has markedly impacted the landscape of natural language processing (NLP), signaling the advent of large language models. Central to the Transformer architecture is the self-attention mechanism, a special kind of layer that distinguishes it from preceding models such as ResNets. This innovation has yielded unprecedented performance not only in machine translation and text summarization but also in areas beyond NLP, including computer vision, speech recognition, and robotics. The flexibility and efficiency of Transformers underscore their integral role in the progression of artificial intelligence. Despite their widespread use, the theoretical foundations underlying their success remain underexplored. \n\nFollowing Sander et al. (2022), recent studies by Geshkovski et al. (2024b) and Geshkovski et al. (2023) have proposed a mathematical framework to analyze Transformers as interacting particle systems, demonstrating that tokens, when modeled as particles, exhibit clustering under certain conditions on the Key, Query, and Value matrices. These works primarily focus on full (mean-field) attention mechanisms, where each token can interact with every other token. Building upon this foundation, our research extends the analysis to causal attention mechanisms, wherein each token is restricted to interact only with preceding tokens. This distinction is crucial, as causal attention is prevalent in Transformer models employed in generative AI and known as decoder architectures. \n\nCausal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output.",
            "reference_string": "[273877976 | Karagodin et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Making Text Embedders Few-Shot Learners",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 37,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15700, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2223752606",
                    "name": "Chaofan Li"
                },
                {
                    "authorId": "2316642216",
                    "name": "Minghao Qin"
                },
                {
                    "authorId": "2051175765",
                    "name": "Shitao Xiao"
                },
                {
                    "authorId": "2322614361",
                    "name": "Jianlyu Chen"
                },
                {
                    "authorId": "2282542078",
                    "name": "Kun Luo"
                },
                {
                    "authorId": "2276488152",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "2301158639",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2240687341",
                    "name": "Zheng Liu"
                }
            ],
            "abstract": "Large language models (LLMs) with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context. Recognizing the potential of this capability, we propose leveraging the ICL feature in LLMs to enhance the process of text embedding generation. To this end, we introduce a novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. Our approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks. Additionally, we have investigated how to effectively utilize LLMs as embedding models, including various attention mechanisms, pooling methods, etc. Our findings suggest that retaining the original framework often yields the best results, underscoring that simplicity is best. Experimental results on the MTEB and AIR-Bench benchmarks demonstrate that our approach sets new state-of-the-art (SOTA) performance. Our model, code and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding .",
            "corpus_id": 272831970,
            "sentences": [
                {
                    "corpus_id": "272831970",
                    "title": "Making Text Embedders Few-Shot Learners",
                    "text": "The attention mechanism in LLM-based embedding models is typically unidirectional, aligned with the next-token prediction task fundamental to their pre-training (Touvron et al., 2023). \n\nHowever, recent studies indicate that unidirectional attention may limit the model's capacity for representation learning. Evidence suggests that bidirectional attention is more effective at integrating contextual information, resulting in improved performance on certain tasks. For example, LLM2Vec (BehnamGhader et al., 2024) introduces an additional training phase with a masked token prediction task, preconditioning the model for bidirectional attention. Approaches such as NV-Embed (Lee et al., 2024a) and GritLM (Muennighoff et al., 2024) replace unidirectional attention with bidirectional attention during the embedding training phase, often employing mean pooling or more sophisticated latent attention layers to obtain representations for queries and passages. \n\nDespite these advances, we argue that incorporating bidirectional attention during embedding finetuning creates a mismatch with the model's pre-training design, potentially undermining its incontext learning and generative properties. To address the trade-off between enhancing embedding representations for specific tasks and preserving the model's inherent generative properties for deep semantic pattern understanding, our approach retains the unidirectional attention mechanism, consistent with the majority of existing embedding methods. \n\nWe use the [EOS] token's output embedding as the vector representation for queries and passages, positioning it at the end of inputs to capture both semantic and ICL patterns through causal attention mechanisms, thereby aligning with the foundational pretraining methodology of LLMs. Specifically, given the tokenized input sequence T: [BOS], t 1 , ..., t N is sent into the LLM (Figure 1): \n\nThe text embedding is taken from the output embedding of the special token [EOS].",
                    "score": 0.547681917964005,
                    "section_title": "REPRESENTATION METHOD",
                    "char_start_offset": 10522,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 187,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 958
                        },
                        {
                            "start": 961,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1503
                        },
                        {
                            "start": 1506,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1896
                        },
                        {
                            "start": 1899,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 706,
                            "end": 732,
                            "matchedPaperCorpusId": "1289517"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6025390625
                }
            ],
            "relevance_judgement": 0.6025390625,
            "relevance_judgment_input_expanded": "# Title: Making Text Embedders Few-Shot Learners\n# Venue: arXiv.org\n# Authors: Chaofan Li, Minghao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao, Defu Lian, Zheng Liu\n## Abstract\nLarge language models (LLMs) with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context. Recognizing the potential of this capability, we propose leveraging the ICL feature in LLMs to enhance the process of text embedding generation. To this end, we introduce a novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. Our approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks. Additionally, we have investigated how to effectively utilize LLMs as embedding models, including various attention mechanisms, pooling methods, etc. Our findings suggest that retaining the original framework often yields the best results, underscoring that simplicity is best. Experimental results on the MTEB and AIR-Bench benchmarks demonstrate that our approach sets new state-of-the-art (SOTA) performance. Our model, code and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding .\n## REPRESENTATION METHOD\nThe attention mechanism in LLM-based embedding models is typically unidirectional, aligned with the next-token prediction task fundamental to their pre-training (Touvron et al., 2023). \n\nHowever, recent studies indicate that unidirectional attention may limit the model's capacity for representation learning. Evidence suggests that bidirectional attention is more effective at integrating contextual information, resulting in improved performance on certain tasks. For example, LLM2Vec (BehnamGhader et al., 2024) introduces an additional training phase with a masked token prediction task, preconditioning the model for bidirectional attention. Approaches such as NV-Embed (Lee et al., 2024a) and GritLM (Muennighoff et al., 2024) replace unidirectional attention with bidirectional attention during the embedding training phase, often employing mean pooling or more sophisticated latent attention layers to obtain representations for queries and passages. \n\nDespite these advances, we argue that incorporating bidirectional attention during embedding finetuning creates a mismatch with the model's pre-training design, potentially undermining its incontext learning and generative properties. To address the trade-off between enhancing embedding representations for specific tasks and preserving the model's inherent generative properties for deep semantic pattern understanding, our approach retains the unidirectional attention mechanism, consistent with the majority of existing embedding methods. \n\nWe use the [EOS] token's output embedding as the vector representation for queries and passages, positioning it at the end of inputs to capture both semantic and ICL patterns through causal attention mechanisms, thereby aligning with the foundational pretraining methodology of LLMs. Specifically, given the tokenized input sequence T: [BOS], t 1 , ..., t N is sent into the LLM (Figure 1): \n\nThe text embedding is taken from the output embedding of the special token [EOS].",
            "reference_string": "[272831970 | Li et al. | 2024 | Citations: 37]"
        },
        {
            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 81,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327246234",
                    "name": "Junhua Liu"
                },
                {
                    "authorId": "2325881142",
                    "name": "Bin Fu"
                }
            ],
            "abstract": "Multilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.",
            "corpus_id": 273532366,
            "sentences": [
                {
                    "corpus_id": "273532366",
                    "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
                    "text": "Currently, MLLMs mainly use decoder, encoder-decoder, and prefix language model (Prefix LM) structures. The early pre-training model BERT used the encoder structure, which is now rarely used independently, so this article mainly introduces these three structures of MLLMs. \n\nDecoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models. Recent research suggests that model performance can be enhanced through decoder-less training [67] or instruction fine-tuning [68] on decoder-based models. However, these methods have primarily been tested on English-centric models, and their effectiveness on MLLMs remains unclear. The PaLM 2 model has reported excellent results using similar approaches for multilingual model tasks, suggesting this could be a viable optimization strategy for MLLMs. \n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token. mT5 continues to use this structure. \n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure.",
                    "score": 0.5682713077791788,
                    "section_title": "D. Training MLLMs",
                    "char_start_offset": 44592,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 272
                        },
                        {
                            "start": 275,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 629
                        },
                        {
                            "start": 630,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1082
                        },
                        {
                            "start": 1085,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1702
                        },
                        {
                            "start": 1703,
                            "end": 1899
                        },
                        {
                            "start": 1900,
                            "end": 2053
                        },
                        {
                            "start": 2054,
                            "end": 2090
                        },
                        {
                            "start": 2093,
                            "end": 2234
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5791015625
                }
            ],
            "relevance_judgement": 0.5791015625,
            "relevance_judgment_input_expanded": "# Title: Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact\n# Venue: arXiv.org\n# Authors: Junhua Liu, Bin Fu\n## Abstract\nMultilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.\n## D. Training MLLMs\nCurrently, MLLMs mainly use decoder, encoder-decoder, and prefix language model (Prefix LM) structures. The early pre-training model BERT used the encoder structure, which is now rarely used independently, so this article mainly introduces these three structures of MLLMs. \n\nDecoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models. Recent research suggests that model performance can be enhanced through decoder-less training [67] or instruction fine-tuning [68] on decoder-based models. However, these methods have primarily been tested on English-centric models, and their effectiveness on MLLMs remains unclear. The PaLM 2 model has reported excellent results using similar approaches for multilingual model tasks, suggesting this could be a viable optimization strategy for MLLMs. \n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token. mT5 continues to use this structure. \n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure.",
            "reference_string": "[273532366 | Liu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "TWT: Table with Written Text for Controlled Data-to-Text Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 47,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.18653/v1/2021.findings-emnlp.107",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2021.findings-emnlp.107?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2021.findings-emnlp.107, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2841012",
                    "name": "Tongliang Li"
                },
                {
                    "authorId": "2153681740",
                    "name": "Lei Fang"
                },
                {
                    "authorId": "153249455",
                    "name": "Jian-Guang Lou"
                },
                {
                    "authorId": "1707275",
                    "name": "Zhoujun Li"
                }
            ],
            "abstract": "Large pre-trained neural models have recently shown remarkable progress in text generation. In this paper, we propose to generate text conditioned on the structured data (table) and a pre\ufb01x (the written text) by leveraging the pre-trained models. We present a new data-to-text dataset, T able with W ritten T ext (TWT), by repurposing two existing datasets: ToTTo and TabFact. TWT contains both factual and logical statements that are faithful to the structured data, aiming to serve as a useful benchmark for controlled text generation. Compared with existing data-to-text task settings, TWT is more intuitive, the pre\ufb01x (usually provided by the user) controls the topic of the generated text. Existing methods usually output hallucinated text that is not faithful on TWT. Therefore, we design a novel approach with table-aware attention visibility and copy mechanism over the table. Experimental results show that our approach outperforms state-of-the-art methods under both automatic and human evaluation metrics.",
            "corpus_id": 244119798,
            "sentences": [
                {
                    "corpus_id": "244119798",
                    "title": "TWT: Table with Written Text for Controlled Data-to-Text Generation",
                    "text": "Typically, the encoder-decoder based models generate text starting from the beginning, and the decoder adopts a causal mask to force the state of each decoder time step s t i only attend to the state from the previous time steps, s t|t\u2264t i , to avoid seeing tokens \"from the future\". We consider this type of attention as unidirectional. In our task, we have the input prefix as the written text. Tokens in the prefix should be visible to each other. Therefore, we adopt the causal with prefix mask: bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens.",
                    "score": 0.7528913294726668,
                    "section_title": "Decoder Self-attention Visibility",
                    "char_start_offset": 14430,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 607
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57763671875
                }
            ],
            "relevance_judgement": 0.57763671875,
            "relevance_judgment_input_expanded": "# Title: TWT: Table with Written Text for Controlled Data-to-Text Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Tongliang Li, Lei Fang, Jian-Guang Lou, Zhoujun Li\n## Abstract\nLarge pre-trained neural models have recently shown remarkable progress in text generation. In this paper, we propose to generate text conditioned on the structured data (table) and a pre\ufb01x (the written text) by leveraging the pre-trained models. We present a new data-to-text dataset, T able with W ritten T ext (TWT), by repurposing two existing datasets: ToTTo and TabFact. TWT contains both factual and logical statements that are faithful to the structured data, aiming to serve as a useful benchmark for controlled text generation. Compared with existing data-to-text task settings, TWT is more intuitive, the pre\ufb01x (usually provided by the user) controls the topic of the generated text. Existing methods usually output hallucinated text that is not faithful on TWT. Therefore, we design a novel approach with table-aware attention visibility and copy mechanism over the table. Experimental results show that our approach outperforms state-of-the-art methods under both automatic and human evaluation metrics.\n## Decoder Self-attention Visibility\nTypically, the encoder-decoder based models generate text starting from the beginning, and the decoder adopts a causal mask to force the state of each decoder time step s t i only attend to the state from the previous time steps, s t|t\u2264t i , to avoid seeing tokens \"from the future\". We consider this type of attention as unidirectional. In our task, we have the input prefix as the written text. Tokens in the prefix should be visible to each other. Therefore, we adopt the causal with prefix mask: bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens.",
            "reference_string": "[244119798 | Li et al. | 2021 | Citations: 11]"
        },
        {
            "title": "An Analysis and Mitigation of the Reversal Curse",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 21,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2311.07468",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66261602",
                    "name": "Ang Lv"
                },
                {
                    "authorId": "2266486204",
                    "name": "Kaiyi Zhang"
                },
                {
                    "authorId": "1889683",
                    "name": "Shufang Xie"
                },
                {
                    "authorId": "2071635049",
                    "name": "Quan Tu"
                },
                {
                    "authorId": "2266420263",
                    "name": "Yuhan Chen"
                },
                {
                    "authorId": "2263887786",
                    "name": "Ji-Rong Wen"
                },
                {
                    "authorId": "2172312251",
                    "name": "Rui Yan"
                }
            ],
            "abstract": "Recent research observed a noteworthy phenomenon in large language models (LLMs), referred to as the ``reversal curse.'' The reversal curse is that when dealing with two entities, denoted as $a$ and $b$, connected by their relation $R$ and its inverse $R^{-1}$, LLMs excel in handling sequences in the form of ``$aRb$,'' but encounter challenges when processing ``$bR^{-1}a$,'' whether in generation or comprehension. For instance, GPT-4 can accurately respond to the query ``Tom Cruise's mother is?'' with ``Mary Lee Pfeiffer,'' but it struggles to provide a satisfactory answer when asked ``Mary Lee Pfeiffer's son is?'' In this paper, we undertake the first-ever study of how the reversal curse happens in LLMs. Our investigations reveal that the reversal curse can stem from the specific training objectives, which become particularly evident in the widespread use of next-token prediction within most causal language models. We hope this initial investigation can draw more attention to the reversal curse, as well as other underlying limitations in current LLMs.",
            "corpus_id": 265150001,
            "sentences": [
                {
                    "corpus_id": "265150001",
                    "title": "An Analysis and Mitigation of the Reversal Curse",
                    "text": "Bi-Directional \n\nConverting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask, as doing so would introduce positional information that the model has never encountered during training, in which stage a query vector is only allowed to calculate the inner product with its preceding key vectors. This is evident in Eq.4: the relative position n \u2212 m is always non-positive during training but is positive when q m needs to attend to k >m . To address this issue, we propose a modification to the inner product between q m and k n for arbitrary values of m and n in a causal language model, as follows: \n\n(5) This adjustment ensures that when a query vector calculates an inner product with subsequent keys, there is no unexpected relative position information compared to training, as long as the relative distance between m and n does not exceed the maximum context length which is not within the scope of this paper. \n\nTo implement the Eq.5: when n \u2264 m, we calculate the attention weights as usual; In cases where n > m, we incorporate positional information with R \u22a4 \u03b8 , the transposition of R \u03b8 . Because R \u22a4 \u03b8,m is equivalent to R \u03b8,\u2212m for any given position m, we have: \n\n(6) Figure 3 illustrates this modification of attention calculation, where purple lines and squares denote that attention weights are calculated using the standard R \u03b8 matrix, and yellow indicates that the query attends to its succeeding keys within the extended bidirectional attention mechanism. The annotated numbers indicate the relative distance between a query and a key vector, with all values being non-positive.",
                    "score": 0.5657012873746977,
                    "section_title": "Extending Causal Attention to",
                    "char_start_offset": 17265,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 14
                        },
                        {
                            "start": 17,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 717
                        },
                        {
                            "start": 720,
                            "end": 1034
                        },
                        {
                            "start": 1037,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1291
                        },
                        {
                            "start": 1294,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1714
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5771484375
                }
            ],
            "relevance_judgement": 0.5771484375,
            "relevance_judgment_input_expanded": "# Title: An Analysis and Mitigation of the Reversal Curse\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, Rui Yan\n## Abstract\nRecent research observed a noteworthy phenomenon in large language models (LLMs), referred to as the ``reversal curse.'' The reversal curse is that when dealing with two entities, denoted as $a$ and $b$, connected by their relation $R$ and its inverse $R^{-1}$, LLMs excel in handling sequences in the form of ``$aRb$,'' but encounter challenges when processing ``$bR^{-1}a$,'' whether in generation or comprehension. For instance, GPT-4 can accurately respond to the query ``Tom Cruise's mother is?'' with ``Mary Lee Pfeiffer,'' but it struggles to provide a satisfactory answer when asked ``Mary Lee Pfeiffer's son is?'' In this paper, we undertake the first-ever study of how the reversal curse happens in LLMs. Our investigations reveal that the reversal curse can stem from the specific training objectives, which become particularly evident in the widespread use of next-token prediction within most causal language models. We hope this initial investigation can draw more attention to the reversal curse, as well as other underlying limitations in current LLMs.\n## Extending Causal Attention to\nBi-Directional \n\nConverting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask, as doing so would introduce positional information that the model has never encountered during training, in which stage a query vector is only allowed to calculate the inner product with its preceding key vectors. This is evident in Eq.4: the relative position n \u2212 m is always non-positive during training but is positive when q m needs to attend to k >m . To address this issue, we propose a modification to the inner product between q m and k n for arbitrary values of m and n in a causal language model, as follows: \n\n(5) This adjustment ensures that when a query vector calculates an inner product with subsequent keys, there is no unexpected relative position information compared to training, as long as the relative distance between m and n does not exceed the maximum context length which is not within the scope of this paper. \n\nTo implement the Eq.5: when n \u2264 m, we calculate the attention weights as usual; In cases where n > m, we incorporate positional information with R \u22a4 \u03b8 , the transposition of R \u03b8 . Because R \u22a4 \u03b8,m is equivalent to R \u03b8,\u2212m for any given position m, we have: \n\n(6) Figure 3 illustrates this modification of attention calculation, where purple lines and squares denote that attention weights are calculated using the standard R \u03b8 matrix, and yellow indicates that the query attends to its succeeding keys within the extended bidirectional attention mechanism. The annotated numbers indicate the relative distance between a query and a key vector, with all values being non-positive.",
            "reference_string": "[265150001 | Lv et al. | 2023 | Citations: 21]"
        },
        {
            "title": "GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model",
            "venue": "Interspeech",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09444, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118544111",
                    "name": "Yingying Gao"
                },
                {
                    "authorId": "2261184944",
                    "name": "Shilei Zhang"
                },
                {
                    "authorId": "2261084697",
                    "name": "Chao Deng"
                },
                {
                    "authorId": "2144086249",
                    "name": "Junlan Feng"
                }
            ],
            "abstract": "Pre-trained speech language models such as HuBERT and WavLM leverage unlabeled speech data for self-supervised learning and offer powerful representations for numerous downstream tasks. Despite the success of these models, their high requirements for memory and computing resource hinder their application on resource restricted devices. Therefore, this paper introduces GenDistiller, a novel knowledge distillation framework which generates the hidden representations of the pre-trained teacher model directly by a much smaller student network. The proposed method takes the previous hidden layer as history and implements a layer-by-layer prediction of the teacher model autoregressively. Experiments on SUPERB reveal the advantage of GenDistiller over the baseline distilling method without an autoregressive framework, with 33% fewer parameters, similar time consumption and better performance on most of the SUPERB tasks. Ultimately, the proposed GenDistiller reduces the size of WavLM by 82%.",
            "corpus_id": 270521959,
            "sentences": [
                {
                    "corpus_id": "270521959",
                    "title": "GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model",
                    "text": "Language models are established to predict the embedding of the target token based on the context or the previous tokens of it.In this work, we use generative language model to predict the hidden layer embeddings of teacher model in the hope to involve the interaction of the intermediate layers and avoid utilizing the future information.Three generative architectures are considered: encoder-decoder, prefix decoder and causal decoder [24].The encoder-decoder architecture [25][26][27] consists of two stacks of Transformer blocks to construct encoder and decoder separately.The encoder encodes the input sequence into a common history and the decoder generates the target sequence based on the common history in an autoregressive way.The prefix decoder architecture [28,29] performs bidirectional attention over the prefix tokens and unidirectional attention on generated tokens.The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input through a unidirectional attention mask.In our work, we select the causal decoder architecture as the backbone of our generative distiller since it is more concise and is capable to predict the hidden layer outputs of the teacher model autoregressively.",
                    "score": 0.6768230885390735,
                    "section_title": "Generative Language Models",
                    "char_start_offset": 5733,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 127,
                            "end": 339
                        },
                        {
                            "start": 339,
                            "end": 442
                        },
                        {
                            "start": 442,
                            "end": 577
                        },
                        {
                            "start": 577,
                            "end": 737
                        },
                        {
                            "start": 737,
                            "end": 882
                        },
                        {
                            "start": 882,
                            "end": 1015
                        },
                        {
                            "start": 1015,
                            "end": 1228
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 437,
                            "end": 441,
                            "matchedPaperCorpusId": "257900969"
                        },
                        {
                            "start": 479,
                            "end": 483,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 769,
                            "end": 773,
                            "matchedPaperCorpusId": "247951931"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.576171875
                }
            ],
            "relevance_judgement": 0.576171875,
            "relevance_judgment_input_expanded": "# Title: GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model\n# Venue: Interspeech\n# Authors: Yingying Gao, Shilei Zhang, Chao Deng, Junlan Feng\n## Abstract\nPre-trained speech language models such as HuBERT and WavLM leverage unlabeled speech data for self-supervised learning and offer powerful representations for numerous downstream tasks. Despite the success of these models, their high requirements for memory and computing resource hinder their application on resource restricted devices. Therefore, this paper introduces GenDistiller, a novel knowledge distillation framework which generates the hidden representations of the pre-trained teacher model directly by a much smaller student network. The proposed method takes the previous hidden layer as history and implements a layer-by-layer prediction of the teacher model autoregressively. Experiments on SUPERB reveal the advantage of GenDistiller over the baseline distilling method without an autoregressive framework, with 33% fewer parameters, similar time consumption and better performance on most of the SUPERB tasks. Ultimately, the proposed GenDistiller reduces the size of WavLM by 82%.\n## Generative Language Models\nLanguage models are established to predict the embedding of the target token based on the context or the previous tokens of it.In this work, we use generative language model to predict the hidden layer embeddings of teacher model in the hope to involve the interaction of the intermediate layers and avoid utilizing the future information.Three generative architectures are considered: encoder-decoder, prefix decoder and causal decoder [24].The encoder-decoder architecture [25][26][27] consists of two stacks of Transformer blocks to construct encoder and decoder separately.The encoder encodes the input sequence into a common history and the decoder generates the target sequence based on the common history in an autoregressive way.The prefix decoder architecture [28,29] performs bidirectional attention over the prefix tokens and unidirectional attention on generated tokens.The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input through a unidirectional attention mask.In our work, we select the causal decoder architecture as the backbone of our generative distiller since it is more concise and is capable to predict the hidden layer outputs of the teacher model autoregressively.",
            "reference_string": "[270521959 | Gao et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.06225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354284757",
                    "name": "Biao Zhang"
                },
                {
                    "authorId": "2165469946",
                    "name": "Fedor Moiseev"
                },
                {
                    "authorId": "2343748926",
                    "name": "Joshua Ainslie"
                },
                {
                    "authorId": "1658871094",
                    "name": "P. Suganthan"
                },
                {
                    "authorId": "2352024723",
                    "name": "Min Ma"
                },
                {
                    "authorId": "9692128",
                    "name": "Surya Bhupatiraju"
                },
                {
                    "authorId": "2275184616",
                    "name": "Federico Lebron"
                },
                {
                    "authorId": "2273534960",
                    "name": "Orhan Firat"
                },
                {
                    "authorId": "2319608",
                    "name": "Armand Joulin"
                },
                {
                    "authorId": "2349772191",
                    "name": "Zhe Dong"
                }
            ],
            "abstract": "While decoder-only large language models (LLMs) have shown impressive results, encoder-decoder models are still widely adopted in real-world applications for their inference efficiency and richer encoder representation. In this paper, we study a novel problem: adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches to achieve a more favorable quality-efficiency trade-off. We argue that adaptation not only enables inheriting the capability of decoder-only LLMs but also reduces the demand for computation compared to pretraining from scratch. We rigorously explore different pretraining objectives and parameter initialization/optimization techniques. Through extensive experiments based on Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to 1.6B), we demonstrate the effectiveness of adaptation and the advantage of encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs achieve comparable (often better) pretraining performance but substantially better finetuning performance than their decoder-only counterpart. For example, Gemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning. Encoder-decoder adaptation also allows for flexible combination of different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B by $>$3\\%. The adapted encoder representation also yields better results on SuperGLUE. We will release our checkpoints to facilitate future research.",
            "corpus_id": 277626724,
            "sentences": [
                {
                    "corpus_id": "277626724",
                    "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
                    "text": "Pretraining LLMs is both compute and time intensive. To reduce the amount of training required, we propose to adapt existing decoder-only LLMs to encoder-decoder and leverage pretrained decoder-only checkpoints for initialization, as shown in Figure 1. Due to this, we keep the encoder-decoder architecture as similar as possible to original decoder-only model, only introducing changes when necessary. This results in the following architecture: \n\n1. Encoder has exactly the same architecture as the decoder-only model, but self-attention is switched from causal to bidirectional. We provide ablations in Section 6 that illustrate the critical effect of bidirectional attention on downstream performance. \n\n2. In each Decoder block, FFN and self-attention parts are identical to the corresponding parts in decoder-only models, and cross-attention has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder. \n\nWe base our study on Gemma 2 (Team et al., 2024). But note our approach is highly flexible and isn't restricted to specific decoder-only architectures. We can easily apply our method to other model families, such as LLaMA (Dubey et al., 2024), QWen (Yang et al., 2024), and DeepSeek (Liu et al., 2024a). In theory, we can also adapt decoder-only models from different families, such as pairing LLaMA models with QWen models. \n\nIn addition, our approach allows for unbalanced encoderdecoder models, where the decoder is significantly smaller than the encoder. This provides better support for applications where input processing capabilities are more important than generative capacity. For example, for summarization, deep understanding of the input text is often more important than the generation part, as it doesn't need to generate any new information. As a result, generation time is significantly reduced, while providing competitive quality.",
                    "score": 0.5655866129360188,
                    "section_title": "Architecture",
                    "char_start_offset": 6279,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 52
                        },
                        {
                            "start": 53,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 446
                        },
                        {
                            "start": 449,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 705
                        },
                        {
                            "start": 708,
                            "end": 962
                        },
                        {
                            "start": 965,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1116
                        },
                        {
                            "start": 1117,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1389
                        },
                        {
                            "start": 1392,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1821
                        },
                        {
                            "start": 1822,
                            "end": 1913
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57080078125
                }
            ],
            "relevance_judgement": 0.57080078125,
            "relevance_judgment_input_expanded": "# Title: Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation\n# Venue: arXiv.org\n# Authors: Biao Zhang, Fedor Moiseev, Joshua Ainslie, P. Suganthan, Min Ma, Surya Bhupatiraju, Federico Lebron, Orhan Firat, Armand Joulin, Zhe Dong\n## Abstract\nWhile decoder-only large language models (LLMs) have shown impressive results, encoder-decoder models are still widely adopted in real-world applications for their inference efficiency and richer encoder representation. In this paper, we study a novel problem: adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches to achieve a more favorable quality-efficiency trade-off. We argue that adaptation not only enables inheriting the capability of decoder-only LLMs but also reduces the demand for computation compared to pretraining from scratch. We rigorously explore different pretraining objectives and parameter initialization/optimization techniques. Through extensive experiments based on Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to 1.6B), we demonstrate the effectiveness of adaptation and the advantage of encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs achieve comparable (often better) pretraining performance but substantially better finetuning performance than their decoder-only counterpart. For example, Gemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning. Encoder-decoder adaptation also allows for flexible combination of different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B by $>$3\\%. The adapted encoder representation also yields better results on SuperGLUE. We will release our checkpoints to facilitate future research.\n## Architecture\nPretraining LLMs is both compute and time intensive. To reduce the amount of training required, we propose to adapt existing decoder-only LLMs to encoder-decoder and leverage pretrained decoder-only checkpoints for initialization, as shown in Figure 1. Due to this, we keep the encoder-decoder architecture as similar as possible to original decoder-only model, only introducing changes when necessary. This results in the following architecture: \n\n1. Encoder has exactly the same architecture as the decoder-only model, but self-attention is switched from causal to bidirectional. We provide ablations in Section 6 that illustrate the critical effect of bidirectional attention on downstream performance. \n\n2. In each Decoder block, FFN and self-attention parts are identical to the corresponding parts in decoder-only models, and cross-attention has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder. \n\nWe base our study on Gemma 2 (Team et al., 2024). But note our approach is highly flexible and isn't restricted to specific decoder-only architectures. We can easily apply our method to other model families, such as LLaMA (Dubey et al., 2024), QWen (Yang et al., 2024), and DeepSeek (Liu et al., 2024a). In theory, we can also adapt decoder-only models from different families, such as pairing LLaMA models with QWen models. \n\nIn addition, our approach allows for unbalanced encoderdecoder models, where the decoder is significantly smaller than the encoder. This provides better support for applications where input processing capabilities are more important than generative capacity. For example, for summarization, deep understanding of the input text is often more important than the generation part, as it doesn't need to generate any new information. As a result, generation time is significantly reduced, while providing competitive quality.",
            "reference_string": "[277626724 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Which Discriminator for Cooperative Text Generation?",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022,
            "reference_count": 43,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2204.11586",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.11586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2129106958",
                    "name": "Antoine Chaffin"
                },
                {
                    "authorId": "90745780",
                    "name": "Thomas Scialom"
                },
                {
                    "authorId": "1782552",
                    "name": "S. Lamprier"
                },
                {
                    "authorId": "1767493",
                    "name": "Jacopo Staiano"
                },
                {
                    "authorId": "1703777",
                    "name": "Benjamin Piwowarski"
                },
                {
                    "authorId": "1801242",
                    "name": "Ewa Kijak"
                },
                {
                    "authorId": "1735666",
                    "name": "V. Claveau"
                }
            ],
            "abstract": "Language models generate texts by successively predicting probability distributions for next tokens given past ones. A growing field of interest tries to leverage external information in the decoding process so that the generated texts have desired properties, such as being more natural, non toxic, faithful, or having a specific writing style. A solution is to use a classifier at each generation step, resulting in a cooperative environment where the classifier guides the decoding of the language model distribution towards relevant texts for the task at hand. In this paper, we examine three families of (transformer-based) discriminators for this specific task of cooperative decoding: bidirectional, left-to-right and generative ones. We evaluate the pros and cons of these different types of discriminators for cooperative generation, exploring respective accuracy on classification tasks along with their impact on the resulting sample quality and computational performances. We also provide the code of a batched implementation of the powerful cooperative decoding strategy used for our experiments, the Monte Carlo Tree Search, working with each discriminator for Natural Language Generation.",
            "corpus_id": 248377166,
            "sentences": [
                {
                    "corpus_id": "248377166",
                    "title": "Which Discriminator for Cooperative Text Generation?",
                    "text": "By default, attention layers as defined in [34] are bidirectional: every token can attend to tokens at every position. When it comes to discrimination, models based on such bidirectional attention are commonly used since \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than [...] a left-to-right model\" [9]. However, while it brings some capacity to the model, it also makes it non auto-regressive: when a token is added at the end of a sequence, every hidden states need to be re-computed. \n\nOne way to train a transformer based LM for text generation is to use unidirectional attention masks [29]. In this unidirectional setting, any extra token added at the end of a sequence does not change the already calculated hidden states, since previous tokens do not attend to it. Thus, starting from an already classified sequence  1: \u22121 , classifying  1: only requires to compute  attention scores, rather than the whole set of  2 scores per self-attention layer, as it would be required in the bidirectional setting. In common discriminative tasks, this does not matter since only entire sequences are discriminated. Hence, none of the hidden states needs to be reused for another next sample. However, for a use in autoregressive cooperative decoding, where input sequences are often the continuation of already discriminated ones unidirectional attention allows to reuse contextual encoding of previous tokens, hence greatly speeding up the process. \n\nHowever, even with unidirectional discriminators, evaluating every possible continuation of a given sequence is intractable since, for a vocabulary of size |V |, it requires |V | forward passes at each decoding step. |V | being in the order of ten thousand, discriminating every possible continuation of decoding sequences is too costly. Thus, cooperative approaches have to circumvent this issue by limiting the number of sequences actually evaluated by the discriminator. For example, DAS pre-filters potential continuations on the nucleus of the LM distribution [16]. This choice necessarily biases the resulting generated distribution.",
                    "score": 0.5773964079058465,
                    "section_title": "Choosing the Right Teammate",
                    "char_start_offset": 5982,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 542
                        },
                        {
                            "start": 545,
                            "end": 651
                        },
                        {
                            "start": 652,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 1066
                        },
                        {
                            "start": 1067,
                            "end": 1166
                        },
                        {
                            "start": 1167,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1501
                        },
                        {
                            "start": 1504,
                            "end": 1720
                        },
                        {
                            "start": 1721,
                            "end": 1841
                        },
                        {
                            "start": 1842,
                            "end": 1977
                        },
                        {
                            "start": 1978,
                            "end": 2074
                        },
                        {
                            "start": 2075,
                            "end": 2143
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 43,
                            "end": 47,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 355,
                            "end": 358,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 2069,
                            "end": 2073,
                            "matchedPaperCorpusId": "127986954"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5556640625
                }
            ],
            "relevance_judgement": 0.5556640625,
            "relevance_judgment_input_expanded": "# Title: Which Discriminator for Cooperative Text Generation?\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Antoine Chaffin, Thomas Scialom, S. Lamprier, Jacopo Staiano, Benjamin Piwowarski, Ewa Kijak, V. Claveau\n## Abstract\nLanguage models generate texts by successively predicting probability distributions for next tokens given past ones. A growing field of interest tries to leverage external information in the decoding process so that the generated texts have desired properties, such as being more natural, non toxic, faithful, or having a specific writing style. A solution is to use a classifier at each generation step, resulting in a cooperative environment where the classifier guides the decoding of the language model distribution towards relevant texts for the task at hand. In this paper, we examine three families of (transformer-based) discriminators for this specific task of cooperative decoding: bidirectional, left-to-right and generative ones. We evaluate the pros and cons of these different types of discriminators for cooperative generation, exploring respective accuracy on classification tasks along with their impact on the resulting sample quality and computational performances. We also provide the code of a batched implementation of the powerful cooperative decoding strategy used for our experiments, the Monte Carlo Tree Search, working with each discriminator for Natural Language Generation.\n## Choosing the Right Teammate\nBy default, attention layers as defined in [34] are bidirectional: every token can attend to tokens at every position. When it comes to discrimination, models based on such bidirectional attention are commonly used since \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than [...] a left-to-right model\" [9]. However, while it brings some capacity to the model, it also makes it non auto-regressive: when a token is added at the end of a sequence, every hidden states need to be re-computed. \n\nOne way to train a transformer based LM for text generation is to use unidirectional attention masks [29]. In this unidirectional setting, any extra token added at the end of a sequence does not change the already calculated hidden states, since previous tokens do not attend to it. Thus, starting from an already classified sequence  1: \u22121 , classifying  1: only requires to compute  attention scores, rather than the whole set of  2 scores per self-attention layer, as it would be required in the bidirectional setting. In common discriminative tasks, this does not matter since only entire sequences are discriminated. Hence, none of the hidden states needs to be reused for another next sample. However, for a use in autoregressive cooperative decoding, where input sequences are often the continuation of already discriminated ones unidirectional attention allows to reuse contextual encoding of previous tokens, hence greatly speeding up the process. \n\nHowever, even with unidirectional discriminators, evaluating every possible continuation of a given sequence is intractable since, for a vocabulary of size |V |, it requires |V | forward passes at each decoding step. |V | being in the order of ten thousand, discriminating every possible continuation of decoding sequences is too costly. Thus, cooperative approaches have to circumvent this issue by limiting the number of sequences actually evaluated by the discriminator. For example, DAS pre-filters potential continuations on the nucleus of the LM distribution [16]. This choice necessarily biases the resulting generated distribution.",
            "reference_string": "[248377166 | Chaffin et al. | 2022 | Citations: 4]"
        },
        {
            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 64,
            "citation_count": 16,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-long.355.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.01185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146677401",
                    "name": "Nihal Jain"
                },
                {
                    "authorId": "2358258",
                    "name": "Dejiao Zhang"
                },
                {
                    "authorId": "38123220",
                    "name": "Wasi Uddin Ahmad"
                },
                {
                    "authorId": "50219006",
                    "name": "Zijian Wang"
                },
                {
                    "authorId": "144647318",
                    "name": "Feng Nan"
                },
                {
                    "authorId": "2187045812",
                    "name": "Xiaopeng Li"
                },
                {
                    "authorId": "144745483",
                    "name": "Ming Tan"
                },
                {
                    "authorId": "1701451",
                    "name": "Ramesh Nallapati"
                },
                {
                    "authorId": "31631000",
                    "name": "Baishakhi Ray"
                },
                {
                    "authorId": "50339091",
                    "name": "Parminder Bhatia"
                },
                {
                    "authorId": "47646605",
                    "name": "Xiaofei Ma"
                },
                {
                    "authorId": "144028698",
                    "name": "Bing Xiang"
                }
            ],
            "abstract": "Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44% relative improvement on the Semantic Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark.",
            "corpus_id": 258461112,
            "sentences": [
                {
                    "corpus_id": "258461112",
                    "title": "ContraCLM: Contrastive Learning For Causal Language Model",
                    "text": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations. \n\nTaking the encoder-only models in Table 7a for illustration, on average, BERT-Base (Devlin et al., 2019) and Roberta-Base (Liu et al., 2019)   between CodeGen and the BERT models trained on programming languages, i.e., CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021), decreases or even diminishes when evaluated on the code search tasks, the performance gap is still significant as both the model size and pretraining data in CodeGen are much larger than those used by the encoder-only models in Table 7b. Similar trends were observed in the performance gap between the decoder-only and encoderdecoder models on both natural language (Lewis et al., 2020;Raffel et al., 2020) and programming language (Ahmad et al., 2021;Wang et al., 2021). The large performance gap severely limits the decoder-only models used in many discriminative tasks. To this end, contrastive learning shows the promise to largely bridge the gap. As seen in Table 7a, on STS, CONTRACLM reduces the relative performance gap from 67.24% (absolute 21.12%) to 16.17% (absolute 7.33%) regarding BERT-Base, and from 84.62% (absolute 26.64%) to 28.24% (absolute 12.8%). Similarly, Table 7b shows that CONTRACLM outperforms encoder-decoder models and performs comparably to the encoder-only model, GraphCodeBERT. Gao et al. (2021) showed that the dropout-based augmentation is an effective strategy for unsupervised contrastive learning, and the follow-up works (Chuang et al., 2022;Wu et al., 2022) endorse the effectiveness.",
                    "score": 0.549964930876218,
                    "section_title": "D.1 Bridge the Gap on Discriminative Tasks",
                    "char_start_offset": 38682,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 273
                        },
                        {
                            "start": 276,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1429
                        },
                        {
                            "start": 1430,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1785
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 359,
                            "end": 380,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 504,
                            "end": 523,
                            "matchedPaperCorpusId": "211171605"
                        },
                        {
                            "start": 542,
                            "end": 560,
                            "matchedPaperCorpusId": "221761146"
                        },
                        {
                            "start": 928,
                            "end": 948,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 994,
                            "end": 1014,
                            "matchedPaperCorpusId": "232185260"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55224609375
                }
            ],
            "relevance_judgement": 0.55224609375,
            "relevance_judgment_input_expanded": "# Title: ContraCLM: Contrastive Learning For Causal Language Model\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Nihal Jain, Dejiao Zhang, Wasi Uddin Ahmad, Zijian Wang, Feng Nan, Xiaopeng Li, Ming Tan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Xiaofei Ma, Bing Xiang\n## Abstract\nDespite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44% relative improvement on the Semantic Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark.\n## D.1 Bridge the Gap on Discriminative Tasks\nCompared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations. \n\nTaking the encoder-only models in Table 7a for illustration, on average, BERT-Base (Devlin et al., 2019) and Roberta-Base (Liu et al., 2019)   between CodeGen and the BERT models trained on programming languages, i.e., CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021), decreases or even diminishes when evaluated on the code search tasks, the performance gap is still significant as both the model size and pretraining data in CodeGen are much larger than those used by the encoder-only models in Table 7b. Similar trends were observed in the performance gap between the decoder-only and encoderdecoder models on both natural language (Lewis et al., 2020;Raffel et al., 2020) and programming language (Ahmad et al., 2021;Wang et al., 2021). The large performance gap severely limits the decoder-only models used in many discriminative tasks. To this end, contrastive learning shows the promise to largely bridge the gap. As seen in Table 7a, on STS, CONTRACLM reduces the relative performance gap from 67.24% (absolute 21.12%) to 16.17% (absolute 7.33%) regarding BERT-Base, and from 84.62% (absolute 26.64%) to 28.24% (absolute 12.8%). Similarly, Table 7b shows that CONTRACLM outperforms encoder-decoder models and performs comparably to the encoder-only model, GraphCodeBERT. Gao et al. (2021) showed that the dropout-based augmentation is an effective strategy for unsupervised contrastive learning, and the follow-up works (Chuang et al., 2022;Wu et al., 2022) endorse the effectiveness.",
            "reference_string": "[258461112 | Jain et al. | 2022 | Citations: 16]"
        },
        {
            "title": "Pseudo-Bidirectional Decoding for Local Sequence Transduction",
            "venue": "Findings",
            "year": 2020,
            "reference_count": 42,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.136.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.11694, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "150341221",
                    "name": "Wangchunshu Zhou"
                },
                {
                    "authorId": "50251691",
                    "name": "Tao Ge"
                },
                {
                    "authorId": "2087261268",
                    "name": "Ke Xu"
                }
            ],
            "abstract": "Local sequence transduction (LST) tasks are sequence transduction tasks where there exists massive overlapping between the source and target sequences, such as grammatical error correction and spell or OCR correction. Motivated by this characteristic of LST tasks, we propose Pseudo-Bidirectional Decoding (PBD), a simple but versatile approach for LST tasks. PBD copies the representation of source tokens to the decoder as pseudo future context that enables the decoder self-attention to attends to its bi-directional context. In addition, the bidirectional decoding scheme and the characteristic of LST tasks motivate us to share the encoder and the decoder of LST models. Our approach provides right-side context information for the decoder, reduces the number of parameters by half, and provides good regularization effects. Experimental results on several benchmark datasets show that our approach consistently improves the performance of standard seq2seq models on LST tasks.",
            "corpus_id": 211003689,
            "sentences": [
                {
                    "corpus_id": "211003689",
                    "title": "Pseudo-Bidirectional Decoding for Local Sequence Transduction",
                    "text": "which facilitates the decoder to generate \"went\" instead of \"goes\". While ideally, the encoder-decoder attention may be able to capture the information provided by the future token representation copied from the encoder, the pseudo-bidirectional decoding mechanism makes the decoder self-attention module more effective by allowing it to directly attend to future information, thus probably leads to better empirical performance. \n\nPseudo-bidirectional Attention Mask While the proposed approach is versatile and applicable to RNN-based seq2seq models, in this paper, we focus on the transformer model [Vaswani et al., 2017] which yields state-of-the-art performance in most natural language generation tasks. The pseudo-bidirectional decoding copies the representation of tokens from the N +1 th position to the end of the input sequence in the corresponding encoder layer to the decoder when predicting the N th output token. This is easy to implement for auto-regressive decoding during inference. However, a straightforward way to make the model behaves consistently during training and inference requires us to change the decoder input dynamically for each decoding step instead of feeding the entire output sequence into the decoder and use a causal attention mask to ensure the causality of the decoder. This would hinder the transformer model from being trained in parallel, thus makes the training much less efficient. \n\nTo address this problem, we propose a novel masking strategy referred as \"pseudo-bidirectional attention mask\". As il-lustrated in Figure 2, we concatenate the representation of the input sequences to that of the output sequences to form the Key and the Value in the decoder self-attention module, while the Query remains to be the representation of the output sequences. The pseudo-bidirectional attention mask makes the decoder self-attention bidirectional by allowing the query tokens to attend to pseudo future tokens copied from the encoder, thus retaining the causality of the decoder and enabling parallel training.",
                    "score": 0.611798588689347,
                    "section_title": "Pseudo Future Context Modeling",
                    "char_start_offset": 8425,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 429
                        },
                        {
                            "start": 432,
                            "end": 709
                        },
                        {
                            "start": 710,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1427
                        },
                        {
                            "start": 1430,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1801
                        },
                        {
                            "start": 1802,
                            "end": 2052
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54736328125
                }
            ],
            "relevance_judgement": 0.54736328125,
            "relevance_judgment_input_expanded": "# Title: Pseudo-Bidirectional Decoding for Local Sequence Transduction\n# Venue: Findings\n# Authors: Wangchunshu Zhou, Tao Ge, Ke Xu\n## Abstract\nLocal sequence transduction (LST) tasks are sequence transduction tasks where there exists massive overlapping between the source and target sequences, such as grammatical error correction and spell or OCR correction. Motivated by this characteristic of LST tasks, we propose Pseudo-Bidirectional Decoding (PBD), a simple but versatile approach for LST tasks. PBD copies the representation of source tokens to the decoder as pseudo future context that enables the decoder self-attention to attends to its bi-directional context. In addition, the bidirectional decoding scheme and the characteristic of LST tasks motivate us to share the encoder and the decoder of LST models. Our approach provides right-side context information for the decoder, reduces the number of parameters by half, and provides good regularization effects. Experimental results on several benchmark datasets show that our approach consistently improves the performance of standard seq2seq models on LST tasks.\n## Pseudo Future Context Modeling\nwhich facilitates the decoder to generate \"went\" instead of \"goes\". While ideally, the encoder-decoder attention may be able to capture the information provided by the future token representation copied from the encoder, the pseudo-bidirectional decoding mechanism makes the decoder self-attention module more effective by allowing it to directly attend to future information, thus probably leads to better empirical performance. \n\nPseudo-bidirectional Attention Mask While the proposed approach is versatile and applicable to RNN-based seq2seq models, in this paper, we focus on the transformer model [Vaswani et al., 2017] which yields state-of-the-art performance in most natural language generation tasks. The pseudo-bidirectional decoding copies the representation of tokens from the N +1 th position to the end of the input sequence in the corresponding encoder layer to the decoder when predicting the N th output token. This is easy to implement for auto-regressive decoding during inference. However, a straightforward way to make the model behaves consistently during training and inference requires us to change the decoder input dynamically for each decoding step instead of feeding the entire output sequence into the decoder and use a causal attention mask to ensure the causality of the decoder. This would hinder the transformer model from being trained in parallel, thus makes the training much less efficient. \n\nTo address this problem, we propose a novel masking strategy referred as \"pseudo-bidirectional attention mask\". As il-lustrated in Figure 2, we concatenate the representation of the input sequences to that of the output sequences to form the Key and the Value in the decoder self-attention module, while the Query remains to be the representation of the output sequences. The pseudo-bidirectional attention mask makes the decoder self-attention bidirectional by allowing the query tokens to attend to pseudo future tokens copied from the encoder, thus retaining the causality of the decoder and enabling parallel training.",
            "reference_string": "[211003689 | Zhou et al. | 2020 | Citations: 3]"
        },
        {
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "venue": "Empirical Software Engineering",
            "year": 2023,
            "reference_count": 226,
            "citation_count": 76,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.11396",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.11396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2148256392",
                    "name": "Zibin Zheng"
                },
                {
                    "authorId": "2115304",
                    "name": "Kai-Chun Ning"
                },
                {
                    "authorId": "2254800142",
                    "name": "Jiachi Chen"
                },
                {
                    "authorId": "2214155529",
                    "name": "Yanlin Wang"
                },
                {
                    "authorId": "2274095496",
                    "name": "Wenqing Chen"
                },
                {
                    "authorId": "2217902484",
                    "name": "Lianghong Guo"
                },
                {
                    "authorId": "2233023641",
                    "name": "Weicheng Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.",
            "corpus_id": 261064777,
            "sentences": [
                {
                    "corpus_id": "261064777",
                    "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
                    "text": "This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture. While GPT (Radford et al., 2018) and GPT-2 (Radford et al., 2018) did not exhibit the same level of performance as GPT-3 (Brown et al., 2020), with the increase in model size and the amount of data used for pretraining, GPT-3 (Brown et al., 2020) showcased a remarkable few-shot capability that earlier models did not possess. Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b). \n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a).",
                    "score": 0.613538563053057,
                    "section_title": "Model Architecture",
                    "char_start_offset": 12603,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1067
                        },
                        {
                            "start": 1070,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1396
                        },
                        {
                            "start": 1397,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1853
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 179,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 179,
                            "end": 198,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 403,
                            "end": 423,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 508,
                            "end": 528,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52880859375
                }
            ],
            "relevance_judgement": 0.52880859375,
            "relevance_judgment_input_expanded": "# Title: Towards an Understanding of Large Language Models in Software Engineering Tasks\n# Venue: Empirical Software Engineering\n# Authors: Zibin Zheng, Kai-Chun Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, Weicheng Wang\n## Abstract\nLarge Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.\n## Model Architecture\nThis constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture. While GPT (Radford et al., 2018) and GPT-2 (Radford et al., 2018) did not exhibit the same level of performance as GPT-3 (Brown et al., 2020), with the increase in model size and the amount of data used for pretraining, GPT-3 (Brown et al., 2020) showcased a remarkable few-shot capability that earlier models did not possess. Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b). \n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a).",
            "reference_string": "[261064777 | Zheng et al. | 2023 | Citations: 76]"
        },
        {
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 150,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.15347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108418112",
                    "name": "Seungcheol Park"
                },
                {
                    "authorId": "2281832669",
                    "name": "Jaehyeon Choi"
                },
                {
                    "authorId": "2281792832",
                    "name": "Sojin Lee"
                },
                {
                    "authorId": "2281746333",
                    "name": "U. Kang"
                }
            ],
            "abstract": "How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.",
            "corpus_id": 267312283,
            "sentences": [
                {
                    "corpus_id": "267312283",
                    "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
                    "text": "We elaborate on Transformer architecture which is dominantly used for PLMs [12,136,137,173] in natural language processing (NLP). Transformer [139] is a language model that refines token embeddings reflecting the contextual information through a self-attention mechanism [21,111,139] to utilize them in downstream tasks. Transformer is composed of multiple layers consisting of multi-head attention (MHA) and feed-forward network (FFN) sublayers. \n\nMHA sublayers capture the contextual inter-token information via a self-attention mechanism and FFN sublayers polish intra-token information with a position-wise feedforward network. Original Transformer architecture [139] comprises Encoder and Decoder where Encoder encodes input token embeddings considering contextual information and Decoder generates output tokens by decoding the encoding results. However, we use encoder-only and decoder-only Transformers according to downstream tasks based on the previous works demonstrating the impressive performance of encoder-only [25,36,83,98,121] and decoder-only [12,124,136,173] PLMs. We illustrate encoder-only and decoder-only Transformer architectures in Fig. 1. Encoder-only Transformers (a) [25,36,83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,37], natural language inference [140,148], and question answering [117,118]. On the other hand, decoder-only Transformers (b) [12,124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration.",
                    "score": 0.6574776817089234,
                    "section_title": "Transformer Architecture",
                    "char_start_offset": 4639,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 446
                        },
                        {
                            "start": 449,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1164
                        },
                        {
                            "start": 1165,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1725
                        },
                        {
                            "start": 1726,
                            "end": 1850
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 75,
                            "end": 79,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 142,
                            "end": 147,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 271,
                            "end": 275,
                            "matchedPaperCorpusId": "6506243"
                        },
                        {
                            "start": 275,
                            "end": 279,
                            "matchedPaperCorpusId": "8495258"
                        },
                        {
                            "start": 279,
                            "end": 283,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 666,
                            "end": 671,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1030,
                            "end": 1033,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1061,
                            "end": 1065,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1199,
                            "end": 1202,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1453,
                            "end": 1456,
                            "matchedPaperCorpusId": "16639476"
                        },
                        {
                            "start": 1490,
                            "end": 1494,
                            "matchedPaperCorpusId": "3432876"
                        },
                        {
                            "start": 1519,
                            "end": 1524,
                            "matchedPaperCorpusId": "47018994"
                        },
                        {
                            "start": 1524,
                            "end": 1528,
                            "matchedPaperCorpusId": "11816014"
                        },
                        {
                            "start": 1579,
                            "end": 1583,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52587890625
                }
            ],
            "relevance_judgement": 0.52587890625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Compression Algorithms for Language Models\n# Venue: arXiv.org\n# Authors: Seungcheol Park, Jaehyeon Choi, Sojin Lee, U. Kang\n## Abstract\nHow can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.\n## Transformer Architecture\nWe elaborate on Transformer architecture which is dominantly used for PLMs [12,136,137,173] in natural language processing (NLP). Transformer [139] is a language model that refines token embeddings reflecting the contextual information through a self-attention mechanism [21,111,139] to utilize them in downstream tasks. Transformer is composed of multiple layers consisting of multi-head attention (MHA) and feed-forward network (FFN) sublayers. \n\nMHA sublayers capture the contextual inter-token information via a self-attention mechanism and FFN sublayers polish intra-token information with a position-wise feedforward network. Original Transformer architecture [139] comprises Encoder and Decoder where Encoder encodes input token embeddings considering contextual information and Decoder generates output tokens by decoding the encoding results. However, we use encoder-only and decoder-only Transformers according to downstream tasks based on the previous works demonstrating the impressive performance of encoder-only [25,36,83,98,121] and decoder-only [12,124,136,173] PLMs. We illustrate encoder-only and decoder-only Transformer architectures in Fig. 1. Encoder-only Transformers (a) [25,36,83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,37], natural language inference [140,148], and question answering [117,118]. On the other hand, decoder-only Transformers (b) [12,124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration.",
            "reference_string": "[267312283 | Park et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation",
            "venue": "Psychometrika",
            "year": 2021,
            "reference_count": 72,
            "citation_count": 21,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s11336-021-09823-9.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9166894, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2136597350",
                    "name": "Bj\u00f6rn E. Hommel"
                },
                {
                    "authorId": "2138754315",
                    "name": "Franz-Josef M. Wollang"
                },
                {
                    "authorId": "2137277509",
                    "name": "Veronika Kotova"
                },
                {
                    "authorId": "50999175",
                    "name": "H. Zacher"
                },
                {
                    "authorId": "5278036",
                    "name": "S. Schmukle"
                }
            ],
            "abstract": "Algorithmic automatic item generation can be used to obtain large quantities of cognitive items in the domains of knowledge and aptitude testing. However, conventional item models used by template-based automatic item generation techniques are not ideal for the creation of items for non-cognitive constructs. Progress in this area has been made recently by employing long short-term memory recurrent neural networks to produce word sequences that syntactically resemble items typically found in personality questionnaires. To date, such items have been produced unconditionally, without the possibility of selectively targeting personality domains. In this article, we offer a brief synopsis on past developments in natural language processing and explain why the automatic generation of construct-specific items has become attainable only due to recent technological progress. We propose that pre-trained causal transformer models can be fine-tuned to achieve this task using implicit parameterization in conjunction with conditional generation. We demonstrate this method in a tutorial-like fashion and finally compare aspects of validity in human- and machine-authored items using empirical data. Our study finds that approximately two-thirds of the automatically generated items show good psychometric properties (factor loadings above .40) and that one-third even have properties equivalent to established and highly curated human-authored items. Our work thus demonstrates the practical use of deep neural networks for non-cognitive automatic item generation.",
            "corpus_id": 242033930,
            "sentences": [
                {
                    "corpus_id": "242033930",
                    "title": "Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation",
                    "text": "One of the most recent and arguably substantial paradigm shifts since the initial advance of distributional semantics was sparked by the introduction of the transformer model by Vaswani et al. (2017). Its model architecture holds numerous advantages when applied to sequential data such as natural language. First, sequential data can be processed in parallel by transformer models, reducing the resources required to train such a model. Sequential information (i.e., the order of words) is preserved by a process termed positional encoding, which engrains each word in a sentence with its intended sequential position. As a consequence, larger and more competent language models can be trained. Second, and of central importance to the design, transformer models learn through a mechanism referred to as self-attention. In essence, self-attention refers to the concept of determining the relevance of a word in relation to the relevance of other words in the input sequence. We provide more details on how attention is computed in the next section of this article. In particular, these two features allow the transformer model to learn long-range dependencies better than LSTMs. \n\nSince the publication of Vaswani et al.'s (2017) paper, a plethora of transformer implementations have been released with various modifications. One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados & Pilehvar, 2018;Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation.",
                    "score": 0.5289319305733715,
                    "section_title": "Transformer Models and the Attention Mechanism",
                    "char_start_offset": 18001,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1546
                        },
                        {
                            "start": 1547,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 1737
                        },
                        {
                            "start": 1738,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 2057
                        },
                        {
                            "start": 2058,
                            "end": 2239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 178,
                            "end": 199,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1715,
                            "end": 1736,
                            "matchedPaperCorpusId": "264847728"
                        },
                        {
                            "start": 1835,
                            "end": 1870,
                            "matchedPaperCorpusId": "13696533"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52392578125
                }
            ],
            "relevance_judgement": 0.52392578125,
            "relevance_judgment_input_expanded": "# Title: Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation\n# Venue: Psychometrika\n# Authors: Bj\u00f6rn E. Hommel, Franz-Josef M. Wollang, Veronika Kotova, H. Zacher, S. Schmukle\n## Abstract\nAlgorithmic automatic item generation can be used to obtain large quantities of cognitive items in the domains of knowledge and aptitude testing. However, conventional item models used by template-based automatic item generation techniques are not ideal for the creation of items for non-cognitive constructs. Progress in this area has been made recently by employing long short-term memory recurrent neural networks to produce word sequences that syntactically resemble items typically found in personality questionnaires. To date, such items have been produced unconditionally, without the possibility of selectively targeting personality domains. In this article, we offer a brief synopsis on past developments in natural language processing and explain why the automatic generation of construct-specific items has become attainable only due to recent technological progress. We propose that pre-trained causal transformer models can be fine-tuned to achieve this task using implicit parameterization in conjunction with conditional generation. We demonstrate this method in a tutorial-like fashion and finally compare aspects of validity in human- and machine-authored items using empirical data. Our study finds that approximately two-thirds of the automatically generated items show good psychometric properties (factor loadings above .40) and that one-third even have properties equivalent to established and highly curated human-authored items. Our work thus demonstrates the practical use of deep neural networks for non-cognitive automatic item generation.\n## Transformer Models and the Attention Mechanism\nOne of the most recent and arguably substantial paradigm shifts since the initial advance of distributional semantics was sparked by the introduction of the transformer model by Vaswani et al. (2017). Its model architecture holds numerous advantages when applied to sequential data such as natural language. First, sequential data can be processed in parallel by transformer models, reducing the resources required to train such a model. Sequential information (i.e., the order of words) is preserved by a process termed positional encoding, which engrains each word in a sentence with its intended sequential position. As a consequence, larger and more competent language models can be trained. Second, and of central importance to the design, transformer models learn through a mechanism referred to as self-attention. In essence, self-attention refers to the concept of determining the relevance of a word in relation to the relevance of other words in the input sequence. We provide more details on how attention is computed in the next section of this article. In particular, these two features allow the transformer model to learn long-range dependencies better than LSTMs. \n\nSince the publication of Vaswani et al.'s (2017) paper, a plethora of transformer implementations have been released with various modifications. One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados & Pilehvar, 2018;Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation.",
            "reference_string": "[242033930 | Hommel et al. | 2021 | Citations: 21]"
        },
        {
            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 113,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308036602",
                    "name": "Andrea Posada"
                },
                {
                    "authorId": "2091163163",
                    "name": "D. Rueckert"
                },
                {
                    "authorId": "2126502908",
                    "name": "Felix Meissen"
                },
                {
                    "authorId": "1399796227",
                    "name": "Philip M\u00fcller"
                }
            ],
            "abstract": "Since the Transformer architecture emerged, language model development has grown, driven by their promising potential. Releasing these models into production requires properly understanding their behavior, particularly in sensitive domains like medicine. Despite this need, the medical literature still lacks practical assessment of pre-trained language models, which are especially valuable in settings where only consumer-grade computational resources are available. To address this gap, we have conducted a comprehensive survey of language models in the medical field and evaluated a subset of these for medical text classification and conditional text generation. The subset includes 53 models with 110 million to 13 billion parameters, spanning the Transformer-based model families and knowledge domains. Different approaches are employed for text classification, including zero-shot learning, enabling tuning without the need to train the model. These approaches are helpful in our target settings, where many users of language models find themselves. The results reveal remarkable performance across the tasks and datasets evaluated, underscoring the potential of certain models to contain medical knowledge, even without domain specialization. This study thus advocates for further exploration of model applications in medical contexts, particularly in computational resource-constrained settings, to benefit a wide range of users. The code is available on https://github.com/anpoc/Language-models-in-medicine.",
            "corpus_id": 270703043,
            "sentences": [
                {
                    "corpus_id": "270703043",
                    "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
                    "text": "The emergence of pre-trained language models represented a paradigm shift, driving research toward designing more efficient architectures and refining pre-training strategies. These pre-trained models have been commonly adapted or specialized to downstream tasks via fine-tuning, which involves transferring knowledge by further training a model on new data. There are significant advantages demonstrated by these models in language understanding and model performance in various tasks [9,13]. \n\nELMo is one of the earliest attempts at pre-trained language models [15]. This model was developed to capture context-aware word representations by pre-training a bidirectional Long Short-Term Memory (biLSTM) network and fine-tuning it for subsequent downstream tasks. Later the Transformer architecture was introduced, revolutionizing the NLP field by offering highly parallelizable structures and self-attention mechanisms. The Transformer [6] follows the autoencoder archetype, from which three families of models arose: (1) BERT-family or encoder-only models, (2) GPT-family or decoder-only models, and (3) text-to-text or encoder-decoder models. In Fig. 1, the graphical representations of these  Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively. families are shown.",
                    "score": 0.6491592979539362,
                    "section_title": "Pre-trained language models",
                    "char_start_offset": 5957,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 493
                        },
                        {
                            "start": 496,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 764
                        },
                        {
                            "start": 765,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1642
                        },
                        {
                            "start": 1643,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1772
                        },
                        {
                            "start": 1773,
                            "end": 1870
                        },
                        {
                            "start": 1871,
                            "end": 1926
                        },
                        {
                            "start": 1927,
                            "end": 2097
                        },
                        {
                            "start": 2098,
                            "end": 2208
                        },
                        {
                            "start": 2209,
                            "end": 2228
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 564,
                            "end": 568,
                            "matchedPaperCorpusId": "3626819"
                        },
                        {
                            "start": 938,
                            "end": 941,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5234375
                }
            ],
            "relevance_judgement": 0.5234375,
            "relevance_judgment_input_expanded": "# Title: Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings\n# Venue: arXiv.org\n# Authors: Andrea Posada, D. Rueckert, Felix Meissen, Philip M\u00fcller\n## Abstract\nSince the Transformer architecture emerged, language model development has grown, driven by their promising potential. Releasing these models into production requires properly understanding their behavior, particularly in sensitive domains like medicine. Despite this need, the medical literature still lacks practical assessment of pre-trained language models, which are especially valuable in settings where only consumer-grade computational resources are available. To address this gap, we have conducted a comprehensive survey of language models in the medical field and evaluated a subset of these for medical text classification and conditional text generation. The subset includes 53 models with 110 million to 13 billion parameters, spanning the Transformer-based model families and knowledge domains. Different approaches are employed for text classification, including zero-shot learning, enabling tuning without the need to train the model. These approaches are helpful in our target settings, where many users of language models find themselves. The results reveal remarkable performance across the tasks and datasets evaluated, underscoring the potential of certain models to contain medical knowledge, even without domain specialization. This study thus advocates for further exploration of model applications in medical contexts, particularly in computational resource-constrained settings, to benefit a wide range of users. The code is available on https://github.com/anpoc/Language-models-in-medicine.\n## Pre-trained language models\nThe emergence of pre-trained language models represented a paradigm shift, driving research toward designing more efficient architectures and refining pre-training strategies. These pre-trained models have been commonly adapted or specialized to downstream tasks via fine-tuning, which involves transferring knowledge by further training a model on new data. There are significant advantages demonstrated by these models in language understanding and model performance in various tasks [9,13]. \n\nELMo is one of the earliest attempts at pre-trained language models [15]. This model was developed to capture context-aware word representations by pre-training a bidirectional Long Short-Term Memory (biLSTM) network and fine-tuning it for subsequent downstream tasks. Later the Transformer architecture was introduced, revolutionizing the NLP field by offering highly parallelizable structures and self-attention mechanisms. The Transformer [6] follows the autoencoder archetype, from which three families of models arose: (1) BERT-family or encoder-only models, (2) GPT-family or decoder-only models, and (3) text-to-text or encoder-decoder models. In Fig. 1, the graphical representations of these  Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively. families are shown.",
            "reference_string": "[270703043 | Posada et al. | 2024 | Citations: 0]"
        },
        {
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 169,
            "influential_citation_count": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.02309",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2043490",
                    "name": "Erik Nijkamp"
                },
                {
                    "authorId": "50376014",
                    "name": "Hiroaki Hayashi"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                },
                {
                    "authorId": "2118860628",
                    "name": "Yingbo Zhou"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly. In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a\"free lunch\"hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored. We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.",
            "corpus_id": 258461229,
            "sentences": [
                {
                    "corpus_id": "258461229",
                    "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
                    "text": "Model Architecture In representation learning with transformers (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020), two schemes of modeling are prevalent which differ in their attention masks for the contextualization of hidden vectors. For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2020). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks. \n\nLearning Algorithm The choice of encoder or decoder-based model architectures typically guides the selection of learning algorithms for language modeling. Encoder-based models may be trained with the task of masked language modeling in the form of denoising span corruptions (Devlin et al., 2019;Raffel et al., 2020).",
                    "score": 0.5815841171406922,
                    "section_title": "COMPONENTS: ARCHITECTURE, OBJECTIVE, SAMPLING, DATA",
                    "char_start_offset": 9494,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1017
                        },
                        {
                            "start": 1018,
                            "end": 1098
                        },
                        {
                            "start": 1099,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1294
                        },
                        {
                            "start": 1295,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1543
                        },
                        {
                            "start": 1544,
                            "end": 1702
                        },
                        {
                            "start": 1705,
                            "end": 1859
                        },
                        {
                            "start": 1860,
                            "end": 2022
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 64,
                            "end": 85,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 85,
                            "end": 104,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 104,
                            "end": 124,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 995,
                            "end": 1016,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5146484375
                }
            ],
            "relevance_judgement": 0.5146484375,
            "relevance_judgment_input_expanded": "# Title: CodeGen2: Lessons for Training LLMs on Programming and Natural Languages\n# Venue: arXiv.org\n# Authors: Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, S. Savarese, Yingbo Zhou\n## Abstract\nLarge language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly. In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a\"free lunch\"hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored. We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.\n## COMPONENTS: ARCHITECTURE, OBJECTIVE, SAMPLING, DATA\nModel Architecture In representation learning with transformers (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020), two schemes of modeling are prevalent which differ in their attention masks for the contextualization of hidden vectors. For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2020). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks. \n\nLearning Algorithm The choice of encoder or decoder-based model architectures typically guides the selection of learning algorithms for language modeling. Encoder-based models may be trained with the task of masked language modeling in the form of denoising span corruptions (Devlin et al., 2019;Raffel et al., 2020).",
            "reference_string": "[258461229 | Nijkamp et al. | 2023 | Citations: 169]"
        },
        {
            "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 241,
            "influential_citation_count": 39,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.05961, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2101317786",
                    "name": "Parishad BehnamGhader"
                },
                {
                    "authorId": "1666183192",
                    "name": "Vaibhav Adlakha"
                },
                {
                    "authorId": "2269460274",
                    "name": "Marius Mosbach"
                },
                {
                    "authorId": "3335364",
                    "name": "Dzmitry Bahdanau"
                },
                {
                    "authorId": "2748188",
                    "name": "Nicolas Chapados"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                }
            ],
            "abstract": "Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.",
            "corpus_id": 269009682,
            "sentences": [
                {
                    "corpus_id": "269009682",
                    "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
                    "text": "Text embedding models aim to encode the semantic content of natural language text in vector representations which then facilitate various natural language processing (NLP) tasks, such as semantic textual similarity, information retrieval, and clustering. For many years, the dominating paradigm for building such models relied on pre-trained bidirectional encoders or encoder-decoders such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), which are typically adapted for text embedding tasks by following a multi-step training pipeline consisting of weakly-and fully-supervised contrastive training (Ni et al., 2022;Li et al., 2023a;Xiao et al., 2023, inter alia). Only recently, the community started to adopt decoder-only LLMs for embedding text (Muennighoff, 2022;Ma et al., 2023;Wang et al., 2023;Springer et al., 2024;Li & Li, 2024). \n\nWe speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position i is influenced solely by the representations of preceding tokens at positions 0, 1, . . . , i \u2212 1. Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence. \n\nOvercoming this architectural limitation of decoder-only LLMs for text embedding tasks is highly appealing as these models come with several advantages compared to their encoder-",
                    "score": 0.5541727512104795,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 849
                        },
                        {
                            "start": 852,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1491
                        },
                        {
                            "start": 1494,
                            "end": 1672
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 398,
                            "end": 419,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 427,
                            "end": 448,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 610,
                            "end": 627,
                            "matchedPaperCorpusId": "245144556"
                        },
                        {
                            "start": 778,
                            "end": 794,
                            "matchedPaperCorpusId": "16299141"
                        },
                        {
                            "start": 834,
                            "end": 848,
                            "matchedPaperCorpusId": "265066823"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51220703125
                }
            ],
            "relevance_judgement": 0.51220703125,
            "relevance_judgment_input_expanded": "# Title: LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders\n# Venue: arXiv.org\n# Authors: Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, Siva Reddy\n## Abstract\nLarge decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.\n## Introduction\nText embedding models aim to encode the semantic content of natural language text in vector representations which then facilitate various natural language processing (NLP) tasks, such as semantic textual similarity, information retrieval, and clustering. For many years, the dominating paradigm for building such models relied on pre-trained bidirectional encoders or encoder-decoders such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), which are typically adapted for text embedding tasks by following a multi-step training pipeline consisting of weakly-and fully-supervised contrastive training (Ni et al., 2022;Li et al., 2023a;Xiao et al., 2023, inter alia). Only recently, the community started to adopt decoder-only LLMs for embedding text (Muennighoff, 2022;Ma et al., 2023;Wang et al., 2023;Springer et al., 2024;Li & Li, 2024). \n\nWe speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position i is influenced solely by the representations of preceding tokens at positions 0, 1, . . . , i \u2212 1. Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence. \n\nOvercoming this architectural limitation of decoder-only LLMs for text embedding tasks is highly appealing as these models come with several advantages compared to their encoder-",
            "reference_string": "[269009682 | BehnamGhader et al. | 2024 | Citations: 241]"
        },
        {
            "title": "The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives",
            "venue": "J. Medical Syst.",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 83,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10916-024-02045-3.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10873461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265474928",
                    "name": "M. Cascella"
                },
                {
                    "authorId": "2265474397",
                    "name": "Federico Semeraro"
                },
                {
                    "authorId": "4239573",
                    "name": "J. Montomoli"
                },
                {
                    "authorId": "8663537",
                    "name": "Valentina Bellini"
                },
                {
                    "authorId": "2239310513",
                    "name": "Ornella Piazza"
                },
                {
                    "authorId": "2198023579",
                    "name": "E. Bignami"
                }
            ],
            "abstract": "Within the domain of Natural Language Processing (NLP), Large Language Models (LLMs) represent sophisticated models engineered to comprehend, generate, and manipulate text resembling human language on an extensive scale. They are transformer-based deep learning architectures, obtained through the scaling of model size, pretraining of corpora, and computational resources. The potential healthcare applications of these models primarily involve chatbots and interaction systems for clinical documentation management, and medical literature summarization (Biomedical NLP). The challenge in this field lies in the research for applications in diagnostic and clinical decision support, as well as patient triage. Therefore, LLMs can be used for multiple tasks within patient care, research, and education. Throughout 2023, there has been an escalation in the release of LLMs, some of which are applicable in the healthcare domain. This remarkable output is largely the effect of the customization of pre-trained models for applications like chatbots, virtual assistants, or any system requiring human-like conversational engagement. As healthcare professionals, we recognize the imperative to stay at the forefront of knowledge. However, keeping abreast of the rapid evolution of this technology is practically unattainable, and, above all, understanding its potential applications and limitations remains a subject of ongoing debate. Consequently, this article aims to provide a succinct overview of the recently released LLMs, emphasizing their potential use in the field of medicine. Perspectives for a more extensive range of safe and effective applications are also discussed. The upcoming evolutionary leap involves the transition from an AI-powered model primarily designed for answering medical questions to a more versatile and practical tool for healthcare providers such as generalist biomedical AI systems for multimodal-based calibrated decision-making processes. On the other hand, the development of more accurate virtual clinical partners could enhance patient engagement, offering personalized support, and improving chronic disease management.",
            "corpus_id": 267701011,
            "sentences": [
                {
                    "corpus_id": "267701011",
                    "title": "The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives",
                    "text": "The innovative transformer architecture has paved the way for the development of various LLMs, each distinguished by its unique characteristics [3]. Recent advancements in language modeling have led to the emergence of three predominant categories, classified based on the fundamental modules employed in their construction. Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial. The application of diverse processes and datasets allows for the provision of a spectrum of tools tailored to meet the evolving demands of natural language understanding and generation [5]. \n\nBefore the rise of LLMs, traditional deep-learning models grappled with numerous technical challenges, including inadequate sequence and semantic understanding, along with computational complexity. This complexity necessitated a substantial number of parameters, as evident in convolutional neural networks, to achieve satisfactory outcomes. Additionally, issues such as vanishing gradients (e.g., in recurrent neural networks) posed challenges in capturing long-term dependencies, while sequential computation impeded the efficiency of training and inference processes, particularly for extended sequences [3]. The true innovation stemmed from optimizing pre-trained language 1 3 models to suit the specific demands of chat-oriented tasks, thereby achieving enhanced performance in applications that involve dialog-finetuned versions for conversational interactions. This complex process provides the use of different approaches.",
                    "score": 0.6295920984591005,
                    "section_title": "Development of LLMs for Chatbots and Enhanced human-like Interaction",
                    "char_start_offset": 4223,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 324
                        },
                        {
                            "start": 325,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1513
                        },
                        {
                            "start": 1516,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1857
                        },
                        {
                            "start": 1858,
                            "end": 2127
                        },
                        {
                            "start": 2128,
                            "end": 2383
                        },
                        {
                            "start": 2384,
                            "end": 2446
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51123046875
                }
            ],
            "relevance_judgement": 0.51123046875,
            "relevance_judgment_input_expanded": "# Title: The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives\n# Venue: J. Medical Syst.\n# Authors: M. Cascella, Federico Semeraro, J. Montomoli, Valentina Bellini, Ornella Piazza, E. Bignami\n## Abstract\nWithin the domain of Natural Language Processing (NLP), Large Language Models (LLMs) represent sophisticated models engineered to comprehend, generate, and manipulate text resembling human language on an extensive scale. They are transformer-based deep learning architectures, obtained through the scaling of model size, pretraining of corpora, and computational resources. The potential healthcare applications of these models primarily involve chatbots and interaction systems for clinical documentation management, and medical literature summarization (Biomedical NLP). The challenge in this field lies in the research for applications in diagnostic and clinical decision support, as well as patient triage. Therefore, LLMs can be used for multiple tasks within patient care, research, and education. Throughout 2023, there has been an escalation in the release of LLMs, some of which are applicable in the healthcare domain. This remarkable output is largely the effect of the customization of pre-trained models for applications like chatbots, virtual assistants, or any system requiring human-like conversational engagement. As healthcare professionals, we recognize the imperative to stay at the forefront of knowledge. However, keeping abreast of the rapid evolution of this technology is practically unattainable, and, above all, understanding its potential applications and limitations remains a subject of ongoing debate. Consequently, this article aims to provide a succinct overview of the recently released LLMs, emphasizing their potential use in the field of medicine. Perspectives for a more extensive range of safe and effective applications are also discussed. The upcoming evolutionary leap involves the transition from an AI-powered model primarily designed for answering medical questions to a more versatile and practical tool for healthcare providers such as generalist biomedical AI systems for multimodal-based calibrated decision-making processes. On the other hand, the development of more accurate virtual clinical partners could enhance patient engagement, offering personalized support, and improving chronic disease management.\n## Development of LLMs for Chatbots and Enhanced human-like Interaction\nThe innovative transformer architecture has paved the way for the development of various LLMs, each distinguished by its unique characteristics [3]. Recent advancements in language modeling have led to the emergence of three predominant categories, classified based on the fundamental modules employed in their construction. Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial. The application of diverse processes and datasets allows for the provision of a spectrum of tools tailored to meet the evolving demands of natural language understanding and generation [5]. \n\nBefore the rise of LLMs, traditional deep-learning models grappled with numerous technical challenges, including inadequate sequence and semantic understanding, along with computational complexity. This complexity necessitated a substantial number of parameters, as evident in convolutional neural networks, to achieve satisfactory outcomes. Additionally, issues such as vanishing gradients (e.g., in recurrent neural networks) posed challenges in capturing long-term dependencies, while sequential computation impeded the efficiency of training and inference processes, particularly for extended sequences [3]. The true innovation stemmed from optimizing pre-trained language 1 3 models to suit the specific demands of chat-oriented tasks, thereby achieving enhanced performance in applications that involve dialog-finetuned versions for conversational interactions. This complex process provides the use of different approaches.",
            "reference_string": "[267701011 | Cascella et al. | 2024 | Citations: 83]"
        },
        {
            "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 179,
            "citation_count": 18,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2171652249",
                    "name": "Qizhi Pei"
                },
                {
                    "authorId": "47767791",
                    "name": "Lijun Wu"
                },
                {
                    "authorId": "1944690382",
                    "name": "Kaiyuan Gao"
                },
                {
                    "authorId": "151068900",
                    "name": "Jinhua Zhu"
                },
                {
                    "authorId": "2290062348",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2290024261",
                    "name": "Zun Wang"
                },
                {
                    "authorId": "2267250090",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "2257028545",
                    "name": "Rui Yan"
                }
            ],
            "abstract": "The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this review, we provide an extensive analysis of recent advancements achieved through cross modeling of biomolecules and natural language. (1) We begin by outlining the technical representations of biomolecules employed, including sequences, 2D graphs, and 3D structures. (2) We then examine in depth the rationale and key objectives underlying effective multi-modal integration of language and molecular data sources. (3) We subsequently survey the practical applications enabled to date in this developing research area. (4) We also compile and summarize the available resources and datasets to facilitate future work. (5) Looking ahead, we identify several promising research directions worthy of further exploration and investment to continue advancing the field. The related resources and contents are updating in \\url{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling}.",
            "corpus_id": 268247581,
            "sentences": [
                {
                    "corpus_id": "268247581",
                    "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
                    "text": "The standard Transformer [115] and its variants [139], [140] adopt the encoder-decoder framework (Figure 6d),where the encoder processes and contextualize the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention. Models such as BART [139] and T5 [140] are with this architecture and demonstrating its effectiveness across a broad spectrum of applications. In biomolecule and text cross modeling scenario, this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models. This enriched interaction allows for a deeper understanding of the input sequences. Subsequently, the encoder-decoder attention mechanism empowers the decoder to generate outputs tailored for specific biological tasks. For example, MolT5 [21] shows strong retrieval and generation abilities between molecule and text on several downstream tasks. Similarly, the advanced BioT5 [22] and BioT5+ [107] also take the T5 framework and achieve much superior performances on both biomolecule understanding and generation tasks.",
                    "score": 0.5981869886752406,
                    "section_title": "Encoder-decoder",
                    "char_start_offset": 33192,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 761
                        },
                        {
                            "start": 762,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1197
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 30,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 48,
                            "end": 53,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 55,
                            "end": 60,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 304,
                            "end": 309,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 317,
                            "end": 322,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 916,
                            "end": 920,
                            "matchedPaperCorpusId": "248376906"
                        },
                        {
                            "start": 1054,
                            "end": 1058,
                            "matchedPaperCorpusId": "263834780"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5068359375
                }
            ],
            "relevance_judgement": 0.5068359375,
            "relevance_judgment_input_expanded": "# Title: Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey\n# Venue: arXiv.org\n# Authors: Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, Rui Yan\n## Abstract\nThe integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this review, we provide an extensive analysis of recent advancements achieved through cross modeling of biomolecules and natural language. (1) We begin by outlining the technical representations of biomolecules employed, including sequences, 2D graphs, and 3D structures. (2) We then examine in depth the rationale and key objectives underlying effective multi-modal integration of language and molecular data sources. (3) We subsequently survey the practical applications enabled to date in this developing research area. (4) We also compile and summarize the available resources and datasets to facilitate future work. (5) Looking ahead, we identify several promising research directions worthy of further exploration and investment to continue advancing the field. The related resources and contents are updating in \\url{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling}.\n## Encoder-decoder\nThe standard Transformer [115] and its variants [139], [140] adopt the encoder-decoder framework (Figure 6d),where the encoder processes and contextualize the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention. Models such as BART [139] and T5 [140] are with this architecture and demonstrating its effectiveness across a broad spectrum of applications. In biomolecule and text cross modeling scenario, this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models. This enriched interaction allows for a deeper understanding of the input sequences. Subsequently, the encoder-decoder attention mechanism empowers the decoder to generate outputs tailored for specific biological tasks. For example, MolT5 [21] shows strong retrieval and generation abilities between molecule and text on several downstream tasks. Similarly, the advanced BioT5 [22] and BioT5+ [107] also take the T5 framework and achieve much superior performances on both biomolecule understanding and generation tasks.",
            "reference_string": "[268247581 | Pei et al. | 2024 | Citations: 18]"
        },
        {
            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00539, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314473248",
                    "name": "Mingcong Lu"
                },
                {
                    "authorId": "2314649002",
                    "name": "Jiangcai Zhu"
                },
                {
                    "authorId": "2314113733",
                    "name": "Wang Hao"
                },
                {
                    "authorId": "2314323587",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2314311430",
                    "name": "Shusheng Zhang"
                },
                {
                    "authorId": "2314110211",
                    "name": "Kailai Shao"
                },
                {
                    "authorId": "2314192630",
                    "name": "Chao Chen"
                },
                {
                    "authorId": "2314343132",
                    "name": "Nan Li"
                },
                {
                    "authorId": "2324104105",
                    "name": "Feng Wang"
                },
                {
                    "authorId": "2324103820",
                    "name": "Xin Lu"
                }
            ],
            "abstract": "Multi-turn dialogues are a key interaction method between humans and Large Language Models (LLMs), as conversations extend over multiple rounds, keeping LLMs' high generation quality and low latency is a challenge. Mainstream LLMs can be grouped into two categories based on masking strategy: causal LLM and prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform causal ones in scenarios that heavily depend on historical context such as multi-turn dialogues or in-context learning, thanks to their bidirectional attention on prefix sequences. However, prefix LLMs have an inherent inefficient training problem in multi-turn dialogue datasets. In addition, the attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to reduce generation latency. In this paper, we propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to address these problems. Specifically, we apply alternate bidirectional and unidirectional attention on queries and answers in the dialogue history. In this way, ISM is able to maintain the high quality of prefix LLM and low generation latency of causal LLM, simultaneously. Extensive experiments illustrate that our ISM achieves significant performance.",
            "corpus_id": 271600495,
            "sentences": [
                {
                    "corpus_id": "271600495",
                    "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                    "text": "Recent developments in Large Language Models (LLMs) such as GPT-3.5 Ouyang et al. [2022] and GPT-4 Achiam et al. [2023] have attracted significant attention. Due to the powerful generation capability, LLMs have made remarkable achievements in different kinds of Natural Language Process (NLP) tasks through a unified generative paradigm. Specifically, the most natural and common way to interact with LLMs is through multi-turn dialogues. However, as the number of dialogue rounds increases, ensuring high quality and low latency of the generated answer by LLMs is a challenge. \n\nExisting language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], Raffel et al. [2020], Lewis et al. [2020], Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], Dong et al. [2019], and Decoder-Only Brown et al. [2020], Touvron et al. [2023a,b], Du et al. [2022]. Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders Brown et al. [2020], Touvron et al. [2023a] and prefix decoders Du et al. [2022]. The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention. Our ISM utilizes alternate bidirectional and unidirectional attention on queries and answers in prefix sequences. All of them generate answer in auto-regressive.",
                    "score": 0.7388980292593058,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 577
                        },
                        {
                            "start": 580,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1701
                        },
                        {
                            "start": 1702,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 718,
                            "end": 738,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 740,
                            "end": 759,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 822,
                            "end": 840,
                            "matchedPaperCorpusId": "147704286"
                        },
                        {
                            "start": 859,
                            "end": 878,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 906,
                            "end": 922,
                            "matchedPaperCorpusId": "247519241"
                        },
                        {
                            "start": 1183,
                            "end": 1202,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1247,
                            "end": 1263,
                            "matchedPaperCorpusId": "247519241"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.505859375
                }
            ],
            "relevance_judgement": 0.505859375,
            "relevance_judgment_input_expanded": "# Title: Intermittent Semi-working Mask: A New Masking Paradigm for LLMs\n# Venue: arXiv.org\n# Authors: Mingcong Lu, Jiangcai Zhu, Wang Hao, Zheng Li, Shusheng Zhang, Kailai Shao, Chao Chen, Nan Li, Feng Wang, Xin Lu\n## Abstract\nMulti-turn dialogues are a key interaction method between humans and Large Language Models (LLMs), as conversations extend over multiple rounds, keeping LLMs' high generation quality and low latency is a challenge. Mainstream LLMs can be grouped into two categories based on masking strategy: causal LLM and prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform causal ones in scenarios that heavily depend on historical context such as multi-turn dialogues or in-context learning, thanks to their bidirectional attention on prefix sequences. However, prefix LLMs have an inherent inefficient training problem in multi-turn dialogue datasets. In addition, the attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to reduce generation latency. In this paper, we propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to address these problems. Specifically, we apply alternate bidirectional and unidirectional attention on queries and answers in the dialogue history. In this way, ISM is able to maintain the high quality of prefix LLM and low generation latency of causal LLM, simultaneously. Extensive experiments illustrate that our ISM achieves significant performance.\n## Introduction\nRecent developments in Large Language Models (LLMs) such as GPT-3.5 Ouyang et al. [2022] and GPT-4 Achiam et al. [2023] have attracted significant attention. Due to the powerful generation capability, LLMs have made remarkable achievements in different kinds of Natural Language Process (NLP) tasks through a unified generative paradigm. Specifically, the most natural and common way to interact with LLMs is through multi-turn dialogues. However, as the number of dialogue rounds increases, ensuring high quality and low latency of the generated answer by LLMs is a challenge. \n\nExisting language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], Raffel et al. [2020], Lewis et al. [2020], Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], Dong et al. [2019], and Decoder-Only Brown et al. [2020], Touvron et al. [2023a,b], Du et al. [2022]. Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders Brown et al. [2020], Touvron et al. [2023a] and prefix decoders Du et al. [2022]. The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention. Our ISM utilizes alternate bidirectional and unidirectional attention on queries and answers in prefix sequences. All of them generate answer in auto-regressive.",
            "reference_string": "[271600495 | Lu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Scaling Laws Behind Code Understanding Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2211900186",
                    "name": "Jiayi Lin"
                },
                {
                    "authorId": "2260535450",
                    "name": "Hande Dong"
                },
                {
                    "authorId": "2260831632",
                    "name": "Yutao Xie"
                },
                {
                    "authorId": "2261093743",
                    "name": "Lei Zhang"
                }
            ],
            "abstract": "The scaling law is becoming a fundamental law in many machine learning areas. That is, test error falls off with the power law when increasing training data, model size, and computing resource. However, whether this law is suitable for the task of code understanding is not well studied, and most current language models for code understanding are about 100M parameters, which are relatively\"small\"compared to large language models. In this paper, we conduct extensive experiments to investigate the scaling law for the code understanding task by varying training data, model size, and computing resource. We validate that the test error of code understanding models falls off with the power law when using larger models, indicating that the scaling law is suitable for the code understanding task. Besides, we apply different scales of models to two downstream code understanding tasks, and find that the performance increases with larger scale of models. Finally, we train a large-scale code understanding model named CoLSBERT with 1.5B parameters on a large dataset using more computing resource, which outperforms previous work by a large margin. We will release our code and the CoLSBERT model when our paper is published.",
            "corpus_id": 267759571,
            "sentences": [
                {
                    "corpus_id": "267759571",
                    "title": "Scaling Laws Behind Code Understanding Model",
                    "text": "The Transformer architecture has emerged as the predominant model to date [36]. It employs a stacking mechanism of multiple transformer blocks, each comprising a self-attention layer and an MLP layer. Two primary self-attention structures are identified: \n\n\u2022 Full attention [4,5]. This bidirectional attention operates between every pair of tokens in the sequence. Notably, both subsequent and preceding tokens can assimilate information from each other. The robust information encoding capability afforded by full attention designates the model as a transformer-encoder, specialized for tasks involving understanding. \u2022 Masking attention [2,3]. In this directional attention model, interactions occur exclusively from left to right within the token sequence. Consequently, only the subsequent token can integrate information from preceding tokens, creating a unidirectional flow. Masking attention aligns with standard causal language modeling and is denoted as a transformerdecoder, tailored for tasks centered around generation.",
                    "score": 0.6019107219597292,
                    "section_title": "Transformer Architecture",
                    "char_start_offset": 6391,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 254
                        },
                        {
                            "start": 257,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 454
                        },
                        {
                            "start": 455,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 1031
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 74,
                            "end": 78,
                            "matchedPaperCorpusId": "221702858"
                        },
                        {
                            "start": 274,
                            "end": 277,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 277,
                            "end": 279,
                            "matchedPaperCorpusId": "211171605"
                        },
                        {
                            "start": 639,
                            "end": 642,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5048828125
                }
            ],
            "relevance_judgement": 0.5048828125,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws Behind Code Understanding Model\n# Venue: arXiv.org\n# Authors: Jiayi Lin, Hande Dong, Yutao Xie, Lei Zhang\n## Abstract\nThe scaling law is becoming a fundamental law in many machine learning areas. That is, test error falls off with the power law when increasing training data, model size, and computing resource. However, whether this law is suitable for the task of code understanding is not well studied, and most current language models for code understanding are about 100M parameters, which are relatively\"small\"compared to large language models. In this paper, we conduct extensive experiments to investigate the scaling law for the code understanding task by varying training data, model size, and computing resource. We validate that the test error of code understanding models falls off with the power law when using larger models, indicating that the scaling law is suitable for the code understanding task. Besides, we apply different scales of models to two downstream code understanding tasks, and find that the performance increases with larger scale of models. Finally, we train a large-scale code understanding model named CoLSBERT with 1.5B parameters on a large dataset using more computing resource, which outperforms previous work by a large margin. We will release our code and the CoLSBERT model when our paper is published.\n## Transformer Architecture\nThe Transformer architecture has emerged as the predominant model to date [36]. It employs a stacking mechanism of multiple transformer blocks, each comprising a self-attention layer and an MLP layer. Two primary self-attention structures are identified: \n\n\u2022 Full attention [4,5]. This bidirectional attention operates between every pair of tokens in the sequence. Notably, both subsequent and preceding tokens can assimilate information from each other. The robust information encoding capability afforded by full attention designates the model as a transformer-encoder, specialized for tasks involving understanding. \u2022 Masking attention [2,3]. In this directional attention model, interactions occur exclusively from left to right within the token sequence. Consequently, only the subsequent token can integrate information from preceding tokens, creating a unidirectional flow. Masking attention aligns with standard causal language modeling and is denoted as a transformerdecoder, tailored for tasks centered around generation.",
            "reference_string": "[267759571 | Lin et al. | 2024 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "Moreover, in terms of architecture, we directly feed the outputs of bidirectional encoders into the causal decoder, rather than relying on cross attention (Vaswani et al., 2017). Besides, multiple bidirectional encoders can be mounted to the causal language model, but the encoder-decoder architecture usually has only one encoder. \n\nNon-causal encoders as System 1, and causal language models as System 2. Cognition is usually categorized into two levels (Kahneman, 2011;Bengio, 2019): System 1 (i.e., intuitive, and unconscious) and System 2 (i.e., sequential, conscious, planning, and reasoning). In the proposed framework, the modules can be regarded as an implementation of these two levels, respectively. To be specific, non-causal encoders pretrained by masked data modeling, such as BERT (Devlin et al., 2019) and BEiT (Bao et al., 2022), are used as a perception layer to encode various input modalities. The encoding modules can be viewed as System 1. After we obtain the input representations, we  feed them to the causal language model, which has shown promising performance on commonsense reasoning (Chowdhery et al., 2022) and planning (Huang et al., 2022). The universal task layer is designed to play a role of System 2 in our method. \n\nNatural language interface between users and pretrained models. The universal task layer based on causal language modeling enables users to interact with pretrained non-causal encoders using natural language. First, language can be used as a programming language for the underlying pretrained or finetuned models, which is compiled by the universal interface. For example, we can write text-based instructions (Ouyang et al., 2022) and explanations (Wei et al., 2022) to repurpose and guide the model behaviors. Second, the universal interface enables the models to present the results using free texts, making predictions directly understandable and explainable. Third, the proposed framework natively supports multi-turn conversational interactions. In each turn, we can feed the encoded input to the interface layer and then generate response results in a semi-causal manner.",
            "score": 0.8501627739177744,
            "section_title": "body",
            "char_start_offset": 3968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 796,
                    "end": 817,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396240234375
        },
        {
            "corpus_id": "276423946",
            "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
            "text": "Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems. \n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51]. \n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way. \n\nNon-causal nlg strategies perform much worse in English than their causal counterparts [49]. However, to our knowledge, no prior research has been conducted on non-causal nlg in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the mlm task could be successfully exploited in Spanish nlg.",
            "score": 0.7985783516462949,
            "section_title": "Causality in generative transformer language models",
            "char_start_offset": 6569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "231645376"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "234785837"
                },
                {
                    "start": 1035,
                    "end": 1038,
                    "matchedPaperCorpusId": "231715684"
                },
                {
                    "start": 1038,
                    "end": 1041,
                    "matchedPaperCorpusId": "247126308"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "218487230"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "208224776"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "212657570"
                },
                {
                    "start": 1142,
                    "end": 1145,
                    "matchedPaperCorpusId": "267201220"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "265629619"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "211069634"
                },
                {
                    "start": 1638,
                    "end": 1642,
                    "matchedPaperCorpusId": "60441316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
            "score": 0.79231747357041,
            "section_title": "Prefix (Non-Causal) Language Model",
            "char_start_offset": 19013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1798
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "277349741",
            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
            "text": "Models like T5 (Raffel et al., 2020) and BART (Lewis et al., 2019) utilize both encoding and decoding mechanisms, enabling them to perform a wide range of tasks, including translation and summarization. The encoder applies stacked self-attention layers to encode the input sequence, and the decoder performs cross-attention on these representations and autoregressively generates the output. \n\n\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022). \n\nTo scale the capacity of LLMs efficiently, the Mixture of Experts (MoE) technique can be exploited to combine the above architectures, such as in Swich Transformer (Fedus et al., 2022) and GLaM (Du et al., 2022). MoE involves sparsely activating a subset of model parameters (the \"experts\") for each input, allowing the model to handle a vast number of parameters without incurring prohibitive computational costs. This is achieved by employing a trainable gating mechanism to route each input token to the most relevant subset of experts. \n\nApart from the mainstream Transformer architecture, there are also emerging architectures proposed to alleviate the inherent issues of Transformers (e.g., the quadratic complexity) such as State-Space Models (SSMs) (Gu et al., 2021), Mamba (Gu and Dao, 2023), and RWKV (Peng et al., 2023).",
            "score": 0.7908673976962467,
            "section_title": "Architecture",
            "char_start_offset": 34135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 36,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 731,
                    "end": 756,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1228,
                    "end": 1248,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1258,
                    "end": 1275,
                    "matchedPaperCorpusId": "245124124"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "276771845",
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "text": "Pre-trained generative models often employ three types of attention mask patterns: bidirectional, causal, and prefix masking (Figures 3 and 4 in Raffel et al. (2020)). For our focus on non-generative tasks, we limited the explorations to causal attention and bidirectional attention. \n\nBidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence. \n\nGiven that Gemma models are pre-trained with causal attention, we investigated the impact of both bidirectional and causal attention masks during fine-tuning to maximize the performance of Gemma Encoder models.",
            "score": 0.7818135545632132,
            "section_title": "Attention Masking",
            "char_start_offset": 4030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 165,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 347,
                    "end": 368,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "270702559",
            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
            "text": "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
            "score": 0.7587938703243283,
            "section_title": "Architecture of LLMs",
            "char_start_offset": 7065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 749
                },
                {
                    "start": 751,
                    "end": 898
                },
                {
                    "start": 898,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1243
                },
                {
                    "start": 1243,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1430
                },
                {
                    "start": 1432,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 1513,
                    "end": 1535,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "270832367",
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "text": "The contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
            "score": 0.7532888559138182,
            "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
            "char_start_offset": 5984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 1118,
                    "end": 1122,
                    "matchedPaperCorpusId": "219463386"
                },
                {
                    "start": 1124,
                    "end": 1128,
                    "matchedPaperCorpusId": "209832341"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "234785837"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "265629619"
                },
                {
                    "start": 1794,
                    "end": 1798,
                    "matchedPaperCorpusId": "231924507"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "231715684"
                },
                {
                    "start": 1806,
                    "end": 1810,
                    "matchedPaperCorpusId": "247126308"
                },
                {
                    "start": 1850,
                    "end": 1854,
                    "matchedPaperCorpusId": "218487230"
                },
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "208224776"
                },
                {
                    "start": 1921,
                    "end": 1925,
                    "matchedPaperCorpusId": "210839508"
                },
                {
                    "start": 1927,
                    "end": 1931,
                    "matchedPaperCorpusId": "212657570"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "244119798",
            "title": "TWT: Table with Written Text for Controlled Data-to-Text Generation",
            "text": "Typically, the encoder-decoder based models generate text starting from the beginning, and the decoder adopts a causal mask to force the state of each decoder time step s t i only attend to the state from the previous time steps, s t|t\u2264t i , to avoid seeing tokens \"from the future\". We consider this type of attention as unidirectional. In our task, we have the input prefix as the written text. Tokens in the prefix should be visible to each other. Therefore, we adopt the causal with prefix mask: bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens.",
            "score": 0.7528913294726668,
            "section_title": "Decoder Self-attention Visibility",
            "char_start_offset": 14430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 607
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "260435365",
            "title": "Pre-Trained Language Models for Text Generation: A Survey",
            "text": "Then, GPT-2 [153] explored the transfer capacity of language models for zero-shot generation task, highlighting the significance of sufficient data. Furthermore, GPT-3 [14] showed that massive model parameters can significantly improve the downstream generation tasks, with a few examples or prompts. CTRL [90] is proposed as a conditional causal LM to generate text based on control codes that govern style, content, and task-specific behavior. Causal LMs are simple and straightforward for text generation, but they have several structural and algorithmic limitations: Causal LMs encode the tokens just from left to right, thus ignore the bidirectional information on the input side. Moreover, causal LMs are not specially designed for the sequence-to-sequence generation tasks, thus in practice they do not achieve high performance in tasks such as summarization and translation [153]. \n\n4.1.3 Prefix Language Models. Upon a single Transformer, prefix LMs adopt bidirectional encoding scheme in the input side and natural left-to-right generation pattern in the output side. By utilizing the mixture attention mask, the tokens in the input text  can attend to each other, while the tokens in the target text  can only attend to all input tokens and previous generated tokens. \n\nUniLM [36] was the first prefix LM. Compared to causal LMs, UniLM used prefix attention mask to solve conditional generation tasks, similar to the encoder-decoder architecture. UniLMv2 [5] and GLM [39] improved vanilla prefix masking strategy by introducing permuted language modeling in XLNet [198]. Although prefix LMs have several advantages, Raffel et al. [154] compared single-Transformer prefix LMs to Transformer-based encoder-decoder LMs and concluded that adding explicit encoder-decoder attention is more effective to capture conditional dependencies. \n\n4.1.4 Encoder-Decoder Language Models. Encoder-decoder LMs follow the standard Transformer architecture for text generation, consisting of stacks of both encoder and decoder layers.",
            "score": 0.7502802759774916,
            "section_title": "Standard Architecture",
            "char_start_offset": 34171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 17,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 882,
                    "end": 887,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1466,
                    "end": 1469,
                    "matchedPaperCorpusId": "211572655"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "232270067"
                },
                {
                    "start": 1575,
                    "end": 1580,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1641,
                    "end": 1646,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.473876953125
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "For example, we can use task instructions (Ouyang et al., 2022) to repurpose the model, and use demonstrations of some examples to conduct few-shot learning. \n\nNon-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available. In addition, non-causal encoder pretrained by the masked language modeling objective achieves competitive performance on cross-lingual transfer (Conneau et al., 2020), which makes it effective to adapt models to the multilingual setting. \n\nSemi-causal language modeling as a meta-pretraining task. Semi-causal language modeling plays the role of linking together non-causal encoders and the causal language model. It is a meta task in the sense of universal interface pretraining of pretrained encoders. Specifically, non-causal encoders learn to represent various input data, and a causal language model serves as a universal task layer. Non-causal encoders dock with a causal language model, so that we can benefit from both modeling methods described as above. In comparison with previous encoder-decoder pretraining (such as prefix language modeling, and T5; Raffel et al. 2020), our task non-causally encodes random spans of the whole sequence, while generating the rest via causal language modeling. Moreover, in terms of architecture, we directly feed the outputs of bidirectional encoders into the causal decoder, rather than relying on cross attention (Vaswani et al., 2017).",
            "score": 0.7470007912363181,
            "section_title": "body",
            "char_start_offset": 2111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 160,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 388,
                    "end": 409,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 995,
                    "end": 1017,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1714,
                    "end": 1733,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "271600495",
            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
            "text": "Recent developments in Large Language Models (LLMs) such as GPT-3.5 Ouyang et al. [2022] and GPT-4 Achiam et al. [2023] have attracted significant attention. Due to the powerful generation capability, LLMs have made remarkable achievements in different kinds of Natural Language Process (NLP) tasks through a unified generative paradigm. Specifically, the most natural and common way to interact with LLMs is through multi-turn dialogues. However, as the number of dialogue rounds increases, ensuring high quality and low latency of the generated answer by LLMs is a challenge. \n\nExisting language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], Raffel et al. [2020], Lewis et al. [2020], Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], Dong et al. [2019], and Decoder-Only Brown et al. [2020], Touvron et al. [2023a,b], Du et al. [2022]. Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders Brown et al. [2020], Touvron et al. [2023a] and prefix decoders Du et al. [2022]. The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention. Our ISM utilizes alternate bidirectional and unidirectional attention on queries and answers in prefix sequences. All of them generate answer in auto-regressive.",
            "score": 0.7388980292593058,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 718,
                    "end": 738,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 740,
                    "end": 759,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 822,
                    "end": 840,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 859,
                    "end": 878,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 906,
                    "end": 922,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 1183,
                    "end": 1202,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1247,
                    "end": 1263,
                    "matchedPaperCorpusId": "247519241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "263829839",
            "title": "LLM for SoC Security: A Paradigm Shift",
            "text": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
            "score": 0.7325500628597399,
            "section_title": "A. Preliminaries",
            "char_start_offset": 37836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 776,
                    "end": 781,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 791,
                    "end": 796,
                    "matchedPaperCorpusId": "252780443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "270870458",
            "title": "When Search Engine Services Meet Large Language Models: Visions and Challenges",
            "text": "Introduced by \"Attention is All You Need\" [118], transformers revolutionized NLP by utilizing self-attention mechanisms over recurrent layers.This innovation allows simultaneous word processing in sentences, enhancing efficiency and linguistic comprehension.We, here, compare encoderonly, decoder-only, and encoder-decoder models of transformers, exemplified by BERT, GPT, and BART, respectively [32], [71].\n\n\u2022 Encoder-Only Models: BERT exemplifies this category with its bidirectional training enhancing context understanding.Its encoder transforms input sequences into contextualized representations, aiding in various NLP tasks [49].\n\n\u2022 Decoder-Only Models: GPT and related models emphasize text generation through stacked decoder layers.They predict subsequent words based on previous ones, enabling coherent text generation [1].\u2022 Encoder-Decoder Models: BART combines both approaches for robust language understanding and generation.This architecture supports a wide range of tasks including summarization and translation [59].Each model type, from encoder-only to encoder-decoder, offers unique capabilities for specific NLP applications.The evolution from basic transformer models to specialized ones like BERT, GPT, and BART highlights rapid advancements in NLP technology [32], [71].",
            "score": 0.717584342305756,
            "section_title": "Foundation Models of LLMs",
            "char_start_offset": 14381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 407
                },
                {
                    "start": 409,
                    "end": 527
                },
                {
                    "start": 527,
                    "end": 636
                },
                {
                    "start": 638,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 833
                },
                {
                    "start": 833,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1032
                },
                {
                    "start": 1032,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1292
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1027,
                    "end": 1031,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1287,
                    "end": 1291,
                    "matchedPaperCorpusId": "235368340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32568359375
        },
        {
            "corpus_id": "269981935",
            "title": "Bitune: Bidirectional Instruction-Tuning",
            "text": "But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2020) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020;Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling.As there is simply more unlabeled data available for pretraining, training a decoder-only architecture on unlabeled data, and then finetuning it for tasks with instruction-tuning, is the best modus operandi of today (Wang et al., 2022).However, with this switch to decoder-only architectures, we lost bidirectional attention in the process.As we know this can improve feature representations for instructions, we set out to re-introduce bidirectional attention, such that it can be integrated into pretrained decoder-only LLMs.\n\nOur new method Bitune adds bidirectional attention to decoder-only architectures and combines it with causal attention to generate two sets of instruction features, using two different sets of weights.These features are then integrated, utilizing learnable mixing coefficients, and later used as the KV-cache for response generation.Notably, the autoregressive response generation process remains unaffected by the bidirectional attention and continues to be causal.",
            "score": 0.7167230908070252,
            "section_title": "Introduction",
            "char_start_offset": 1637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1520
                },
                {
                    "start": 1522,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 269,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 293,
                    "end": 314,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 419,
                    "end": 439,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1209,
                    "end": 1228,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "As shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders. \n\nBackbone Network We use Transformer (Vaswani et al., 2017) to build the models. Given an input sequence, we first pack their vector representations together. Then we feed the vectors into a multilayer Transformer, which encodes the input to contextualized representations. In each Transformer block, there is a multi-head self-attention layer and a feed-forward network layer that are used to aggregate the hidden states of the previous layer. Moreover, attention masks are used to control the context access. We use a triangular matrix as the attention mask for the universal task layer, so that it processes the input from left to right. For the bidirectional encoder, we allow all the tokens to access each other. After obtaining the output vectors of the universal task layer, we use a softmax classifier to predict over the vocabulary. The weight matrix is shared with the input token embeddings. \n\nConnector As shown in Figure 2, there is a connector layer between the universal task layer and various bidirectional encoders. The connectors project vector representations of bidirectional encoders before feeding them into the general-purpose interface. Moreover, the connectors are used to match the output dimensions of foundation models with the universal task layer. We empirically find that both linear projection and feed-forward network work well in our experiments.",
            "score": 0.7166540556959149,
            "section_title": "Model Architecture",
            "char_start_offset": 7682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1696
                },
                {
                    "start": 1699,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 197,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 483,
                    "end": 504,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "269302556",
            "title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
            "text": "These models can broadly fall into three categories: encoder-only, decoder-only, and encoder-decoder models.Encoder-only models, such as BERT, specialize in understanding and interpreting language.In contrast, decoder-only models like GPT excel in generating coherent and contextually relevant text.Encoder-decoder models, like T5, combine both abilities, efficiently performing various NLP tasks from translation to summarization.\n\nAs an encoder-only model, BERT introduces a paradigm in NLP with its innovative bi-directional attention mechanism, which analyzes text from both directions simultaneously, unlike its predecessors like transformer which processed text in a single direction (either left-to-right or right-to-left).This feature allows BERT to attain a comprehensive context understanding, significantly improving its language nuance comprehension.On the other hand, decoder-only models such as GPT, including variants like ChatGPT, utilize a unidirectional self-attention mechanism.This design makes them particularly effective in predicting subsequent words in a sequence, thus excelling in tasks like text completion, creative writing, language translation, and code generation [75].Additionally, as an encoder-decoder model, T5 uniquely transforms a variety of NLP tasks as text generation problems.For example, it reframes sentiment analysis from a classification task to a text generation task, where input like \"Sentiment: Today is sunny\" would prompt T5 to generate an output such as \"Positive\".This text-to-text approach underscores T5's versatility and adaptability across diverse language tasks.\n\nThe evolution of LLMs has seen the emergence of advanced models like GPT-3 [97], LaMDA [98], PaLM [99], and Vicuna [68].These models represent significant advances in NLP, distinguished by their enhanced capabilities in comprehending and generating complex, fine-grained language.Their training methods are usually more sophisticated, involving larger datasets and more powerful computational resources.This scaling up has led to unprecedented language understanding and generation capabilities, exhibiting emergent properties such as in-context learning (ICL), adaptability, and flexibility.",
            "score": 0.7150173233503316,
            "section_title": "Foundation Models (FMs)",
            "char_start_offset": 12664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 431
                },
                {
                    "start": 433,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1620
                },
                {
                    "start": 1622,
                    "end": 1742
                },
                {
                    "start": 1742,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 2025
                },
                {
                    "start": 2025,
                    "end": 2214
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.487060546875
        },
        {
            "corpus_id": "266755678",
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "text": "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
            "score": 0.7140742294667604,
            "section_title": "Decoder-only Architecture",
            "char_start_offset": 35165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1591
                },
                {
                    "start": 1594,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 1951,
                    "end": 1955,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Notable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020]. T5 in particular was recently used as the foundation for the T0 model [Sanh et al., 2021], which leveraged large-scale multitask finetuning to achieve strong zero-shot generalization, outperforming decoder-only models an order of magnitude larger. \n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) . Most notably, the CD architecture makes up the backbone of the GPT series of models [Radford et al., 2018, 2019, Brown et al., 2020] as well as many other recent record-breaking LLMs [Zeng et al., 2021, Kim et al., 2021, Smith et al., 2022, Thoppilan et al., 2022, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. \n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e.",
            "score": 0.7114237560953935,
            "section_title": "Architectures",
            "char_start_offset": 10211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 135,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1321,
                    "end": 1342,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1438,
                    "end": 1456,
                    "matchedPaperCorpusId": "237485423"
                },
                {
                    "start": 1476,
                    "end": 1500,
                    "matchedPaperCorpusId": "238582964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.315673828125
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "An example of encoder-decoder architecture is the transformer model proposed in [24]. Its encoder and decoder blocks are stacked with multiple layers. As shown in Figure 3, the transformer encoder layer consists of a self-attention layer and a position-wise feed-forward layer. In addition to these two layers, decoder consists of a third cross-attention layer, which is responsible for attending to encoder output. \n\nEncoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks. T5 [25] uses encoder-decoder architecture. As stated in T5, using encoder-decoder structure helped to achieve good performance regarding classification as well as for generative tasks. \n\nAlthough encoder-decoder models end up having twice as many parameters as their decoder-only or encoder-only counterparts, they still have similar computational cost. Compared to PrefixLM models where the parameters are shared, here, the input and target are independently processed and use separate sets of parameters. Unlike decoder-only language models that are trained to generate the input, encoder-decoder models output target tokens. \n\nThe original transformer consisted of encoder-decoder blocks and was initially used for sequence-to-sequence tasks, such as NMT. However, it was discovered that, with the change in how the input is fed to the model, the single-stack (decoder or encoder) could also complete sequence-sequence model tasks. As a result, the subsequent models started containing either an encoder or decoder architecture. Below, we discuss these architectural variants of the original transformer model.",
            "score": 0.7109203804130689,
            "section_title": "Encoder-Decoder-Based Model",
            "char_start_offset": 15589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "274992300",
            "title": "Segment-Based Attention Masking for GPTs",
            "text": "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding. \n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design. \n\nThe most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2020). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder. However, PrefixLM requires training from scratch and is limited to single-turn inputs, overlooking scenarios with multiple prefill phases, as often encountered in chat-based systems. \n\nIn contrast, our approach enables the easy enhancement of SOTA decoder-only models by unlocking the potential of bidirectional attention in non-generated segments through lightweight finetuning. Trained on massive corpora with causal masking, these models can be enhanced with limited hardware and just a few hours of fine-tuning, enabling them to effectively use bidirectional attention during the prefill phase.",
            "score": 0.7048680968509291,
            "section_title": "Related Work",
            "char_start_offset": 3399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 616,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 950,
                    "end": 970,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1278,
                    "end": 1299,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "274789267",
            "title": "LLMs are Also Effective Embedding Models: An In-depth Overview",
            "text": "The development of language models has seen significant changes in neural architecture design. Early models like BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) primarily utilized encoder-only architectures, which focus on bidirectional context, enabling the model to consider both left and right context during training. In contrast, recent LLMs have adopted decoder-only architectures (Jiang et al., 2023a;OpenAI, 2023;Touvron et al., 2023b), which are driven by the need for models to excel at generative tasks, such as text completion and creative writing. While encoder-only models provide strong contextual embeddings, decoder-only models have demonstrated superior performance in generative tasks due to their autoregressive nature. \n\nEmbedding with Bi-directional Contextualization. Current generative LLMs predominantly use mono-directional attention, focusing on a unidirectional flow of information. This approach simplifies the model architecture and aligns well with autoregressive tasks where future tokens are predicted based on past context. However, the lack of bidirectional attention can limit the model's ability to fully capture dependencies within the entire sequence. To address this, some models like Gecko (Lee et al., 2024b) andLLM2vec (BehnamGhader et al., 2024) propose incorporating bidirectional attention mechanisms within existing LLMs, enabling the model to consider both past and future tokens simultaneously. This enhancement aims to improve the quality of sequence embeddings by leveraging a more comprehensive understanding of the input text. In addition, GritLM (Muennighoff et al., 2024) unifies embedding tasks and generative tasks into a single model with bidirectional attention through generative representational instruction tuning. Recently, however, BGE-ICL (Li et al., 2024b) argues that enabling bidirectional attention during embedding fine-tuning misalign with the model's original pre-training setup, which could compromise its effectiveness in generation tasks. \n\nLow-Rank Adaption.",
            "score": 0.6978348615823007,
            "section_title": "Model Architecture",
            "char_start_offset": 24975,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 2023
                },
                {
                    "start": 2026,
                    "end": 2044
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.425537109375
        },
        {
            "corpus_id": "267402678",
            "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
            "text": "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision.",
            "score": 0.6940490946818718,
            "section_title": "Different structures for combining encoders and decoders.",
            "char_start_offset": 10124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2294
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49267578125
        },
        {
            "corpus_id": "268247581",
            "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
            "text": "The Transformer model can be specialized into encoder-only (Figure 6a) and decoder-only (Figure 6c) designs to suit different purposes. Encoder-only models [25] specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks [24]. Encoder-only models are typically designed for representation learning objective obtain strong representations for text and biomolecule. In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task [54]. Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives.",
            "score": 0.6927092989241093,
            "section_title": "Encoder/Decoder-only",
            "char_start_offset": 32051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1121
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "246815222"
                },
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "258762343"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.465087890625
        },
        {
            "corpus_id": "257505035",
            "title": "The Life Cycle of Knowledge in Big Language Models: A Survey",
            "text": "Currently, pre-trained language models usually acquire various knowledge from pure text through self-supervised learning on a large-scale text corpus. In this section, we will first introduce several widely used learning objectives (Qiu et al., 2020), and then discuss the learning mechanisms behind them. \n\nCausal Language modeling aims to autoregressively predict the next token in the input sequence, which is the most popular pre-training tasks (Radford et al., 2019b;Brown et al., 2020;Ouyang et al., 2022;Scao et al., 2022) and has demonstrated excellent effectiveness in capturing context dependency and text generation paradigms. One limitation of causal language modeling is unidirectional, which can only capture contextual information from left to right. \n\nMasked Language Modeling aims to mask some tokens in the input randomly, and then predict the masked token conditioned on the rest of sequence (Devlin et al., 2019;Liu et al., 2019c). Unlike causal language modeling, which can only obtain information in a unidirectional manner, masked language modeling can capture contextual information from both left-to-right and right-to-left directions. \n\nSeq2seq Masked Language Modeling uses an encoder-decoder architecture for pre-training, which first feeds the encoder with masked sequence, and the decoder is supposed to predict the masked tokens autoregressively (Raffel et al., 2020;Song et al., 2019). \n\nDenoising Autoencoder first corrupts the input sequence with randomly mask symbols, then feed the input into a bidirectional encoder, and the likelihood of the whole original input is calculated with an auto-regressive decoder (Lewis et al., 2020a).",
            "score": 0.6840930210502405,
            "section_title": "Learning from Text Data",
            "char_start_offset": 12176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1669
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 250,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 449,
                    "end": 472,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 491,
                    "end": 511,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 911,
                    "end": 932,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 932,
                    "end": 950,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 1377,
                    "end": 1398,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1398,
                    "end": 1416,
                    "matchedPaperCorpusId": "146808476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2298583984375
        },
        {
            "corpus_id": "246035820",
            "title": "CM3: A Causal Masked Multimodal Model of the Internet",
            "text": "For example, masked encoder-only models such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) excel in non-generative fine-tuning tasks. Masked encoder-decoder models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) excel in both discriminative and generative fine-tuning. Brown et al. (2020) on the other hand, showed that causal language models (de-facto, decoder only) are capable of non-trivial performance without the need of fine-tuning by simply prompting with appropriate string to control the generated outputs Radford et al. (2019); Brown et al. (2020); Artetxe et al. (2021). \n\nThere are pros and cons to both masked and causal language modeling in the context of prompting. Masking offers the critical ability to encode bi-directionality within the prompts at the cost of only decoding roughly 15% of the tokens of the input sequence during training (Devlin et al., 2018;Liu et al., 2019;Lewis et al., 2019). Conversely, decoder-only causal language models decode every token in the input sequence in the training but are typically limited to left-only contexts. Empirically, more work has also been done on scaling causal decoder-only rather than their counterparts. \n\nIn an effort to get most of the best of both worlds, we introduce a novel objective that combines the benefit of per-token generation with optional bi-directionality specifically tailored to prompting. For a document of size s we select n \u223c Clamp(Poisson(1), 1, 16) masks and for each of those masks we select span m \u223c (U nif orm(0, s), U nif orm(0, s)) which does not intersect with any other m. These values are chosen to, on average, select relatively few relatively long spans, which we expect will allow the model to learn to infill long spans.",
            "score": 0.6837079970213193,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 567,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.130615234375
        },
        {
            "corpus_id": "270521959",
            "title": "GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model",
            "text": "Language models are established to predict the embedding of the target token based on the context or the previous tokens of it.In this work, we use generative language model to predict the hidden layer embeddings of teacher model in the hope to involve the interaction of the intermediate layers and avoid utilizing the future information.Three generative architectures are considered: encoder-decoder, prefix decoder and causal decoder [24].The encoder-decoder architecture [25][26][27] consists of two stacks of Transformer blocks to construct encoder and decoder separately.The encoder encodes the input sequence into a common history and the decoder generates the target sequence based on the common history in an autoregressive way.The prefix decoder architecture [28,29] performs bidirectional attention over the prefix tokens and unidirectional attention on generated tokens.The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input through a unidirectional attention mask.In our work, we select the causal decoder architecture as the backbone of our generative distiller since it is more concise and is capable to predict the hidden layer outputs of the teacher model autoregressively.",
            "score": 0.6768230885390735,
            "section_title": "Generative Language Models",
            "char_start_offset": 5733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1228
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 441,
                    "matchedPaperCorpusId": "257900969"
                },
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.576171875
        },
        {
            "corpus_id": "271974388",
            "title": "Legilimens: Practical and Unified Content Moderation for Large Language Model Services",
            "text": "The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT series models [9,55,57] are developed based on the causal decoder architecture. So far, the causal decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as LLaMA [73,74], Dolly [17,18], and Falcon [61]. \n\nThe prefix decoder architecture (a.k.a., non-causal decoder) revises the masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens [23] and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and auto-regressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding. Existing representative LLMs based on prefix decoders include GLM [24,89] and U-PaLM [72]. \n\nwhere \u2295 denotes concatenating the previous output tokens to the end of the input sequence until a special sentence ending token (usually denoted as [eos]) is generated. The first decoding method is greedy search, which predicts the most likely token at each step based on the previously generated tokens. The other decoding method is sampling, which randomly samples the next token based on the probability distribution to enhance the randomness and diversity during generation. \n\nFrom the inference process and the self-attention mechanism of LLMs we know that LLMs output the first token  1 of r leveraging the information of p, and output the last token (i.e., [eos]) with the information of both p and r, i.e., \n\nwhere H (\u2022) denotes the inference function of LLMs. We utilize this inference process as a feature extractor for the downstream content moderation task, which we elaborate on in \u00a74.",
            "score": 0.6739635543084452,
            "section_title": "Basic Component.",
            "char_start_offset": 11466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "253018395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408203125
        },
        {
            "corpus_id": "267211690",
            "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
            "text": "Recently, pretrained language models based on transformers have been dominant in the NLP field, exhibiting variations in both components and objectives. Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as Devlin et al. (2019) and Clark et al. (2020). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019;Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2020), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives. Recently, Tay et al., 2023 proposed that various pretraining objectives can be recast as each other. They introduced the UL2 framework based on a pretraining objective called Mixture-of-Denoisers (MoD), which combines different pretraining paradigms. They compared decoder-only and encoder-decoder models trained with the MoD objective and found that encoder-decoder models often perform better. Notably, by using the MoD objective and moderately scaling up the model, they achieved state-of-the-art performance on a diverse set of NLP tasks including understanding and generation tasks.",
            "score": 0.6680890848886132,
            "section_title": "Pretraining objectives",
            "char_start_offset": 5796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1590
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 330,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 335,
                    "end": 354,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 511,
                    "end": 533,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 609,
                    "end": 630,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 782,
                    "end": 801,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2314453125
        },
        {
            "corpus_id": "254812339",
            "title": "A Framework and Operational Procedures for Metaverses-Based Industrial Foundation Models",
            "text": "Transformer [8], the sole attention-based architecture, is first proposed in the NLP area, which has then become the main framework used for constructing NLP big models. Typical structures of big NLP models include encoder-based models, decoder-based models, and encoder-decoder models which are paired with different training strategies and used for different tasks. Encoder-based NLP models, such as BERT [9], UniLM [10], XLM [11], ELECTRA [12], and so on, simultaneously consider the context features for each token and adopt autoencoding (AE) objective to train the model with masked language modeling-based self-supervision. Different from the bidirectional design in encoder-based models, decoder-based models only consider tokens before the current position and use autoregression (AR) objectives during the training process. Typical decoder-based NLP models include GPT [13], GPT-2 [14], GPT-3 [15], ELMo [16], CPM-1 [17], and so on. Encoder-based models have advantages in the modeling of context features and are widely used in language understanding. But due to the masking operations, there are gaps between the inputs at the pretraining stage and finetuning stages. Decoder-based methods adopt unidirectional designs, which are suitable for generative language tasks. XLNET [18] explores the combination of advantages from both AE and AR by token permutation. Encoder-decoder frameworks combine the representation and task-specific modules in series, which are widely used for sequence-to-sequence tasks, such as question answering and machine translation. T5 [19], Switch-Transformer [20], and BART [21] are typical encoder-decoder-based big language models.",
            "score": 0.6655660431224572,
            "section_title": "A. NLP Models",
            "char_start_offset": 5707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1672
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 15,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 925,
                    "end": 929,
                    "matchedPaperCorpusId": "227238757"
                },
                {
                    "start": 1287,
                    "end": 1291,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1573,
                    "end": 1577,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1598,
                    "end": 1602,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3125
        },
        {
            "corpus_id": "221701867",
            "title": "Deep Learning Based Abstractive Text Summarization: Approaches, Datasets, Evaluation Measures, and Challenges",
            "text": "However, there is only a single-layer encoder. e encoder and decoder employ an LSTM unit, but while the encoder utilises bidirectional LSTM, the decoders use unidirectional LSTM, as shown in Figure 13. To understand the summary generated by the backward decoder, the attention mechanism is applied in both the backward decoder and the encoder. Moreover, to address the problem of out-of-vocabulary words, an attention mechanism is employed in both decoders. \n\nA double attention pointer network, which is referred to as (DAPT), was applied to generate an abstractive text summarisation model [49]. \n\ne encoder utilised bidirectional LSTM, while the decoder utilised unidirectional LSTM. e encoder key features were extracted using a selfattention mechanism. At the decoder, the beam search was employed. Moreover, more coherent and accurate summaries were generated. \n\ne repetition problem was addressed using an improved coverage mechanism with a truncation parameter. e model was optimised by generating a training model that is based on RL and scheduled sampling.",
            "score": 0.6637474693846566,
            "section_title": "LSTM RN.",
            "char_start_offset": 47512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1066
                }
            ],
            "ref_mentions": [
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "210930338"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289306640625
        },
        {
            "corpus_id": "271329267",
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "text": "Transformer architectures have revolutionized NLP tasks, such as sequence modeling, by effectively capturing 2 https://paperswithcode.com/ long-range dependencies and modeling relationships between words. The advantages of the transformer architecture include enhanced parallelization, faster training, and the ability to model long-range dependencies efficiently. The attention mechanism allows the model to focus on relevant parts of the input sequence, contributing to its success in handling variable-length sequences without sacrificing performance. Recognizing the shift from encoder-decoder to decoder-only architectures, understanding pre-training strategies, and the advantages of transformer models provide a more nuanced perspective on their capabilities in Generative AI and various NLP tasks. Here, we will distinguish between the original encoder-decoder architecture and the decoder-only architecture and the pre-training strategies of transformer models. \n\nEncoder-Decoder Architecture. The encoder-decoder architecture serves as a fundamental structure in Transformer models, employed for sequence-to-sequence tasks such as machine translation, where an input sequence (source language) is transformed into an output sequence [18], [110]. In an encoder-decoder architecture, the model consists of two main components featuring multiple layers of self-attention and feedforward layers: an encoder and a decoder network. The encoder network processes the input sequence, capturing relevant information and creating a contextualized representation that encompasses semantic and syntactic details of the input. Subsequently, the decoder network, in turn, utilizes this contextualized representation from the encoder to generate the output sequence step by step. At each step, the decoder attends to various parts of the encoder's output, facilitating the alignment of source and target language information. Both the encoder and decoder components typically employ the self-attention mechanism [18]. This mechanism enables the model to weigh the importance of different positions in the input sequence during the generation of the output sequence, thereby allowing for the capture of long-range dependencies.",
            "score": 0.6611942520903131,
            "section_title": "D. Architecture of Transformer Models",
            "char_start_offset": 51023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1249,
                    "end": 1254,
                    "matchedPaperCorpusId": "259501685"
                },
                {
                    "start": 2007,
                    "end": 2011,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.391845703125
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) . Sometimes called a prefix language model, this approach was introduced by [Liu et al., 2018] and was later explored as an architectural variant by [Raffel et al., 2020, Wu et al., 2021]. Despite single-task finetuning performance nearly on par with encoder-decoder models [Raffel et al., 2020], it has seen limited adoption in the literature. \n\nEncoder-only. As an aside, we note that another popular architectural variant is to only use a Transformer encoder layer stack. This model architecture underlies the ubiquitous BERT [Devlin et al., 2018] and its derivatives. However, this architecture is limited to producing the same number of tokens as it was fed as input, considerably limiting its applicability and making it only rarely used in the zero-shot setting [Tamborrino et al., 2020]. We therefore omit it from consideration. For full language modeling, all tokens in a sequence are used during training. For prefix language modeling, we randomly select a prefix size, and hence only half of the tokens are used on average to derive the loss. At inference time, the prefix would be over the input/conditioning information. Finally, for masked language modeling, we mask 15% of the tokens, in spans of 3 tokens on average. We use sentinel tokens to replace spans (not represented here), and the model outputs subsequently each sentinel followed by its prediction of the content masked by the sentinel. \n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.",
            "score": 0.6587721321355524,
            "section_title": "Architectures",
            "char_start_offset": 11973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 469,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 649,
                    "end": 670,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270263671875
        },
        {
            "corpus_id": "267312283",
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "text": "We elaborate on Transformer architecture which is dominantly used for PLMs [12,136,137,173] in natural language processing (NLP). Transformer [139] is a language model that refines token embeddings reflecting the contextual information through a self-attention mechanism [21,111,139] to utilize them in downstream tasks. Transformer is composed of multiple layers consisting of multi-head attention (MHA) and feed-forward network (FFN) sublayers. \n\nMHA sublayers capture the contextual inter-token information via a self-attention mechanism and FFN sublayers polish intra-token information with a position-wise feedforward network. Original Transformer architecture [139] comprises Encoder and Decoder where Encoder encodes input token embeddings considering contextual information and Decoder generates output tokens by decoding the encoding results. However, we use encoder-only and decoder-only Transformers according to downstream tasks based on the previous works demonstrating the impressive performance of encoder-only [25,36,83,98,121] and decoder-only [12,124,136,173] PLMs. We illustrate encoder-only and decoder-only Transformer architectures in Fig. 1. Encoder-only Transformers (a) [25,36,83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,37], natural language inference [140,148], and question answering [117,118]. On the other hand, decoder-only Transformers (b) [12,124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration.",
            "score": 0.6574776817089234,
            "section_title": "Transformer Architecture",
            "char_start_offset": 4639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 79,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 142,
                    "end": 147,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "6506243"
                },
                {
                    "start": 275,
                    "end": 279,
                    "matchedPaperCorpusId": "8495258"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 666,
                    "end": 671,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1030,
                    "end": 1033,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1061,
                    "end": 1065,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1199,
                    "end": 1202,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1453,
                    "end": 1456,
                    "matchedPaperCorpusId": "16639476"
                },
                {
                    "start": 1490,
                    "end": 1494,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1519,
                    "end": 1524,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 1524,
                    "end": 1528,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "274992300",
            "title": "Segment-Based Attention Masking for GPTs",
            "text": "The introduction of the transformer architecture (Vaswani et al., 2017) has significantly advanced the field of natural language processing (NLP). Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford   * Equal contribution. \n\nFigure 1: Causal and MAS attention. The plot shows binary values, where the y-axis represents the index of the current token, and the x-axis represents the set of indices of tokens it can attend to. MAS is inspired by the observation that input prompts are provided to the model as a whole, so they can be masked together in blocks, allowing access to future tokens within the same block of the prompt. \n\nFigure 2: Model performance on the Commonsense Reasoning benchmark for seven LLMs. L,M,Q stand for Llama, Mistral and Qwen respectively. et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI. \n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approxi-mately twice the number of parameters compared to decoder-only models with equivalent capacity. Efforts such as those by Dong et al. (2019), Raffel et al. (2020), and Tay et al. (2023) explored unified architectures, where a model's parameters are trained from scratch to function as both encoder and decoder. Despite their potential efficiency, these approaches failed to gain widespread adoption. In contrast, the remarkable success of decoderonly models, exemplified by GPT-3 (Brown et al., 2020), has shifted the field's focus toward architectures almost exclusively based on causal attention. \n\nWhile the popularity of GPT models continues to grow, a key limitation is their inability to fully leverage information from future tokens.",
            "score": 0.6573764721289808,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 71,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1221,
                    "end": 1242,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1527,
                    "end": 1545,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1547,
                    "end": 1567,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1573,
                    "end": 1590,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 1885,
                    "end": 1905,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50537109375
        },
        {
            "corpus_id": "274305758",
            "title": "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?",
            "text": "Better results also make models with bidirectional encoders have a variety of applications and improvements. But as the models become larger, due to the inherent complexity of bidirectional models, they require more time and computing power than one-way models, and driven by interests, it is clear that pure one-way models have a higher cost performance at present. Of course, understanding and being able to do are certainly different. With the improvement of computing power, the situation may change in the future. GPT and BERT are among the earliest foundational models, representing two distinct approaches in natural language processing. GPT is a one-way or causal language model that predicts the next word in a sequence by considering only the preceding words. In contrast, BERT is a bidirectional language model, which processes both forward and backward context simultaneously to improve feature extraction and representation capacity. Both models are based on the Transformer [28] architecture, which consists of two main components: the encoder and decoder. Transformer introduces a self-attention mechanism. For each position in the sequence, the self-attention mechanism computes the query, the key and value vectors, and the attention weight, and uses the attention weight to weigh the sum of the value vectors to get the final output representation. In the Transformer encoder architecture, each encoding layer's multi-head self-attention sublayer will interact with each position in the input sequence with all other positions in the sequence, so that the selfattention layer when encoding a word will consider the entire sentence's words when encoding a word. However, in the decoder, the self-attention layer uses a mask matrix so that each position in the sequence can only see the sequence before it and the positions behind it will be hidden. GPT employs the Transformer decoder, which uses masked self-attention to only attend to past tokens in the sequence, making it suitable for tasks like text generation. On the other hand, BERT utilizes the Transformer encoder, which allows it to attend to all tokens in the sequence, both before and after each word, making it ideal for tasks that require deep understanding of context, such as question answering and text classification.",
            "score": 0.6572551212028048,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.701171875
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "Large-scale language model pretraining has achieved strong performance across various downstream tasks and aroused extensive research interest. The difference between the models mainly lies in the pretraining objective and model architecture. GPT (Radford et al., 2018;2019;Brown et al., 2020) pretrains causal language models with decoder-only Transformers, demonstrating intriguing properties of few-shot and in-context learning. Recent efforts (Rae et al., 2021;Du et al., 2021;Smith et al., 2022;Hoffmann et al., 2022;Thoppilan et al., 2022;Chowdhery et al., 2022) focus on scaling up in terms of data and model size. In order to implement bidirectional encoding, Devlin et al. (2019) propose the masked language modeling objective. Clark et al. (2020) introduce the replaced token detection task to improve pretraining efficiency. Furthermore, some efforts investigate frameworks that can handle both natural language understanding and generation tasks. T5 (Raffel et al., 2020) introduces an encoder-decoder framework that converts all tasks into a text-to-text format. BART (Lewis et al., 2020) is a sequence-to-sequence model pretrained by reconstructing the original text from corrupted documents. UniLM (Dong et al., 2019;Bao et al., 2020) presents to jointly optimize unidirectional, bidirectional and sequence-to-sequence language modeling objectives controlled by different self-attention masks. Wang et al. (2022b), Tay et al. (2022), andArtetxe et al. (2022) study the effects of different pretraining objectives and architectures on downstream generalization. Specifically, causal language models are good at zero-shot or in-context learning, while non-causal models perform better for finetuning. In our work, we combine the best of both worlds by introducing semi-causal language modeling. So we can obtain decent finetuning performance and benefit from the capability of in-context learning. Moreover, the unification enables us to build a general-purpose interface to various foundation models.",
            "score": 0.6549190553973225,
            "section_title": "Language Model Pretraining",
            "char_start_offset": 43537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 293,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 481,
                    "end": 500,
                    "matchedPaperCorpusId": "51876975"
                },
                {
                    "start": 668,
                    "end": 688,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 962,
                    "end": 983,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1081,
                    "end": 1101,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1232,
                    "end": 1249,
                    "matchedPaperCorpusId": "211572655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3232421875
        },
        {
            "corpus_id": "265329916",
            "title": "Large Language Models and Information Retrieval",
            "text": "The research investigates fundamental architectural designs integral to Large Language Models (LLMs): the encoder-decoder architecture and the decoder-only architecture. These structural frameworks have played a pivotal role in advancing language understanding and generation, laying the foundation for innovative applications across diverse domains. \n\nThe encoder-decoder architecture, a widely embraced structure in Natural Language Processing (NLP) tasks, comprises two essential components: an encoder and a decoder. The encoder processes the input sequence, generating a fixed-length context vector. Conversely, the decoder receives this context vector and produces the output sequence. Typically, recurrent layers such as LSTM or GRUs construct the encoder, processing the input sequentially and capturing contextual information. Attention mechanisms further enhance this architecture by enabling the decoder to selectively focus on different parts of the input during decoding. \n\nIn contrast, the decoder-only architecture revolves around a solitary transformer-based decoder, devoid of an explicit encoder. This architectural model generates the output sequence based on a learned positional representation of the input sequence. Initially introduced in encoder-decoder models, transformer-based architectures have been adapted for decoder-only models. They incorporate selfattention mechanisms, allowing the model to weigh the significance of each input token when generating the output token. The transformer decoder processes tokens in parallel, enhancing computational efficiency and significantly accelerating training and inference processes. These architectural paradigms have significantly propelled the field of language modeling. Encoder-decoder architectures have notably augmented language understanding by efficiently capturing contextual relationships in both input and output sequences, crucial for translation and summarization tasks. On the other hand, the decoder-only architecture, particularly with transformer models, has immensely elevated generation capabilities. The self-attention mechanism's ability to consider all input tokens simultaneously results in coherent and contextually appropriate text generation. Understanding these architectural designs is fundamental in the development of sophisticated LLMs adept at both understanding and generating human-like text. Their flexibility and effectiveness continue to drive innovation, paving the way for a multitude of applications in the realm of natural language processing.",
            "score": 0.6539640403737148,
            "section_title": "LLM Architecture",
            "char_start_offset": 13306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2401
                },
                {
                    "start": 2402,
                    "end": 2559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43359375
        },
        {
            "corpus_id": "264405877",
            "title": "GenDistiller: Distilling Pre-trained Language Models based on Generative Models",
            "text": "The language models can predict the embedding of the target token based on the context or the previous tokens of it. In this work, we use generative language model to predict the hidden layer outputs of teacher network in the hope to take the interaction of the hidden layers into account and avoid seeing the future information. Three language model architectures are considered: encoder-decoder, causal decoder, and prefix decoder [27]. The encoder-decoder architecture [3,28,29] consists of two stacks of Transformer blocks as the encoder and decoder, in which encoder is to encode the input sequence as a common history for the generated sequence and decoder generates the target sequence autoregressively. The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input and itself through a unidirectional attention mask. The prefix decoder architecture [34,35] performs bidirectional attention over the prefix tokens and unidirectional attention only on generated tokens. In our work, we treat the original feature as the prefix tokens and the target hidden layers as the sequence to be generated one-by-one. Therefore, we select the prefix decoder architecture and modify it to some extend to build our distiller which can bidirectionally encode the input features and predict the output layers autoregressively, meanwhile, the required parameters are less than the encoder-decoder architecture.",
            "score": 0.6535016157929564,
            "section_title": "Generative Language Models",
            "char_start_offset": 6038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1431
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "257900969"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 475,
                    "end": 478,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 751,
                    "end": 755,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 892,
                    "end": 895,
                    "matchedPaperCorpusId": "253018395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482666015625
        },
        {
            "corpus_id": "270703043",
            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
            "text": "The emergence of pre-trained language models represented a paradigm shift, driving research toward designing more efficient architectures and refining pre-training strategies. These pre-trained models have been commonly adapted or specialized to downstream tasks via fine-tuning, which involves transferring knowledge by further training a model on new data. There are significant advantages demonstrated by these models in language understanding and model performance in various tasks [9,13]. \n\nELMo is one of the earliest attempts at pre-trained language models [15]. This model was developed to capture context-aware word representations by pre-training a bidirectional Long Short-Term Memory (biLSTM) network and fine-tuning it for subsequent downstream tasks. Later the Transformer architecture was introduced, revolutionizing the NLP field by offering highly parallelizable structures and self-attention mechanisms. The Transformer [6] follows the autoencoder archetype, from which three families of models arose: (1) BERT-family or encoder-only models, (2) GPT-family or decoder-only models, and (3) text-to-text or encoder-decoder models. In Fig. 1, the graphical representations of these  Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively. families are shown.",
            "score": 0.6491592979539362,
            "section_title": "Pre-trained language models",
            "char_start_offset": 5957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 938,
                    "end": 941,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "265511054",
            "title": "Recent Advances in Text Analysis",
            "text": "In a traditional encoder-decoder pair, the encoder maps the input sequence into a fixed-length vector, and the decoder has access to this vector only.The attention mechanism allows the encoder to pass all the hidden states (not just the final encoded vector) to the decoder, along with annotation vectors and attention weights to tell the decoder which part of information to \"pay attention to\".The attention mechanism was shown to be much more effective than RNNs in processing long documents.Vaswani et al. (2017) proposed a special architecture called transformer that uses self-attention within each of the encoder and decoder and cross-attention between them.The transformer has become the most popular architecture in NLP.For example, the encoder part of the transformer is the building block of models like BERT (see below), and the decoder part of the transformer is the building block of models like GPT (Radford et al. 2018) for text generation.",
            "score": 0.6478227249752091,
            "section_title": "Commonly used neural network architectures",
            "char_start_offset": 35204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 395
                },
                {
                    "start": 395,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 955
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 515,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31494140625
        },
        {
            "corpus_id": "271843410",
            "title": "Survey: Transformer-based Models in Data Modality Conversion",
            "text": "Moreover, the Reformer model [86] introduces efficiency improvements that enable the processing of very long documents, significantly reducing memory usage and computation time without sacrificing the quality of text generation. \n\n4.1.3 Encoder-Decoder. Encoder-decoder PLMs are designed to handle a wide array of complex NLP tasks that involve both understanding input text (encoding) and generating new text based on that understanding (decoding). These models have been pivotal in advancing the capabilities of NLP applications, from machine translation to summarization and question-answering. Facebook AI developed BART (Bidirectional and Auto-Regressive Transformers) [97]. BART combines bidirectional encoding (similar to BERT) with autoregressive decoding (similar to GPT), making it particularly effective for text generation tasks that require a deep understanding of context, such as summarization and translation. mBART (multilingual BART) [29], an extension into multilingual contexts, is a sequence-to-sequence model pre-trained on large-scale monolingual corpora across multiple languages. This pre-training gives mBART the deep understanding of linguistic subtleties it needs to do translation work, even in languages with few resources. This makes the benefits of advanced NLP models available to everyone, regardless of language. Google introduced T5 or Text-to-Text Transformer [116]. It redefines the paradigm by framing all NLP tasks as a text-to-text problem. The model handles every task, from translation to summarization, by converting one type of text into another using a consistent approach. This innovative perspective has simplified the application of transfer learning in NLP. In addition, BigBird [97] is an encoder-decoder model that proposes a sparse attention mechanism, which reduces complexity and time consumption for tasks such as question answering and summarization. \n\nManuscript submitted to ACM Autoprompt [142] Masked attention Decoder-only GPT-3 OpenAI GPT-3 Dataset Transformer-XL [34] Self-attention Decoder-only Transformer-XL WikiText-103 Dynamic Evaluation [87] Self-attention Decoder-only RNNs Penn Treebank",
            "score": 0.6456475501118077,
            "section_title": "TB Architecture in NLP",
            "char_start_offset": 15399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "234942970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27783203125
        },
        {
            "corpus_id": "202565479",
            "title": "Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems",
            "text": "As discussed before, Transformer is the first model that entirely relies on the self-attention mechanism for both the encoder and the decoder. The Transformer uses the self-attention mechanism to learn a representation of a sentence by relating different positions of that sentence. Like many of the sequence modelling methods, Transformer follows the encoder-decoder architecture in which the input is given to the encoder and the results of the encoder is passed to the decoder to create the output sequence. The difference between Transformer (which is a self-attentional model) and other sequence models (such as recurrencebased and convolution-based) is that the encoder and decoder architecture is only based on the self-attention mechanism. The Transformer also uses multi-head attention which intends to give the model the ability to look at different representations of the different positions of both the input (encoder self-attention), output (decoder self-attention) and also between input and output (encoder-decoder attention) (Vaswani et al., 2017). It has been used in a variety of NLP tasks such as mathematical language understanding [110], language modeling (Dai et al., 2018), machine translation (Vaswani et al., 2017), question answering (Devlin et al., 2018), and text summarization (Liu et al., 2018).",
            "score": 0.6455864096689986,
            "section_title": "Transformer",
            "char_start_offset": 9490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1325
                }
            ],
            "ref_mentions": [
                {
                    "start": 1041,
                    "end": 1063,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1217,
                    "end": 1239,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2198486328125
        },
        {
            "corpus_id": "252383196",
            "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design",
            "text": "Based on their network structure, attention-based NNs can be classified into three categories: i) encoder-decoder, ii) encoder-only, and iii) decoder-only networks. The encoder-decoder NNs are mainly designed for sequenceto-sequence tasks, such as machine translation [2]. One of the most widely used encoder-decoder network is the Transformer, which is constructed by a stack of encoder and decoder blocks. Figure 2 illustrates the structure, where N l , D hid and R fft represent input length, hidden size and FFN expand ratio respectively. Each encoder starts with a multi-head attention module, followed by a feed-forward network (FFN) consisting of two linear (fully-connected) layers. Finally, residual addition [22] and layer normalization (LN) [23] are used after FFN. Within each multi-head attention, the inputs are first mapped to query (Q), key (K) and value (V) matrices through three different linear layers. The query matrix is then multiplied with K T , followed by a softmax operation to get the score matrix (S). The generated S is multiplied with V and the resultant matrix will flow into another linear layer, which generates the final output matrix of the multi-head attention. Similar to the encoder, the decoder employs two multi-head attention modules and one FFN, where the difference is that the inputs of the query and key matrices in the second attention module come from the last encoder. \n\nBased on the original encoder-decoder structure of Transformer, different variants have been proposed. The encoderonly networks, such as BERT [3] and XLM [24], are autoencoding models that have been widely applied to NLP tasks, such as sequence classification [25]. The Vision Transformer (ViT) [5] also lies in this category. An extra linear projection layer is introduced at the beginning, while its encoder layers correspond to the encoder part of the original Transformer. Finally, the decoder-only networks represent the autoregressive models designed for NLP tasks, such as language modeling [26].",
            "score": 0.6426471624967549,
            "section_title": "A. Attention-Based Neural Networks",
            "char_start_offset": 5003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2023
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 271,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1562,
                    "end": 1565,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1574,
                    "end": 1578,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 1680,
                    "end": 1684,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1715,
                    "end": 1718,
                    "matchedPaperCorpusId": "225039882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2000732421875
        },
        {
            "corpus_id": "221702858",
            "title": "Efficient Transformers: A Survey",
            "text": "It is important to note the differences in how the Transformer blocks are used. Transformers can primarily be used in three ways, namely: (1) encoder-only (e.g., for classification), (2) decoder-only (e.g., for language modeling), and (3) encoder-decoder (e.g., for machine translation). In encoder-decoder mode, there are usually multiple multi-headed self-attention modules, including a standard self-attention in both the encoder and the decoder, along with an encoder-decoder cross-attention that allows the decoder to utilize information from the encoder. This influences the design of the self-attention mechanism. In the encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs. \n\nThe mode of usage of a Transformer model generally depends on the target application. Given an input sequence, the sequence is typically passed through an encoder stack. At this stage, there might be too options. For multi-class classification, a linear layer with Softmax outputs typically projects the sequence representation down to the number of classes. In the case of BERT (Devlin et al., 2018), this is a [CLS] token that is appended to the start of the sequence as a prefix. Recent work has also explored the usage of Encoder-Decoder architectures for classification, such as T5 (Raffel et al., 2019). Decoder-only models are typically used for generation and are trained using a language modeling objective (of predicting the next token). Due to the nature of the loss, these models are often superior for open ended generation (Brown et al., 2020).",
            "score": 0.6341083283265432,
            "section_title": "Transformer Mode",
            "char_start_offset": 9721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 1509,
                    "end": 1530,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1717,
                    "end": 1738,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497802734375
        },
        {
            "corpus_id": "261064777",
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "text": "The Transformer architecture, proposed by Vaswani et al. in 2017(Vaswani et al., 2017), has emerged as the leading choice for developing large language models (LLMs) due to its exceptional parallelizability and capacity (Zhao et al., 2023b). This scalability allows language models to be expanded to include hundreds or even thousands of billions of parameters, enabling them to capture more complex language patterns and improve performance on various tasks. In general, large language models can be categorized into three main architecture types: encoder-decoder structures, causal-decoder, and prefix decoder (Zhao et al., 2023b), each with its own characteristics and applications. \n\nEncoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation. Encoder-decoder pretrained model. Encoder-decoder pretrained models, such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020), have demonstrated excellent performance across various downstream tasks. However, with the development of LLM there are only a few large language models based on the encoder-decoder architecture, such as Flan-T5 (Chung et al., 2022) and CodeT5+ (Wang et al., 2023f). \n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process.",
            "score": 0.6326310010003747,
            "section_title": "Model Architecture",
            "char_start_offset": 10774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 1308,
                    "end": 1329,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2919921875
        },
        {
            "corpus_id": "9256367",
            "title": "Encode, Review, and Decode: Reviewer Module for Caption Generation",
            "text": "Encoder-decoder is a framework for learning a transformation from one representation to another. In this framework, an encoder network first encodes the input into a context vector, and then a decoder network decodes the context vector to generate the output. The encoder-decoder framework was recently introduced for sequence-to-sequence learning based on recurrent neural networks (RNNs) with applications to machine translation [3,15], where the input is a text sequence in one language and the output is a text sequence in the other language. More generally, the encoder-decoder framework is not restricted to RNNs and text; e.g., encoders based on convolutional neural networks (CNNs) are used for image captioning [17]. Since it is often difficult to encode all the necessary information in a single context vector (often regarded as the bottleneck), attentive encoder-decoder introduces attention mechanism to the encoder-decoder framework. Attention mechanism breaks down the encoder-decoder bottleneck by conditioning the generative process in the decoder on the encoder hidden states, rather than on one single context vector only. Improvements by the attention mechanism are shown on various tasks, including machine translation [1], image captioning [19], and text summarization [12]. \n\nHowever, there remain two important issues to address for the attentive encoder-decoder models. First, the attention mechanism proceeds in a sequential manner and thus lacks global modeling abilities. More specifically, at the generation step t, the decoded token is conditioned on the attention results at the current time step ht , but has no information about future attention results ht with t > t. For example, when there are multiple objects in the image, the caption tokens generated at the beginning focuses on the first one or two objects and is unaware of the other objects, which is potentially suboptimal due to the lack of global modeling abilities.",
            "score": 0.6324582994195505,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 434,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 434,
                    "end": 437,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "1055111"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "1918428"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343505859375
        },
        {
            "corpus_id": "252595927",
            "title": "Bidirectional Language Models Are Also Few-shot Learners",
            "text": "Transformer-based language models (Vaswani et al., 2017) can be broadly categorized into bidirectional and unidirectional models. Bidirectional models are models that use a denoising pre-training objective (such as masked language modeling), allowing them to utilize bidirectional context when learning language representations. Unidirectional language models are models with a causal-or a left-to-right-language modeling objective (such as next token prediction), restricting them to be unidirectional when learning representations (Liu et al., 2021). \n\nThe T5 family of models, such as T5 v1.1 and mT5, and BART-style models (Lewis et al., 2019) are bidirectional, while GPT-style models, such as GPT-2, GPT-3, and XGLM are unidirectional. \n\nUsually, but not always, bidirectional models are paired with an encoder-decoder architecture, while unidirectional models are paired with a decoder-only architecture (Devlin et al., 2019;Raffel et al., 2020;Xue et al., 2021;Radford et al., 2019;Brown et al., 2020;Lin et al., 2021;Wang et al., 2022). BERT-style models are an example of an exception. BERT-style models are bidirectional, but they cannot be easily utilized for prompting and text generation since they are encoder-only (Wang & Cho, 2019). Of the available bidirectional models, T5 models are the only models with a long enough sequence length (unlimited with their relative position embeddings) to support many in-context prompt examples and with a large enough number of parameters to be effective zeroshot and few-shot performers (Radford et al., 2019;Brown et al., 2020;Kaplan et al., 2020). See Appendix J for a survey of popular open source language models. Aside from sequence length and model size, BART is not purely trained on the span denoising objective SAP exploits, but is also trained on many other corruption objectives like \"Sentence Permutation.\"",
            "score": 0.631382711709569,
            "section_title": "UNIDIRECTIONAL AND BIDIRECTIONAL LANGUAGE MODELS",
            "char_start_offset": 5822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1874
                }
            ],
            "ref_mentions": [
                {
                    "start": 911,
                    "end": 932,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 932,
                    "end": 952,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 952,
                    "end": 969,
                    "matchedPaperCorpusId": "225040574"
                },
                {
                    "start": 990,
                    "end": 1009,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1230,
                    "end": 1248,
                    "matchedPaperCorpusId": "60441316"
                },
                {
                    "start": 1565,
                    "end": 1584,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257568359375
        },
        {
            "corpus_id": "273323719",
            "title": "JurEE not Judges: safeguarding llm interactions with small, specialised Encoder Ensembles",
            "text": "Most LLMs are built upon the decoder-only transformer architecture [60,50], and their recent success is largely due to their ability to scale to tens or even hundreds of billions of parameters [5,52]. This extreme scale allows them to be highly versatile, with the capability to generalize effectively to novel domains through zero-shot and few-shot learning. However, this versatility comes with significant trade-offs, including high operational costs and substantial computational demands. \n\nIn contrast, encoder-only models like BERT [15] and DeBERTa [23] are optimized for understanding and analyzing text, making them particularly efficient for targeted tasks such as text classification, named entity recognition, and sentiment analysis. These models typically have fewer parameters, which leads to faster inference and lower computational overhead. While encoder-only models excel in specific, narrow tasks and offer greater adaptability through efficient fine-tuning [76,63,26], they are less versatile compared to decoder-only models. However, for domain-specific applications, encoder-only models often provide a more sustainable and cost-effective solution, balancing performance with resource efficiency. \n\nThe difference in attention mechanisms is reflected in the training objectives of these models. Encoder-only models typically use Masked Language Modeling (MLM), where the objective is to predict masked tokens given their context: \n\nDecoder-only models, on the other hand, are trained to predict the next token in a sequence: \n\nEncoder models benefit from bidirectional attention, allowing them to consider the entire context of an input simultaneously, which is particularly useful for classifying completed texts. This bidirectional understanding, combined with their MLM training objective, often makes encoders preferable for many classification tasks. However, decoder models have gained prominence due to their sample efficiency, using 100% of tokens for training compared to the typical 15% masked tokens in encoder training. This efficiency, along with their versatility in reformulating tasks as text generation problems, has led to the recent trend of scaling decoder models to much larger sizes.",
            "score": 0.6308271240234617,
            "section_title": "Transformer Architectures: Encoders vs Decoders",
            "char_start_offset": 8969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 71,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368896484375
        },
        {
            "corpus_id": "267701011",
            "title": "The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives",
            "text": "The innovative transformer architecture has paved the way for the development of various LLMs, each distinguished by its unique characteristics [3]. Recent advancements in language modeling have led to the emergence of three predominant categories, classified based on the fundamental modules employed in their construction. Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial. The application of diverse processes and datasets allows for the provision of a spectrum of tools tailored to meet the evolving demands of natural language understanding and generation [5]. \n\nBefore the rise of LLMs, traditional deep-learning models grappled with numerous technical challenges, including inadequate sequence and semantic understanding, along with computational complexity. This complexity necessitated a substantial number of parameters, as evident in convolutional neural networks, to achieve satisfactory outcomes. Additionally, issues such as vanishing gradients (e.g., in recurrent neural networks) posed challenges in capturing long-term dependencies, while sequential computation impeded the efficiency of training and inference processes, particularly for extended sequences [3]. The true innovation stemmed from optimizing pre-trained language 1 3 models to suit the specific demands of chat-oriented tasks, thereby achieving enhanced performance in applications that involve dialog-finetuned versions for conversational interactions. This complex process provides the use of different approaches.",
            "score": 0.6295920984591005,
            "section_title": "Development of LLMs for Chatbots and Enhanced human-like Interaction",
            "char_start_offset": 4223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2383
                },
                {
                    "start": 2384,
                    "end": 2446
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51123046875
        },
        {
            "corpus_id": "258947629",
            "title": "How Powerful are Decoder-Only Transformer Neural Models?",
            "text": "To create a decoder-only model, the vanilla architecture is modified in two ways. First, the connection to the encoder is removed. Second, the cross-attention which allows the decoder to conditionally attend to the encoder output at each layer of the decoder is eliminated. These, along with the entire encoder, are surrounded by a dashed yellow line in 1 to visualize what is eliminated. As mentioned previously, this superficially suggests that encoder-only and decoder-only architectures are identical as seen in 2. \n\n2) Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in [1]. The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences. \n\nIf any of the above are violated, the model can't be reasonably considered a decoder-only model as it is no longer capable of auto-regressive next token prediction.",
            "score": 0.6249536155941164,
            "section_title": "1) Modifying the Vanilla Transformer to form a Decoderonly Model:",
            "char_start_offset": 3733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1807
                }
            ],
            "ref_mentions": [
                {
                    "start": 1357,
                    "end": 1360,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35009765625
        },
        {
            "corpus_id": "257482688",
            "title": "An Overview on Language Models: Recent Developments and Outlook",
            "text": "The transformer architecture [82] can capture long-term dependencies and important sequence components by exploiting a self-attention mechanism. Unlike the recurrent structure of RNNs, a transformer is easy to parallelize in both training and inference. Its structure is shown in Fig. 6. It consists of an encoder and a decoder. Before being sent to the encoder, the input textual sequence is first converted to an embedding through an embedding layer plus positional embedding. Multi-head attention, which is an ensemble of multiple self-attention mechanisms, enables the transformer to capture more robust and diverse attention between tokens. The other parts in the transformer encoder include feed-forward layers, residual connections, and normalization layers. The difference between the transformer encoder and decoder is that the transformer decoder has an additional masked multi-head attention layer. The masking ensures the decoder can only access preceding tokens of the current one, which makes the decoder auto-regressive.\n\nBased on different purposes, transformers have encoder-only, decoder-only, and encoder-decoder three variants as shown in Table 1 and Fig. 7. Encoder-only models can access all positions given an input and utilize bi-directional contexts to predict words. They are suitable for tasks requiring understanding full sentences, such as text classification.\n\nTransformer decoder-only models can only use previous words to predict the current word (namely, auto-regressive models). They are good at text generation tasks such as story generation. Transformer encoder-decoder models can access all words in the encoding phase, and words before the current word in the decoding phase. They are suitable for sequence-to-sequence tasks such as translation and summarization.",
            "score": 0.6240115710821161,
            "section_title": "Transformers",
            "char_start_offset": 21464,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41259765625
        },
        {
            "corpus_id": "258049081",
            "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
            "text": "The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.",
            "score": 0.6178151133597688,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "260435365",
            "title": "Pre-Trained Language Models for Text Generation: A Survey",
            "text": "Existing PLMs for text generation adopt either a single Transformer or a Transformer-based encoderdecoder as the backbone. PLMs, such as GPT-3 [14] and UniLM [36], use a single Transformer encoder/decoder to simultaneously implement the process of input encoding and output decoding. This includes three major variants: masked LMs, causal LMs, and prefix LMs, with different attention mask strategies. In contrast, PLMs built upon Transformer encoder-decoder perform input encoding and output decoding separately. In the following, we describe these four variants in detail. \n\n4.1.1 Masked Language Models. Masked LMs use a full-attention Transformer encoder. Equipped with the full attention, models are usually pre-trained with masked language modeling (MLM) task, i.e., predicting the masked tokens using the bidirectional information. The most representative model is BERT [35], which is used extensively in natural language understanding (NLU). However, due to the discrepancy between the pre-training task of masked LMs and the downstream generation function, masked LMs are rarely utilized for text generation tasks [198]. It is more common to use masked LMs as the encoder part for text generation, allowing to leverage the excellent bidirectional encoding capacities. For example, Rothe et al. [161] proposed to initialize both the encoder and decoder of the generation model with BERT [35], which yields comparable performance with other PLMs specially designed for text generation. \n\n4.1.2 Causal Language Models. Similar to Transformer decoder, causal LMs adopt the diagonal mask matrix. Causal LMs are designed for language modeling, which is to determine the probability of a given sequence of words occurring in a sentence. Causal LMs are straightforward for text generation, predicting the next word conditioned on all previous words. \n\nIn the literature, GPT [152] was the first causal LM for the text generation task. Then, GPT-2 [153] explored the transfer capacity of language models for zero-shot generation task, highlighting the significance of sufficient data.",
            "score": 0.6173113295026684,
            "section_title": "Standard Architecture",
            "char_start_offset": 32235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1850
                },
                {
                    "start": 1853,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1123,
                    "end": 1128,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1303,
                    "end": 1308,
                    "matchedPaperCorpusId": "198967997"
                },
                {
                    "start": 1395,
                    "end": 1399,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35791015625
        },
        {
            "corpus_id": "1179542",
            "title": "Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks",
            "text": "We emphasized the specific variants of those networks that are often used in the encoder-decoder model; a conditional language model based on RNNs (a conditional RNN-LM) and a pre-trained CNN for transfer learning. Then, we introduced the simple encoder-decoder model followed by the attention mechanism, which together form the central topic of this paper, the attention-based encoder-decoder model. \n\nWe presented four recent applications of the attention-based encoder-decoder models; machine translation (Sec. IV-A), image caption generation (Sec. IV-B), video description generation (Sec. IV-C) and speech recognition (Sec. IV-D). We gave a concise description of the attention-based model for each of these applications together with the model's performance on benchmark datasets. Furthermore, each description was accompanied with a figure visualizing the behaviour of the attention mechanism. \n\nIn the examples discussed above, the attention mechanism was primarily considered as a means to building a model that can describe the input multimedia content in natural language, meaning the ultimate goal of the attention mechanism was to aid the encoder-decoder model for multimedia content description. However, this should not be taken as the only possible application of the attention mechanism. Indeed, as recent work such as the pointer networks [65] suggests, future applications of attention mechanisms could run the range of AI-related tasks. \n\nBeside superior performance it delivers, an attention mechanism can be used to extract the underlying mapping between two entirely different modalities without explicit supervision of the mapping. From Figs. 2, 5, 7 and 8, it is clear that the attention-based models were able to infer -in an unsuperivsed way -alignments between different modalities (multimedia and its text description) that agree well with our intuition. This suggests that this type of attention-based model can be used solely to extract these underlying, often complex, mappings from a pair of modalities, where there is not much prior/domain knowledge. As an example, attention-based models can be used in neuroscience to temporally and spatially map between the neuronal activities and a sequence of stimuli [74].",
            "score": 0.616980959190446,
            "section_title": "V. RELATED WORK: ATTENTION-BASED NEURAL NETWORKS",
            "char_start_offset": 41449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2246
                }
            ],
            "ref_mentions": [
                {
                    "start": 2241,
                    "end": 2245,
                    "matchedPaperCorpusId": "12645140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325927734375
        },
        {
            "corpus_id": "268041362",
            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
            "text": "LLMs are a class of extensive artificial intelligence models characterized by their massive scale with billions of parameters [14]. Scaling up LLMs allows them to learn more intricate and accurate language representations, resulting in improved performance across diverse downstream Natural Language Processing (NLP) tasks, particularly excelling in Natural Language Generation (NLG) challenges [15,16]. The brief comparison of different structures of the LLMs mentioned can be seen in Table 1. Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. [18] The vanilla Transformer architecture [17], a sequence-to-sequence model, has emerged as a foundational framework for diverse LLMs, utilizing encoders and decoders with self-attention mechanisms as its core components, thanks to its exceptional parallelism and capacity. Based on the masking methods utilized by various attention mechanisms in the model, the current LLMs can be divided into three categories, i.e., Encoder-Decoder, Decoder-only, and Encoder-only. \n\nThe decoder-only category further includes distinctions such as causal decoders and prefix decoders, illustrated in Figure 1. \n\nIn the following subsection, we shall introduce different types of LLMs based on various Transformer architectures.",
            "score": 0.6169377472596641,
            "section_title": "GENERAL METHODS",
            "char_start_offset": 4546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44970703125
        },
        {
            "corpus_id": "263831258",
            "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain",
            "text": "While the masked encoder introduced in BERT (Devlin et al., 2019) was used in pre-training, masked reconstruction was not used in downstream tasks which mainly focused on obtaining a representation of the entire input. Thus, they focused on the bidirectional encoder architecture, and did not consider other attention masking schemes. \n\nCausal attention masks can be used to differentiate between encoding and decoding, i.e. full attention for encoding and causal attention for decoding. Dong et al. (2019) introduced various attention masking strategies for a unified Transformer architecture in the context of NLP. While the various masking strategies correspond to different downstream tasks in natural language processing (e.g. full attention/bidirectional encoding for extractive question answering and full causal/unidirectional decoding for long text generation), it is unclear which paradigm time series forecasting fits in. On the one hand, we could argue that past time steps should not attend to future time steps, on the other hand, attending to future time steps could help in extracting seasonal information for example. Figure 10 illustrates the various attention masking schemes for the masked encoder architecture.",
            "score": 0.6167721287836916,
            "section_title": "D.3 ATTENTION MASKS",
            "char_start_offset": 44444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1231
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 65,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 488,
                    "end": 506,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369873046875
        },
        {
            "corpus_id": "271600495",
            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
            "text": "In this section, we first give the mathematical description of causal LLM, prefix LLM, and our ISM. Then, we elaborate on the benefits of our ISM compared to causal and prefix LLM, respectively. In addition, we introduce how to implement our ISM on existing prefix and causal LLMs. Given a sequence of input vectors X = (x 1 , \u2022 \u2022 \u2022 , x n ), the output of standard Softmax Self-Attention (SSA) layer is \n\nwhere P corresponds to the output projection, V, K, Q corresponds to the value, key, and query transformation, respectively. \n\nSince the softmax attention of standard transformers is non-linear, its theoretical analysis becomes complicated even for a single layer. For this reason, theoretical approaches Von Oswald et al. [2023], Zhang et al. [2023] to analyze transformers have often dropped the softmax function from the attention, resorting to the Linear Self-Attention (LSA) layer, \n\n3.1.2 Causal LLM, Prefix LLM, and ISM for Multi-turn dialogue \n\nHere, we briefly introduce the attentions of causal LM, prefix LM, and ISM in their LSA version for multi-turn dialogue. Given a multi-turn dialogue sample including the prompt \n\nThe most classic form of attention is categorized as full (or bidirectional) attention, shown in equation 3.2, in which each input x j (j = 1, \u2022 \u2022 \u2022 , n) can attend to all positions. Full attention is typically used in the transformer encoder. \n\nFurthermore, another transformer decoder for in-context learning uses the auto-regressive attention \n\nwhich restricts each token x j to attend only to previous positions (and itself) from 1 to j. This restriction is due to the role of the decoder as a causal language model (causal LLM) which predicts the next token in the context of the previously generated ones. \n\nThe original transformer uses a full attention based encoder and an auto-regressive attention based decoder. However, NLP researchers have a preference for models that are either encoder-only (e.g. BERTDevlin et al.",
            "score": 0.6158526746084972,
            "section_title": "Methodology",
            "char_start_offset": 6850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41650390625
        },
        {
            "corpus_id": "261064777",
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "text": "This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture. While GPT (Radford et al., 2018) and GPT-2 (Radford et al., 2018) did not exhibit the same level of performance as GPT-3 (Brown et al., 2020), with the increase in model size and the amount of data used for pretraining, GPT-3 (Brown et al., 2020) showcased a remarkable few-shot capability that earlier models did not possess. Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b). \n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a).",
            "score": 0.613538563053057,
            "section_title": "Model Architecture",
            "char_start_offset": 12603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 179,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 179,
                    "end": 198,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 403,
                    "end": 423,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 508,
                    "end": 528,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "259203988",
            "title": "Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health",
            "text": "A language model (LM) is a statistical model that computes the (joint) probability of a sequence of words (or tokens). Research on language models has been going on for a long period of time [15]. In 2017 the transformer model introduced by Vaswani et al. [16] became the foundational architecture for most modern language models including ChatGPT. The transformer architecture includes an encoder of bidirectional attention blocks and a decoder of unidirectional attention blocks. Based on the modules used for model development, most recent LMs can be grouped into three categories: encoder-only LMs such as BERT (Bidirectional Encoder Representations from Transformers) [17] and its variants, decoder-only LMs such as the GPT (Generative Pre-trained Transformer) family [18][19][20] and encoder-decoder LMs such as T5 (Text-to-Text Transfer Transformer) [21] and BART (Bidirectional and AutoRegressive Transformers) [22]. Encoder-only and encoder-decoder language models are usually trained with an infilling (\"masked LM\" or \"span corruption\") objective along with an optional downstream task, while decoder-only LMs are trained with autoregressive language models that predict the next token given the previous tokens. \n\nAlthough the encoder-only and encoder-decoder models have achieved state-of-the-art performance across a variety of natural language processing (NLP) tasks, they have the downside that requires significant amount of task-specific data for fine-tuning the model to adapt to the specific tasks. This process needs to update the model parameters and adds complexity to model development and deployment. \n\nUnlike those models, when GPT-3 [19] was released, it demonstrated that large decoder-only language models trained on large text corpus gained significantly increased capability [23] for natural language generation. After training, the models can be directly applied to various unseen downstream tasks through in-context learning such as zero-shot, one-shot or few-shot prompting [19].",
            "score": 0.6127072509686886,
            "section_title": "Overview of General LLMs",
            "char_start_offset": 2979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 857,
                    "end": 861,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.335205078125
        },
        {
            "corpus_id": "220055529",
            "title": "FastSpec: Scalable Generation and Detection of Spectre Gadgets Using Neural Embeddings",
            "text": "Although recurrent models with attention mechanisms learn the representations of long sequences, attention-only models, namely Transformer architectures [63], are shown to be highly effective in terms of computational complexity and performance on long-range dependencies. Similar to seq2seq architecture, Transformer architecture consists of encoder-decoder model. The main difference of Transformer is that recurrent models are not used in encoder or decoder units. Instead, the encoder unit is composed of L hidden layers where each layer has a multi-head self-attention mechanism with A attention heads and a fully connected feed-forward network. The input embedding vectors are fed into the multi-head attention and the output of the encoder stack is formed by feed-forward network which takes the output of the attention sub-layer. The decoder unit also has L hidden layers, and it has the same sublayers with encoder. In addition to one multi-head attention unit and one feed-forward network, the decoder unit has an extra multi-head attention layer that processes the encoder stack output. In order to process the information in the sequence order, positional embeddings are used with token embeddings where both embedding vectors have a size of H. \n\nKeeping the same Transformer architecture, Devlin et al. [9] introduced a new language representation model called BERT (Bidirectional Encoder Representations from Transformers) which surpasses the state-of-the-art scores on language representation learning. BERT is designed to pre-train the token representation vectors of deep bidirectional Transformers. For the detailed description of the architecture, we refer the readers to [9,63]. The heavy part of the training is handled by processing unlabeled data in an unsupervised manner. The unsupervised phase is called pre-training which consists of masked language model training and next sentence prediction procedures. The supervised phase is referred to as fine-tuning where the model representations are further trained with labeled data for a text classification task. Both phases are further explained in detail for Spectre gadget detection model in Section 5. \n\n3 Related Work",
            "score": 0.6126124541448423,
            "section_title": "Transformer and BERT",
            "char_start_offset": 12589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2178
                },
                {
                    "start": 2181,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1316,
                    "end": 1319,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1691,
                    "end": 1694,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1694,
                    "end": 1697,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "211003689",
            "title": "Pseudo-Bidirectional Decoding for Local Sequence Transduction",
            "text": "which facilitates the decoder to generate \"went\" instead of \"goes\". While ideally, the encoder-decoder attention may be able to capture the information provided by the future token representation copied from the encoder, the pseudo-bidirectional decoding mechanism makes the decoder self-attention module more effective by allowing it to directly attend to future information, thus probably leads to better empirical performance. \n\nPseudo-bidirectional Attention Mask While the proposed approach is versatile and applicable to RNN-based seq2seq models, in this paper, we focus on the transformer model [Vaswani et al., 2017] which yields state-of-the-art performance in most natural language generation tasks. The pseudo-bidirectional decoding copies the representation of tokens from the N +1 th position to the end of the input sequence in the corresponding encoder layer to the decoder when predicting the N th output token. This is easy to implement for auto-regressive decoding during inference. However, a straightforward way to make the model behaves consistently during training and inference requires us to change the decoder input dynamically for each decoding step instead of feeding the entire output sequence into the decoder and use a causal attention mask to ensure the causality of the decoder. This would hinder the transformer model from being trained in parallel, thus makes the training much less efficient. \n\nTo address this problem, we propose a novel masking strategy referred as \"pseudo-bidirectional attention mask\". As il-lustrated in Figure 2, we concatenate the representation of the input sequences to that of the output sequences to form the Key and the Value in the decoder self-attention module, while the Query remains to be the representation of the output sequences. The pseudo-bidirectional attention mask makes the decoder self-attention bidirectional by allowing the query tokens to attend to pseudo future tokens copied from the encoder, thus retaining the causality of the decoder and enabling parallel training.",
            "score": 0.611798588689347,
            "section_title": "Pseudo Future Context Modeling",
            "char_start_offset": 8425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 2052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54736328125
        },
        {
            "corpus_id": "267627936",
            "title": "History, Development, and Principles of Large Language Models-An Introductory Survey",
            "text": "This taxonomy is based on the transformer architecture, the backbone of LLMs, comprising core components like the encoder, decoder, and their variations, which together enable their capabilities [77]. Specifically, encoders are designed to process and interpret input sequences, while decoders generate output sequences based on this interpretation [84]. In some models, both components are integrated into an encoder-decoder framework to handle a diverse set of tasks effectively [33]. Conversely, other models focus on specific applications by using either the encoder alone for understanding tasks or the decoder alone for text generation. The detail information about each type of models is as outlined below: \n\n\u2022 Encoder-only models, such as BERT [71], primarily focus on understanding and interpreting input data rather than generating it. They excel in tasks that require strong contextual understanding, such as sentiment analysis, named entity recognition, and text classification. These models are highly effective for analyzing text but are less suitable for text generation tasks due to their limited generation capabilities and computational intensity when handling large texts. \u2022 Encoder-decoder LLMs, like T5 [62], combine the strengths of both encoders and decoders, making them versatile and powerful for a range of applications. These models are particularly effective for tasks that require both text understanding and generation, such as machine translation, text summarization, and question answering. The encoder-decoder architecture provides a unified approach to these tasks, allowing for a comprehensive understanding and generation of text. However, the complexity of this dual architecture can result in more challenging training processes and slower inference times compared to other model types. \u2022 Decoder-only LLMs, such as GPT-3 [10], are optimized for generating text, making them ideal for content creation, conversational AI, and creative writing. These models are known for their excellent text-generation capabilities and are effective in few-shot learning scenarios, where they can generate coherent and contextually relevant responses from minimal input. However, decoder-only models can face limitations in understanding context deeply and are often resource-intensive, requiring significant computational power to generate high-quality text outputs.",
            "score": 0.6111578140480063,
            "section_title": "PRINCIPLES AND TAXONOMY OF LARGE LANGUAGE MODELS",
            "char_start_offset": 28732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2389
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "248118752"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1995849609375
        },
        {
            "corpus_id": "275544523",
            "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
            "text": "While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.",
            "score": 0.6085607077984898,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "270064259",
            "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
            "text": "The causal attention mask in decoder-only LLMs is introduced for next-token prediction task (Vaswani et al., 2017). In principle, causal mask in decoder blocks prevents information leakage by allowing the decoder to attend only to previous positions during auto-regressive text generation. However, it is observed that unidirectional attention limits the model's representation power, as evidenced by the poor performance of GPT models compared to similarly sized BERT or T5 models on natural language understanding benchmarks (e.g., Wang et al., 2019). In recent, LLM2Vec (BehnamGhader et al., 2024) introduces additional training phase with a specially designed masked token prediction to warm-up the bidirectional attention. GRIT (Muennighoff et al., 2024) utilizes a hybrid objective with both bidirectional representation learning and causal generative training. In contrast, we simply remove the causal attention mask of decoder-only LLM during the contrastive learning and find it works compellingly well as demonstrated by our results. As a result, we go with simple solution.",
            "score": 0.6079586461305985,
            "section_title": "BIDIRECTIONAL ATTENTION",
            "char_start_offset": 9939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1084
                }
            ],
            "ref_mentions": [
                {
                    "start": 534,
                    "end": 552,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492919921875
        },
        {
            "corpus_id": "277130726",
            "title": "A Survey of Large Language Model for Drug Research and Development",
            "text": "where \n\nThe attention mechanism in transformers not only enhances the efficiency of processing sequence data through self-attention and multi-head attention design, but also enhances the model's understanding and production of sequences, which is useful for a variety of NLP applications, such as text creation, machine translation, and questionanswering systems. The discipline of NLP has improved tremendously as a result of the outstanding performance in these tasks. Furthermore, based on different training strategies, the transformer framework can be categorized into three types: encoder-only models like BERT, decoder-only models, and encoder-decoder models which are complete transformers. Additionally, architectural modifications can be made by adjusting one or more layers to suit specific target tasks.",
            "score": 0.6057176803517892,
            "section_title": "B. TRANSFORMER MODEL ARCHITECTURE",
            "char_start_offset": 25123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 8,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 815
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.256103515625
        },
        {
            "corpus_id": "269009682",
            "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
            "text": "in an unsupervised manner using only a set of unordered sentences. These unsupervised approaches typically create two different representations of the same sentence for contrastive learning. The methods vary in how they form these representations -perturbing the input sentence (Wu et al., 2020), or using different model instances (Carlsson et al., 2021). SimCSE (Gao et al., 2021), the approach used in this work, generates two representations of the same sentence by passing it through the model twice with different dropout masks. \n\nTurning decoder-only LLMs into text encoders While decoder-only LLMs have outperformed bidirectional encoders across a large variety of language understanding tasks (Brown et al., 2020;Touvron et al., 2023;Jiang et al., 2023a, inter alia), their impact on sentence representation learning remains limited. The most common approaches in literature use the final hidden state of the last token as the sentence embedding (Neelakantan et al., 2022;Ma et al., 2023;Wang et al., 2023). \n\nThere are few works that explore the limitations of using a causal attention mask when adapting decoder-only LLMs for text classification and sentence representation tasks. Li et al. (2023b) experiment with removing the causal mask of Llama-2 during supervised fine-tuning for text classification and NER tasks. Similarly, Duki\u0107 & \u0160najder (2024) enable bidirectional attention for a group of layers during supervised fine-tuning on NER and chunking. In the context of sentence representation learning, Li & Li (2024) explore enabling bidirectional attention in the last layer of a decoder-only model during supervised contrastive fine-tuning on STS tasks. \n\nConcurrent to our work, several works have focused on converting decoder-only-LLMs to text encoders in supervised and unsupervised manner. Jiang et al. (2023b) and Lei et al. (2024) prompt the language model to summarize the input text in one word, and take the last layer's hidden embedding for the last token as the text's representation.",
            "score": 0.6039909990901764,
            "section_title": "Unsupervised text encoders Another line of work has explored training text embedders",
            "char_start_offset": 22008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 355,
                    "matchedPaperCorpusId": "235613354"
                },
                {
                    "start": 364,
                    "end": 382,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 702,
                    "end": 722,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 981,
                    "end": 997,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 1521,
                    "end": 1535,
                    "matchedPaperCorpusId": "265066823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28515625
        },
        {
            "corpus_id": "267759571",
            "title": "Scaling Laws Behind Code Understanding Model",
            "text": "The Transformer architecture has emerged as the predominant model to date [36]. It employs a stacking mechanism of multiple transformer blocks, each comprising a self-attention layer and an MLP layer. Two primary self-attention structures are identified: \n\n\u2022 Full attention [4,5]. This bidirectional attention operates between every pair of tokens in the sequence. Notably, both subsequent and preceding tokens can assimilate information from each other. The robust information encoding capability afforded by full attention designates the model as a transformer-encoder, specialized for tasks involving understanding. \u2022 Masking attention [2,3]. In this directional attention model, interactions occur exclusively from left to right within the token sequence. Consequently, only the subsequent token can integrate information from preceding tokens, creating a unidirectional flow. Masking attention aligns with standard causal language modeling and is denoted as a transformerdecoder, tailored for tasks centered around generation.",
            "score": 0.6019107219597292,
            "section_title": "Transformer Architecture",
            "char_start_offset": 6391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 254
                },
                {
                    "start": 257,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1031
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 277,
                    "end": 279,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 639,
                    "end": 642,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5048828125
        },
        {
            "corpus_id": "273549296",
            "title": "Smart ETL and LLM-based contents classification: the European Smart Tourism Tools Observatory experience",
            "text": "Generative AI (GenAI) is a currently popular subset of AI that involves algorithms that generate new content based on their training data, including images, text, and audio. Large Language Models (LLMs) are a specific category of generative models explicitly designed to understand, generate, and manipulate human language. Among these, Transformer-based models [18] have gained prominence, which, according to NVIDIA, \"70 percent of arXiv papers on AI posted in the last two years mention transformers\" (March 25, 2022). These models effectively capture context and dependencies using self-attention mechanisms and excel at NLP (Natural Language Processing) tasks, text generation, and context understanding. \n\nThe transformer architecture can be divided into5: \n\n\u2022 Encoder: takes a sequence of words (like a sentence or paragraph) and processes it to understand the meaning and context of each word in relation to the others. It generates vectors of numerical representations (embeddings) that represent the input in a way the machine can understand. Among the foundational models6 in the encoder-only architecture, BERT (Bidirectional Encoder Representations from Transformers) [5] stands out. Learning from left and right contexts during pre-training improves its understanding of natural language. \n\nIt is well suited for text classification, question answering, and other comprehension-based applications. \n\n\u2022 Decoder: takes the encoded information (the embeddings) and generates a new sequence of words. It produces the output one word at a time, considering the context provided by the encoded information. GPT (Generative Pre-trained Transformer) [12], a well-known foundational model of the decoder-only architecture, generates text by predicting the next word in a sequence, making it suitable for various generative tasks such as text generation, language modeling, and conversational agents. \n\nCombining both components results in encoder-decoder models that use both an encoder to process the input and a decoder to generate the output. The transformer model was the one that introduced this architecture.",
            "score": 0.6018840292406612,
            "section_title": "Generative AI and large language models",
            "char_start_offset": 14197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1904
                },
                {
                    "start": 1907,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1181,
                    "end": 1184,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2049560546875
        },
        {
            "corpus_id": "237491949",
            "title": "Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions",
            "text": "Recently, Transformer-based models (Vaswani et al., 2017) have allowed huge improvements in performance across multiple NLP tasks. The inclusion of this architecture has led the field of NLP to investigate the inner workings of this architecture in several tasks. One of its core components, the attention mechanism, which provides a distribution of scores over the input tokens, has been often presented as showing the relative importance of the inputs. Some works have criticized the use of attention weights as model explanations (Jain and Wallace, 2019;Serrano and Smith, 2019;Pruthi et al., 2020), demonstrating that attention weights distributions can be modified without affecting the final prediction. However, these studies have mainly analyzed encoder-only or decoder-only architectures like BERT (Devlin et al., 2019) or GPT-2 (Radford et al., 2019), which are based on self-attention mechanisms.\n\nNonetheless, NMT models use the encoderdecoder Transformer architecture, which adds the encoder-decoder attention mechanism, in charge of distributing the information flow from the encoder representations of the source input tokens into the decoder. (Voita et al., 2019) analyze the effect of pruning different attention heads in a Transformer NMT model and conclude that the encoderdecoder attention mechanism is the most critical one. (Raganato et al., 2020) show that encoder selfattention weights can be interchanged by predefined non-learnable patterns without hindering the translation performance. These results provide evidence about the relevance of the encoder-decoder attention mechanism on NMT, which we believe needs further investigation. In this work we analyze the encoder-decoder attention weights and shed light on their impact on the decoder representations and final predictions, showing how alignment errors can also give information about the model's decision-making process.\n\nResearch in NMT interpretability has mainly focused on understanding source words importance when predicting a target word. The word alignment task (Och and Ney, 2003) has served to compare explanation methods against human-annotated source-target word alignments. Encoder-decoder attention weights have been used to provide sourcetarget word alignments (Zenkel et al",
            "score": 0.6016880265158935,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 57,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 533,
                    "end": 557,
                    "matchedPaperCorpusId": "67855860"
                },
                {
                    "start": 557,
                    "end": 581,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 581,
                    "end": 601,
                    "matchedPaperCorpusId": "202583616"
                },
                {
                    "start": 807,
                    "end": 828,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1159,
                    "end": 1179,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 1346,
                    "end": 1369,
                    "matchedPaperCorpusId": "211259041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343017578125
        },
        {
            "corpus_id": "275544523",
            "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
            "text": "We categorize the input tokens as either context tokens or span tokens and use the attention mask shown in Figure 2. Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details). \n\nDuring training, an input sequence includes one or more spans of span tokens surrounded by context tokens. During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling.",
            "score": 0.6002155759966012,
            "section_title": "Modifying Attention",
            "char_start_offset": 8300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76513671875
        },
        {
            "corpus_id": "249974508",
            "title": "Controllable protein design with language models",
            "text": "Transformers are a current revolution in NLP. Their success derives from the evolution of a series of concepts built on top of each other, with the attention mechanism possibly being the most notable of these advances. The attention mechanism originated as a solution to traditional sequence-to-sequence (seq2seq) models, which are widely used for tasks that process sequences from one domain to another, such as machine translation or text summarization. In seq2seq models, the input is stepwise-processed in a module termed encoder to produce a context vector passed to a decoder, which is responsible for generating an output (Fig. 3a). Traditionally, encoder and decoder architectures have usually been RNNs or LSTMs (Fig. 2b), and the context vector corresponded to the final output of the last encoder step (word) (Fig. 2b). Due to this inherently sequential nature, RNNs presented the major drawback of degrading performance and increasing training times with sequence length 15 . The introduction of the attention mechanism provided a solution to this problem by allowing the decoder to analyse the whole input and focus on specific parts of it, a notion similar to attention in the human mind. A simplified example in English-French translation would be focusing on the input word 'home' , when outputting the word 'maison' (Fig. 3a).\n\nAlthough attention mechanisms had been ubiquitously applied in many types of neural networks, they became particularly prominent in 2017, when researchers at Google published their seminal work ' Attention is all you need' , which introduced an architecture that not only applied attention between the modules but also throughout them 39 . This new design permitted the encoder to focus on specific parts of the input sequence, producing a much better performance in many tasks 39 . The model was termed the Transformer and gave its name to all similar architectures that followed in subsequent years (Fig. 2a).\n\nThe transformer's encoder and decoder modules contain a stack of six submodules or layers (N) that process inputs from the previous layer in a parallel fashion, enabling much faster training times (Fig. 3b). The encoder submodules contain two layers: a self-attention layer, which applies the attention mechanism to the input sequence itself,",
            "score": 0.6001752079355862,
            "section_title": "attention mechanism and transformers",
            "char_start_offset": 16963,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 983,
                    "end": 985,
                    "matchedPaperCorpusId": "3397190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2509765625
        },
        {
            "corpus_id": "267617160",
            "title": "A Survey on Transformer Compression",
            "text": "Table 7 presents the model cards of several representative LLMs and LVMs with public details. Vanilla Transformer [11] is based on the encoder-decoder architecture [133]. In this architecture, the encoder stacks multi-head self-attention layers to encode the input sequence into latent representations, while the decoder performs cross-attention on these representations and generates target sequences in an autoregressive manner. BERT [91] is designed to pre-train deep bidirectional representations from unlabeled text by considering both left and right context in all layers. GPT-series models [4], [2] have effectively demonstrated the power of incontext learning using decoder-only architectures [147], [179], [180]. These decoder architectures incorporate unidirectional attention masks to ensure that each input token can only attend to previous tokens, including itself. The input and output tokens are processed in a similar manner within the decoder. In the following, we will provide separate reviews from the perspectives of CV and NLP on the current structural innovations in Transformer-based architectures.",
            "score": 0.5998515356992038,
            "section_title": "Representative Transformer-Based Models",
            "char_start_offset": 54490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1121
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 164,
                    "end": 169,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 597,
                    "end": 600,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25537109375
        },
        {
            "corpus_id": "268247581",
            "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
            "text": "The standard Transformer [115] and its variants [139], [140] adopt the encoder-decoder framework (Figure 6d),where the encoder processes and contextualize the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention. Models such as BART [139] and T5 [140] are with this architecture and demonstrating its effectiveness across a broad spectrum of applications. In biomolecule and text cross modeling scenario, this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models. This enriched interaction allows for a deeper understanding of the input sequences. Subsequently, the encoder-decoder attention mechanism empowers the decoder to generate outputs tailored for specific biological tasks. For example, MolT5 [21] shows strong retrieval and generation abilities between molecule and text on several downstream tasks. Similarly, the advanced BioT5 [22] and BioT5+ [107] also take the T5 framework and achieve much superior performances on both biomolecule understanding and generation tasks.",
            "score": 0.5981869886752406,
            "section_title": "Encoder-decoder",
            "char_start_offset": 33192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1197
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 30,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 48,
                    "end": 53,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 55,
                    "end": 60,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 304,
                    "end": 309,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 317,
                    "end": 322,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "248376906"
                },
                {
                    "start": 1054,
                    "end": 1058,
                    "matchedPaperCorpusId": "263834780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5068359375
        },
        {
            "corpus_id": "245385261",
            "title": "Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization",
            "text": "Below, we describe these two attention mechanisms. \n\n(i) The Bidirectional Self-Attention Mechanism: In the original BERTSUM architecture, the bidirectional self-attention mechanism (Devlin et al., 2019) is utilized by the BERT encoder to generate the encoded representation of the input text. In the bidirectional self-attention mechanism, when a pair of sentences are combined together and given as input to the BERT encoder, both sentences will give attention to each other. Thus, when we utilize the bidirectional self-attention mechanism (see Figure 2a) in the PreQFAS model, both the query and the document will not only give attention to themselves, but also they will give attention to each other to provide the encoded representation of the concatenated input. \n\n(ii) The Query-Document Attention Mechanism: Dong et al. (2019) proposed the sequence-tosequence language modeling objective for text sequences that are consisted of two segments. In such text sequences, each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment. Following this approach, we propose the Query-Document (QD) attention mechanism, where each token in the query can only attend to the tokens which are within the query while the tokens in the document can attend to all tokens in both the query and the document bidirectionally. The intuition here is that in the original PreQFAS model, the bidirectional self-attention allows the query to also attend to the document and thus the query segment might get influenced by the document segment. As a consequence, the final encoded representation of the concatenated input may lose some query related information and the decoder may produce summaries that may not be fully relevant to the query. \n\nTo avoid such scenarios, we allow the query segment to only attend to itself whereas the document segment is allowed to provide a query focused representation by attending to both the query and to itself.",
            "score": 0.5973880084921255,
            "section_title": "The PreQFAS model for the SD-QFAS task",
            "char_start_offset": 24575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 53,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 203,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 817,
                    "end": 835,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.642578125
        },
        {
            "corpus_id": "249192274",
            "title": "E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation",
            "text": "In recent years, we have witnessed numerous pretrained language models (PLMs) that achieved tremendous success in the community of NLP. Based on the model architectures, these PLMs can be classified into three main categories: decoder-only (autoregressive) LMs, encoder-only (auto-encoding) LMs and encoderdecoder (sequence-to-sequence) LMs. Auto-regressive LMs aim to predict the future words towards a sequence of words, such as GPT [19] and its variants. Such auto-regressive models are wellsuitable for language generation, but they are unidirectional and usually fail short in the representation learning for understanding the sentence [20]. Thus, researchers turn to focus on autoencoding LMs that introduce a bidirectional masked language modeling (MLM) objective to predict the masked text token based on the context. The most representative auto-encoding LMs are BERT [13] and its variants, e.g., ROBERTA [14] and DeBERTa [15]. \n\nIn order to combine the advantages of auto-regressive LMs and auto-encoding LMs, seq2seq LMs are sequentially proposed, which firstly employ a separate encoder to model the source text and then use a left-to-right LM to decode the conditional target text. For example, as the typical seq2seq LMs, BART [3] and T5 [4] first corrupt the text with various noising functions on the encoder side and then train the models to reconstruct the original text in an auto-regressive manner. The encoder-decoder paradigm makes the seq2seq LMs not only generally suitable for text generation, but also well for text understanding tasks. In practice, for text understanding tasks, BART employs the final hidden state of the final decoder token as the sentence representation and introduces an additional MLP layer to output the prediction, while T5 converts the classification tasks as \"text-to-text\" generation tasks and directly generates the target texts, e.g., sentiment polarity for sentiment analysis task.",
            "score": 0.5963509335522507,
            "section_title": "Pretrained Language Models",
            "char_start_offset": 7514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 645,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1241,
                    "end": 1244,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2225341796875
        },
        {
            "corpus_id": "267301268",
            "title": "Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling",
            "text": "Pre-trained language models (PLMs) built upon the Transformer architecture have demonstrated exceptional performance across many natural language understanding (NLU) tasks (Radford et al., 2018;Devlin et al., 2019;Yang et al., 2019;Clark et al., 2020;Raffel et al., 2020). Typically, achieving state-of-the-art (SOTA) results in tasks such as sequence classification and sequence labeling Figure 1: Layer-wise causal mask removal from decoder block groups in a decoder-only LLM. Here, the causal mask is removed from the top eight decoder blocks of the Llama2-7B model to enable bidirectionality during fine-tuning, which proves beneficial for many SL tasks. \n\ninvolves a two-step process: pre-training on unlabeled corpora, followed by fine-tuning on taskspecific data -a process often referred to as transfer learning (Ruder et al., 2019;Raffel et al., 2020). Two prevailing architectures emerged, each coupled with a compatible pre-training paradigm: (1) the decoder-only architecture, utilizing causal language modeling (CLM) for pre-training, and (2) the encoder-only architecture, with the masked language modeling (MLM) pre-training objective. \n\nIn transfer learning experiments that juxtapose models of a comparable number of parameters, MLM-based encoders consistently outperformed CLM-based decoders on NLU tasks (Devlin et al., 2019).2 However, a shift in strategy emerged within the NLP community when encoder models ceased being scaled up to the same magnitude of parameters and pre-training data as their decoder counterparts. Consequently, there has been a pronounced trend toward scaling decoder models to multiple billion parameters, leading to a proliferation of large language models (LLMs). Combining LLM text generation capabilities with various prompting strategies can boost the performance on many NLU tasks, eliminating the need for fine-tuning model parameters (Liu et al., 2023).",
            "score": 0.5963451771838766,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 214,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 214,
                    "end": 232,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 251,
                    "end": 271,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 820,
                    "end": 840,
                    "matchedPaperCorpusId": "186206211"
                },
                {
                    "start": 840,
                    "end": 860,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1323,
                    "end": 1344,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2335205078125
        },
        {
            "corpus_id": "255546225",
            "title": "A Survey of Learning-based Automated Program Repair",
            "text": "Existing pre-trained models generally adopt the encoder-decoder transformer architecture, which can be classified into three types: encoder-only, decoder-only, and encoder-decoder models. Encoderonly models (e.g., CodeBERT [48]) usually pre-train a bidirectional transformer where tokens can attend to each other. Encoder-only models are good at understanding tasks (e.g., code search), but their bidirectionality nature requires an additional decoder for generation tasks. Decoderonly models (e.g., CodeGPT [22]) are pre-trained using unidirectional language modeling that only allows tokens to attend to the previous tokens and themselves to predict the next token. Decoder-only models are good at auto-regressive tasks like code completion, but the unidirectional framework is sub-optimal for understanding tasks. Encoder-decoder models (e.g., CodeT5 [152]) often make use of denoising pre-training objectives that corrupt the source input and require the decoder to recover them. Compared to encoder-only and decoder-only models that favor understanding and auto-regressive tasks, encoder-decoder models can support generation tasks like code summarization.\n\nInspired by the success of pre-trained models in NLP, many recent attempts have been adopted to boost numerous code-related tasks (such as program repair) with pre-trained models (e.g., CodeBERT) [48,54]. In the context of APR, an encoder stack takes a sequence of code tokens as input to map a buggy code = [ 1 , . . . , ] into a fixed-length intermediate hidden state, while the decoder stack takes the hidden state vector as an input to generate the output sequence of tokens = [ 1 , . . . , ]. Researchers treat the APR problem as a generation task, and consider encoder-decoder or encoder-only (with an additional decoder) pre-trained models, which are usually evaluated by the BFP dataset from Tufano et al. [177].\n\nWe summarize",
            "score": 0.5960614833795332,
            "section_title": "Universal Pre-trained Model-based APR Techniques",
            "char_start_offset": 119048,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 854,
                    "end": 859,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 1363,
                    "end": 1366,
                    "matchedPaperCorpusId": "221761146"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275634765625
        },
        {
            "corpus_id": "258546803",
            "title": "Stylized Data-to-text Generation: A Case Study in the E-Commerce Domain",
            "text": "The transformer [45] model, which consists of an encoder and a decoder, was initially proposed for machine translation. The encoder learns interactions between the left and right context words with the bidirectional attention mechanism, while the decoder is designed to generate the target translation text using the unidirectional attention mechanism. Due to the remarkable capability of attention mechanisms, many transformer-based models have emerged in the NLP domain. According to the architecture, current transformer-based models for NLP can be roughly classified into three groups: transformer encoder-based models, transformer decoder-based models, and full transformer-based models [34]. The typical transformer encoder-based models are BERT [9] and RoBERTa [26], both of which adopt the transformer encoder as the backbone and are trained by the masked language modeling technique. Regarding the transformer decoder-based model, a classic example is GPT [35], which is developed based on the transformer decoder and achieves comparable performance with the above two models (i.e., BERT and RoBERTa) on many natural language understanding tasks. Regarding the full transformer-based model, BART [22] and T5 [36] are two representative models that were proposed for tackling natural language generation tasks. \n\nTo date, these transformer-based models have achieved compelling success in various NLP tasks, such as movie script generation [57], personalized answer generation [8], and query suggestion [30]. Inspired by their remarkable performance, we utilize the transformer encoder and decoder for our text representation and generation, respectively.",
            "score": 0.5954832750217066,
            "section_title": "Transformer-based Models for Natural Language Processing",
            "char_start_offset": 14772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1663
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 20,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 752,
                    "end": 755,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1205,
                    "end": 1209,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1448,
                    "end": 1452,
                    "matchedPaperCorpusId": "246445772"
                },
                {
                    "start": 1485,
                    "end": 1488,
                    "matchedPaperCorpusId": "245502546"
                },
                {
                    "start": 1511,
                    "end": 1515,
                    "matchedPaperCorpusId": "244591851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419921875
        },
        {
            "corpus_id": "267028408",
            "title": "APPT: Boosting Automated Patch Correctness Prediction via Fine-Tuning Pre-Trained Models",
            "text": "Recently, Pre-trained models (e.g., BERT) have significantly improved performance across a wide range of natural language processing (NLP) tasks, such as machine translation and text classification [29]- [31]. Typically, the models are pre-trained to derive generic language representations by self-supervised training on large-scale unlabeled data and then are transferred to benefit multiple downstream tasks by fine-tuning on limited data annotation. \n\nExisting pre-trained models usually adopt the encoderdecoder architectures, where an encoder encodes an input sequence as a fixed-length vector representation, and a decoder generates an output sequence based on the input representation. Encoder-only models (e.g., BERT [29]) usually pre-train a bidirectional transformer in which each token can attend to each other. Encoder-only models are good at understanding tasks (e.g., code search), but their bidirectionality nature requires an additional decoder for generation tasks, where this decoder initializes from scratch and cannot benefit from the pre-training tasks. Decoder-only models (e.g., GPT [30]) are pre-trained using unidirectional language modeling that only allows tokens to attend to the previous tokens and itself to predict the next token. Decoder-only models are good at auto-regressive tasks like code completion, but the unidirectional framework is suboptimal for understanding tasks. Encoder-decoder models (e.g., T5 [31]) often make use of denoising pre-training objectives that corrupt the source input and require the decoder to recover them. Compared to encoder-only and decoderonly models that favor understanding and auto-regressive tasks, encoder-decoder models can support generation tasks like code summarization. In this work, we treat the patch correctness assessment as a binary classification task, and we consider encoder-only models to get embeddings of code snippets according to existing work [32].",
            "score": 0.5946945711030877,
            "section_title": "Pre-trained Model",
            "char_start_offset": 10618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 208,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1107,
                    "end": 1111,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1444,
                    "end": 1448,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272216796875
        },
        {
            "corpus_id": "258049081",
            "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
            "text": "Though the decoder-only Language Model (LM) is simply a decoder, it is still difficult to be compared with an Encoder-Decoder (ED) structure because this decoder handles both the source sequence and the target sequence together. To facilitate the comparison between the ED and LM structure, we propose to analyze a Regularized Encoder-Decoder (RED) framework as illustrated in Figure 2. It is a variant of the traditional ED framework while replicating the behaviors of an LM. Compared with the traditional ED structure, the RED framework mainly has the following different components: An unidirectional cross attention attends to both the source matrix and the target matrix simultaneously; a source auto-encoder recovers the input source; a parameter sharing mechanism shares the parameters between the encoder and the decoder; a layer-wise coordination component makes each decoder layer attending to the corresponding encoder layer output; a consecutive positional encoding utilizes a positional encoding starting from the length of the source tokens in the decoder. \n\nUnidirectional Cross Attention. The main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features. To simulate this mechanism in the LM, as illustrated in Figure 2, the RED framework uses unidirectional cross attention ATT l which attends to both the source matrix G E l and the target matrix G D l simultaneously. Since it attends to all features with one attention, the output matrix Q D l of the attention layer becomes less sensitive to the input source matrix G E l especially when it has already generated many words and G D l becomes relatively long.",
            "score": 0.5932544491672942,
            "section_title": "Regularized Encoder-Decoder",
            "char_start_offset": 9858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2998046875
        },
        {
            "corpus_id": "263134374",
            "title": "Augmenting LLMs with Knowledge: A survey on hallucination prevention",
            "text": "The Transformer architecture [19] marked a groundbreaking advancement in the field of NLP. Since its inception, Transformers have become the backbone of various state-ofthe-art language models, underpinning many of the recent developments in the realm of augmented language models. At its core, the Transformer architecture revolutionized sequence-to-sequence modeling through the introduction of the attention mechanism. Unlike earlier recurrent neural networks (RNNs) [21] [22] and convolutional neural networks (CNNs) [23], Transformers rely on self-attention mechanisms to capture dependencies between elements in a sequence, making them highly parallelizable and efficient for processing long-range dependencies. \n\nThe architecture consists of two main components: the encoder and the decoder. The encoder processes the input sequence, while the decoder generates the output sequence. Each component comprises multiple layers, with each layer containing a multi-head self-attention mechanism and feedforward neural networks. These self-attention mechanisms enable Transformers to capture contextual information efficiently, making them ideal for tasks that involve understanding and generating sequences of data. \n\nIn the context of language modeling, Transformers can be adapted to function as decoder-only models. In decoderonly Transformers, the encoder component, which is used for encoding input sequences, is removed. These models retain the core Transformer architecture but focus exclusively on generating sequences of tokens, making them particularly suitable for autoregressive language modeling tasks. \n\nDecoder-only Transformers operate in an autoregressive manner. They generate sequences one token at a time, with each token's prediction conditioned on the previously generated tokens. This autoregressive approach allows them to produce coherent and contextually relevant text. Decoder-only Transformers have been instrumental in various text generation tasks, including machine translation, text summarization, and text completion. \n\nSince the introduction of the Transformer architecture, numerous variants and extensions have emerged, each tailored to address specific challenges in NLP. These variants include models such as BERT (Bidirectional Encoder Representations from Transformers) [24], GPT (Generative Pre-trained Transformer) [18] [2], and T5 (Text-to-Text Transfer Transformer) [25], among others.",
            "score": 0.5932083790270963,
            "section_title": "D. Transformers",
            "char_start_offset": 7494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2052
                },
                {
                    "start": 2055,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 475,
                    "end": 479,
                    "matchedPaperCorpusId": "15375627"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "2542741"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33984375
        },
        {
            "corpus_id": "1008367",
            "title": "Encoder-decoder with focus-mechanism for sequence labelling based spoken language understanding",
            "text": "Inspired by the success of the attention mechanism [16] in Natural Language Processing (NLP) field, we first applied an attention-based encoder-decoder [17] to treat the sequence labelling based SLU as a language translation problem. In order to consider the previous and the future information, we modelled the encoder with a bidirectional LSTM (BLSTM), and the decoder with an unidirectional LSTM. The attention mechanism takes the weighted average of scores provided by the matches between inputs around position A and output at position B. There are two main limitations of attention model in sequence labelling task: \n\n\u2022 Input and output in the sequence labelling are aligned while the attention model scores the overall input words. \n\n\u2022 The alignment could be learned by the attention model, but is difficult to approach with limited annotated data in sequence labelling task (unlike Machine Translation in which paired data is easier obtained). \n\nTo address the limitations of the attention mechanism in sequence labelling, we propose the focus mechanism which is emphasizing the aligned encoder's hidden states. The remainder of the paper is organized as follows. Section 2 discusses related research. Section 3 describes the BLSTM-LSTM based the encoder-decoder, the attention and focus mechanisms. Section 4 reports the experiment results. Finally, Section 5 draws conclusions.",
            "score": 0.5920197947226666,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1387
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "14223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.436767578125
        },
        {
            "corpus_id": "251719567",
            "title": "PanDa: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation",
            "text": "In recent years, we have witnessed numerous Transformer [15]-based pretrained language models (PLMs) [1], [2], [3], [16], [5], [4], [17], [18], [19], [20] that achieved tremendous success in various natural language understanding (NLU) and generation (NLG) tasks. Based on the model architectures, these PLMs can be classified into three groups: 1) decoder-only PLMs (e.g., GPT-3 [5]), 2) encoder-only PLMs (e.g., BERT [1]), and 3) encoder-decoder PLMs (e.g., T5 [4]). \n\nDue to different pretraining functions, these PLMs exhibit different abilities when performing NLP tasks. Specifically, decoder-only PLMs generally adopt the auto-regressive language modeling objectives, which aim to predict the future words towards a sequence of words [5]. The representative decoder-only PLMs are GPT-3 [5] and its variants. Such autoregressive models are well-suitable for language generation tasks, but they are unidirectional and usually fall short in the representation learning for understanding the sentence [21]. Thus, researchers turn to focus on discriminative (encoderonly) PLMs that introduce a bidirectional masked language modeling (MLM) objective [1] to predict the masked text token based on the context. The most representative encoder-only PLMs are BERT [1] and its variants, e.g., RoBERTa [2] and DeBERTa [3]. Additionally, to combine the advantages of decoder-only PLMs and encoder-only PLMs, encoder-decoder PLMs are sequentially proposed (e.g., T5 [4] and BART [17]), which firstly employ a separate encoder to model the source text and then use a left-to-right LM to decode the conditional target text.",
            "score": 0.591951207807752,
            "section_title": "II. RELATED WORKS A. Pretrained Language Model",
            "char_start_offset": 5309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1614
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 104,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "258865774"
                },
                {
                    "start": 419,
                    "end": 422,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 1151,
                    "end": 1154,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244384765625
        },
        {
            "corpus_id": "265033213",
            "title": "An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology",
            "text": "On the contrary, decoder-only models like GPT predict the next word based on preceding words, making them suitable for text generation tasks. This architecture is inherently designed for generating new sequences, given a context. Finally, encoder-decoder models, such as T5 (38) and Bidirectional Auto-Regressive Transformers (BART) (39), combine both approaches. They use an encoder to process the input text and a decoder to generate the output, providing a balance between context understanding and sequence generation. T5 casts all tasks as a text-to-text problem, predicting output text from input, while BART is trained to reconstruct original text from a corrupted version, allowing it to generate more coherent text. Despite this broad categorization, many models combine specific elements of the encoder and decoder architecture to fulfill specific goals. The rationale behind the adoption of a particular architecture, be it encoder-only, decoder-only, or a combination of both, revolves around the model's goal and the nature of the linguistic task it aims to excel in. \n\nIn spite of the remarkable language understanding capabilities, these models still require task-specific fine-tuning to attain optimal performance (40). This fine-tuning process entails exposing the model to data pertaining to the target task and modifying its parameters accordingly. The employment of these models has considerably diminished the necessity for copious amounts of task-specific data and enabled the development of highly precise and efficient NLP systems for various applications, such as language translation, question answering, and sentiment analysis (41). LLMs, especially those like GPT-3 and beyond, have demonstrated potential for \"zero-shot\" or \"few-shot\" learning. In a zero-shot scenario, these models attempt tasks they've never explicitly trained on, leveraging their extensive pre-training on vast datasets. For instance, ChatGPT, when used in a generalized context like chat-based applications, can generate responses in a manner that seems it was fine-tuned for a plethora of tasks. This vast and implicit understanding enables it to generalize impressively across numerous tasks.",
            "score": 0.5911476805077078,
            "section_title": "Large Language Models",
            "char_start_offset": 16583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "236964637"
                },
                {
                    "start": 1654,
                    "end": 1658,
                    "matchedPaperCorpusId": "229702498"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.161376953125
        },
        {
            "corpus_id": "258685677",
            "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
            "text": "Following the success of large language models (LLMs) such as BERT [Devlin et al., 2019] and GPT [Radford et al., 2019] in natural language processing (NLP), recent years witness a surge of research work of LLMs in the code domain, leading to new SoTA results on a wide spectrum of code-related tasks. Typically, code-based LLMs can be categorized into three architectures: encoderonly models [Feng et al., 2020, Guo et al., 2021, Wang et al., 2022a], decoder-only models [Lu et al., 2021, Chen et al., 2021, Fried et al., 2022, Nijkamp et al., 2023b], and encoder-decoder models [Ahmad et al., 2021, Wang et al., 2021b, Niu et al., 2022, Chakraborty et al., 2022]. For encoder-only and decoder-only models, they are often ideal for either understanding tasks such as code retrieval [Husain et al., 2019] or generation tasks such as code synthesis [Chen et al., 2021, Hendrycks et al., 2021] respectively. For encoder-decoder models, they can be adapted to both code understanding and generation but do not always achieve better performance [Wang et al., 2021b, Ahmad et al., 2021] than decoder-only or encoder-only models. In this work, we propose a new family of encoder-decoder code large language models that can flexibly operate in various modes, including encoder-only, decoder-only, and encoder-decoder models. \n\nPrior code LLMs are also limited by their pretraining tasks, which are not perfect to transfer the models to some downstream tasks.",
            "score": 0.5911287763975052,
            "section_title": "Related Work",
            "char_start_offset": 6554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1451
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 88,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 97,
                    "end": 119,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 393,
                    "end": 411,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 429,
                    "end": 450,
                    "matchedPaperCorpusId": "248512635"
                },
                {
                    "start": 472,
                    "end": 488,
                    "matchedPaperCorpusId": "231855531"
                },
                {
                    "start": 527,
                    "end": 551,
                    "matchedPaperCorpusId": "252668917"
                },
                {
                    "start": 580,
                    "end": 599,
                    "matchedPaperCorpusId": "232185260"
                },
                {
                    "start": 599,
                    "end": 619,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 619,
                    "end": 637,
                    "matchedPaperCorpusId": "246077487"
                },
                {
                    "start": 637,
                    "end": 664,
                    "matchedPaperCorpusId": "249674577"
                },
                {
                    "start": 1041,
                    "end": 1060,
                    "matchedPaperCorpusId": "237386541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10577392578125
        },
        {
            "corpus_id": "240264395",
            "title": "Entity Linking of Sound Recordings and Compositions with Pre-trained Language Models",
            "text": "Language Models (LMs) use various statistical and probabilistic techniques to determine the probability of a particular sequence of words occurring in a sentence. They analyze text segments to provide a basis for their word prediction. Recently, LMs have been used in many natural language processing (NLP) tasks and have produced great results. \n\nOne of the most successful LM is BERT (Bidirectional Encoder Representations from Transformers), published by researchers at Google AI Language (Devlin et al., 2018). It has achieved top results on many NLP tasks, such as Question Answering, Natural Language Inference, etc (Jiang and de Marneffe, 2019;Qu et al., 2019). The biggest technical innovation of BERT is the application of the bidirectional training of Transformer, a popular attention model, to language modeling. This has enabled a deeper sense of language context and improved on the old technique of looking at a text sequence from left to right. Transformer (Vaswani et al., 2017) is an attentional mechanism that learns contextual relations between words in a text. It involves two separate mechanisms, an encoder that reads the text input and a decoder that makes a prediction for the task. Since BERT is a Language Model, only the encoder mechanism is used. The transformer encoder reads the entire word sequence of words at once and is therefore not directional. This feature allows the model to learn the context of a word based on the entire sequence.",
            "score": 0.5905458243545789,
            "section_title": "Language Models",
            "char_start_offset": 9083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1471
                }
            ],
            "ref_mentions": [
                {
                    "start": 622,
                    "end": 651,
                    "matchedPaperCorpusId": "202789710"
                },
                {
                    "start": 651,
                    "end": 667,
                    "matchedPaperCorpusId": "153312701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3984375
        },
        {
            "corpus_id": "51977850",
            "title": "Hierarchical Attention: What Really Counts in Various NLP Tasks",
            "text": "In recent years, long short-term memory, convolutional and recurrent networks with attention mechanisms have been very successful on a variety of NLP tasks. Most of the tasks In NLP can be formulated as sequence to sequence problems. Moreover, encoders and decoders are vital components in sequence to sequence models while processing the inputs and outputs. \n\nAn Attention mechanism works as a connector between the encoders and decoders and help the decoder to decide which parts of the source text to pay attention to. Thus an attention mechanism can integrates sequence models and transduction models, so it is able to connect two words in a single passage or paragraph without regard to their positions. \n\nAttention mechanisms have become integral components in various NLP models. For example, in machine translation tasks, attention mechanism-based models [1,2,3] have ever been the state-ofthe-art; in sentence embedding, self attention based model is now the state-of-the-art [4]; in machine reading comprehension, almost every recently-published model, such as BIDAF [5], Match-LSTM [6], Reinforcement Ranker Reader [7], and R-NET [8], contains attention mechanism; in abstractive summarization model [12] which has also once been the state-of-the-art, attention mechanism is very necessary, and in poem generation [9], attention mechanism is also widely used. More surprisingly, Vaswani, et al. (2017) showed that their model Transformer which relies solely on the attention mechanisms can outperform those RNN or LSTM-based existing models in machine translation tasks. Thus, they stated that \"Attention is all you need\". However, we note that the potential issue with the existing attention mechanisms is that the basic attention mechanism learns only the low-level features while the multi-level attention mechanism learns only the high-level features. This may make it difficult for the model to capture the intermediate feature information, especially when the source texts are long. In order to address this issue, we present Ham which introduces a hierarchical mechanism into the existing multi-level attention mechanisms.",
            "score": 0.5889901213124069,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 863,
                    "end": 866,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 868,
                    "end": 870,
                    "matchedPaperCorpusId": "19244028"
                },
                {
                    "start": 1141,
                    "end": 1144,
                    "matchedPaperCorpusId": "12501880"
                },
                {
                    "start": 1390,
                    "end": 1412,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333251953125
        },
        {
            "corpus_id": "250492705",
            "title": "A new hope for network model generalization",
            "text": "In this section, we introduce attention, the mechanism behind Transformers; detail the idea of pre-training and fine-tuning; and present insights from adapting Transformers to CV. \n\nSequence modeling with attention Transformers are built around the attention mechanism, which maps an input sequence to an output sequence of the same length. Every output encodes its own information and its context, i.e., information inferred from related elements in the sequence. For a detailed explanation, we refer to [35] and excellent online guides [6,18]. Computing attention is efficient as all elements in the sequence can be processed in parallel with matrix operations that are highly optimized on most hardware. While attention originated as an improvement to recurrent neural networks (RNNs), Vaswani et al. [35] realized that it could replace them entirely. The authors propose an architecture for translation tasks that contains: an embedding layer mapping words to vectors; a transformer encoder encoding the input sequence; and a transformer decoder generating an output sequence based on the encoded input (Fig. 2a). Each transformer block alternates between attention and linear layers, i.e., between encoding context and refining features. \n\nPre-training and fine-tuning Transformers are used for a wide range of NLP tasks, and the prevailing strategy is to use pre-training and fine-tuning. We explain this approach on the example of BERT [13], one of the most widely used transformer models. BERT uses only the transformer encoder, followed by a small and replaceable decoder.2 BERT is pre-trained with a task that requires learning language structure. Concretely, a fraction of words in the input sequence is masked out, and the decoder is tasked to predict the original words from the encoded input sequence (Fig. 2b). Conceptually, this is only possible if the encoding includes sufficient context to infer the missing word. Afterward, the unique pre-trained model can be fine-tuned to many different tasks by replacing the small decoder with task-specific ones, e.g., language understanding, question answering, or text generation [12,13,41]. The model has already learned to encode language context and only needs to learn to extract the task-relevant information from this context.",
            "score": 0.5887264440762536,
            "section_title": "BACKGROUND ON TRANSFORMERS",
            "char_start_offset": 6236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2294921875
        },
        {
            "corpus_id": "248973005",
            "title": "Unsupervised English Intelligent Machine Translation in Wireless Network Environment",
            "text": "e encoder-decoder model, first introduced in [16], is still widely used. e model encodes source language sentences into a fixed-length vector using a recurrent neural network. For each moment in time, a new target vocabulary is predicted by using a combination of the target vocabulary outputs from prior moments. e quality of the translations produced by this method is on par with that of more traditional statistically based methods. To account for the varying lengths of sentences, semantic information is represented as a fixed-length vector. Each phrase will have varied amounts of information in its corresponding vector, which directly influences the neural machine translation model's translation quality for longer sentences. For the long-distance dependency problem, [17] suggested a bidirectional recurrent neural network model based on attention mechanisms. Using the bidirectional recurrent neural network, the model is able to learn not only the relationship between current and past words but also the relationship between the present word and the following words. e attention mechanism is a weight calculation mechanism between the encoder and the decoder. When decoding and predicting the current target vocabulary, not only the decoder output at the previous moment but also the weighted summation result obtained by calculating the weight according to the current hidden layer state as well as the hidden layer state for the encoder at all times is combined. Reference [18] improves the attention mechanism and proposes two concepts, global attention mechanism and local attention mechanism, to further alleviate the problem of long-distance dependence. Reference [19] proposes to use convolutional neural networks to implement machine translation models. e model uses the convolution method to continuously expand from local features to global features and the characteristics of parallel computing to optimize sequence-to-sequence machine translation tasks. Reference [20] proposed an NMT model based entirely on attention and proposed a self-attention mechanism to encode bilingual sentences and learn their semantic information through the self-attention mechanism. \n\nNew machine translation methods were studied using a neural network encoder-decoder structure that was presented in [21]. Based on neural networks, the data sparsity issue in traditional statistical machine translation models was greatly improved. \n\neir approach also provides the groundwork for future neural network-based machine translation applications.",
            "score": 0.5879167292931325,
            "section_title": "Related Work",
            "char_start_offset": 5077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2189
                },
                {
                    "start": 2192,
                    "end": 2313
                },
                {
                    "start": 2314,
                    "end": 2439
                },
                {
                    "start": 2442,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "5758868"
                },
                {
                    "start": 1684,
                    "end": 1688,
                    "matchedPaperCorpusId": "208527264"
                },
                {
                    "start": 1990,
                    "end": 1994,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2308,
                    "end": 2312,
                    "matchedPaperCorpusId": "203043291"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2017822265625
        },
        {
            "corpus_id": "272208875",
            "title": "Advancements in Deep Learning-Based Image Captioning",
            "text": "The decoder part usually uses variants of Recurrent Neural Networks (RNN) such as Long Short-Term Memory Networks (LSTMs) or Gated Recurrent Units (GRUs) to deal with tasks that do not involve long sequences of indeterminate data, such as generating image captioning. Unlike the traditional encoder-decoder structure, the Encoder-Decoder with Attention model introduces an attention mechanism. As the decoder generates each word or phrase, the attention mechanism allows the model to dynamically focus on different regions of the image encoder, thus enabling the model to better capture the correspondence between the image and the text, and improve the accuracy and fluency of the generated descriptions. This attention mechanism helps the model to pay more attention to the image regions that are relevant to the currently generated word, thus making the generated text more descriptive. The topic of Jingqiang Chen, Hai Zhuge's research is to add captions to news images, while News Image Captioning contains more detailed information than general image captioning, such as entity names and events, they generate captions for news images through a multimodal attention encoder-decoder model. Their research methodology consists of four main parts: the text encoder encodes the text through an RNN model, and the image encoder completes the encoding of the image by obtaining the vector representation of the image through Oxford VGGNet. The decoder decodes the words through the RNI model. In order to balance the text encoding and image encoding, they developed a multimodal attention mechanism based on the traditional attention mechanism. In their experimental data analysis, it can be seen that NNattSim, which combines text and image encoding, outperforms NNattImg, which only pays attention to the image input, and NNattTxt, which focuses on the text summarization problem. Umemura, K et al. They also used the attention mechanism to connect the encoder to the decoder, where the encoder is used to convert the image into a vector and the decoder is used to generate the text, which can be used to generate an accurate description of the image based on the output of the encoder.",
            "score": 0.5872309263004,
            "section_title": "Method based on encoder decoder structure",
            "char_start_offset": 10064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244384765625
        },
        {
            "corpus_id": "266903047",
            "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing",
            "text": "When inserting a stream of chunk sequences into a model that contains a memory module in a sequential order and reading them at the same time, we implicitly follow the causal order or temporal order between the chunks, i.e. each chunk can access all past chunks up to the current step but no future chunks. We only decide whether to allow the bidirectional dependency between positions within the chunk. This means when we use the memory inside an attention layer, we should assume the attention is unidirectional, not bidirectional. \n\nAn encoder architecture or the encoder tower in the encoder-decoder architecture expects only bidirectional inputs. Also, while a Transformer",
            "score": 0.587113607585574,
            "section_title": "ATTENDRE Layer",
            "char_start_offset": 17740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 677
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.296142578125
        },
        {
            "corpus_id": "277150812",
            "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content",
            "text": "Following the evaluation of encoder-only models, we also assess several transformer-based encoder-decoder models. BART (Lewis, 2019) is a denoising autoencoder that combines a bidirectional encoder with an autoregressive decoder, excelling in text generation and comprehension tasks. mBART (Liu, 2020) extends BART to a multilingual context, pre-trained as a sequenceto-sequence denoising autoencoder on large-scale monolingual corpora across multiple languages, making it effective for neural machine translation. T5 (Raffel et al., 2020) frames all NLP tasks as text-to-text transformations, enabling a unified approach to various tasks such as translation, summarization, and question answering.",
            "score": 0.5847065330741866,
            "section_title": "Encoder-Decoder models:",
            "char_start_offset": 15775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 698
                }
            ],
            "ref_mentions": [
                {
                    "start": 518,
                    "end": 539,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1497802734375
        },
        {
            "corpus_id": "260886785",
            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
            "text": "In this section, we draw a comparison between the computations of Transformer encoders and decoders to deepen our understanding of the fresh challenges that surface within the realm of GLMs. \n\nCumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss.",
            "score": 0.584164594249019,
            "section_title": "Quantization Challenges on GLMs",
            "char_start_offset": 9244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44384765625
        },
        {
            "corpus_id": "268531330",
            "title": "Advancing Time Series Classification with Multimodal Language Modeling",
            "text": "In the realm of natural language processing (NLP), the advent of Pre-trained Language Models (PLM) has marked a paradigm shift from traditional static word vector representations, such as Word2Vec [31], to dynamic, context-aware embeddings.PLM leverages vast amounts of textual data to learn rich, nuanced language representations before being fine-tuned for specific tasks, embodying a more holistic approach to language understanding.The evolution of PLM can be broadly categorized into three architectural paradigms: Decoder-only, Encoder-only, and Encoder-Decoder models.Each of these paradigms brings its unique approach to language modeling and representation.(1) Decoder-only models, such as GPT [36], focus on generating text based on the preceding context.This design is particularly adept at tasks involving language generation, where the model predicts subsequent tokens given a sequence of prior tokens.GPT and its successors exemplify this approach, demonstrating remarkable proficiency in text completion, creative writing, and more.(2) Encoder-only models, epitomized by BERT [13], specialize in understanding and interpreting the context of a given text fragment.By processing text in a bidirectional manner, these models excel at tasks requiring deep contextual understanding, such as sentiment analysis, named entity recognition, and question answering.(3) Encoder-Decoder models, like BART [23], combine the strengths of both encoders and decoders to handle a wide range of tasks from translation to summarization.These models are designed to encode the input text into an intermediate representation, which the decoder then uses to generate the output text, making them versatile tools for both understanding and generation tasks.\n\nSubsequent research has explored the application of these PLM to downstream tasks, leveraging strategies such as Fine-tuning [13], where the pre-trained model is slightly adjusted to perform specific tasks; Prompt engineering [4], which frames tasks in a way that the model can understand; Instruction Tuning [34], which adapts models to follow human-like instructions; and Reinforcement Learning from Human Feedback (RLHF), which refines models based on qualitative feedback.",
            "score": 0.5840907790111582,
            "section_title": "C.2 Pre-trained Language Model",
            "char_start_offset": 39682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1371
                },
                {
                    "start": 1371,
                    "end": 1533
                },
                {
                    "start": 1533,
                    "end": 1750
                },
                {
                    "start": 1752,
                    "end": 2228
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21533203125
        },
        {
            "corpus_id": "271329382",
            "title": "A Foundation Model for Soccer",
            "text": "One of the largest breakthroughs in recent years with respect to sequence modeling was the introduction of the transformer model [Vaswani et al., 2017].A transformer is a neural architecture that transforms an input sequence into an output sequence.The driving idea behind transformers is the attention mechanism.At each step in the processing, this mechanism decides which other parts of the input sequence are most important for predicting the subsequent word.Transformers are widely applicable in a variety of downstream tasks.For example, this model could be trained to translate text (sequences of words) from one language to another, or for creating a chat bot.\n\nTransformers come in several different flavours as well.An encoder-only transformer architecture only features encoding layers and omits any sort of decoding process.Encoder-only models are widely useful in natural language understanding and in creating highly valuable text embeddings which can later be applied to a variety of downstream tasks, such as text classification and search.A notable encoder-only model is Google's BERT [Devlin et al., 2019], which features a bidirectional transformer, allowing it to make use of both preceding and succeeding words when analyzing a sequence.Conversely, decoder-only transformer architectures only feature decoding layers, and their main purpose lies in generating new words or sequences based on the inputted representations.Popular decoder-only models include OpenAI's GPT models [Radford et al., 2019].Encoder-decoder models, as seen in the original transformer paper [Vaswani et al., 2017], feature both encoders and decoders.These models are commonly used for sequence-to-sequence mapping tasks, such as translating text from one language to another.",
            "score": 0.5840081070820247,
            "section_title": "Deep Learning for Sequence Modeling",
            "char_start_offset": 1576,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 667
                },
                {
                    "start": 669,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1441
                },
                {
                    "start": 1441,
                    "end": 1520
                },
                {
                    "start": 1520,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1770
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 151,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1101,
                    "end": 1122,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1586,
                    "end": 1608,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27880859375
        },
        {
            "corpus_id": "259287245",
            "title": "Leveraging Cross-Utterance Context For ASR Decoding",
            "text": "While typically a transformer [15] may consist of a bidirectional encoder, followed by a causal decoder, for language modelling we use the decoder-only variant transformer. This consists of alternating multi-headed self-attention with a causal mask, and feed-forward modules. \n\nGiven a word sequence w = (w1, .., wT ) causal language models are trained to estimate the conditional probability of P (wt|w<t). Word sequence probabilities can then be obtained by an expansion resulting in P (w), when working with loglikelihoods this equates to: T t=1 log P (wt|w<t). For the purpose of decoding these likelihoods can be treated as scores and combined with the AM through a log-linear interpolation.",
            "score": 0.5831438624797483,
            "section_title": "Transformer Language Modelling",
            "char_start_offset": 3407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 696
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2142333984375
        },
        {
            "corpus_id": "268041362",
            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
            "text": "The decoder-only model [18], functioning independently without an encoder, can act as a language model designed primarily for next-step prediction [19][20][21]. During the training of language models, decoder-only model takes on the task of generating the target sequence. \n\n2.1.1 Causal Decoder. The causal decoder architecture employs unidirectional attention masking to ensure that each input token can only attend to past tokens and itself. Both input and output tokens are processed in a similar manner within the decoder. The schematic for this architecture is depicted in the middle panel of Figure 1. \n\nGPT Series. The Generative Pre-trained Transformer (GPT) model has garnered significant attention and interest in the field of NLP [22], and the technical evolution of the GPT series models is illustrated in cutting-edge technology built upon the Transformer architecture, the versatility and robust performance of the GPT model establish it as a universal solution for various NLP tasks. \n\nGPT-1. Proposed by the OpenAI team, GPT-1 [20], short for Generative Pre-trained Transformer 1 [17], serves as the foundational model for the GPT-series, establishing the key architecture and the fundamental principle of modeling natural language text, specifically predicting the next word. GPT-1 adopts a decoder-only Transformer architecture, implementing a semi-supervised approach for language understanding through a combination of unsupervised pretraining and supervised fine-tuning. \n\nGPT-2. GPT-2 [23], is an extension of the GPT-1 architecture [20], where the parameter scale is increased to 1.5 billion. Trained on the extensive WebText dataset, GPT-2 aims to perform diverse tasks through unsupervised language modeling, eliminating the need for explicit fine-tuning with labeled data. GPT-2 utilizes a probabilistic form for multitask solving, denoted as  ( |, ), predicting the output based on input and task information. This approach is reminiscent of similar methods found in [24].",
            "score": 0.5825564041689149,
            "section_title": "Decoder-only Transformer Architecture",
            "char_start_offset": 6274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 1508,
                    "end": 1512,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44775390625
        },
        {
            "corpus_id": "237562914",
            "title": "Language Models as a Knowledge Source for Cognitive Agents",
            "text": "Current language models (LMs) generally have an encode, decode, or encode-decode architecture, as represented in Figure 1. This architecture can classify and/or produce sequences from sequences (Sutskever et al., 2014), which is particularly apt for natural language processing given the sequential nature of language. The encode component (a deep neural network) learns a context vector, a representation of individuals words (or tokens) in the context of sequences in which they appear in a training set; the LM auto-encodes this context via presentation of sequences during training. The encode process is bidirectional, meaning that tokens that come both before and after a particular token influence the context vector that is learned for a given word (both \"robot\" and \"box\" inform the encoding of the \"move\" feature in the context vector). Auto-encoder LMs are generally used for text classification, such as sentiment analysis, grammar classification, or word labeling (e.g., entity type, part of speech). \n\nThe decode component produces outputs that predict the next item in a sequence. Decoding is unidirectional; only words that come before (or after, but not both) a particular word influence production of that word. Decoding is autoregressive, meaning that when a token/word is produced as output, that generated token can then influence the production of a subsequent one. Auto-regression thus allows a chain of tokens to be produced from a fixed input sequence. Decoders are the engines of sequence/sentence completion in LMs and are used for natural language generation (NLG). \n\nAs suggested by the figure, a particular LM can include an encoder stack, a decoder stack, or both. Encoder-decoder LMs, such as T5 (Raffel et al., 2020), are used for sequence-to-sequence transformations, such as language translation and summarization. \n\nLanguage models were made more practical by the emergence of the transformer architecture (Vaswani et al., 2017). Transformer networks are easier to parallelize than recurrent neural networks resulting in faster training with GPUs.",
            "score": 0.5822458794453738,
            "section_title": "Language Models and Their Uses",
            "char_start_offset": 4540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 1728,
                    "end": 1749,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25244140625
        },
        {
            "corpus_id": "276775091",
            "title": "NLI4DB: A Systematic Review of Natural Language Interfaces for Databases",
            "text": "The advent of the Transformer architecture [134] has resulted in considerable success of LLMs in natural language processing tasks. The models effectively capture the deep structure and semantic information of language through pre-training and fine-tuning [94]. Decoder-only, encoder-only and encoder-decoder are the principal structures of LLMs. \n\n(i) The decoder-only model, represented by GPT [18,98], exclusively comprises a decoder and generates output sequences progressively through an autoregressive approach. The model is suitable for generative tasks such as text generation and dialogue systems [110]. However, the model exhibits limited effectiveness when processing long texts due to the autoregressive nature. Additionally, the model does not directly handle input information, posing a challenge of unidirectional information transmission. \n\n(ii) The encoder-only model, represented by BERT [37], contains only an encoder and extracts context through bidirectional training. This architecture is applicable to tasks involving context comprehension and supervised learning. Lacking a direct output generation mechanism, the model is unsuitable for generative tasks. In addition, the model cannot handle variable-length outputs in seq2seq tasks. \n\n(iii) The encoder-decoder model, represented by T5 [111], consists of an encoder and a decoder. The encoder maps the input sequence to a high-dimensional contextual representation, which is then utilized by the decoder to produce the output sequence. The architecture excels in tasks requiring global information transfer, such as machine translation and summary generation [76]. However, the computational resource demands of the model are high, and the complexity of information transfer may lead to performance degradation in certain tasks. \n\nLLMs contribute to the development of NLIDB. Notably, the growing popularity of GPT [18,98]  The advent of intricate deep learning architectures has prompted a focus on accurately interpreting natural language and generating structured language by optimizing LLMs. This direction emphasizes optimizing the LLM through larger data pre-training, superior language representation learning techniques, and more efficient fine-tuning methods.",
            "score": 0.5820164102870424,
            "section_title": "Text2SQL with LLM",
            "char_start_offset": 82277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 48,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "240420063"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 606,
                    "end": 611,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1312,
                    "end": 1317,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1635,
                    "end": 1639,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1891,
                    "end": 1895,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1895,
                    "end": 1898,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14208984375
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "Model Architecture In representation learning with transformers (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020), two schemes of modeling are prevalent which differ in their attention masks for the contextualization of hidden vectors. For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2020). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks. \n\nLearning Algorithm The choice of encoder or decoder-based model architectures typically guides the selection of learning algorithms for language modeling. Encoder-based models may be trained with the task of masked language modeling in the form of denoising span corruptions (Devlin et al., 2019;Raffel et al., 2020).",
            "score": 0.5815841171406922,
            "section_title": "COMPONENTS: ARCHITECTURE, OBJECTIVE, SAMPLING, DATA",
            "char_start_offset": 9494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 85,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 85,
                    "end": 104,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 104,
                    "end": 124,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 995,
                    "end": 1016,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "269791266",
            "title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models",
            "text": "In the context of LLMs, the \"encoder-decoder\" and \"decoderonly\" architectures are prominently used for NLP tasks.\n\nEncoder-decoder architectures [95,118,119] consist of two main components: an encoder f enc and a decoder f dec .The encoder and decoder components are typically implemented using transformers [95], which employ attention mechanisms to capture long-range dependencies in the input and output sequences.The encoder takes the input sequence X = (x 1 , x 2 , . . ., x N ) and maps it into a sequence of latent representations H = (h 1 , h 2 , . . ., h N ) that capture the contextual information, and the decoder generates the output sequence Y = (y 1 , y 2 , . . ., y T ) based on H. Mathematically, the encoding process can be expressed as H = f enc (X), and the entire latent sequence H is generated at once from X.The decoder, however, generates the output sequence Y sequentially: y t = f dec (y <t , H) where y <t = (y 1 , y 2 , . . ., y t\u22121 ).\n\nDecoder-only architectures [120,121,122], on the other hand, are a variant of the transformer architecture that uses only the decoder component.It is particularly suitable for language modeling tasks, where the goal is to predict the next token given the previous tokens.The decoder-only architecture can be mathematically expressed as y t = f dec (y <t ).\n\nTokenization is a preprocessing method to break the input text into a sequence of tokens, the basic data unit in language models.The number of tokens is finite, and each token can correspond to a word, sub-word, or a single letter.During inference, the input text is converted to a sequence of tokens and fed to the models, which predict the output tokens that are then converted back to text.The tokenization has a great impact on the performance of the language models, as it affects how the models perceive text.",
            "score": 0.5813567914160923,
            "section_title": "LLM Architectures",
            "char_start_offset": 9206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 115,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 962
                },
                {
                    "start": 964,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1320
                },
                {
                    "start": 1322,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1715
                },
                {
                    "start": 1715,
                    "end": 1837
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2120361328125
        },
        {
            "corpus_id": "257019916",
            "title": "Complex QA and language models hybrid architectures, Survey",
            "text": "This left to right generation can be very rich like writing a story or answering a question (e.g. used by ChatGPT). The input prompt can be formatted with appropriate instructions (prompt & instructions engineering) to use the generated text as a task-specific model (e.g. classification, summarization, decomposition...). This is often called causal or autoregressive attention (non-causal decoders exist but has limited adoption in the literature). GPT family of models are one of the most well-known decoder-only models, known for their ability to generate human-like text. Encoders-Decoders (e.g. T5 [21], BART [22], BigBird [23]) encode the input text and decode it into a different form (text-to-text mapping), they are suitable for translation, summarization, or generating a response to a question. They consist of both an encoder and a decoder, where the encoder generates a fixed-size representation of the input text, and the decoder generates the output text. T5 is known for its multi-tasks ability, BART is mainly used for text generation and summarization, and BigBird allows to process much longer sequence of texts than other models. \n\n3 Analyzing: complexity, skills, tasks, and limits In order to design a system able to answer complex questions, we first propose to analyze targeted questions complexity, identify required skills and tasks, as well as limitations to handle. This analysis allows to properly define the problem to be solved gradually and constraints to integrate in order to compose among the different complementary solving approaches further reviewed (training: section 5, hybridization: section 6, prompting: section 7, experience: section 8). This analysis could also be done \"a posteriori\" if a system fails to properly answer in order to better characterize or identify the causes.",
            "score": 0.580880696334917,
            "section_title": "Modular approaches before transformers",
            "char_start_offset": 17482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1823
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216796875
        },
        {
            "corpus_id": "261100939",
            "title": "Pre-Trained Model-Based Automated Software Vulnerability Repair: How Far are We?",
            "text": "Pre-trained models have significantly improved performance across a wide range of natural language processing (NLP) and code-related tasks such as machine translation, defect detection, and code classification [34], [35]. Typically, the models are pre-trained to derive generic vector representation by self-supervised training on a large-scale unlabeled corpus and then are transferred to benefit multiple downstream tasks by fine-tuning on a limited labeled corpus [36]. \n\nExisting pre-trained models generally adopt the encoderdecoder transformer architecture, which can be classified into three types: encoder-only, decoder-only, and encoderdecoder models. Encoder-only models (e.g., CodeBERT [22]) usually pre-train a bidirectional transformer where tokens can attend to each other. Encoder-only models are good at understanding tasks (e.g., code search), but their bidirectionality nature requires an additional decoder for generation tasks. Decoder-only models (e.g., GPT [37]) are pre-trained using unidirectional language modeling that only allows tokens to attend to the previous tokens and themselves to predict the next token. Decoder-only models are good at auto-regressive tasks like code completion, but the unidirectional framework is sub-optimal for understanding tasks. Encoder-decoder models (e.g., T5 [38]) often make use of denoising pre-training objectives that corrupt the source input and require the decoder to recover them. Compared to encoder-only and decoder-only models that favor understanding and auto-regressive tasks, encoder-decoder models can support generation tasks well like code summarization. \n\nIn the context of vulnerability repair, an encoder stack takes a sequence of code tokens as input to map a vulnerable function X i = [x 1 , . . . , x n ] into a fixed-length intermediate hidden state, while the decoder stack takes the hidden state vector as an input to generate the output sequence of tokens Y i = [y 1 , . . . , y n ].",
            "score": 0.5805930569501608,
            "section_title": "Pre-trained Model",
            "char_start_offset": 10761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1321,
                    "end": 1325,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2705078125
        },
        {
            "corpus_id": "265308945",
            "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
            "text": "Then, each higher-level block takes the output hidden states from the previous block as input, represents them with the MHA and FFN layers, and feeds them to the next block. \n\nThe final hidden state outputted from the last block is fed into a linear layer called language modeling head, and the output logits is transformed into a probability distribution over the target vocabulary through the softmax operation. \n\nNote that the slight difference between the Encoder and Decoder blocks is that the latter additionally interfaces with the Encoder's output via a cross-attention (CA) layer before feeding into the FFN layer. \n\nSuch a binary structure was originally designed for sequence-to-sequence modeling in machine translation tasks. \n\nSubsequently, several variations have been proposed aiming at more general language modeling objectives such as MLM and CLM. The BERT series [61,141] harnesses only the Encoder with MLM to enhance bidirectional information, serving as a discriminative model. Conversely, the GPT series [23,178,179] utilizes only the Decoder with CLM, focusing on unidirectional generative models. T5 [181] and BART [125] variants, however, treat each NLP task as a text-to-text conversion, leveraging both Encoder and Decoder. The decoder-only generative model architecture has recently become the predominant choice for current LLMs. Notable examples include Llama [221,222], OPT [262], \n\nBloom [236], GLM [66,258], and Mistral [4,100], among others. \n\nAttention Mechanism. The attention mechanism [13], as the core design of the Transformer implemented in the MHA layer, computes a weighted representation of each token in the input sequence based on its relevance to others. \n\nSpecifically, as illustrated in Fig. 1(a), the word-embedded token sequence  \u2208 R \u00d7  , concatenating long contexts and user prompts with total length , gives rises to three embedding matrices, i.e., a linear projection layer query  \u2208 R \u00d7  , key  \u2208 R \u00d7  and value  \u2208 R \u00d7  , ,  := split  \u00d7  ,, ,  ,, \u2208 R   \u00d7 (  +  +  ) \n\nThen, for the attention kernel operations",
            "score": 0.5804863039048025,
            "section_title": "MLM : arg max",
            "char_start_offset": 9234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 2019
                },
                {
                    "start": 2022,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1124,
                    "end": 1129,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2447509765625
        },
        {
            "corpus_id": "237434245",
            "title": "Generate & Rank: A Multi-task Framework for Math Word Problems",
            "text": "Pre-trained language models have obtained stateof-the-art results in many NLP benchmarks (Wang et al., 2018a(Wang et al., , 2019a. These models are usually based on Transformer layers (Vaswani et al., 2017) and trained on large corpus with self-supervised tasks. According to their architectures, pre-trained language models can be categorized into three types: encoder-only, decoder-only and encoderdecoder models. BERT (Devlin et al., 2019) is an encoder-only model which firstly proposes masked token prediction and next sentence prediction to train a language representation model. Following this, many other models are proposed like RoBERTa (Liu et al., 2019b) and SpanBERT (Joshi et al., 2020). Decoder-only models are typically auto-regressive models trained to estimate the probability distribution of a text corpus, including GPT2 (Radford et al., 2019), GPT3 (Brown et al., 2020) and XLNet (Yang et al., 2019). Encoderdecoder models like BART  and T5 (Raffel et al., 2020) use the encoder-decoder architecture and are trained on sequence-to-sequence tasks such as text denoising and translation.",
            "score": 0.5801092582587222,
            "section_title": "Pre-trained Language Model",
            "char_start_offset": 22420,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 108,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 108,
                    "end": 129,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 184,
                    "end": 206,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 421,
                    "end": 442,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 679,
                    "end": 699,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 840,
                    "end": 862,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 961,
                    "end": 982,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07171630859375
        },
        {
            "corpus_id": "272423598",
            "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
            "text": "Another often overlooked key point is the design of attention masks within the transformer layers in large lan- guage models. In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text. Mathematically, the causal attention mask M \u2208 R T \u00d7T for a sequence of length T is defined as: \n\nThis ensures that each position i only attends to previous positions (including itself), thus implementing the causal attention mechanism. The final attention weights are computed as: \n\nwhere Q is the query vectors, K is the key vectors, V is the value vectors, d k is the dimension of the key vectors, and M is the causal attention mask. However, for multimodal information involving both visual and textual inputs, the visual modality is only used as a conditional input to the language model. During the unidirectional decoding process of the language model, this design weakens the bidirectional attention interactions obtained from the visual encoder, reducing them to unidirectional attention interactions. To explore the impact of different attention masks, we design three distinct attention masks to enhance and investigate better interactions within visual tokens and between visual and text tokens, as illustrated in Figure 3. \n\nFirstly, the Full Visual Mask modifies the causal attention mask to enable more extensive interactions among visual tokens across different frames. This mask can be represented as follows: \n\nSecondly is Frame-wise Block Mask, which limits the attention to adjacent visual tokens within the same frame. This is defined as follows: \n\nFinally, we proposed Frame-wise Block Causal Attention Mask (FwBC), which combines the characteristics of the previous causal and block visual attention masks by incorporating broader visual token interactions within the frame while maintaining causal inference mode across video frames. This can be presented as:",
            "score": 0.5794494120616847,
            "section_title": "Frame-wise Block Causal Attention Mask",
            "char_start_offset": 12069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "219792650",
            "title": "I-BERT: Inductive Generalization of Transformer to Arbitrary Context Lengths",
            "text": "Attention Mechanism Attention mechanisms were introduced as a supplement to encoder-decoder RNN architectures to address the problem of long-term dependencies between source and target tokens. This serves a purpose similar to token alignments [13], used in statistical machine translation models. Two varieties, Luong [14] and Bahndau [15] attention, take different approaches to combine alignment weights and a context vector to generate the next hidden token and/or an output token. Vaswani et al. [3] introduced a layered encoder-decoder architecture that relies only on self-attention between layers and attention on the output of the encoder. It uses scaled dot-product attention to obtain the context vector, where the scores are computed by dot-products of keys and queries. \n\nAuto-encoder Language Model A Transformer encoder is often sufficient for language understanding and generation tasks [16], which are often fine-tuned via transfer learning to solve NLP tasks. Devlin et al. introduced BERT, which has a similar architecture but is pre-trained to learn bidirectional representations. Based on the BERT architecture, recent models such as GPT-2 [4] and MegatronLM [7] have achieved better performance by increasing the number of parameters by an order of magnitude. On the other hand, models such as Albert [17] reduce the number of parameters while retaining performance. XL-Net [5] integrates techniques from Transformer-XL [11] which tries to extend the context length of Transformer models using recurrence and relative positional encoding [18], and introduces a novel pre-training objective that permutes the input sequence. It also incorporates a two-stream attention mechanism, consisting of a content stream identical to self-attention and a query-stream that does not have information about the current position. \n\nInductive Bias of Neural Network The parallel nature of the self-attention mechanism shortens its critical computation path length to O(1), which enables efficient parallelism at the cost of losing the inductive bias. In particular, the fact that Transformers cannot generalize learned algorithmic rules beyond observed context lengths has been proved theoretically [9] and empirically [10].",
            "score": 0.5774451006874035,
            "section_title": "Background",
            "char_start_offset": 4647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "13259913"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 335,
                    "end": 339,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1395,
                    "end": 1398,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1559,
                    "end": 1563,
                    "matchedPaperCorpusId": "3725815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.351318359375
        },
        {
            "corpus_id": "248377166",
            "title": "Which Discriminator for Cooperative Text Generation?",
            "text": "By default, attention layers as defined in [34] are bidirectional: every token can attend to tokens at every position. When it comes to discrimination, models based on such bidirectional attention are commonly used since \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than [...] a left-to-right model\" [9]. However, while it brings some capacity to the model, it also makes it non auto-regressive: when a token is added at the end of a sequence, every hidden states need to be re-computed. \n\nOne way to train a transformer based LM for text generation is to use unidirectional attention masks [29]. In this unidirectional setting, any extra token added at the end of a sequence does not change the already calculated hidden states, since previous tokens do not attend to it. Thus, starting from an already classified sequence  1: \u22121 , classifying  1: only requires to compute  attention scores, rather than the whole set of  2 scores per self-attention layer, as it would be required in the bidirectional setting. In common discriminative tasks, this does not matter since only entire sequences are discriminated. Hence, none of the hidden states needs to be reused for another next sample. However, for a use in autoregressive cooperative decoding, where input sequences are often the continuation of already discriminated ones unidirectional attention allows to reuse contextual encoding of previous tokens, hence greatly speeding up the process. \n\nHowever, even with unidirectional discriminators, evaluating every possible continuation of a given sequence is intractable since, for a vocabulary of size |V |, it requires |V | forward passes at each decoding step. |V | being in the order of ten thousand, discriminating every possible continuation of decoding sequences is too costly. Thus, cooperative approaches have to circumvent this issue by limiting the number of sequences actually evaluated by the discriminator. For example, DAS pre-filters potential continuations on the nucleus of the LM distribution [16]. This choice necessarily biases the resulting generated distribution.",
            "score": 0.5773964079058465,
            "section_title": "Choosing the Right Teammate",
            "char_start_offset": 5982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 2069,
                    "end": 2073,
                    "matchedPaperCorpusId": "127986954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5556640625
        },
        {
            "corpus_id": "273877976",
            "title": "Clustering in Causal Attention Masking",
            "text": "The introduction of the Transformer architecture Vaswani et al. (2017) has markedly impacted the landscape of natural language processing (NLP), signaling the advent of large language models. Central to the Transformer architecture is the self-attention mechanism, a special kind of layer that distinguishes it from preceding models such as ResNets. This innovation has yielded unprecedented performance not only in machine translation and text summarization but also in areas beyond NLP, including computer vision, speech recognition, and robotics. The flexibility and efficiency of Transformers underscore their integral role in the progression of artificial intelligence. Despite their widespread use, the theoretical foundations underlying their success remain underexplored. \n\nFollowing Sander et al. (2022), recent studies by Geshkovski et al. (2024b) and Geshkovski et al. (2023) have proposed a mathematical framework to analyze Transformers as interacting particle systems, demonstrating that tokens, when modeled as particles, exhibit clustering under certain conditions on the Key, Query, and Value matrices. These works primarily focus on full (mean-field) attention mechanisms, where each token can interact with every other token. Building upon this foundation, our research extends the analysis to causal attention mechanisms, wherein each token is restricted to interact only with preceding tokens. This distinction is crucial, as causal attention is prevalent in Transformer models employed in generative AI and known as decoder architectures. \n\nCausal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output.",
            "score": 0.576454061388143,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2278
                },
                {
                    "start": 2279,
                    "end": 2518
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 70,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 792,
                    "end": 812,
                    "matchedPaperCorpusId": "239616044"
                },
                {
                    "start": 832,
                    "end": 857,
                    "matchedPaperCorpusId": "258564860"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "276961508",
            "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
            "text": "Causal Sequence Modeling. State-of-the-art large language models [1,8] adopt a decoder-only Transformers [42] for causal modeling of language sequences and autoregressive generation, a method commonly known as the GPT [1] style approach. In the vision domain, images can be quantized into discrete tokens [41] and flattened from 2D to 1D, enabling generation via the next-token prediction paradigm, as seen in models like VQGAN [7], Llam-aGen [35] etc. [19,20,43,43,50]. These methods have demonstrated impressive generative performance. However, this token-by-token image generation approach is inefficient, especially when dealing with high-resolution images. Additionally, since the generation can only proceed in a specific token order, it encounters difficulties in zeroshot inference that require non-causal dependencies, such as inpainting and outpainting. \n\nMasked Sequence Modeling. Unlike causal sequence modeling, another mainstream approach to sequence modeling is the encoder-only [29,42,48,51] architecture with bidirectional attention, where multiple tokens in a sequence are randomly masked and then predicted. This method is widely used in BERT [16] for language modeling. In the vision domain, MaskGIT etc. [2,3,9,10,21,45,49,51] adopts the same approach for image modeling and generation. By leveraging bidirectional attention, this masked image generation method eliminates the constraints of causal dependencies, enabling multi-token generation in a single step with a randomized order, leading to significantly faster inference. However, due to the absence of KV cache in bidirectional attention, inference efficiency remains limited. The position of the next predicted token is integrated as a query within the attention mechanism. The corresponding probabilistic model, formalized in Eq. ( 8).",
            "score": 0.5756368246397903,
            "section_title": "Related Work",
            "char_start_offset": 5186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "20282961"
                },
                {
                    "start": 428,
                    "end": 431,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 994,
                    "end": 998,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1162,
                    "end": 1166,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1225,
                    "end": 1228,
                    "matchedPaperCorpusId": "246680316"
                },
                {
                    "start": 1228,
                    "end": 1230,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "matchedPaperCorpusId": "257767316"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "253553243"
                },
                {
                    "start": 1241,
                    "end": 1244,
                    "matchedPaperCorpusId": "254563906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39404296875
        },
        {
            "corpus_id": "277622080",
            "title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text",
            "text": "Since the introduction of BERT (Bidirectional Encoder Representations from Transformers) in 2018, encoder-only transformer architectures have remained foundational to both industry-scale and research-driven natural language processing (NLP) [Devlin et al., 2019]. Although recent advances have centered around large-scale decoder-only models such as GPT [Radford et al., 2018, Achiam et al., 2023], LLaMA [Touvron et al., 2023], and DeepSeek [Guo et al., 2025], prized for their generative capabilities, BERT and its derivatives continue to play a central role in real-world applications. Its sustained popularity can be attributed to its versatility and effectiveness across critical tasks, including dense retrieval [Gao et al., 2023], content moderation and classification [Kowsari et al., 2019], and the extraction of sensitive or regulated information in compliance-driven environments [Nadeau and Sekine, 2007]. \n\nWhile decoder-based models dominate applications requiring coherent generation and fluent language synthesis, encoder-only transformers offer unique advantages rooted in their bidirectional attention mechanisms. Unlike causal decoders, BERT-style models allow each token to attend to both preceding and succeeding context, yielding semantically enriched embeddings. This bidirectional encoding has proven especially valuable in scenarios that depend on fine-grained semantic discrimination. Furthermore, architectural advances in recent years have significantly modernized the encoder stack, with innovations in computational efficiency (e.g., Flash Attention [Dao et al., 2022]), extended sequence modeling, and parameter optimization, reaffirming the relevance of the encoder paradigm. BioClinicalBERT [Alsentzer et al., 2019] \u2717 \u2717 \u2713 \u2713 BioBERT [Lee et al., 2020] \u2717 \u2717 \u2713 \u2713 Clinical Longformer [Li et al., 2022] \n\nModernBERT [Warner et al., 2024] exemplifies this next-generation design, achieving a notable Pareto improvement over the original BERT across speed, memory footprint, and representational fidelity.",
            "score": 0.5739059216930513,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1829
                },
                {
                    "start": 1832,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 262,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 776,
                    "end": 798,
                    "matchedPaperCorpusId": "119297355"
                },
                {
                    "start": 1580,
                    "end": 1598,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1765,
                    "end": 1783,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43310546875
        },
        {
            "corpus_id": "271709396",
            "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
            "text": "There are three common LLM architectures, the Encoder-Decoder architecture, exemplified by the traditional transformer model. This architecture comprises six encoders and six decoders, data input into the system will first passes through the encoder, where it undergoes sequential feature extraction via the model's self-attention mechanism. Subsequently, the decoders utilize the word vectors produced by the encoders to generate outputs, this technique is common to see in machine translation tasks, where the encoder processes word vectors from one language through several attention layers and feedforward networks, thereby creating representations of the context. The decoder then uses this information to incrementally construct the correct translated text. A recent example of this architecture is the CodeT5+ model, launched by Salesforce AI Research in 2023 [30]. This model is an enhancement of the original T5 architecture, which designed to improve performance in code understanding and generation tasks. It incorporates a flexible architecture and diversified pre-training objectives to optimize its effectiveness in these specialized areas. This development highlights the competency of Encoder-Decoder architectures in tackling increasingly complex NLP challenges. \n\nThe Encoder-only architecture, as the name suggests it eliminates the decoder from the entire structure making the data more compact. Unlike RNNs, this architecture is stateless and uses a masking mechanism that allows input processing without relying on hidden states, and also accelerating parallel processing speeds and providing excellent contextual awareness. BERT (Bidirectional Encoder Representations from Transformers) is a representative model of this architecture, this model is a large language model built solely on the encoder architecture. BERT leverages the encoder's powerful feature extraction capabilities and pre-training techniques to learn bidirectional representations of text, achieving outstanding results in sentiment analysis and contextual analysis [31]. \n\nThe Decoder-only archiecture, in the transformer framework primarily involves the decoder receiving processed word vectors and generating output. Utilizing the decoder to directly generate text accelerates tasks such as text generation and sequence prediction. This characteristic with high scalability is known as auto-regressiveness, which is why popular models like GPT use this architecture.",
            "score": 0.5736126933064911,
            "section_title": "B. Model Architecture",
            "char_start_offset": 11840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2064
                },
                {
                    "start": 2067,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2327
                },
                {
                    "start": 2328,
                    "end": 2462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2919921875
        },
        {
            "corpus_id": "8063399",
            "title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model",
            "text": "Neural machine translation has shown promising results lately. Most NMT methods follow the encoder-decoder framework proposed by , which typically consists of two RNNs: the encoder RNN reads the source sentence and transform it into vector representation; the decoder RNN takes the vector representation and generates the target sentence word by word. The decoder will stop once a special symbol denoting the end of the sentence is generated. This encoder-decoder framework can be used on general sequence-to-sequence tasks (Sutskever et al., \u2020 Work done while Shi was an intern at Microsoft Research. 2014), like question answering and text summarization. After some modification, for example replacing the RNN encoder with a CNN, the model can also be applied to tasks like image captioning . In the following discussion, we focus on the task of machine translation.\n\nIn the original encoder-decoder model, although the encoder RNN generates a set of hidden states, one at each position of the source sentence, the decoder only takes the last one. This design in effect compresses the variable-length source sentence into a fixed-length context vector, with the information of each source word implicitly stored in the context vector. Thus the decoder cannot easily make full use of the whole sequence of encoder hidden states. To make it more flexible and generalize the fixed-length representation to a variablelength one, it was proposed to use attention mechanism for machine translation .\n\nAttention mechanism was first proposed to allow models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014).\n\nIn , attention mechanism was applied to machine translation to learn an alignment between source words and target words. Fig. 1 shows a sample alignment given by attention mechanism.\n\nWith the ability of learning alignments between different modalities from attention mechanism, attention-based encoder-decoder model is more powerful than just encoder-decoder and has been used for many tasks like question answering (Hermann et al., 2015), speech recognition (Bahdanau Each row is a distribution of how the corresponding target word (English) is aligned to source word. Darker color denotes higher weight",
            "score": 0.5735789635980194,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1679,
                    "end": 1698,
                    "matchedPaperCorpusId": "17195923"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253662109375
        },
        {
            "corpus_id": "266999234",
            "title": "Graph Language Models",
            "text": "Hence, GLMs can be applied to texts and -more interestingly -interleaved inputs of text and graph. In this joint setting, P and M each consists of four sub-matrices that correspond to self-attention between tokens from (i) graph-tograph, (ii) text-to-text, (iii) text-to-graph and (iv) graph-to-text. Graph-to-graph sub-matrices are formatted as described above for \u2113GLM and gGLM, respectively. Text-to-text sub-matrices are standard matrices from conventional sequence transformers. We introduce new T2G and G2T relative positions for text-to-graph, and graph-to-text connections, respectively. With this, the model can learn interaction strength between the two modalities. Similar to G2G in gGLM, we initialize T2G and G2T parameters from +\u221e. See example matrices in \u00a7A. \n\nUni-and Bidirectional LMs If a LM's self-attention is unidirectional, information can only propagate along the direction of arrows in Fig. 2b for the \u2113GLM. This means that, e.g., the representation of the node black poodle is independent of the rest of the graph. We could augment the graph with inverse relations to enable bidirectional information flow with unidirectional LMs, but in this work, we restrict our analysis to bidirectional models. \n\nT5 We use T5 (Raffel et al., 2020) -a bidirectional encoder with unidirectional decoder -as base LM to instantiate GLMs. In T5, relative distances in P group into so-called buckets, and each bucket maps to one learned positional bias in B P for each head. Positional biases are shared across layers. The decoder is not needed to encode graphs, but can be used to generate sequences, such as text or linearized graphs in future work.",
            "score": 0.5732275425730122,
            "section_title": "Preliminary: Graph Transformers (GT)",
            "char_start_offset": 17910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1658
                }
            ],
            "ref_mentions": [
                {
                    "start": 1239,
                    "end": 1259,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2137451171875
        },
        {
            "corpus_id": "257496129",
            "title": "Representation Learning for Stack Overflow Posts: How Far Are We?",
            "text": "Transformer-based language models have revolutionized the landscape of representation learning in natural language processing (NLP) [10,26,36]. Their efficacy in capturing text semantics has led to unparalleled performance in various applications, such as sentiment analysis [43], POS tagging [45], and question answering [34]. The vanilla transformer architecture [48] is composed of the encoder and decoder components. Based on the usage of these components, transformer-based language models can be categorized into three types: encoder-only, decoder-only, and encoderdecoder models. \n\nEncoder-only models exclusively leverage the encoder stacks of the vanilla transformer [48] architecture. BERT [10] stands as a prominent encoder-only representation model, which learns a bidirectional contextual representation of text. BERT proposes the Masked Language Modeling (MLM) task at the pre-training phase. In MLM, the input data is corrupted by randomly masking 15% of the tokens, and then the BERT model learns to reconstruct the original data by predicting the masked words. BERT is extensively pre-trained on large-scale datasets, which learn a meaningful representation that is reusable for various tasks, thus eliminating the process of training language models from scratch and saving time and resources. \n\nIn contrast, Decoder-only models consist solely of the decoder components of the original transformer architecture. A notable instance of such models is the GPT [35], GPT operates under a causal language modeling (CLM) framework during its training phase. CLM is a strategy where the model predicts the next token in a sequence while only considering preceding tokens. In other words, this design restricts the model from accessing future tokens in the sequence. \n\nBridging the above approaches, textitEncoder-decoder models integrate both the encoder and decoder components of the transformer architecture. Popular encoder-decoder models involve T5 [37] and BART [23]. The T5 model [37] advocates a unified text-to-text framework that converts various language tasks into a consistent text-to-text format.",
            "score": 0.57187813725853,
            "section_title": "Transformer-based Language Models",
            "char_start_offset": 6666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 275,
                    "end": 279,
                    "matchedPaperCorpusId": "85459677"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "202122780"
                },
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "153312701"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1978,
                    "end": 1982,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280517578125
        },
        {
            "corpus_id": "272423612",
            "title": "State-space models are accurate and efficient neural operators for dynamical systems",
            "text": "Transformers [51] revolutionized sequence modeling by discarding the recurrent structure in favor of a self-attention mechanism. The self-attention mechanism enables Transformers to directly model dependencies between any two positions in a sequence, allowing for better parallelization and scalability. The original Transformer architecture, particularly the encoder-decoder structure, has been exceptionally successful in machine translation. Subsequent adaptations, such as BERT [10] and GPT [3], have achieved state-of-the-art results in a wide range of NLP tasks, including text understanding and text generation. Despite their success, the quadratic complexity of the self-attention mechanism remains a challenge for processing long sequences.",
            "score": 0.5695966558094738,
            "section_title": "Transformers",
            "char_start_offset": 13912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 749
                }
            ],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2213134765625
        },
        {
            "corpus_id": "235125595",
            "title": "Pretrained Language Models for Text Generation: A Survey",
            "text": "Pretrained language models (PLMs) are pretrained with a mass of unlabelled text data and can be fine-tuned on downstream generation tasks. PLMs encode massive linguistic and world knowledge into vast amounts of parameters, which can enhance the understanding of language and improve the generation quality. The idea of pretraining is inspired by human beings, i.e., we transfer and reuse our old knowledge of what we have learned in the past to understand new knowledge and handle a variety of new tasks. In this way, PLMs can successfully perform on new tasks with their old experience and knowledge. \n\nOwing to the great achievements that Transformer [Vaswani et al., 2017] has made, almost all PLMs employ the backbone of Transformer. For the text generation tasks, some of PLMs utilize the standard Transformer architecture following basic encoder-decoder framework, while the others apply a decoder-only Transformer. Next, we will introduce these two methods successively. \n\nEncoder-decoder Transformer. A standard Transformer utilizes the encoder-decoder architecture, which is composed of two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism. Based on aforementioned architecture, models such as MASS [Song et al., 2019], T5 [Raffel et al., 2020], and BART [Lewis et al., 2020] have improved quality of the generated text. \n\nDecoder-only Transformer. Models such as GPT [Radford et al., 2019;Brown et al., 2020] and CTRL [Keskar et al., 2019] employ a single Transformer decoder blocks, which is typically used for language modeling. They apply unidirectional self-attention masking that each token can only attend to previous tokens. \n\nBesides language modeling, several works also utilize the decoder-only achitecture to generate text conditioned on input text. However, these models do not have an independent module to encode input sequence.",
            "score": 0.5690819990349456,
            "section_title": "Standard Architectures for Text Generation",
            "char_start_offset": 6547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 675,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1337,
                    "end": 1356,
                    "matchedPaperCorpusId": "146808476"
                },
                {
                    "start": 1557,
                    "end": 1578,
                    "matchedPaperCorpusId": "202573071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2783203125
        },
        {
            "corpus_id": "221702979",
            "title": "Current Limitations of Language Models: What You Need is Retrieval",
            "text": "For various tasks in NLP, non-causal models tend to show better performance-computes trade-off than causal models. For example, T5 outperforms the causal baseline not only in discriminative tasks but also generative tasks such as summarization and translation. BART achieves lower perplexity over the causal baseline on the task of fine-tuning to a conversational dataset. Furthermore, it was recently reported that XLNet achieves a superior performance to the GPT-3 of the corresponding size at Winogrande with few-shot learning (Davison, 2020). UnifiedQA, a T5-like pre-trained model fine-tuned with a general QA dataset, is evaluated with few-shot learning examples a la GPT-3, and it performs nearly on par with about 30 times larger GPT-3 (that is not fine-tuned) (Hendrycks et al., 2020). \n\nHowever, the current approach of non-causal models suffer from some limitations. \n\nFirstly, many real-life tasks are open-ended text generation with the output loosely constrained by the input in a way similar to many of the prompt-output generation tasks a la GPT-2 and longrange unsupervised text generation. However, non-causal models are rarely evaluated on this sort of Figure 2: (Raffel et al., 2019). The performance of the baseline T5 with and without fine-tuning on various datasets. Pre-training is beneficial for all fine-tuning datasets but WMT EnFr, which is the largest dataset. \n\ntasks. This practice unfairly favors a certain family of models, including masked language models, since they tend to perform poorly compared with the causal models. More details can be found in Appendix 6.2.1. While non-causal models excel at reading/encoder-intensive tasks, they perform poorly on some writing/decoder-intensive tasks. There needs to be more effort spent on seamlessly combining causal and non-causal models to achieve the best of both worlds. \n\nSecondly, it is unclear how to fine-tune a non-causal model to perform an arbitrary textual task as GPT-2/3 at least as efficiently as GPT-2/3.",
            "score": 0.5685583435534483,
            "section_title": "Non-Causal Models",
            "char_start_offset": 5937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1854
                },
                {
                    "start": 1857,
                    "end": 2000
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2060546875
        },
        {
            "corpus_id": "272464120",
            "title": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge Devices",
            "text": "The architectural makeup of transformer models is the basis for developing pipeline inference strategies on edge devices. A typical transformer model comprises embedding, encoder, decoder, pooling, and additional specialized layers. Based on their architectural configurations, they can be classified into three primary categories: encoder-decoder, encoder-only and decoder-only models. Encoder-decoder models, such as BART [32] and T5 [33] integrate both encoder and decoder layers. BART is architected for complex sequence-to-sequence tasks, and T5 generalizes this capability with a comprehensive text-to-text methodology applicable to a wide array of NLP challenges. Encoder-only models like BERT [34]",
            "score": 0.5684026745872863,
            "section_title": "II. BACKGROUND AND MOTIVATION A. Transformer Model Structure",
            "char_start_offset": 7316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 705
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1580810546875
        },
        {
            "corpus_id": "273532366",
            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
            "text": "Currently, MLLMs mainly use decoder, encoder-decoder, and prefix language model (Prefix LM) structures. The early pre-training model BERT used the encoder structure, which is now rarely used independently, so this article mainly introduces these three structures of MLLMs. \n\nDecoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models. Recent research suggests that model performance can be enhanced through decoder-less training [67] or instruction fine-tuning [68] on decoder-based models. However, these methods have primarily been tested on English-centric models, and their effectiveness on MLLMs remains unclear. The PaLM 2 model has reported excellent results using similar approaches for multilingual model tasks, suggesting this could be a viable optimization strategy for MLLMs. \n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token. mT5 continues to use this structure. \n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure.",
            "score": 0.5682713077791788,
            "section_title": "D. Training MLLMs",
            "char_start_offset": 44592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2090
                },
                {
                    "start": 2093,
                    "end": 2234
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "268020578",
            "title": "The evolution of transformer models from unidirectional to bidirectional in Natural Language Processing",
            "text": "Transformer models have revolutionized Natural Language Processing (NLP), transitioning from traditional sequential models to innovative architectures based on attention mechanisms. The shift from unidirectional to bidirectional models has been a remarkable development in NLP. This paper mainly focuses on the evolution of NLP caused by Transformer models, with the transition from unidirectional to bidirectional modeling. This paper explores how the transformer model has revolutionized NLP, and the evolution from traditional sequential models to innovative attention-driven architectures. In this paper, it mainly discusses the limitations of traditional NLP models like RNNs, LSTMs and CNN when handling lengthy text sequences and complex dependencies, highlighting how transformer models, employing self-attention mechanisms and bidirectional modeling (e.g., BERT and GPT), have significantly improved NLP tasks. It provides a thorough review of the shift from unidirectional to bidirectional transformer models, offering insights into their utilization and development. Finally, this paper concludes with a summary and outlook for the entire study.",
            "score": 0.5682610811687103,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42919921875
        },
        {
            "corpus_id": "263605499",
            "title": "Label Supervised LLaMA Finetuning",
            "text": "The causal masks CM , as shown in Equation 2, \n\nin decoder blocks prevent information leaking, as the decoder is only allowed to attend to earlier positions in text generation. Bidirectional dependency extraction of the self-attention layer is reduced to single-direction, leading to critical information loss at the token level. Our empirical studies show that using token representations learned with causal masks significantly underperforms in token classification tasks. To address such an issue, we remove the causal masks from LlamaForTokenClassification5 and extract the latent representations H \u2032 for token classification, as illustrated in Figure 2, \n\nThe essential bidirectional information is expected to be replenished in token representations during finetuning as all the tokens can attend to each other. \n\nWe believe the essential global dependency is also helpful in the sequence classification, and hence, remove the causal masks in LlamaForSequenceClassification. With bidirectional self-attention resumed, we have more choices in pooling. We have tested three pooling methods, i.e., max, average, and last, and the experiments show that max-over-time pooling yields better performance than average pooling and last-token pooling in classification tasks without causal masks. \n\n3 Experiment",
            "score": 0.567968015027938,
            "section_title": "Label-supervised unmasked LLaMA",
            "char_start_offset": 8049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1307
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3955078125
        },
        {
            "corpus_id": "252280642",
            "title": "Automatically Recommend Code Updates: Are We There Yet?",
            "text": "These models are pre-trained LMs of Code (CodeLMs) on extensive programming code corpora and, akin to their general-purpose counterparts, can be fine-tuned on specialized coderelated tasks. These pre-trained models can be categorized into three groups: encoder-only models, decoder-only models, and encoder-decoder models. Encoder-based models, like CodeBERT [17], exclusively use a bidirectional transformer encoder with an attention mechanism to learn vectorized embeddings of input code sequences. These models are best suited for non-generation downstream tasks, such as code representation and code clone detection [17]. Decoder-based models, including CodeGPT [61] and Codex [11], employ an autoregressive transformer decoder to generate code sequences. Unlike encoder-only models that focus on input code embeddings, decoder-only models excel in open-ended code generation tasks [11]. Finally, encoder-decoder-based models like CodeT5 [89] feature both a bidirectional transformer encoder and an autoregressive transformer decoder. The encoder computes input code embeddings, while the decoder generates code. This flexibility allows encoder-decoder models to accommodate both non-generation and generation downstream tasks [63,74,87]. These code-focused pre-trained models have demonstrated exceptional performance in tasks like code summarization [74], code search [87], and code completion [39]. The development of advanced pre-trained CodeLMs, encompassing both general-purpose and domain-specific varieties, has significantly expanded the horizons of NLP and AI research, extending its applicability to the realm of software engineering.",
            "score": 0.5672213896077569,
            "section_title": "Language Models",
            "char_start_offset": 3611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1649
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 666,
                    "end": 670,
                    "matchedPaperCorpusId": "231855531"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "256662599"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "246822923"
                },
                {
                    "start": 1238,
                    "end": 1241,
                    "matchedPaperCorpusId": "244908620"
                },
                {
                    "start": 1356,
                    "end": 1360,
                    "matchedPaperCorpusId": "246822923"
                },
                {
                    "start": 1374,
                    "end": 1378,
                    "matchedPaperCorpusId": "244908620"
                },
                {
                    "start": 1400,
                    "end": 1404,
                    "matchedPaperCorpusId": "246823166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26708984375
        },
        {
            "corpus_id": "258461497",
            "title": "Exploring the Protein Sequence Space with Global Generative Models",
            "text": "The Transformer has emerged as the most critical development in AI in the last years (Vaswani et al. 2017), enabling the implementation of a myriad of language models. Its success is mainly attributable to the attention mechanism (Bahdanau et al. 2014), which originated as a solution to traditional sequence-to-sequence (seq2seq) models (Fig. 2). In seq2seq models, the input (a sentence) is stepwise processed in the encoder to produce a context vector passed to the decoder, an architecture that however exhibited degrading performance and increasing times with sequence length. \n\nThe attention mechanism provided a solution to these problems since it allows the decoder to analyze the whole input and focus on specific parts, a notion similar to attention in the human mind. A simplified example of the attention mechanism is to focus on the input word 'home,' when outputting the word 'maison' in an English-to-French translation (Fig. 2a). The Transformer not only mediated the attention mechanism between the two modules but also throughout them, producing a much better performance in many tasks. Following these advances, researchers soon started exploring the modules' performance separately. In this direction, Devlin et al. pre-trained Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al. 2018). BERT is also inspired by the Transformer architecture. Still, given that in this case, the interest lies in creating representations of text input, it only uses the encoder module (Fig. 2b). Models like BERT are trained by corrupting the input tokens in some way, e.g., by masking and trying to reconstruct the original sentence, such as in cloze tests. Soon after, OpenAI released GPT (Generative Pretrained Transformer), the first of a series of highly performing generative models, the most recent being ChatGPT and GPT4. GPT was pre-trained on the classic language modeling task, namely, predicting the next item of a sequence based on the previous onesa task that makes it particularly powerful for language generation. Models trained on this objective are termed autoregressive, and their architecture corresponds to the decoder module (Fig. 2c).",
            "score": 0.5666691540399695,
            "section_title": "Transformer-based language models",
            "char_start_offset": 3721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2185
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2333984375
        },
        {
            "corpus_id": "275544523",
            "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
            "text": "Representation Learning. Text representation learning focuses on understanding contextual relationships within sentences. Traditionally, encoder models dominated this field due to their bidirectional context modeling, using masked language modeling for token-level representations (Devlin et al., 2019;Liu et al., 2019;He et al., 2020;Clark et al., 2020;He et al., 2021) and special tokens with similarity-based optimization for sentencelevel understanding (Gunel et al., 2020;Reimers and Gurevych, 2019;Wu et al., 2020;Carlsson et al., 2021;Gao et al., 2021;Wei et al., 2020). Recent work has explored adapting decoder-only LLMs for text encoding through various methods, including introducing special tokens to the model's vocabulary (Zhang et al., 2024), using last-token or mean-pooled representations (Neelakantan et al., 2022;Wang et al., 2023), or fine-tuning with masked modeling (BehnamGhader et al., 2024) or label supervision (Li et al., 2023;Duki'c and vSnajder, 2024). While some approaches modify the decoder's causal attention to be bidirectional (BehnamGhader et al., 2024;Muennighoff et al., 2024;Li and Li, 2023;Duki'c and vSnajder, 2024;Man et al., 2024), this often compromises the model's text generation abilities. In contrast, MAGNET employs a hybrid attention mechanism that combines causal and bidirectional attention, enabling both robust representation learning and preserved generation capabilities. Text Infilling. Text infilling requires considering both left and right context when generating text in the middle of a sequence. Encoder-decoder models (Raffel et al., 2019;Lewis et al., 2019;Kalinsky et al., 2023) can handle this task by encoding available context and decoding infilled text.",
            "score": 0.5664913255135076,
            "section_title": "Related Works",
            "char_start_offset": 3793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1722
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 302,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 335,
                    "end": 354,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 477,
                    "end": 504,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 520,
                    "end": 542,
                    "matchedPaperCorpusId": "235613354"
                },
                {
                    "start": 542,
                    "end": 559,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 736,
                    "end": 756,
                    "matchedPaperCorpusId": "272524678"
                },
                {
                    "start": 954,
                    "end": 980,
                    "matchedPaperCorpusId": "267301268"
                },
                {
                    "start": 1130,
                    "end": 1156,
                    "matchedPaperCorpusId": "267301268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494140625
        },
        {
            "corpus_id": "265695874",
            "title": "Exploring the reversal curse and other deductive logical reasoning in BERT and GPT-based large language models",
            "text": "Contrasting with the GPT models, BERT is a bidirectional transformer-based encoder model. While their structures are quite similar, a key difference is that BERT does not employ a masking mechanism in its multi-head attention process. During training, all input tokens interact with each other to compute the final loss function. A classification layer is applied on top of the classification token, and binary cross entropy is used as the loss function. This bidirectional structure ensures that both \u27e8name\u27e9 and \u27e8description\u27e9 are updated jointly during training, irrespective of their sequence in the prompt. Once trained, the model has \"learned\" to use information from both directions to make inferences. This means that even if the order of A and B is reversed, their joint presence will still aid the BERT model in accurately predicting the final outcomes. \n\nThe fundamental distinction between decoder and encoder language models resides in their respective methods of managing context and sequence during the training process. GPT's unidirectional focus can limit its ability to infer reverse logical relationships, which is a result of its training mechanism that relies on the sequence of the data presented. On the other hand, BERT's bidirectional nature allows it to effectively grasp the context in both directions, making it more proficient in understanding and predicting relationships in reverse order. This fundamental difference in structure and training methodology makes BERT more versatile for tasks that require an understanding of bidirectional or reverse relationships, while GPT may excel in tasks that benefit from its sequential prediction capabilities. \n\nIn practical cases, the structural differences between GPT and BERT have significant implications for their real-world applications. For tasks involving sequence generation, storytelling, or content creation, where the flow and continuity from one token to the next are paramount, GPT's sequential predictive ability makes it a preferred choice. Its design to predict the next token based on the preceding context aligns well with these requirements. In contrast, BERT's bidirectional understanding is particularly beneficial in tasks requiring nuanced comprehension of context, such as sentiment analysis, text classification, and questionanswering, where understanding the interplay between all parts of the input is crucial.",
            "score": 0.5658743495333667,
            "section_title": "BERT can deduce \"B is A\" from \"A is B\"",
            "char_start_offset": 23581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2409
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.362548828125
        },
        {
            "corpus_id": "265150001",
            "title": "An Analysis and Mitigation of the Reversal Curse",
            "text": "Bi-Directional \n\nConverting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask, as doing so would introduce positional information that the model has never encountered during training, in which stage a query vector is only allowed to calculate the inner product with its preceding key vectors. This is evident in Eq.4: the relative position n \u2212 m is always non-positive during training but is positive when q m needs to attend to k >m . To address this issue, we propose a modification to the inner product between q m and k n for arbitrary values of m and n in a causal language model, as follows: \n\n(5) This adjustment ensures that when a query vector calculates an inner product with subsequent keys, there is no unexpected relative position information compared to training, as long as the relative distance between m and n does not exceed the maximum context length which is not within the scope of this paper. \n\nTo implement the Eq.5: when n \u2264 m, we calculate the attention weights as usual; In cases where n > m, we incorporate positional information with R \u22a4 \u03b8 , the transposition of R \u03b8 . Because R \u22a4 \u03b8,m is equivalent to R \u03b8,\u2212m for any given position m, we have: \n\n(6) Figure 3 illustrates this modification of attention calculation, where purple lines and squares denote that attention weights are calculated using the standard R \u03b8 matrix, and yellow indicates that the query attends to its succeeding keys within the extended bidirectional attention mechanism. The annotated numbers indicate the relative distance between a query and a key vector, with all values being non-positive.",
            "score": 0.5657012873746977,
            "section_title": "Extending Causal Attention to",
            "char_start_offset": 17265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 17,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1714
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5771484375
        },
        {
            "corpus_id": "277626724",
            "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
            "text": "Pretraining LLMs is both compute and time intensive. To reduce the amount of training required, we propose to adapt existing decoder-only LLMs to encoder-decoder and leverage pretrained decoder-only checkpoints for initialization, as shown in Figure 1. Due to this, we keep the encoder-decoder architecture as similar as possible to original decoder-only model, only introducing changes when necessary. This results in the following architecture: \n\n1. Encoder has exactly the same architecture as the decoder-only model, but self-attention is switched from causal to bidirectional. We provide ablations in Section 6 that illustrate the critical effect of bidirectional attention on downstream performance. \n\n2. In each Decoder block, FFN and self-attention parts are identical to the corresponding parts in decoder-only models, and cross-attention has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder. \n\nWe base our study on Gemma 2 (Team et al., 2024). But note our approach is highly flexible and isn't restricted to specific decoder-only architectures. We can easily apply our method to other model families, such as LLaMA (Dubey et al., 2024), QWen (Yang et al., 2024), and DeepSeek (Liu et al., 2024a). In theory, we can also adapt decoder-only models from different families, such as pairing LLaMA models with QWen models. \n\nIn addition, our approach allows for unbalanced encoderdecoder models, where the decoder is significantly smaller than the encoder. This provides better support for applications where input processing capabilities are more important than generative capacity. For example, for summarization, deep understanding of the input text is often more important than the generation part, as it doesn't need to generate any new information. As a result, generation time is significantly reduced, while providing competitive quality.",
            "score": 0.5655866129360188,
            "section_title": "Architecture",
            "char_start_offset": 6279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1913
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "252780086",
            "title": "Understanding HTML with Large Language Models",
            "text": "Encoder-decoder T5 based models perform better across all three tasks. On the Autonomous Web Navigation task, encoder-decoder (WebN-T5) architectures are better or comparable to WebN-LaMDA-1B (Figure 2a). On the Semantic Classification, the smallest encoder-decoder model (WebC-T5-base) performs comparably to much larger decoder-only models (WebC-LaMDA-1B or WebC-PaLM-8B) and the largest encoder-only model (WebC-BERT-large) which has 85M more parameters (Table 2). We also observe that decoder-only PaLM-8B performs worse than much-smaller encoder-decoder T5-large when trained only on HTML data. Finally, on the Description Generation encoder-decoder architecture has higher BLEU score. \n\nOne possible explanation for the strong performance of T5-based moels is the encoder-decoder architecture of these models. Namely, T5 models utilize an encoder with a bidirectional attention mechanism, not present in the LaMDA and PaLM decoders. The bidirectional attention mechanism can process HTML pages from both ends, potentially overcoming the loss of information when tree-structured HTML pages are converted into a fixed linear text sequences.",
            "score": 0.5654223082786051,
            "section_title": "ARCHITECTURE EFFECT: T5-BASED MODELS PERFORM BEST ACROSS ALL TASKS",
            "char_start_offset": 28290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1144
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2220458984375
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "In Section 3.1, we let Mamba be the base model  of retriever. As comparison, the base model  is changed from Mamba to other frequently-used base models, including the Transformer encoderonly models and decoder-only models. In this section, we analyze the differences between using Mamba and these models as . \n\nMamba and Decoder-only vs Encoder-only. Mamba and Transformer decoder-only models share similarities that distinguish them from Transformer encoder-only models. In terms of data, Mamba and decoder-only models are pre-trained on more data than most encoder-only models. In particular, Mamba and Pythia [2] are pre-trained on the same data. In terms of architecture, Mamba and decoder-only models have causal characteristics, which is not as suitable as encoder-only models with bi-directional attention for comprehension tasks like retrieval. Mamba can be reconstructed to be bi-directional, but this would lead to a decrease in efficiency. \n\nMamba vs Decoder-only. Intuitively, Transformer decoderonly model can capture long-term dependencies by self-attention mechanism, while Mamba may be limited by the maximum amount of information that can be compressed in latent states. \n\nHowever, some works [1,3,5,16] analyze that Mamba has some mechanism similar to or even surpassing Transformer: Mamba has implicit attention mechanism with good expressiveness; if each SSM is regarded as one head in multi-head self-attention mechanism, then Mamba has more heads than the Transformer; the softmax in self-attention can cause problems, such as over-smoothing, whereas Mamba does not use softmax and thus may better capture subtle differences between different tokens. \n\nIn addition, Mamba has an additional explicit process of summarizing previous information using the latent states. When calculating a token at position , decoder-only model uses the attention mechanism to access keys and values of all previous tokens.",
            "score": 0.5653659240954104,
            "section_title": "Base Model Comparison",
            "char_start_offset": 8036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1926
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497314453125
        },
        {
            "corpus_id": "270832367",
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "text": "These generative models are exclusively causal, that is, they produce text from left to right by recursively feeding the model with previously generated sequences. As in the case of Recurrent Neural Networks (RNN), decoder-only transformers are expectation-based word predictors. These systems tend to favor structures in which related elements are close along the sequence, such as relative clause attachments to syntactically lower nominals in ambiguous contexts, which fits nicely into English syntax [17]. \n\nHowever, the mutually beneficial congruence between causal language modeling and English may not apply to other languages. Not only does Spanish prefer a higher nominal attachment in the resolution of ambiguous relative clauses, but its syntax is also highly flexible, even within declarative sentences [18]. This is strongly opposed to the more strict subject-verb-object structure of the English language, which allows for few inversion exceptions [19]. \n\nUnlike causal language models, encoder-only non-causal language models generate word embeddings using bidirectional contexts, which means that the model output can be conditioned by both left and right tokens. This eliminates the output sequence's sequential dependencies and allows alternative generation orders. \n\nIn light of this, we depart from the hypothesis that decoderonly (causal) transformer language models may introduce generation bias in target languages with less rigid word ordering than English, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. To put this hypothesis to test, in addition to English, we consider and Spanish, a language with a different grammatical structure and also a broad base of speakers (these languages sum over 1.5 billion and 0.5 billion speakers, respectively, a substantial share of the world's population). However, the approaches in this work can be extended to obtain insights on other languages and NLP tasks. \n\nOur contributions are: A) First, we present a novel information-theoretic approach to study language predictability. We compare the causal context-conditioned entropy and the noncausal context-conditioned entropy of the grammatical category distribution of source natural texts to assess whether their language is more predictable from causal or non-causal language contexts.",
            "score": 0.5644495865814501,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "218470598"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "146854174"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "235555573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3671875
        },
        {
            "corpus_id": "264172191",
            "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
            "text": "Language models are designed to assign probabilities for every possible sequence of generated text. This overarching goal can be achieved through two primary approaches: autoregressive and non-autoregressive language modeling. Autoregressive language models typically concentrate on natural language generation and employ a \"next-word prediction\" pretrain task (Radford et al., 2018(Radford et al., , 2019Brown et al., 2020a). In contrast, non-autoregressive models focus more on natural language understanding, frequently leveraging the masked language modeling objective as their foundational task (Devlin et al., 2019a). Classic models from the BERT family fall under the category of non-autoregressive models (Devlin et (Zoph et al., 2022), which are built upon the encoder-decoder structure, as well as BloombergGPT , Gopher (Rae et al., 2021) and Claude 2 (Models, C.), which are based on decoder architectures. The majority of LLMs are based on decoder-only structures, and a significant reason for this is the leading results achieved by OpenAI in the GPT series (from GPT-1 to , with the decoder-only family of models demonstrating impressive performance. Besides the decoder-only structure, there is another type of architecture known as the prefix-decoder architecture, which has found some degree of application in LLMs. In contrast to the \"next-word prediction\" function used in decoder-only LLMs, the prefix-decoder architecture employs bidirectional attention on prefix tokens, similar to an encoder, while maintaining consistency with the decoder-only LLMs for the prediction of subsequent tokens. Existing representative LLMs based on prefix decoders include GLM130B (Zeng et al., 2022) and U-PaLM (Tay et al., 2022b).",
            "score": 0.5642180600921871,
            "section_title": "Modeling",
            "char_start_offset": 5383,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28076171875
        },
        {
            "corpus_id": "52297912",
            "title": "Bidirectional Attentional Encoder-Decoder Model and Bidirectional Beam Search for Abstractive Summarization",
            "text": "Generative models with encoder-decoder architecture have recently attracted an extensive interest. In it is basic form, the input is encoded into a vector representation. The encoder output is used as an initial input to the decoder. An RNN variant, such as LSTM, is used as a decoder that sequentially generates the output. Most of the previously proposed encoder-decoder models used a bidirectional encoder and unidirectional decoder. The bidirectional decoder topology was used in a few works to solve some specific tasks. For example, a work proposed by (Wang et al. 2016) used a bidirectional decoder as a try to make use of the past and future context information during generating an image caption. Two embeddings were used, sentence embeddings and visual embeddings. The sentence embeddings were encoded using a bidirectional LSTM and the visual embeddings were encoded by CNN. Furthermore, they used a deep bidirectional LSTM architecture to learn higher level embeddings. A standard beam search algorithm was used to select the optimal output sequence. Another work has been proposed by (Liu, Finch, et al. 2016), in which they utilized the agreement between a pair of target-directional LSTMs, one generates sequences from the left-to-right and the other generates sequences from right-to-left, to generate more balanced outputs. Moreover, they introduced two approximate search models which used only a small subset of the entire search space. Their paper was addressing the machine transliteration and grapheme-phoneme problems. Qing Sun (Sun, Lee, and Batra 2017) presented Bidirectional Beam Search to address the Fill-in-the-Blank Image Captioning task. They used both past and future sentence structure to recreate sensible picture depictions. They started by decomposing the bidirectional RNN into two unidirectional RNN. Then, a beam search was performed on one direction while holding the beams in the other direction fixed. We discussed the difference between this work and ours earlier.",
            "score": 0.5638511187358626,
            "section_title": "Bidirectional RNN",
            "char_start_offset": 24365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 575,
                    "matchedPaperCorpusId": "91460"
                },
                {
                    "start": 1097,
                    "end": 1122,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1551,
                    "end": 1577,
                    "matchedPaperCorpusId": "7864745"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "238419670",
            "title": "Pretrained Language Models are Symbolic Mathematics Solvers too!",
            "text": "Attention [4] is a powerful mechanism led to recent achievements in developing strong DNN models in NLP like the transformer architecture [46]. Attention mechanism has also been used in other tasks such as visual explanation [20], video captioning [53], healthcare [12], object detection [13], and speech recognition [29]. The transformer architecture introduced in [46] is an autoencoder that encodes the input data and then decodes them to the target domain. It does not use recurrent modulus and just uses self-attention mechanism. It is a breakthrough in NLP and is the base for many language models including bidirectional encoder representations from transformers, BERT, [16], generative pretrained transformer, GPT-3, [8], Text-to-Text Transfer Transformer, T5, [38] and Google's Meena [1]. It has also been successfully used as a baseline in other tasks such as object detection [9], image generation [11], image colorization [25], video understanding [41], and visual question answering [43]. Furthermore, Yun et al. [54] showed that transformers can universally approximate sequence to sequence functions. Therefore, the transformer is a good choice for transfer learning not only because of their prosperity across different tasks, but also because of its architecture which makes it possible to use the hardware parallelism to train much more big models with much more training data.",
            "score": 0.5637571261692371,
            "section_title": "Transformers in different modalities",
            "char_start_offset": 25603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1395
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "56895339"
                },
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "198318847"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "220419534"
                },
                {
                    "start": 366,
                    "end": 370,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 887,
                    "end": 890,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "227239228"
                },
                {
                    "start": 960,
                    "end": 964,
                    "matchedPaperCorpusId": "102483628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2213134765625
        },
        {
            "corpus_id": "276771845",
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "text": "When processing sequences of varying lengths in batched inputs, padding is essential to create uniform input tensors. However, the choice between left padding (prepending padding tokens) and right padding (appending padding tokens) may impacts model behavior, especially when adapting encoder model from decoder-only models. \n\nIn decoder-only transformer models, such as those used for language generation (e.g., GPT (OpenAI, 2023), Gemma (Gemma Team, 2024a,b) models), left padding is typically used to align the inputs during training or inference, due to the auto-regressive training objective and efficient positional embedding. Decoder-only models are trained in an auto-regressive manner, where the task is to predict the next token based on all previous tokens. Decoder-only transformer models use positional embeddings to encode the order of tokens. Left-padding ensures that the relative positions of the actual tokens remain consistent regardless of the sequence length. \n\nHowever, right padding is acceptable for encoder models because of the way these models process input sequences. Unlike decoder-only models, encoders like those in BERT (Devlin et al., 2019), T5 (Raffel et al., 2020) or other bidirectional transformer models handle the entire input sequence at once, and their attention mechanisms allow tokens to attend to any other token in the input through bidirectional attention. In encoder models, positional embeddings are applied to the entire sequence, including padding tokens. Since the padding tokens are ignored during attention, their positional embeddings don't interfere with the actual computation. Whether the padding is on the right or left does not affect the functionality. \n\nThe choice of padding strategy has implications for the pooling layer, especially in conjunction with causal attention and First-K/Last-K token pooling.",
            "score": 0.5634673674060909,
            "section_title": "Padding Strategies",
            "char_start_offset": 10417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 1152,
                    "end": 1173,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1178,
                    "end": 1199,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5224609375
        },
        {
            "corpus_id": "231925325",
            "title": "Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition",
            "text": "Nowadays, ASR models are widely embedded in systems like smart devices and chatbots. In addition, application of attention mechanism is showing great potential in achieving higher effectiveness and efficiency for ASR. From the middle of last decade, a lot of progress has been made on attention-based Local/Hard [23] At each decoder time step, a set of encoder hidden states (within a window) are attended. \n\nContent-based [24] Attention calculated only using the content information of the encoder hidden states. \n\nLocation-based [25] Attention calculation depends only on the decoder states and not on the encoder hidden states. \n\nHybrid [11] Attention calculated using both content and location information. \n\nSelf [20] Attention calculated over different positions(or tokens) of a sequence itself. \n\n2D [26] Attention calculated over both timeand frequency-domains. \n\nHard monotonic [27] At each decoder time step, only one encoder hidden state is attended. \n\nMonotonic chunkwise [28] At each decoder time step, a chunk of encoder states (prior to and including the hidden state identified by the hard monotonic attention) are attended. \n\nAdaptive monotonic chunkwise [29] At each decoder time step, the chunk of encoder hidden states to be attended is computed adaptively. models. Recently, some survey papers [21], [22] have presented the development of attention-based models on natural language processing (NLP). These survey papers have documented the advancement of a wide range of NLP applications like machine translation, text and document classification, text summarisation, question answering, sentiment analysis, and speech processing. However, the existing literature still lacks a survey specifically targeted on the evolution of attention-based models for ASR. Therefore, we have been motivated to write this paper. The rest of paper is organised as follows. Section II provides a simple explanation of Attention mechanism. A brief introduction to attention-based encoder-decoder architecture is discussed in Section III. Section IV discusses the evolution of offline speech recognition followed by the evolution of streaming speech recognition in Section V. Finally Section VI concludes the paper.",
            "score": 0.5630266033327265,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 312,
                    "end": 316,
                    "matchedPaperCorpusId": "11760007"
                },
                {
                    "start": 423,
                    "end": 427,
                    "matchedPaperCorpusId": "18165915"
                },
                {
                    "start": 640,
                    "end": 644,
                    "matchedPaperCorpusId": "1921173"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 887,
                    "end": 891,
                    "matchedPaperCorpusId": "14345813"
                },
                {
                    "start": 984,
                    "end": 988,
                    "matchedPaperCorpusId": "3538865"
                },
                {
                    "start": 1172,
                    "end": 1176,
                    "matchedPaperCorpusId": "53290672"
                },
                {
                    "start": 1321,
                    "end": 1325,
                    "matchedPaperCorpusId": "218972088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "272367199",
            "title": "A Survey for Large Language Models in Biomedicine",
            "text": "The encoder-decoder architecture, also known as the sequence-to-sequence (seq2seq) model, combines the strengths of both encoder and decoder components. This design makes it suitable for tasks that involve transforming one sequence into another, such as machine translation, text summarization, and question answering. In this architecture, the encoder processes the input sequence and compresses it into a latent representation. The decoder then uses this representation to generate the target sequence [30]. This separation of encoding and decoding allows the model to handle input and output sequences of different lengths and structures effectively. Two examples of encoderdecoder models are the Text-To-Text Transfer Transformer (T5) [31] and Bidirectional and Auto-Regressive Transformers (BART) [32] T5 adopts a unified approach by framing all NLP tasks as text-to-text problems, demonstrating remarkable versatility and strong multitask processing capabilities. BART, on the other hand, combines the bidirectional nature of BERT-like encoders with the autoregressive generation of GPTlike decoders, making it particularly effective for text generation and repair tasks. In biomedical applications, encoder-decoder models have shown significant potential. For instance, BioBART [33] has been adapted for biomedical text generation and summarization tasks. Another notable example is GeneCompass [34], a cross-species large language model designed to decipher gene regulatory mechanisms. These applications highlight the architecture's versatility in addressing complex biomedical challenges, from text processing to unraveling the intricacies of genetic regulation across different species.",
            "score": 0.563008813594237,
            "section_title": "Encoder-Decoder Architecture",
            "char_start_offset": 9252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1402,
                    "end": 1406,
                    "matchedPaperCorpusId": "263610118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.185546875
        },
        {
            "corpus_id": "222381581",
            "title": "Survive the Schema Changes: Integration of Unmanaged Data Using Deep Learning",
            "text": "In recent several years, Natural Language Processing (NLP) has experienced several major advancements including the bi-directional mechanism, attention mechanism, transformer mechanism, and so on. Existing works show that the final hidden state in Bi-LSTM networks cannot capture all important information in a long sentence. Therefore, the attention mechanism was introduced to address the problem by preserving information from all hidden states from encoder cells and aligning them with the current target output. Later such idea was integrated into the transformer architectures, so that encoders had self-attention layers, while decoders had encoder-decoder attention layers. Most recently, to make the transformer architecture more flexible to applications other than language translation, GPT-2 that only uses the decoders' part and BERT that only uses encoders' part are invented and achieve great success in a broad class of NLP problems. Our assumption is that on one hand, more complicated models like GPT-2 and BERT may naturally achieve better accuracy than a simpler model like Bi-LSTM; but on the other hand, these complex models may require significantly higher storage and computational resources, as well as more training data. It is important to know the trade-offs among accuracy, latency, and resource consumption, made by different model architectures. We mainly consider two types of language model architectures: (1) simple and compact sequence models based on customized local character-based embedding and Bi-LSTM; and (2) complex and large pre-trained transformer models, such as GPT-2 [62] and BERT [20]. \n\n1) Sequence Model (Bi-LSTM): Our Bi-LSTM model architecture, includes an embedding layer that has 150 neurons; a Bi-LSTM layer that consists of 512 neurons; and a fullyconnected layer that has 256 neurons. \n\n2) Transformer Model: Moreover, we also consider transformer models based on GPT-2 [62] and BERT [20]. We use a pre-trained GPT-2 small model or a pre-trained BERT base model as the backend, which connects to a frontend classifier composed of four convolutional layers and a fully connected layer.",
            "score": 0.5629636044077031,
            "section_title": "A. Model Architectures",
            "char_start_offset": 20601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1840
                },
                {
                    "start": 1843,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1926,
                    "end": 1930,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "278602876",
            "title": "Variational Prefix Tuning for diverse and accurate code summarization using pre-trained language models",
            "text": "Current mainstream Large Language Models for Code (LLMC) predominantly utilize Multi-head Attention Mechanisms as proposed by Vaswani et al. [27]. These transformer-based models typically feature several layers of Multi-head Attention, equipped with a substantial number of trainable parameters. This architecture enables the models to effectively capture complex term dependencies and contextual information within both source code and natural language. The multi-head attention process is mathematically represented as follows: \n\nwhere \n\nTransformer-based large pre-trained models for code can be divided into three main categories based on their architectures: \n\n1. Encoder-Only: BERT-like models utilize Bidirectional Multi-head Attention. Their primary pre-training tasks include Masked Language Modeling (MLM) and other tasks specifically designed for code [9]. Notable models include CodeBERT [9], GraphCodeBERT [28], and ContraBERT [29]. Encoder-only models are often used for classification or retrieval tasks and can only perform generation tasks such as code summarization when attached to a decoder and fine-tuned. 2. Decoder-Only: GPT-like models employ Multi-head Attention with causal masks, preventing tokens from attending to future positions. They are mainly pre-trained in an autoregressive manner. Notable models include CodeGPT [30], Codex [31], and CodeLlama [11]. Decoderonly models can generally scale well and are often used for generalpurpose generation tasks with prompting, as their large scale of parameters allows them to encode a wide range of knowledge. 3. Encoder-Decoder: These models maintain a transformer-like structure and treat input and output as seq2seq tasks, allowing for flexible pre-training tasks to be applied to both the encoder and the decoder. Notable models include CodeT5 [10] and PLBART [32]. Encoderdecoder models have demonstrated their effectiveness on seq2seq tasks such as code summarization and translation [10,33]. \n\nTransformer architecture.",
            "score": 0.562511229809947,
            "section_title": "Large Language Model of Code (LLMC)",
            "char_start_offset": 8147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 863,
                    "end": 866,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 900,
                    "end": 903,
                    "matchedPaperCorpusId": "211171605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4716796875
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "LLMs on Code Transformers capture dependency among sequence elements through attention mechanism (Bahdanau et al., 2014) and are highly scalable, as shown in natural language processing (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020). Several efforts explore these models for program synthesis (Chen et al., 2021a;Austin et al., 2021;Li et al., 2022;Fried et al., 2022;Nijkamp et al., 2022;Allal et al., 2023) and its effectiveness (Vaithilingam et al., 2022). \n\nAblation Studies Raffel et al. (2020) introduce the concept of non-causal decoder in the form of a Prefix-LM with favorable performance over causal decoders after fine-tuning on down-stream tasks. \n\nThe performance in few-shot generative tasks was not evaluated. Wang et al. (2022) conduct an extensive ablation study over architectures and objectives with the conclusion that decoder-only models with causal language modeling exhibit the strongest zero-shot generalization. Therefore, we limit our investigation to causal and non-causal decoders. Tay et al. (2022a) compare encoder-decoder, decoder-only, and Prefix-LM architectures and report the beneficial performance of encoder-decoder models, while zero-shot generation tasks are not evaluated. The authors later adopt Prefix-LM instead of encoder-decoder in (Tay et al., 2022b). \n\nData Mixtures LaMDA (Thoppilan et al., 2022) was trained on a mixture of various data sources including dialogues, code documents, Q&A data, tutorials, and, Wikipedia. However, the impact of this mixture and the specific sources are unclear. Xie et al. ( 2023) introduces a data selection method based on importance resampling which allows to mix datasets of various sizes, however, the evaluation only covers encoder-only models.",
            "score": 0.5622252884210016,
            "section_title": "RELATED WORK",
            "char_start_offset": 5422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1744
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 207,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 226,
                    "end": 246,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 347,
                    "end": 363,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 445,
                    "end": 472,
                    "matchedPaperCorpusId": "247255943"
                },
                {
                    "start": 493,
                    "end": 513,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 739,
                    "end": 757,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205810546875
        },
        {
            "corpus_id": "246863916",
            "title": "ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer",
            "text": "Transformer's attention mechanism was proposed based on the flow of introducing the attention mechanism into sequence-to-sequence model for machine translation [4,34]. Although the attention mechanism was introduced into the field of NLP as an auxiliary mechanism to compensate for information that cannot be captured by the recurrent neural network, Vaswani et al. [46] proposed the Transformer as a model with attention as the main mechanism. \n\nTransformer's attention mechanism maps a query and key-value pairs to an output. Their particular \"Scaled Dot-Product Attention\" can be described as follows. \n\n, ,  stand for query, key, and value, respectively.   is the dimension of , and scaling factor 1 \n\nwas introduced to prevent from dot product   growing large in magnitude and causing extremely small gradients in softmax function. \n\nThe encoder of the vanilla transformer contains self-attention layers. In a self-attention layer, queries and key-value pairs are come from the same place. The relationship, or attention, between each element of the sequence given as input and all other elements is calculated. In the decoder, each layer uses an output of the previous layer as queries, and key-value pairs come from the output of the encoder. \n\nUNITER is a model that uses only the Encoder part of the Transformer, and embeds the input image and text, and then processes them with the self-attention mechanism. \n\nOur method, ViNTER, is an encoder-decoder model like BART, and its encoder is a modified version based on UNITER.",
            "score": 0.5617864890582442,
            "section_title": "Attention Mechanism in Transformers.",
            "char_start_offset": 10944,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1533
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 163,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 366,
                    "end": 370,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2069091796875
        },
        {
            "corpus_id": "267069342",
            "title": "Revolutionizing Finance with LLMs: An Overview of Applications and Insights",
            "text": "LLMs are primarily built upon the Transformer architecture [119], which has been central to their ability to handle complex language tasks. The Transformer model is structured with two key components: the Encoder and the Decoder, each consisting of multiple layers of self-attention and feed-forward neural networks. This architecture facilitates effective management of long-range dependencies within sequences. \n\nSelf-attention is characterized by its use of queries (Q), keys (K), and values (V), three vectors derived from the input data. Each element in the input sequence is transformed into these three vectors through linear transformation. The self-attention mechanism then computes the attention scores by taking the dot product of the query with all keys. These scores determine how much focus or 'attention' each element in the sequence should have in relation to every other element. The attention scores are normalized using a softmax function, ensuring they sum up to one, thus forming a probability distribution. The final output of the self-attention layer is a weighted sum of the value vectors, where the weights are the softmax-normalized attention scores. This process allows each output element of the self-attention layer to be a combination of the inputs, with the weights specifying the amount of attention given to each input element. The self-attention mechanism's ability to weigh inputs differently allows LLMs to capture complex relationships in the data, such as long-range dependencies, making it exceptionally powerful for tasks that require an understanding of context and sequence. \n\nThe architecture of LLMs typically falls into one of two categories: Decoderonly and Encoder-Decoder. Decoder-only models, such as those in the GPT series [99,100], generate text in a unidirectional manner [12]. Each token in the input sequence attends only to preceding tokens, making them well-suited for tasks like text generation. The Encoder-Decoder models, like T5 [102] and BART [56], feature separate mechanisms for encoding input sequences and decoding them into target sequences. This design allows them to handle a broader range of tasks, including both generation and comprehension. \n\nToken generation in LLMs is a vital process, involving vocabulary creation, probability prediction, and techniques like beam search for sequence generation.",
            "score": 0.5616319713678781,
            "section_title": "Related Work 2.1 Large Language Models",
            "char_start_offset": 3859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2213
                },
                {
                    "start": 2216,
                    "end": 2372
                }
            ],
            "ref_mentions": [
                {
                    "start": 1778,
                    "end": 1782,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1825,
                    "end": 1829,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1990,
                    "end": 1995,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29833984375
        },
        {
            "corpus_id": "267617032",
            "title": "Large Language Models: A Survey",
            "text": "Instead of performing a single attention function with d model dimensional keys, values and queries, it is found to be beneficial to linearly project the queries, keys and values h with different, learned linear projections to d k , d k and d v dimensions, respectively. Positional encoding is incorporated to fuse information about the relative or absolute position of the tokens in the sequence. \n\n2) Encoder-Only: For this family, at each stage, the attention layers can access all the words in the initial sentence. The pre-training of these models usually consist of somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence. Encoder models are great for tasks requiring an understanding of the full sequence, such as sentence classification, named entity recognition, and extractive question answering. One prominent encoder only model is BERT (Bidirectional Encoder Representations from Transformers), proposed in [24]. \n\n3) Decoder-Only: For these models, at each stage, for any word, the attention layers can only access the words positioned before that in the sentence. These models are also sometimes called auto-regressive models. The pretraining of these models is usually formulated as predicting the next word (or token) in the sequence. The decoder-only models are best suited for tasks involving text generation. GPT models are prominent example of this model category.",
            "score": 0.561085262767008,
            "section_title": "1)",
            "char_start_offset": 33581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1484
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2381591796875
        },
        {
            "corpus_id": "224818566",
            "title": "Generalized Conditioned Dialogue Generation Based on Pre-trained Language Model",
            "text": "Recently, pre-trained language model (Peters et al., 2018;Sun et al., 2019;Zhang et al., 2019;Dong et al., 2019) are widely applied to various NLP tasks including dialogue generation. Some approaches (Wolf et al., 2019;Lin et al., 2019) utilize a decoder-only transformer initialized with GPT (Generative Pre-Training) parameters (Radford et al., 2018). This decoder-only architecture encodes dialogue history using only left-to-right attention (i.e. attention is allowed only to previous positions), which does not allow exploit the full context in a sentence. Using bi-directional attention could enable the encoder to leverage richer context information in dialogue history. Zheng et al. (2019) utilizes an encoder-decoder transformer architecture (Vaswani et al., 2017) with a clear separation between encoder and decoder. This makes it more difficult for the fine-tuning process to update the encoder's parameters, as has been shown in some previous work on abstractive summarization (Liu et al., 2018). In contrast, we use BERT (Devlin et al., 2018) for both encoder and decoder. Similar approaches have been used in non-conditioned dialogue (e.g. Bao et al. (2020)). To cope with the difference between encoder and decoder, we use different input representations and self-attention masks, which will be described in detail.",
            "score": 0.560973314694102,
            "section_title": "Pre-training Based Transformer",
            "char_start_offset": 8020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1330
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 58,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 94,
                    "end": 112,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 751,
                    "end": 773,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26806640625
        },
        {
            "corpus_id": "119302203",
            "title": "An Empirical Investigation of Global and Local Normalization for Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search",
            "text": "We changed the standard sequence-to-sequence model to be more suitable for the tagging task. This change also lets us perform controlled experiments pertaining to the amount of input sequence information available to the decoder at each time step. \n\nIn a standard encoder-decoder model with attention, the initial hidden state of the decoder is often some function of the final encoder state so that the decoder's predictions can be conditioned on the full input. For our tagging experiments, instead of influencing the initial decoder state with the encoder, we set it to a vector of zeros. Thus the information about input for prediction is only available via the attention mechanism. In addition to the change above, we also forced the model to attend to only the i th input representation while predicting the i th label. This is enforceable because the output length is equal to the input length and it is also a more suitable structure for a tagging model. With these changes in the decoder, we can precisely control the amount of information about the input available to the decoder at each prediction step. For example, with a unidirectional LSTM encoder, the decoder at i th step only has access to input till the i th token and the prediction history: \n\nThis setting lets us clearly explore the classical notion of label bias arising out of access to partial input at each prediction step (Section 2.3). A bidirectional LSTM encoder, however provides access to all of the input information to the decoder at all the prediction steps.",
            "score": 0.5601834783590168,
            "section_title": "Tagging model for ablation study",
            "char_start_offset": 17640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1543
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342041015625
        },
        {
            "corpus_id": "238744157",
            "title": "Automated Essay Scoring Using Transformer Models",
            "text": "Note. Adapted from the \"Hugging Face Course\", summary of chapter 1 (https://huggingface.co/course/chapter1/9). \n\nThe encoder-decoder architecture describes a sequence-to-sequence model as proposed in the original \"Attention is All You Need\" paper [3]. This type of architecture is particularly trained to transform (in the case of an NLP task) a text sequence of a certain format into a text sequence of another format, such as in translation tasks. \n\nThe encoder architecture includes (as indicated by the name) only the input, or left-hand side, of the original transformer architecture and transforms each input sequence into a numerical representation. Typically, encoder models like BERT, RoBERTa, or ALBERT are specially used for text classification or extractive question answering [4,45,46]. Since AES is a special case of text classification, this type of models is therefore particularly well suited to AES, and accordingly the use of the BERT model has become increasingly prominent in the literature on AES [47]. \n\nFinally, the decoder architecture includes only the output, or right-hand side, of the original transformer model. In the original model, the decoder takes the information of the input sequence from the encoder and generates in a stepwise procedure one token after the other for the output. At each step, the decoder not only considers the information from the decoder, but also combines it with the information given by the output tokens that were already generated. Using only the decoder architecture therefore results in a model that generates new tokens based on the tokens it already generated or based on a sequence of initial tokens it was provided with (typically called \"prompt\"). Decoder models, like the GPT-3 or the Transformer XL, are therefore applied to generate text outputs. By providing specifically designed prompts such models can be used for other tasks such as translation or classification tasks as well though [48]. \n\nAll mentioned models from the different architecture types have in common that they are, in a first step, trained self-supervised on large text corpora to calibrate powerful language models.",
            "score": 0.5594968704959411,
            "section_title": "Table 2. Transformer Architectures",
            "char_start_offset": 17578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 1019,
                    "end": 1023,
                    "matchedPaperCorpusId": "238762159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1622314453125
        },
        {
            "corpus_id": "232404009",
            "title": "Embedding API dependency graph for neural code generation",
            "text": "Neural machine translation models in natural language processing (NLP) field provide helpful inspiration for the code generation task. Recurrent neural network (RNN) [33] and it variants, especially long short-term memory (LSTM) [34], are commonly used as the encoder and decoder in the Seq2Seq model [11]. The Seq2Seq model transforms a source sequence into hidden vectors from which the decoder generates the corresponding target sequence word by word. Cho et al. [14] were the first to propose an RNN-based encoder-decoder neural network and used it for machine translation, laying a solid foundation for subsequent studies on Seq2Seq. Sutskever et al. [11] presented a general end-to-end approach to sequence learning and used two layers of LSTM as the encoder and decoder. The seq2seq model has achieved good results in machine translation.\n\nHowever, one disadvantage of the Seq2Seq model is that the last hidden state of the RNN in the encoder, namely the context vector, contains only a limited amount of information. The greater the text length is, the more the information could be lost; thus, the Seq2Seq model does not perform well on long text generation tasks [35]. The attention mechanism [36] can improve neural machine translation by selectively focusing on parts of the source sentence during translation. For example, Luong et al. [37] proposed an attention-based Seq2Seq approach, which applied both global and local attention mechanisms. Vaswani et al. [38] proposed the Transformer architecture, which is based solely on attention mechanisms. Currently, the Seq2Seq model with attention has been widely adopted in neural machine translation.",
            "score": 0.5594434285826466,
            "section_title": "Neural Machine Translation Models",
            "char_start_offset": 6752,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 170,
                    "matchedPaperCorpusId": "14711886"
                },
                {
                    "start": 301,
                    "end": 305,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 656,
                    "end": 660,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1173,
                    "end": 1177,
                    "matchedPaperCorpusId": "1055111"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "17195923"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.206787109375
        },
        {
            "corpus_id": "238744157",
            "title": "Automated Essay Scoring Using Transformer Models",
            "text": "For the application of a transformer model, it is fundamental to understand the three basic architecture types of a transformer: encoder models, decoder models, and encoder-decoder models. Table 2 provides a brief overview of the three types, the tasks they are predominantly used for, and exemplary models that have been implemented and trained based on the respective type. \n\nIn general, transformer models are neural networks based on the so-called attention mechanism and were originally introduced in the context of language translation. The attention mechanism was presented in 2014 by Bahdanau et al. [35]. They showed that instead of encoding a text from the source language into a vector representation and then decoding the vector representation into the text of the target language, the attention mechanism allows to avoid this bottleneck of a vector representation to be avoided between the encoder and decoder by allowing the model to directly search for relevant tokens in the source text when predicting the next token for the target text. 4  In 2017, Vaswani et al. [3] then showed that \"Attention is All You Need\". While the encoder and decoder models for translation tasks before were mainly based on RNNs, the authors demonstrated that not only can the described vector representation be replaced by an attention mechanism but the encoder and decoder models themselves can be implemented based on the attention mechanism alone. They implemented a self-attention mechanism, in which different attention layers tell the model to focus on informative words and neglect irrelevant words. They showed that this way the model achieves new performance records on several For a detailed introduction to the attention mechanism, we recommend Andrew Ng's lecture on Attention Model Intuition: https://www.youtube.com/watch?v=SysgYptB198 translation benchmarks while having a fraction of the training cost compared to the best models previously used. 5  A major advantage of the transformer architecture is that it allows parallel processing of the input data and is not affected by the vanishing gradient problem. This makes it possible to train with larger and larger datasets, resulting in better and better language models [36].",
            "score": 0.5592720990490192,
            "section_title": "Methodological Background on Transformer Models",
            "char_start_offset": 14514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2240
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2445068359375
        },
        {
            "corpus_id": "257220255",
            "title": "Abstractive Text Summarization using Attentive GRU based Encoder-Decoder",
            "text": "Nallapati et al [2] has proposed baseline encoder and decoder architecture where LSTM has been used. Bidirectional as well as unidirectional LSTM was used at encoder and decoder correspondingly. Word level and sentence level bidirectional GRU was used. Performance of basic encoder and decoder model has been improved through Bahdanau et al [1]. See et al. [3] offered a detailed study of numerous abstractive text summarization models for pointer-generator and RNN seq2seq models that are based on sequence-to-sequence encoder-decoder architecture. Sutskever et al. [4] proposed a multilayer LSTM based end-to-end solution to sequence learning. The input for the encoder was a fixed length of text, and the output for the decoder was the same. Lin et al [5] proposed global encoding mechanism of abstractive text summarization. In this paper, we have designed GRU based encoder and decoder with one extra attention layer. Shi et al [6] proposed to \"improve seq2seq models, making them capable of handling different challenges, such as saliency, fluency and human readability, and generate high-quality summaries\". Generally speaking, most of these techniques differ in one of these three categories: network structure, parameter inference, and decoding/generation. Luong et al [7] examines two simple and effective classes of attention mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. Ksenov et al [8] proposed \"the encoder and decoder of a Transformer-based neural model on the BERT language model\". Recently, a model proposed as \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\" [9] which captures the simplicity of BERT (Devlin et al.) [10] and GPT (Radford et al.) [11] and others pre-training schemes. BART opens many ways to thinking for fine-tuning in text summarization application.",
            "score": 0.5590029555179608,
            "section_title": "Related Works",
            "char_start_offset": 1958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1949
                }
            ],
            "ref_mentions": [
                {
                    "start": 567,
                    "end": 570,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 933,
                    "end": 936,
                    "matchedPaperCorpusId": "54448559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2607421875
        },
        {
            "corpus_id": "254877381",
            "title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers",
            "text": "Denoising with Encoder-Decoder Framework Unlike DiffuSeq (Gong et al., 2022) using encoder-only Transformer architecture, we propose using an encoder-decoder Transformers architecture to model the input and output text sequences. For z 0 \u03b8 (z t , w x , t), we use the encoder to process the input sequences w x and use the decoder to model the noisy output sequence z t . Following the previous work (Li et al., 2022), we inject time step information t by adding time step embedding to z t . Using the encoder-decoder architecture has computational convenience during generation because the input sequences w x only require one forward computation through the encoder network during the whole reverse process. Considering the reverse process requires thousands of iterations to generate the output sequences of high quality, the saving of computational resources can be significant. \n\nDuring training and generation, the function z 0 \u03b8 generates denoised samples at the sequence level. Therefore making predictions from the denoising function z 0 \u03b8 resembles the non-autoregressive natural language generation. In this regard, we use a decoder with full attention matrices instead of causal attention matrices to model z t at the sequence level.",
            "score": 0.5589520690271904,
            "section_title": "Diffusion Model",
            "char_start_offset": 10965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1245
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 417,
                    "matchedPaperCorpusId": "249192356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1895751953125
        },
        {
            "corpus_id": "249940391",
            "title": "An Improved Transformer-Based Neural Machine Translation Strategy: Interacting-Head Attention",
            "text": "e transformer architecture resolves NMT solely by relying on the attention algorithm [5]. It has been proved that the transformer-based models are superior to the models using RNNs and CNN [1-4, 8, 9]. Like RNNs 2\n\nComputational Intelligence and Neuroscience and CNN, the standard transformer-based model employs the encoder-to-decoder structure for NMT [14]. is structure maps the source sequence to a hidden state matrix as a natural language understanding (NLU) task and views the matrix elements as the context vectors or conditions for producing the target sequence. Encoder and decoder blocks are stacked in the encoder-to-decoder structure.\n\nEach encoder block usually comprises a multihead selfattention layer and a feedforward layer with residual connection [15], followed by a normalization layer [16]. As the core component of the encoder, the multihead selfattention layer captures the hidden representations of all the tokens within the source sequence.\n\nis operation mainly depends on the SAN, which learns the mutual attention score of any two tokens in the source sequence. It should be noted that the learned attention scores constitute an asymmetric square matrix, because of the learned parameters. For example, a ij , the attention score from the i-th token to the j-th token, is not equal to a ji , the attention score from the j-th token to the i-th token. Specifically, the SAN computes the attention scores by the scaled dot product attention algorithm. Since each token is visible to the others, the encoder can capture the feature of each token in two directions. ere are two primary functions of the encoder: (1) learning the hidden representations of the input sequence as a condition for natural language generation (NLG) tasks, for example, NMT; (2) completing downstream NLP tasks, such as sentiment classification or labeling by transfer learning, after being trained independently as a masked language model (MLM) [17] and connected to specific networks. e decoder blocks have a similar structure as encoder blocks. e only difference lies in an additional sublayer, which computes the attention scores between the representations of the source sequence given by the encoder and the current",
            "score": 0.5586850504527621,
            "section_title": "Transformer.",
            "char_start_offset": 5401,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "214607918"
                },
                {
                    "start": 767,
                    "end": 771,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1707763671875
        },
        {
            "corpus_id": "224709351",
            "title": "An Empirical Study for Vietnamese Constituency Parsing with Pre-training",
            "text": "In recent years, attention has become a popular and powerful mechanism in deep learning community. It brings a strong development in not only NLP, but also a patch of domains such as recommendation [20], image processing [21], speech recognition [22]. The first version of attention was proposed is sequence to sequence (Seq2Seq) architecture for machine translation [23], [24]. They independently exported similar architecture including two RNNs namely encoder linking to decoder. However, there is an issue in seq2seq architecture that might lose some information when processing long sentences. Therefore, Align & Translate [25], [26] born to exploit the context vector to align the source and target. In year 2006, hierarchical attention network (HAN) [27] that attention can be effectively used on two levels: word and sentence level. Thereby it allows the system to pay less or more attention to individual word and sentence accordingly when assembling the representation of a document. Transformer neural network architecture [15] is one of the revolutionized studies in NLP field. It based entirely on attention mechanism, multi-headed self-attention is proposed instead of encoder-decoder architecture .The intuition behind Transformer got a lot of attentions, pave the way for the development of self-attention-based models such as the Bidirectional Encoder Representations from Transformers (BERT) archived novel result in many NLP tasks. A group of BERTbased models demonstrate high performances, such as XLNET [28], RoBERTa [11], DistilBERT [29], ALBERT [30] and PhoBERT [12] have been proposed recently.",
            "score": 0.5586331345067194,
            "section_title": "V. RELATED WORK",
            "char_start_offset": 10952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1617
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "51607831"
                },
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "3446654"
                },
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162353515625
        },
        {
            "corpus_id": "201698358",
            "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel",
            "text": "Transformer (Vaswani et al., 2017) is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations (Vaswani et al., 2017), language understanding (Devlin et al., 2018), sequence prediction (Dai et al., 2019), image generation (Child et al., 2019), video activity classification , music generation (Huang et al., 2018a), and multimodal sentiment analysis (Tsai et al., 2019a). Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, Transformer is a feed-forward model that concurrently processes the entire sequence. At the core of the Transformer is its attention mechanism, which is proposed to integrate the dependencies between the inputs. There are up to three types of attention within the full Transformer model as exemplified with neural machine translation application (Vaswani et al., 2017): 1) Encoder self-attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence. 2) Decoder self-attention considers the target sentence (e.g., predicted target sequence for translation) as input, generating a sequence of decoded representations 1 , where each decoded token depends on previous decoded tokens. 3) Decoder-encoder attention considers both encoded and decoded sequences, generating a sequence with the same length as the decoded sequence. It should be noted that some applications has only the decoder self-attention such as sequence prediction (Dai et al., 2019). In all cases, the Transformer's attentions follow the same general mechanism.\n\nAt the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence. We note that this operation is order-agnostic to the permutation in the input sequence (order is encoded with extra positional em",
            "score": 0.5582342163688865,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 33,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 158,
                    "end": 182,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 308,
                    "end": 330,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1023,
                    "end": 1045,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2298583984375
        },
        {
            "corpus_id": "213682343",
            "title": "A CNN-Transformer Hybrid Approach for Crop Classification Using Multitemporal Multisensor Images",
            "text": "The transformer architecture [37] proposed by Google, which is different from the previous RNN-like model for modeling sequence information, has shown great vitality in the field of NLP. In transformer architecture, the self-attention is a sequential encoding mechanism similar to RNN and LSTM, and it improves the expression ability of the relationship between word sequences to get better performance on various NLP tasks. In addition to the excellent expression ability of the relationship between sequence information, the self-attention is much better than the RNN-like model in parallel ability because it inputs the entire sequence at a time for training, which can greatly improve the training speed of a sequence model. \n\nFor the sequence modeling and sequence classification, there are different task paradigms. For sequence modeling, a typical task is the language modeling. For a sentence sequence U = {u 1 , . . ., u n }, the standard language modeling objective can be used to maximize the likelihood: L(U ) = i log P (u i |u i\u2212k , . . ., u i\u22121 ; \u0398). When using transformer architecture to build the language model, the parallel training of sentence sequences can be achieved through forward masking mechanism which acts on the encoder and decoder. The mask mechanism shield the future word embedding, so that only leftward information of the sequence can be seen, while rightward information is blind. In contrast to the language model task, in the classification task, the output is the classification label rather than the sequence. And the label is visible to the entire sequence information, so the classification model can utilize all of the sequence information. Therefore, in the sequence classification task, we only take advantage of the encoder module in transformer without mask mechanism. \n\nIn the architecture, we use the multilayer transformer encoder module for sequence model, and it is a variant of transformer. As shown in Fig. 4, there are two sublayers in encoder module: multihead self-attention is the first part followed by the second positionwise fully connected feed-forward network. In addition, there are residual connections and layer normalizations in each encoder. \n\nThis module takes SSU features and position embeddings as input, so h 0 can be formed as",
            "score": 0.5568991866357932,
            "section_title": "2) Multilayer Transformer Encoder Module:",
            "char_start_offset": 17478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2209
                },
                {
                    "start": 2212,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "267751060",
            "title": "Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers",
            "text": "Our results obtained from the investigation of iterative vs. bi-directional attention as well as our experiments across multiple tasks and modalities clearly show that bi-directional attention offers advantages in a number of settings, both in terms of performance and efficiency. However, it is worth noting that by simultaneously refining the tokens alongside the latents, BiXT does not decouple the model's depth from the input, unlike Perceiver models [18]. Therefore, very deep BiXT variants might potentially face difficulties in settings of extremely long sequences paired with limited compute and memory. However, we suspect most such scenarios to benefit from some form of preprocessing via a modality-specific input tokenizer, similar to the input-adapter-based concept used in Perceiver-IO [19] -shifting most applications again into regions where BiXT performs effectively and efficiently. \n\nGiven the current popularity of natural language processing tasks, we would further like to note that BiXT in its current form is an encoder-based architecture (similar to BERT-like models), and we expect it to perform well on tasks that require understanding and modeling of entire sequenceswhich is what our results obtained in Section 3.5 / Table 4 on the LRA tasks indicate. However, as BiXT circumvents the expensive token self-attention of Transformers via our proposed bi-directional cross-attention, causal masking as commonly used in decoder-only methods for generative language tasks is not directly applicable to BiXT's current attention mechanism, as information from later tokens would be able to 'leak' to earlier ones via the latent refinement. One possibility to establish causality in this setup could be to assign groups of tokens to specific latents by masking the bidirectional cross-attention and latent refinement accordingly (while trading off some processing resolution at training time), but we expect there to be numerous potential ways and leave this as an interesting area for future follow-up research.",
            "score": 0.5567987909451289,
            "section_title": "Limitations & Discussion",
            "char_start_offset": 30869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "236635379"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47265625
        },
        {
            "corpus_id": "277626915",
            "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
            "text": "LLMs process textual inputs by converting them into dense vector representations through a sequence of tokenization , embedding and attention-based transformations. Tokenization involves the process of splitting the input text into subword units (tokens) using a model-specific vocabulary (e.g., SentencePiece 84 , Byte-Pair Encoding 85 ). The tokens are mapped to continuous vectors via learned embedding layers and passed through multiple self-attention layers that capture contextual relationships between tokens. \n\nLLMs can follow different architectural designs: encoder-only (e.g., BERT 2 ), decoder-only (e.g., Qwen 52 ), and encoder-decoder (e.g., T5 49 ). Encoder-based models process the full input bidirectionally and are suited for classification and regression . Decoder-only models generate text autoregressively with causal masking . Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks . The architecture choices impact the structure and pooling strategies used to extract unified representations from the variable-length token sequences. \n\nPooling refers to the process of aggregating a sequence of token-level representations produced by a language model into a single fixed-dimensional embedding. Encoder-based models often use the hidden state corresponding to the special [CLS] token or apply mean-pooling across token embeddings. Decoder-only models typically use the final hidden state of the last non-padding token. For encoder-decoder models, pooling is applied over the encoder-side hidden states.",
            "score": 0.5564929920297301,
            "section_title": "B.5 Large Language Models",
            "char_start_offset": 39674,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 336,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 659,
                    "end": 661,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.311279296875
        },
        {
            "corpus_id": "246863916",
            "title": "ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer",
            "text": "Transformer was proposed as an encoder-decoder model. Both the encoder and decoder parts include a self-attention mechanism. Language models designed to be pre-trained using large unsupervised datasets have been proposed and have been shown to perform well in various natural language processing tasks. Typical examples include Generative Pre-trained Transformer (GPT) [38] and Bidirectional Encoder Representations from Transformers (BERT) [11]. Whereas these methods use only decoder and encoder parts, respectively, the Bidirectional and Auto-Regressive Transformer (BART) [26] has been proposed as an encoder-decoder model with the intention of becoming more versatile model. In recent years, the Transformer architecture has made a significant impact, not only on NLP, but also on other modalities. Moreover, multimodal applications such as those combining vision and language, have attracted the attention of researchers. There are two main approaches that combine vision and language processing in Transformer models, including two-stream and single-stream models. Two-stream models [31,45] utilize separate Transformers for each modality, and a cross-modality module is adopted. Singlestream models [44] directly input the text and visual embeddings into a single Transformer. Su et al. [44] argued for the usefulness of using single-stream methods, as opposed to ViLBERT [31] and LXMERT [45], each of which adopts a two-stream approach. They explained that the network architecture of the attention pattern in the cross-modal Transformer is restricted in ViLBERT and LXMERT, and proposed VL-BERT as a unified architecture based on Transformer models, without any restriction on the attention patterns,",
            "score": 0.5564625880931471,
            "section_title": "Transformer-based Language Models for Vision and Language",
            "char_start_offset": 7247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 576,
                    "end": 580,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1094,
                    "end": 1097,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1207,
                    "end": 1211,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 1380,
                    "end": 1384,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1396,
                    "end": 1400,
                    "matchedPaperCorpusId": "201103729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289306640625
        },
        {
            "corpus_id": "233033666",
            "title": "MuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for Discriminative Music Modeling on Raw Waveforms",
            "text": "Attention mechanisms have enjoyed widespread adoption as a computational module for sequence modeling [2,3,12,44]. Notably, Bahdanau proposed integrating attention with a recurrent neural network (RNN) [2] for machine translation. Vaswani et al. [44] introduced the Transformer architecture, obtaining state-of-the-art performance on machine translation, and subsequently, many other NLP tasks. The Transformer consists of an encoder and decoder, which both use a deep stack of self-attention [6] and pointwise FC layers. It also incorporates positional information by augmenting inputs with absolute position embeddings. The position embeddings can be fixed or learned during training to model the dependency between elements at different positions in the input sequence. Devlin et al. [12] enhanced the Transformer by introducing BERT, or Bidirectional Encoder Representations from Transformers, which only uses the encoder part from the Transformer. BERT's main contribution is incorporating bidirectional information into the encoder by using a masked language modeling (MLM) pretraining task. \n\nThe primary component of the Transformer and its variants like BERT is self-attention [6]. Self-attention is a form of attention that processes a sequence by replacing each element by a weighted average of the rest of the sequence. It also does not suffer from the vanishing/exploding gradi-ent problem that is common in other sequential modeling techniques such as RNNs. \n\nDespite the inherent sequential structure of music, attention has not been widely explored in MIR, particularly for discriminative music tasks. Notable examples of attention in MIR include: [22] and [9] for music generation, [33] for source separation, [25,45] for music tagging, and [36] for chord recognition. Previous studies on attention mechanisms for music tagging tasks focus on recalibrating convolutional features by addition [25] or gating [45], and typically only attend to the channel subspace as opposed to the temporal subspace.",
            "score": 0.5558927384502249,
            "section_title": "Attention and Transformers",
            "char_start_offset": 18100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 107,
                    "matchedPaperCorpusId": "127951164"
                },
                {
                    "start": 110,
                    "end": 113,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "132718639"
                },
                {
                    "start": 1909,
                    "end": 1913,
                    "matchedPaperCorpusId": "132718639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.256591796875
        },
        {
            "corpus_id": "263829839",
            "title": "LLM for SoC Security: A Paradigm Shift",
            "text": "Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens. These models excel in tasks like dialog and story generation that require deep input understanding and coherent output generation. Decoderonly architectures, such as the GPT series [17,18], have gained popularity due to their parameter efficiency, simplicity, generalization, and versatility. \n\nIn Context of SoC Security: Decoder-only models, known for their strength in unconditional generation tasks, shine in areas of SoC security that are predominantly generative. They are tailored for tasks like vulnerability insertion, security policy and property generation, and testbench generation, where the model needs to produce new content based on a given prompt or context. For tasks demanding a deeper understanding before generation, like vulnerability mitigation, they can still offer decent performance but might not be the primary choice. \n\nc) Encoder-Only Model: Encoder-only models process input sequences and output a fixed-size context for each token or the entire sequence. These models are adept at distilling information from input sequences into fixed representations, making them suitable for tasks like classification where the aim is to derive a condensed understanding from the input. These models are referred to as \"encoder-only\" because they prioritize encoding input sequences into meaningful embeddings. The \"decoding\" they do is not about generating novel sequences (as with autoregressive models), but rather about producing specific outputs from the learned embeddings, such as masked token predictions during pretraining. BERT (Bidirectional Encoder Representations of Transformers) [126] developed by Google and its variants: RoBERTa [139], Dis-tilBERT [140], etc are popular examples. These models have lost popularity in recent times. \n\nIn Context of SoC Security: Encoder-only models, with their inherent design to understand and represent data, align well with tasks that require profound analysis. In the SoC security landscape, they are best suited for tasks like security verification and assessment, which demand an in-depth comprehension of the given data without extensive generation. However, when the task requires subsequent generative actions based on the understood context, encoder-only models might not be the ideal choice.",
            "score": 0.555770088885904,
            "section_title": "A. Preliminaries",
            "char_start_offset": 39977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2253
                },
                {
                    "start": 2254,
                    "end": 2399
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292724609375
        },
        {
            "corpus_id": "258284924",
            "title": "Web Interface of NER and RE with BERT for Biomedical Text Mining",
            "text": "GPT, which omits the encoder part of the transformer structure and focuses on sentence generation as the basis of the language model using the decoder part, can predict sentences only from left to right. BERT was developed with the idea that GPT's unidirectional structure limits context understanding, such as in question-and-answer tasks. In contrast to GPT, bidirectional training is possible by ignoring the decoder part of the transformer structure and using the encoder part to predict what will fill the empty space between words in the sentence. Bidirectional training examines words throughout the sentence and predicts which words will fill in the blanks, leading to a better understanding of the context compared to unidirectional training. BERT was created by overlapping 12 transformer encoders, as shown in Figure 2.",
            "score": 0.5550541338866531,
            "section_title": "BERT (Bidirectional Encoder Representations from Transformer)",
            "char_start_offset": 3084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13037109375
        },
        {
            "corpus_id": "258284924",
            "title": "Web Interface of NER and RE with BERT for Biomedical Text Mining",
            "text": "GPT, which omits the encoder part of the transformer structure and focuses on sentence generation as the basis of the language model using the decoder part, can predict sentences only from left to right. BERT was developed with the idea that GPT's unidirectional structure limits context understanding, such as in question-and-answer tasks. In contrast to GPT, bidirectional training is possible by ignoring the decoder part of the transformer structure and using the encoder part to predict what will fill the empty space between words in the sentence. Bidirectional training examines words throughout the sentence and predicts which words will fill in the blanks, leading to a better understanding of the context compared to unidirectional training. BERT was created by overlapping 12 transformer encoders, as shown in Figure 2.",
            "score": 0.5550541338866531,
            "section_title": "BERT (Bidirectional Encoder Representations from Transformer)",
            "char_start_offset": 5654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13037109375
        },
        {
            "corpus_id": "221095881",
            "title": "Spatiotemporal Attention for Multivariate Time Series Prediction and Interpretation",
            "text": "Multivariate time series analysis, classification, and prediction capabilities are crucial for applications in different domains such as healthcare, financial markets, climate science, and performance monitoring of engineering systems [1,2]. Along with the accuracy of decision-making, interpretability remains one of the important aspects of many real-life problems to build user trust and generate domain insights.\n\nLong Short Term Memory (LSTM) networks can capture the long-term temporal dependencies in complex multivariate time series [3]. To address the bottleneck of using a fixedlength vector in the Encoder-Decoder model [4,5], a model based on attention was introduced, which can automatically soft search for important parts of the input sequence in neural machine translation [6]. Inspired by this paper, attention mechanism-based models have been developed for time series prediction [7,8,9,10,11,12]. We compare and contrast some of the notable works in Table 1. In RETAIN [7], the spatial interpretation method is quite complicated, domainspecific, and only meant for a classification task. A dual-stage attention-based recurrent neural network (DA-RNN) [8] has spatial attention in the encoder layer and temporal attention in the decoder. DA-RNN is not causal as it also depends on future inputs during the computation of the spatial weights in the encoding phase. The hierarchy of LSTM networks makes AttentiveChrome [9] non-scalable and computationally intractable in the presence of a large number of variables. Using bidirectional LSTMs to encode, AttentiveChrome is non-causal for binary classification. The Transformer [13] can only highlight sequential attention weights and will not be suitable for spatial interpretability. Though computationally tractable, SAnD [10] is not causal and does not provide spatiotemporal interpretability. ICAtt [14] lacks in spatiotemporal interpretability having only spatial attention. Similar to DA-RNN, the non-causal DSTP-RNN model [12] has spatial attention in the encoding phase, the limitations of which are described in the next section. Another non-causal model developed is a multi-stage attention network [15]. Among the few spat",
            "score": 0.5549813541395352,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "199133603"
                },
                {
                    "start": 238,
                    "end": 240,
                    "matchedPaperCorpusId": "164781583"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "43680425"
                },
                {
                    "start": 634,
                    "end": 636,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 898,
                    "end": 901,
                    "matchedPaperCorpusId": "948039"
                },
                {
                    "start": 903,
                    "end": 905,
                    "matchedPaperCorpusId": "11236812"
                },
                {
                    "start": 905,
                    "end": 908,
                    "matchedPaperCorpusId": "5985448"
                },
                {
                    "start": 911,
                    "end": 914,
                    "matchedPaperCorpusId": "118714376"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "948039"
                },
                {
                    "start": 1435,
                    "end": 1438,
                    "matchedPaperCorpusId": "11236812"
                },
                {
                    "start": 1642,
                    "end": 1646,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "5985448"
                },
                {
                    "start": 1868,
                    "end": 1872,
                    "matchedPaperCorpusId": "204904776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35205078125
        },
        {
            "corpus_id": "238221542",
            "title": "SSS-AE: Anomaly Detection Using Self-Attention Based Sequence-to-Sequence Auto-Encoder in SMD Assembly Machine Sound",
            "text": "This option is primarily chosen for speech recognition [14] or machine translation [10], [18]. 2) Output of the Encoder: This is usually considered in unsupervised learning task [20], [21]. Existing studies using Seq2Seq models mainly use the first option, but they have poor performance on long sequence data. \n\nAttention mechanism is a powerful method to overcome the vulnerability of Seq2Seq to the long sequence data [14], [16], [17]. Attention calculates and selects important hidden states (scoring and focusing) for input y t\u22121 in the entire hidden states of the encoder when the decoder generates output y t from input y t\u22121 . Generally, scoring is similar to similarity measure, and various score functions [22], [23] have proposed, including cosine similarity [24]. Self-attention or intra-attention is a mechanism for calculating the relationship between different positions of the same input. For example, the relationship between different words in the same sentence. By this mechanism, self-attention can enrich the input representation without external information. Self-attention has not only achieved great results in natural language processing [9], [25]- [28], but has also succeeded in many other tasks in various areas [29]. \n\nAuto-encoder [4], [5] is an unsupervised learning model composed of encoder and decoder, and it learns the representation of input data by compressing the input data and reconstructing it back. Auto-encoder successfully performs learning representation in various areas, including text generation [30] and anomaly detection [2], [31], [32]. The task of auto-encoder targeting sequence data is to learn representation that reflect temporal information. Seq2Seq helps auto-encoder learn the representation of sequence data [20], [33]- [35]. This is because Seq2Seq is specialized in extracting temporal features of input sequence. Recently, auto-encoder with attention have also been studied [30], [36]- [38].",
            "score": 0.5546875628215489,
            "section_title": "1) y t\u22121 of Target Sequence y:",
            "char_start_offset": 7752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "18165915"
                },
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "53084313"
                },
                {
                    "start": 184,
                    "end": 188,
                    "matchedPaperCorpusId": "209331480"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "18165915"
                },
                {
                    "start": 427,
                    "end": 431,
                    "matchedPaperCorpusId": "4689304"
                },
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "206742911"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1168,
                    "end": 1172,
                    "matchedPaperCorpusId": "6506243"
                },
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "17804904"
                },
                {
                    "start": 1266,
                    "end": 1269,
                    "matchedPaperCorpusId": "16671942"
                },
                {
                    "start": 1545,
                    "end": 1549,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "matchedPaperCorpusId": "53084313"
                },
                {
                    "start": 1775,
                    "end": 1779,
                    "matchedPaperCorpusId": "1353097"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.157470703125
        },
        {
            "corpus_id": "269009682",
            "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
            "text": "Text embedding models aim to encode the semantic content of natural language text in vector representations which then facilitate various natural language processing (NLP) tasks, such as semantic textual similarity, information retrieval, and clustering. For many years, the dominating paradigm for building such models relied on pre-trained bidirectional encoders or encoder-decoders such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), which are typically adapted for text embedding tasks by following a multi-step training pipeline consisting of weakly-and fully-supervised contrastive training (Ni et al., 2022;Li et al., 2023a;Xiao et al., 2023, inter alia). Only recently, the community started to adopt decoder-only LLMs for embedding text (Muennighoff, 2022;Ma et al., 2023;Wang et al., 2023;Springer et al., 2024;Li & Li, 2024). \n\nWe speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position i is influenced solely by the representations of preceding tokens at positions 0, 1, . . . , i \u2212 1. Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence. \n\nOvercoming this architectural limitation of decoder-only LLMs for text embedding tasks is highly appealing as these models come with several advantages compared to their encoder-",
            "score": 0.5541727512104795,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1672
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 419,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 427,
                    "end": 448,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 610,
                    "end": 627,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 778,
                    "end": 794,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 834,
                    "end": 848,
                    "matchedPaperCorpusId": "265066823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "213176351",
            "title": "Lexical Variation and Sentiment Analysis of Roman Urdu Sentences with Deep Neural Networks",
            "text": "All these variants of RNNs have achieved great success in numerous tasks. However, they are often called as black boxes, lacking interpretability and consume high resources [25]. Research efforts to solve this issue have steadily increased. \n\nThe Attention mechanism was presented to upgrade the RNN encoder decoder sequence-to-sequence network for NMT [5], [26]. Initially, Attention was defined as the process of determining a context vector for the next decoder step that consists of the most relevant information with the encoder hidden states. Seminal contributions have been made by Vaswani et al. [6] when Transformer architecture was proposed for machine translation. It depends only on Attention mechanisms, as the best replacement of either recurrent or convolution neural networks. For sequence processing and language modeling, Transformer has outperformed the recurrent neural network and their variants. \n\nA closer look to the literature on neural networks for sentence classification [5], [6] reveal that Attention predicts based on only recent hidden states (unlike RNN, which predict based on entire history and reminds all the previous hidden states). The objective is to devise and implement a system that consists of Self-Attention to address the problem of complex structure of Roman Urdu Sentences. In this study, a more efficient and lightweight model Self-Attention Bidirectional LSTM is proposed for targeted problem, where Self-Attention takes charge of the complex formation by correlating the whole sentence and determining embedding that consists of the most relevant information. Bidirectional LSTM is integrated to strengthen the network as it extracts context representations to tackle the lexical variation of attended embedding in preceding and succeeding directions. Moreover, it promotes essential embedding by memorizing the contextual information for the long term. The results endorse that the integration of network leads to enhance the Self-Attention's performance. Besides, deficiencies of Bidirectional LSTM are conquered by Self-Attention module in the network.",
            "score": 0.5538971938512485,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 6416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "1633753"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1004,
                    "end": 1007,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28515625
        },
        {
            "corpus_id": "239049412",
            "title": "Improving Non-autoregressive Generation with Mixup Training",
            "text": "Pre-trained language models Pre-trained language models bring significant improvement for both natural language understanding and generation tasks. These models are trained with a large amount of unlabeled data to understand the language and improve the results on small down-stream datasets. For example, BERT (Devlin et al. 2018) pre-trains a encoder based model with masked language task and next sentence prediction task. It significantly improves the performance on natural language understanding tasks, but it is not suitable for generation tasks. UniLM (Dong et al. 2019;Bao et al. 2020) pre-trains encoder based model with three tasks: unidirectional, bidirectional, and prediction, which allow it can be fine-tuned for both natural language understanding and generation tasks. For the encoder-decoder based models (Song et al. 2019;Qi et al. 2020b;Song et al. 2019), these models are pre-trained with sequence-to-sequence tasks to help the down-stream generation tasks. \n\nNon-autoregressive generation Many works (Gu et al. 2017;Kasai et al. 2020) have been proposed to decrease the huge latency in autoregressive generation. The most popular way is to generate tokens in parallel called nonautoregressive generation. However, these works (Gu et al. 2017;Gu and Kong 2020;Qian et al. 2020) mostly focus on translation, and cannot achieve reasonable results on tasks like summarization or question generation. BANG (Qi et al. 2020a) achieves significantly improvement by bridging autoregressive and non-autoregressive generation with large scale pre-training on these tasks. For model architecture, BANG has several differences compared to previous methods: 1) BANG only uses [MASK] tokens with fixed length as the input of decoder. 2) BANG does not predict target tokens length directly, but treats the first [SEP] token as the end of sequences like autoregressive generation.",
            "score": 0.5536331260195007,
            "section_title": "Related Work",
            "char_start_offset": 3741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1885
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 578,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 578,
                    "end": 593,
                    "matchedPaperCorpusId": "211572655"
                },
                {
                    "start": 823,
                    "end": 841,
                    "matchedPaperCorpusId": "207880694"
                },
                {
                    "start": 841,
                    "end": 857,
                    "matchedPaperCorpusId": "210164665"
                },
                {
                    "start": 857,
                    "end": 874,
                    "matchedPaperCorpusId": "207880694"
                },
                {
                    "start": 1423,
                    "end": 1439,
                    "matchedPaperCorpusId": "210164665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137939453125
        },
        {
            "corpus_id": "269502453",
            "title": "A Systematic Literature Review on Large Language Models for Automated Program Repair",
            "text": "Categories.The literature has seen a variety of LLMs supporting NLP and SE research, which can be categorized into three main categories based on their model architectures.(1) Encoder-only LLMs, such as CodeBERT [35], GraphCodeBERT [46], train the encoder part of the Transformer to generate a fixed-dimensional bidirectional representation with Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).MLM aims to predict the original tokens that have been randomly masked out, and NSP predicts whether two given sentences actually follow each other in a text.(2) Decoder-only LLMs, such as CodeGPT [95], train the decoder part of the Transformer to support auto-regressive tasks with Causal Language Modeling (CLM), which aims to predict new tokens in a sequence based on previous tokens.(3) Encoder-decoder LLMs, such as CodeT5 [154], train both encoder and decoder parts of the Transformer to support sequence-to-sequence generation tasks with denoising objectives.We will summarize existing LLMs and how they are leveraged to support program repair in Section 5.1.",
            "score": 0.5534748221197983,
            "section_title": "Model",
            "char_start_offset": 6307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 11,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1078
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "256503496"
                },
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 609,
                    "end": 613,
                    "matchedPaperCorpusId": "231855531"
                },
                {
                    "start": 840,
                    "end": 845,
                    "matchedPaperCorpusId": "237386541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.129638671875
        },
        {
            "corpus_id": "277857043",
            "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
            "text": "Continuing from the architectural foundations of large language models, it is important to distinguish between two major classes of transformer-based designs: encoder-only and decoder-only models. Each follows a unique training paradigm and serves different purposes in natural language understanding or generation tasks. \n\nEncoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input. \n\nThe standard training objective for such models is Masked Language Modelling (MLM), where a random subset of tokens is replaced with a special [MASK] token. The model is then trained to reconstruct the masked tokens using the surrounding unmasked tokens. This approach encourages the network to develop contextualized embeddings grounded in full-sequence comprehension. The loss function for MLM is typically expressed as: \n\nwhere \\M denotes the set of masked positions, x \\M is the unmasked sequence, and \u03b8 represents model parameters. \n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling. \n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step. \n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation.",
            "score": 0.5525162050392021,
            "section_title": "Large language Models (LLMs) and Attention Mechanism",
            "char_start_offset": 11813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291259765625
        },
        {
            "corpus_id": "3397190",
            "title": "Recent Trends in Deep Learning Based Natural Language Processing",
            "text": "One potential problem that the traditional encoder-decoder framework faces is that the encoder at times is forced to encode information which might not be fully relevant to the task at hand. The problem arises also if the input is long or very information-rich and selective encoding is not possible. \n\nFor example, the task of text summarization can be cast as a sequence-to-sequence learning problem, where the input is the original text and the output is the condensed version. Intuitively, it is unrealistic to expect a fixed-size vector to encode all information in a piece of text whose length can potentially be very long. Similar problems have also been reported in machine translation (Bahdanau et al., 2014). \n\nOn the other hand, a certain alignment exists between the input text and the output text in tasks such as text summarization and machine translation, where each token generation step is highly related to a certain part of the input text. \n\nThe attention mechanism attempts to ease the above problems by allowing the decoder to refer back to the input sequence. Specifically during decoding, in addition to the last hidden state and generated token, the decoder is also conditioned on a \"context\" vector calculated based on the input hidden state sequence. (Bahdanau et al., 2014) first applied the attention mechanism to machine translation improving the performance, especially for long sequences. In their work, the attention signal over the input hidden state sequence is determined with a multi-layer perceptron by the last hidden state of the decoder. Visualizing the attention signal over the input sequence during each decoding step, a clear alignment between the source and target language can be demonstrated (Fig. 14). \n\nA similar approach was applied to the task of summarization by (Rush et al., 2015) where each output word in the summary was conditioned on the input sentence through an attention mechanism. With success lurking up in multiple sub fields, attention-based models started gaining traction amongst NLP researchers. \n\nIn image captioning, (Xu et al., 2015) conditioned the LSTM decoder on different parts of the input image during each decoding step. Attention signal was determined by the previous hidden state and CNN features.",
            "score": 0.5515877188619118,
            "section_title": "D. Attention Mechanism",
            "char_start_offset": 42624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2063
                },
                {
                    "start": 2066,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 2087,
                    "end": 2104,
                    "matchedPaperCorpusId": "1055111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289794921875
        },
        {
            "corpus_id": "267126553",
            "title": "Efficient Classification of Malicious URLs: M-BERT\u2014A Modified BERT Variant for Enhanced Semantic Understanding",
            "text": "The Transformer framework has gained significant attention with the introduction of large language models across different languages. It serves as the foundation for various language models and has revolutionized natural language processing tasks. The key principle of the Transformer is to encode and decode input sequences using a self-attention mechanism [19]. \n\nIn the encoder part of the Transformer, the self-attention mechanism plays a crucial role in capturing the relationship between different positions within the input sequence. It computes a weight for each position based on its relationship with other positions in the sequence. By weighting and averaging the encoding vectors of these other positions, the encoder generates a comprehensive encoding vector for each position. This allows each position's encoding vector to encapsulate information from the entire input sequence, resulting in a more robust representation. \n\nIn the decoder part of the Transformer, the self-attention mechanism facilitates the relationship between the input sequence and the output sequence. For each position in the output sequence, the self-attention mechanism calculates a weight based on its relationship with all positions in the input sequence. Similar to the encoder, the decoder then weights and averages the encoding vectors of all positions in the input sequence. This generates a decoding vector for each position in the output sequence that incorporates information from both the input sequence and the generated output sequence. This comprehensive representation aids in generating accurate and contextually relevant output sequences. \n\nBy utilizing the self-attention mechanism in both the encoder and decoder, the Transformer model can effectively capture dependencies and relationships within in-put sequences. This allows it to better represent input and output sequences, thereby improving the performance of various natural language processing tasks. Currently, the Transformer architecture has achieved significant success in various tasks within the field of NLP, such as machine translation [20], text generation [21], [22], text classification [22], [23], questionanswering systems [21], [24], language modeling [25], etc. \n\nParticularly in text classification tasks, BERT models and their variants have become standard models. These remarkable achievements highlight the powerful potential of the Transformer architecture in the NLP field, providing a strong direction for the work presented in this paper.",
            "score": 0.5511651748444588,
            "section_title": "D. TRANSFORMER",
            "char_start_offset": 22471,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2242
                },
                {
                    "start": 2245,
                    "end": 2347
                },
                {
                    "start": 2348,
                    "end": 2527
                }
            ],
            "ref_mentions": [
                {
                    "start": 2132,
                    "end": 2136,
                    "matchedPaperCorpusId": "245986550"
                },
                {
                    "start": 2170,
                    "end": 2174,
                    "matchedPaperCorpusId": "220041960"
                },
                {
                    "start": 2202,
                    "end": 2206,
                    "matchedPaperCorpusId": "245986550"
                },
                {
                    "start": 2208,
                    "end": 2212,
                    "matchedPaperCorpusId": "71152426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291015625
        },
        {
            "corpus_id": "265034116",
            "title": "Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision",
            "text": "We study the impact of combining encoder and decoder blocks and the cross-attention links between them in Table 6. A decoder-only version of the model is constructed by stacking 24 decoder layers and is pre-trained to convergence using next-token prediction. We also construct a balanced model using stacks of 12 layers for both the encoder and decoder blocks, introducing cross-attention layers in the decoder that attend to the embeddings and the output sequence. Both models have \u223c 800 M trainable parameters. We then fine-tune these models on the mutation generation task and compare with the ENBED model having a 2:1 encoder-decoder block ratio. \n\nIntroducing the encoder and cross attention leads to a significant improvement in the pre-training accuracy, demonstrating the suitability of both the architecture as well has the pretraining task, since decoder-only models are restricted to causal objectives like next-token prediction unlike encoders that can handle bi-directional information.",
            "score": 0.5505320737913502,
            "section_title": "Encoder-decoder architecture",
            "char_start_offset": 29813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29150390625
        },
        {
            "corpus_id": "253761197",
            "title": "A survey on knowledge-enhanced multimodal learning",
            "text": "layers of the encoder and the decoder are stacked the one upon the other, containing sub-layers with Multi-Head self-attention and position-wise fully connected feed-forward layers. Residual connections followed by layer normalization are used between the sublayers of the encoder. The decoder has an additional encoder-decoder multi-head attention sub-layer that helps focusing on the appropriate parts of the encoded input sequence. Moreover, the decoder's self-attention modules are modified so that they force the prediction at a certain position to be based only on the known predictions of previous positions. Transformer architectures prove that there is no need for convolutions or recurrent units to achieve state-of-the-art performance in linguistic tasks. Currently, most state-of-the-art VL architectures utilize attention mechanisms within their implementation.\n\nTransformer models for NLP tasks consist of a pretraining stage on a large corpus of data, followed by fine-tuning on certain downstream tasks. Language transformers can be divided in two major categories: autoregressive (AR) and autoencoding (AE) language models, depending on whether pre-training is performed in a unidirectional or a bidirectional way. AR language models attempt to estimate the probability distribution of a text corpus, while AE models learn to reconstruct manipulated inputs with the help of surrounding information. (Yang et al., 2019b) BERT (Devlin et al., 2019) is a popular bidirectional transformer-based language representation model, able to handle a variety of natural language processing tasks by just fine-tuning one additional output layer. It uses masked language modeling (MLM), randomly hiding some input tokens in order to be inferred from the surrounding words. The pre-training stage is based on unlabeled data, which enable parameter learning. Those parameters are then finetuned with labelled data corresponding to some certain tasks. RoBERTa  offers an optimized extension to BERT, suggesting that longer training, more data and larger batch size, as well as training on longer sequences, dynamically altering the masking patterns and removing the next sentence prediction loss are factors that contribute to advanced performance of the original BERT model. XLNet (Yang et al., 2019b",
            "score": 0.5503280909436282,
            "section_title": "Recurrent neural networks",
            "char_start_offset": 15551,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1416,
                    "end": 1436,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1442,
                    "end": 1463,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407958984375
        },
        {
            "corpus_id": "271947604",
            "title": "Understanding the natural language of DNA using encoder\u2013decoder foundation models with byte-level precision",
            "text": "We study the impact of combining encoder and decoder blocks and the cross-attention links between them in Table 6. A decoder-only version of the model is constructed by stacking 24 decoder layers and is pre-trained to convergence using next-token prediction. We also construct a balanced model using stacks of 12 layers for both the encoder and decoder blocks, introducing cross-attention layers in the decoder that attend to the embeddings and the output sequence. Both models have \u223c 800 M trainable parameters. We then fine-tune these models on the mutation generation task and compare with the ENBED model having a 2:1 encoder-decoder block ratio. \n\nIntroducing the encoder and cross attention leads to a significant improvement in the pre-training accuracy, demonstrating the suitability of both the architecture as well has the pretraining task, since decoder-only models are restricted to causal objectives like next-token prediction unlike encoders that can handle bi-directional information.",
            "score": 0.5503071164259661,
            "section_title": "Encoder-decoder architecture",
            "char_start_offset": 29813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "258461112",
            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
            "text": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations. \n\nTaking the encoder-only models in Table 7a for illustration, on average, BERT-Base (Devlin et al., 2019) and Roberta-Base (Liu et al., 2019)   between CodeGen and the BERT models trained on programming languages, i.e., CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021), decreases or even diminishes when evaluated on the code search tasks, the performance gap is still significant as both the model size and pretraining data in CodeGen are much larger than those used by the encoder-only models in Table 7b. Similar trends were observed in the performance gap between the decoder-only and encoderdecoder models on both natural language (Lewis et al., 2020;Raffel et al., 2020) and programming language (Ahmad et al., 2021;Wang et al., 2021). The large performance gap severely limits the decoder-only models used in many discriminative tasks. To this end, contrastive learning shows the promise to largely bridge the gap. As seen in Table 7a, on STS, CONTRACLM reduces the relative performance gap from 67.24% (absolute 21.12%) to 16.17% (absolute 7.33%) regarding BERT-Base, and from 84.62% (absolute 26.64%) to 28.24% (absolute 12.8%). Similarly, Table 7b shows that CONTRACLM outperforms encoder-decoder models and performs comparably to the encoder-only model, GraphCodeBERT. Gao et al. (2021) showed that the dropout-based augmentation is an effective strategy for unsupervised contrastive learning, and the follow-up works (Chuang et al., 2022;Wu et al., 2022) endorse the effectiveness.",
            "score": 0.549964930876218,
            "section_title": "D.1 Bridge the Gap on Discriminative Tasks",
            "char_start_offset": 38682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1785
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 504,
                    "end": 523,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 542,
                    "end": 560,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 928,
                    "end": 948,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 994,
                    "end": 1014,
                    "matchedPaperCorpusId": "232185260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "270711050",
            "title": "A Complete Survey on LLM-based AI Chatbots",
            "text": "The GPT-series models [3], [75] use an autoregressive or causal decoder architecture with a one-way attention mask that lets each input token think about only the elements that came before it and itself while it is being processed (see Fig. 8). This makes the processing more like how a conversation would naturally flow. Both input and output tokens undergo similar processing within this framework. The non-causal or prefix decoder architecture [76] performs bidirectional attention on prefix tokens, i.e., it considers both preceding and subsequent tokens. While autoregressively predicting the output tokens utilizing the same set of parameters used in encoding, it performs unidirectional attention [77], [78]. \n\nThe working principle of LLMs includes a series of steps. The process begins with word embedding, which involves representing words as vectors in a high-dimensional space. Here, similar words are clustered in specific groups or categories. This clustering of words enables the model to understand their meanings, which helps the LLM make accurate predictions. The model is trained on large text corpora like news articles or books, and during training, it learns to predict the likelihood of a word appearing in a specific context. Positional encoding further adds an understanding of word order within sequences, essential for tasks like translation, summarization, and question answering. Then comes the core of these models, the transformer architecture. It consists of the self-attention mechanism that helps understand textual dependencies by al-Fig. 7. Transformer model architecture [74]. locating distinct weights to individual words. This is achieved by computing: \n\nwhere matrices Q (query), K (key), and V (value) represent the current element, other elements, and information to be aggregated, respectively. The similarity between the query and key matrices is calculated through a dot product operation. It is then scaled by 1 \n\nto prevent gradient vanishing problems and subsequently normalized using the SoftMax activation function to produce the attention matrix A. The updated representations are obtained by performing matrix multiplication on A and V. This aggregation of weighted values forms a new representation that captures the inherent associations within the text. Algorithm 1 outlines the step-by-step procedure for this self-attention mechanism.",
            "score": 0.5499243439326633,
            "section_title": "B. Large Language Models (LLMs)",
            "char_start_offset": 19825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2308
                },
                {
                    "start": 2309,
                    "end": 2391
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 25,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "246441975"
                },
                {
                    "start": 704,
                    "end": 708,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 710,
                    "end": 714,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "265066982",
            "title": "Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform",
            "text": "As the FNet paper proposed only an encoder architecture, Kiruluta et al. (2021) first suggested 3 different Seq2Seq architectures to overcome this shortcoming. In all of their proposed Seq2Seq diagrams, they removed the multi-headed self-attention layers in a transformer (both encoder and decoder): either by direct removal, i.e. moving the Fourier token mixing completely outside the transformer by Fourier transforming both inputs, or by replacement with a Fourier layer. Although the latter solution, what they call Hybrid-FNet Seq2Seq, is their best solution, and is more similar to a classical transformer diagram, their results, as reported in Table 3, are a long way from those offered by alternatives such as PEGASUS and Longformer. \n\nFor these reasons, we have designed a new Seq2Seq infrastructure with an encoder without attention and a decoder with attention, which for clarity we can call the Right-side Attention Encoder-Decoder (RAED) model (See Fig. 2). It is based on the idea that an attention-based decoder gives a moderate computational load to the entire model infrastructure because it is generally powered by short text. Furthermore, in the main NLP tasks, where Seq2Seq infrastructure is involved, more emphasis is placed on the output generated by the decoder. We believe, therefore, that the multi-headed self-attention mechanism acting on the decoder tokens through a dense representation of the context via Fourier/Hartley descriptors (i.e. encoder output) is crucial to obtaining AI generated text which is indistinguishable from a human with a reduced computational cost.",
            "score": 0.5491862793119018,
            "section_title": "Right-side Attention Encoder-Decoder",
            "char_start_offset": 16878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1602
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.116943359375
        },
        {
            "corpus_id": "201058516",
            "title": "Attending to Future Tokens for Bidirectional Sequence Generation",
            "text": "To measure the success of our proposed approach, we consider the following three baselines. \n\nEncoder-Decoder Transformer (E&D). First, we compare our bidirectional encoder to a standard encoder-decoder Transformer where the decoder only has access to tokens produced so far to compute its self-attention. We use the implementation of OpenNMT (Klein et al., 2017) and employ the parameters suggested by them, but adjust the learning rate to 0.1, which we found to work better for both datasets. Additionally, we increased the word and hidden dimension size to 768 and the number of attention heads to 12 to match the capacity of our model. Training ran for 50 epochs. Needing both an encoder and a decoder, this leads to a total of about 270M parameters. \n\nEncoder-Decoder Transformer with BERT (E&D+B). The power of our bidirectional decoder stems from two advantages. First, we can initialize our model with the pre-trained BERT-BASE-UNCASED model. Second, the decoding process is bidirectional. It would be possible to transfer the first advantage to an encoder-decoder framework by using BERT embeddings. This is however only possible for the input sequence, because the bidirectionality of BERT requires the entire sequence to be available beforehand. Thus, we modify implementation of OpenNMT to use the BERT model as the encoder. The weights are frozen when training the decoder, which produced better results than allowing the gradients to also flow through the BERT model. Again, with both an en-  2019) present a transformer decoder, GPT2, trained as a language model on large amounts of monolingual text. Radford et al. (2019) showed that it is possible to perform various tasks in a zero-shot setting by priming the language model with an input and letting it generate further words greedily. This setup can be transferred to a supervised setting, where the model is fine-tuned to a dataset by using maximum likelihood estimation to increase the probability of the gold output sequence (Wolf et al., 2019).",
            "score": 0.5488318545010153,
            "section_title": "Baselines",
            "char_start_offset": 16318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 2018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2115478515625
        },
        {
            "corpus_id": "275336956",
            "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
            "text": "The evolution of attention mechanisms in neural networks has significantly influenced the field of artificial intelligence and machine learning, from early work [1,2,3,4,5] to more recent developments [6,7,8,9,10,11,12,13,14]. Originally vaguely inspired by cognitive processes in humans, attention mechanisms have become integral to modern neural network architectures like the Transformer [5]. These mechanisms dynamically allocate computational resources to the most relevant parts of input data, optimizing the processing of information and reducing computational redundancy. Decode-only Transformers [6], designed specifically for autoregressive tasks, have emerged as an efficient subclass of the Transformer architecture, focusing on multimodal tasks like text generation, image modeling, audio modeling, language modeling, and sequential data prediction, among many other modalities (Figure 1). These architectures utilize an embedding layer to convert discrete tokens (which can represent diverse types of data, like text, chemical structures, images/pixels, symbols, and others) into dense vector representations, enabling the model to process flexible inputs. The core of the architecture is the self-attention mechanism, which operates causally to ensure that each token attends only to its past context, maintaining the autoregressive property essential for tasks like generative AI. Multi-head self-attention enhances the model's ability to capture diverse relationships between tokens by allowing parallel attention computations. \n\nIn addition to attention, decode-only Transformers such as those used in LLama foundation models [10] integrate feedforward layers (FF), usually implemented via a multi-layer perceptron (MLP), following the attention mechanism. These layers expand and transform the attention outputs, introducing non-linearity and enabling the model to learn complex patterns. The architecture also employs several residual connections and layer normalization, ensuring stability during training and facilitating the flow of gradients. Positional encodings are incorporated to inject sequence order information into the embeddings [5,15], addressing the lack of inherent order in self-attention computations.",
            "score": 0.5487849895880663,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "784288"
                },
                {
                    "start": 168,
                    "end": 170,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 204,
                    "end": 206,
                    "matchedPaperCorpusId": "235248316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.358642578125
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "We now focus on the relatively new practice of multitask finetuning, where there has not yet been any systematic study of the influence of the architecture and training objective. Notably, the two main papers advocating this practice use completely different approaches: Sanh et al. [2021] finetunes an encoder-decoder model pretrained with span corruption, whereas Wei et al. [2021] finetunes a decoder-only pretrained with full language modeling. It is not immediately clear which approach is more natural: while decoder-only models trained with full language modeling are better at zero-shot generalization (as evidenced in Section 4.1), encoder-decoder models and masked language modeling pretraining have been shown to perform significantly better after finetuning [Raffel et al., 2020]. We therefore evaluate every architecture and objective combination after multitask finetuning. \n\nOur results are outlined in Figure 4. The encoder-decoder pretrained with span corruption offers the best performance after multitask finetuning. Specifically, on EAI-Eval, the best performance is achieved by the encoder-decoder with MLM, and the non-causal decoder with MLM comes in a close second. However, the difference is more significant on T0-Eval, where the encoder-decoder with MLM pretraining outperforms other models by a large margin. Finally, encoder-decoder pretrained with PLM and causal decoder with MLM achieve significantly worse performance than other models. These results are consistent across all levels of pretraining (see early checkpoints in Appendix D). \n\nFinding 2. Encoder-decoder models trained with masked language modeling achieve the best zero-shot performance after multitask finetuning . More broadly, approaches that perform well in the single-task finetuning setting perform well on multitask finetuning. \n\nTable 3: After full or prefix language modeling pretraining, the causal decoder (FLM) exhibits the best zero-shot generalization abilities, followed closely by the non-causal decoder (PLM).",
            "score": 0.5482480400692382,
            "section_title": "After multitask finetuning",
            "char_start_offset": 29597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 770,
                    "end": 791,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27490234375
        },
        {
            "corpus_id": "232135297",
            "title": "MalBERT: Using Transformers for Cybersecurity and Malicious Software Detection",
            "text": "Thus, in order to solve the limitations of both RNN and endocer-decoder architectures, the authors of Transformers [8] have proposed a solution. They rely on the seq2seq encoder-decoder by replacing RNN with attention mechanisms. \n\nThe attention mechanism allows the Transformers to have a very long term memory. A Transformer model can \"attend\" or \"focus\" on all the previous tokens that have been generated. The attention mechanism allows the decoder to go back over the entire sentence and selectively extract the information it needs during decoding. Attention gives the decoder access to all the hidden states of the encoder. However, the decoder still has to make a single prediction for the next word, so we can not just pass a whole sequence to it (we have to pass it some kind of synthesis vector). So it asks the decoder to choose which hidden states to use and which to ignore by weighting the hidden states. The decoder then receives a weighted sum of hidden states to use to predict the next word. \n\nIn this section, we define the context of the Transformer-based approach by presenting most of the approaches related to the architectures of the approaches. The Transformer in NLP is a new architecture that aims at solving sequence to sequence tasks while easily managing long-range dependencies. The Transformer has been proposed in [8]. A Transformer is an architecture that avoids recurrence and relies entirely on an attention mechanism to draw global dependencies between input and output. Prior to Transformers, dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. \n\nTransformers also use an encoder and a decoder, but the elimination of recurrence in favor of attention mechanisms allows for much greater parallelization than methods such as RNNs and CNNs. The transformation is undoubtedly a huge improvement over the seq2seq models based on RNN. But it has its own set of limitations. Attention can only be paid to text strings of fixed length. The text must be divided into a number of segments or pieces before being introduced into the system as input and this causes context fragmentation.",
            "score": 0.5481013572185253,
            "section_title": "Transformer",
            "char_start_offset": 6145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1348,
                    "end": 1351,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2437744140625
        },
        {
            "corpus_id": "235097256",
            "title": "Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus",
            "text": "Similar to Syed et al. (2020), we first pre-train a Transformer-based language model with Masked Language Modeling (MLM) objective on English Wikipedia data extracted using WikiExtractor.1 This equips LM with the ability to predict masked words over a large corpus. Masked Language Modeling leverages bidirectional context of the input, thus enabling better language understanding. Following Masked Language Modeling objective from Devlin et al. (2019), we randomly sample 15% of the tokens from the text stream and replace them with the [MASK] token 80% of the time, by a random token 10% of the time and keep them unchanged 10% of the time, with the objective of predicting the original identity of the masked word based on its bidirectional context. To enable style transfer from a given sentence to target style, we use independently trained language models (LMs) to initialize the encoder and decoder and connect these with randomly initialized attention layers to arrive at a encoder-decoder setup. As discussed by Syed et al. (2020), the Transformer architecture (Vaswani et al., 2017) allows such independent initialization by implicitly aligning encoder-decoder layers via attention mechanism. \n\nPre-training an encoder only transformer on generative task and then leveraging it to initialize as both encoder and decoder as opposed to pretraining a joint encoder-decoder model has several advantages. Transformer-based models with encoder-only (Devlin et al., 2019) or decoder-only (Radford et al., 2018) blocks have been shown to perform well in generative pre-training task. Clearly, pre-training a single transformer block on generative task and then utilizing it as both encoder and decoder blocks has lower computational cost than training the entire encoder-decoder block jointly. Moreover, this also enables us to use the same pre-trained model to initialize both style transfer module and the discriminator models, explained in the following section. This is not only computationally more efficient but it also closely ties the underlying language distribution of the two modules.",
            "score": 0.5479009797762748,
            "section_title": "Pre-trained LM as Encoder-Decoder",
            "char_start_offset": 9535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 29,
                    "matchedPaperCorpusId": "202719307"
                },
                {
                    "start": 432,
                    "end": 452,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1021,
                    "end": 1039,
                    "matchedPaperCorpusId": "202719307"
                },
                {
                    "start": 1070,
                    "end": 1092,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1453,
                    "end": 1474,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291748046875
        },
        {
            "corpus_id": "259251760",
            "title": "Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?",
            "text": "Over the past decade, significant advancements have been made in the field of Natural Language Processing (NLP), leading to the development of powerful language models. One such groundbreaking model is the Transformer [Vaswani et al., 2017], which introduced an attention-based encoderdecoder architecture that has consistently yielded superior performance compared to Recurrent Neural Network (RNN) based architectures, including Long Short-Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] and Gated Recurrent Unit (GRU) [Cho et al., 2014], in various language tasks. The Transformer model's key innovation lies in its ability to effectively capture long-range dependencies within sequences by employing self-attention mechanisms. This enables the model to assign importance to different parts of the input text and establish contextual relationships, resulting in improved language understanding and generation capabilities. Consequently, the Transformer architecture has paved the way for a new generation of state-of-the-art Large Language Models that have inherited its powerful framework exemplified by the Generative Pre-trained Transformer (GPT) [Radford et al., 2018] series, developed by OpenAI, which focus on generative language modeling and utilize a variant of the Transformer architecture, where tokens attend to tokens that appear before in the sentence. This unidirectional attention mechanism, often referred to as left-to-right attention or auto-regressive decoding, allows the models to generate coherent and contextually relevant text referencing the attention assigned to preceding words. Another LLM that is based on the Transformer architecture is the BERT (Bidirectional Encoder Representations from Transformers [Devlin et al., 2019]) model. BERT, introduced by Google AI, is designed to capture bidirectional contextual information from the input text. Unlike the unidirectional attention of GPT, BERT employs a bidirectional attention mechanism. It enables tokens to attend not only to preceding words but also to words appearing after in latter parts of the sentence. By considering the entire context, BERT can effectively capture the dependencies and relationships between words, resulting in a better understanding of the text.",
            "score": 0.5478092955107665,
            "section_title": "Related Works",
            "char_start_offset": 4544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 240,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 461,
                    "end": 495,
                    "matchedPaperCorpusId": "252666016"
                },
                {
                    "start": 527,
                    "end": 545,
                    "matchedPaperCorpusId": "263829435"
                },
                {
                    "start": 1159,
                    "end": 1181,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1743,
                    "end": 1764,
                    "matchedPaperCorpusId": "263829435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.485107421875
        },
        {
            "corpus_id": "272831970",
            "title": "Making Text Embedders Few-Shot Learners",
            "text": "The attention mechanism in LLM-based embedding models is typically unidirectional, aligned with the next-token prediction task fundamental to their pre-training (Touvron et al., 2023). \n\nHowever, recent studies indicate that unidirectional attention may limit the model's capacity for representation learning. Evidence suggests that bidirectional attention is more effective at integrating contextual information, resulting in improved performance on certain tasks. For example, LLM2Vec (BehnamGhader et al., 2024) introduces an additional training phase with a masked token prediction task, preconditioning the model for bidirectional attention. Approaches such as NV-Embed (Lee et al., 2024a) and GritLM (Muennighoff et al., 2024) replace unidirectional attention with bidirectional attention during the embedding training phase, often employing mean pooling or more sophisticated latent attention layers to obtain representations for queries and passages. \n\nDespite these advances, we argue that incorporating bidirectional attention during embedding finetuning creates a mismatch with the model's pre-training design, potentially undermining its incontext learning and generative properties. To address the trade-off between enhancing embedding representations for specific tasks and preserving the model's inherent generative properties for deep semantic pattern understanding, our approach retains the unidirectional attention mechanism, consistent with the majority of existing embedding methods. \n\nWe use the [EOS] token's output embedding as the vector representation for queries and passages, positioning it at the end of inputs to capture both semantic and ICL patterns through causal attention mechanisms, thereby aligning with the foundational pretraining methodology of LLMs. Specifically, given the tokenized input sequence T: [BOS], t 1 , ..., t N is sent into the LLM (Figure 1): \n\nThe text embedding is taken from the output embedding of the special token [EOS].",
            "score": 0.547681917964005,
            "section_title": "REPRESENTATION METHOD",
            "char_start_offset": 10522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1896
                },
                {
                    "start": 1899,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 732,
                    "matchedPaperCorpusId": "1289517"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "250964799",
            "title": "Comparative Analysis of Aesthetic Emotion of Dance Movement: A Deep Learning Based Approach",
            "text": "e language model is a statistical model to calculate the probability of any word order in the text. It is a basic subject of NLP. e neural network language model has only one hidden layer. e maximum penalty likelihood estimation of the random gradient rise method is used to train the network parameters, and softmax is used to calculate the probability of word order. Traditional encoding usually refers to distributed feature representations obtained by learning through neural network language models that can represent complex contextual information in a large-scale corpus in a nonsparse manner. In order to effectively focus on specific local information and tap deeper into feature information, an attention mechanism is proposed. e attention mechanism models the global dependence of input and output so that the model can learn feature information in different representation subspaces, so as to generate more semanticist representations. e calculation formula of the attention mechanism is as follows:\n\nIn terms of parallelism, the multi-head attention model, like CNN, does not rely on the previous calculation and can be well parallelized, which is better than RNN. e sequence-tosequence model is a supervised learning algorithm in which the input is a sequence of tokens, and the generated output is another sequence of tokens. However, when encoding, all semantic information is encoded into a fixed and an unified semantic feature vector and then decoded, which will cause a great loss of semantic information, especially word order information, which greatly reduces the decoding performance of the model. erefore, researchers use the attention mechanism to break the limitation of the traditional encoderdecoder structure that encoding and decoding only depend on the vector of a fixed length and size. e sequence-tosequence structure with the attention mechanism is shown in Figure 4. e codec network with the attention mechanism weights and sums these inputs by saving the intermediate process output of the encoder and training the attention model, and associates the output sequence with the attention model when decoding the output. e introduction of the attention mechanism enables the model to be associated with the required context information during encoding and decoding, and it strengthens this association in the process of iterative learning.\n\nParsimonious Bayes is widely used in sentiment classification for assigning the category with maximum probability to a given review for classifying documents. According to the Bayes' rule, it is expressed by the",
            "score": 0.5475045836300291,
            "section_title": "Relevant Technical and eoretical Basis.",
            "char_start_offset": 17563,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09967041015625
        },
        {
            "corpus_id": "261644305",
            "title": "Effects of Generative Chatbots in Higher Education",
            "text": "Generative models (GPT-style) emphasize modelling the joint distribution of inputs and outputs, generating realistic output data that aligns with the training data distribution. GPT-3, Wu Dao, LamDA, YaLM, PaLM, BLOOM, GLM, LLaMA, and GPT-4 are generative LLMs. \n\nUnsupervised pre-training objectives: Based on their approach for generating and reconstructing sequences, LLMs can be classified into three categories: encoder-only, decoderonly, and encoder-decoder language models. \n\nEncoder-only (auto-encoding) models aim to reconstruct the original input sequence from a compressed and abstract representation of that sequence. The encoding process captures contextual information bidirectionally, making auto-encoding models unsuitable for unconditional sequence generation. However, they are useful for tasks like language understanding, feature extraction, and text representation learning. BERT is an example of an encoder-only model. \n\nDecoder-only (autoregressive) models generate output sequences by predicting the next token given the preceding context. These models generate text sequentially, word by word, taking into account the entire history of the sequence generated so far. This approach allows for coherent and contextually appropriate generation but may be slower and less parallelizable due to the sequential nature of the generation process. GPT-3, GPT-4, PaLM, LaMDA, BLOOM, GLM, PanGu, YaLM, and LLaMa are examples of autoregressive models. \n\nAn encoder-decoder (sequence-to-sequence) model treats each task as sequence-tosequence conversion, which can involve text-to-text or even text-to-image or image-to-text generation. Encoder-decoder models are typically used for tasks that require both content understanding and generation, such as machine translation. XLNet is an example of an encoder-decoder model. \n\nModality: LLMs can be categorized into two groups, based on the number of modalities they operate with unimodal and multimodal language models.",
            "score": 0.5473282066351812,
            "section_title": "Large Language Models for NLP and Their Comparison",
            "char_start_offset": 21324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "6228859",
            "title": "Toward Mention Detection Robustness with Recurrent Neural Networks",
            "text": "Consequently, the concatenated vector \u03b1 i = [l i , r i ] constitutes a distributed representation that is specific to the word at position i but still encapsulates the context information over the whole sentence at the same time. This effectively provides the networks a much richer representation to decode the sentence. The bidirectional network for R e = ELMAN and R d = JORDAN is given on the left of Figure 2. \n\nWe notice that Mesnil et al. (2013) also investigate the bidirectional models for the task of slot filling in spoken language understanding. However, compared to the work presented here, Mesnil et al. (2013) does not use any special transition memory cells (like the GRUs we are employing in this paper) to avoid numerical stability issues (Pascanu et al., 2012). Besides, they form the inputs \u03b1 for the decoding phase from a larger context of the forward and backward encoding outputs, while performing word-wise, independent classification; in contrast, we use only the current output vectors in the forward and backward encodings for \u03b1, but perform recursive computations to decode the sentence via the RNN model R d (demonstrated on the right of Figure 2).",
            "score": 0.546834794143302,
            "section_title": "The Bidirectional Networks",
            "char_start_offset": 15174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1177
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07342529296875
        },
        {
            "corpus_id": "232168537",
            "title": "Domain Controlled Title Generation with Human Evaluation",
            "text": "T5: Text-to-text-Transfer-Transfer model was proposed by Colin Raffel et al. [17] to reframe all NLP-based activities to a single text-to-text format where text strings are both input and output. This formatting enables the T5 model to cast various tasks into this framework including machine translation, classification task, regression task, and summarization task. This framework uses the same model, hyperparameters, and loss function on the tasks mentioned. \n\nThe model architecture is similar to standard vanilla encoder-decoder transformers with similar computational cost and considering half parameters. The input sequence is initially mapped to the embedding sequence and transferred to the encoder afterwards. The encoder comprises a stack of blocks, and each block consists of two subcomponents: a self-attention layer and a feed-forward network. Normalization of the layer is introduced to each subcomponent's input. Normalization only includes rescaling of the activations and no additive bias is introduced. This is followed by a residual skip connection that adds the subcomponent's input to its output. This model considers a different position embedding scheme and these are passed as parameters across all layers of the model. \n\nThe structure of the decoder is similar to the encoder except it incorporates the standard attention process after the self-attention layer encoder's output. The self-attention in the decoder is a type of auto-regressive or causal selfattention that only enables the algorithm to attend to previous outputs. The final decoder block output is fed into a dense layer with a softmax output, the weights of which are shared with the input embedding matrix. Hence, the T5 model is similar to the original Transformer with an exception of the removal of Layer Norm bias and using different position embedding methods.",
            "score": 0.5464817747655248,
            "section_title": "T5 Model",
            "char_start_offset": 12319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1859
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1790771484375
        },
        {
            "corpus_id": "251991124",
            "title": "Adding Visual Information to Improve Multimodal Machine Translation for Low-Resource Language",
            "text": "Word alignment is a prominent issue in machine translation, which analyzes the relationship between the input sequence and the output target sequence, and the translation model selects the most appropriate result for output based on exact relationship. \n\nMultimodal machine translation (MMT) assists the system to translate higher quality target sequences by other information besides the text, such as images, audio, and video, which are incorporated into the translation model by different ways to assist the system. It has been shown that [6], by matching image information with text and performing training, produced better results than text-only translation. \n\nFor now, plain text-based machine translation with an encoder-decoder architecture is a widely used technique. \n\nis model accepts a sequence as input and encodes the information in the sequence into an intermediate representation. e decoder then decodes the intermediate representation into the target language. LSTM has been shown to be a good solution to the long-time sequence dependence problem. Sutskever et al. [4] proposed to encode the input sequence with an LSTM and decode that vector with another LSTM and obtain the output sequence. However, for long sentences, this encoder-decoder structure is unable to encode all the information. Bahdanau et al. [5] proposed attention mechanism and the problem was solved. In lowresource language translation, the model architecture based on attention mechanism has good performance. Bahdanau et al. [5] have good performance in WAT2021 with bidirectional RNN (BRNN) encoder and dual attention RNN decoder for multimodal machine translation. \n\nIn this paper, we investigate improving on the text-only machine translation model by adding image information as an auxiliary modality to improve the quality of the translated target sentences. In our experiments, we extract image features by different models and then add image features information to the encoder side of the baseline, fuse it with the initial word encoding information and position embedding information, and train an encoder containing image features, followed by a standard decoder module at the same time, to form a complete encoder and decoder model, thus improving the quality of English to Hindi translation.",
            "score": 0.5460576562513213,
            "section_title": "Introduction",
            "char_start_offset": 2126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 542,
                    "end": 545,
                    "matchedPaperCorpusId": "16585731"
                },
                {
                    "start": 1083,
                    "end": 1086,
                    "matchedPaperCorpusId": "7961699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.187744140625
        },
        {
            "corpus_id": "252968120",
            "title": "On effects of Knowledge Distillation on Transfer Learning",
            "text": "This section focuses on the origin and motivation for developing an attention mechanism in neural networks. Attention mechanism refers to the act of attending to just a relevant region of input instead of the whole input image or sentences. In contrast to encoding a variable-length sequence into a fixed-length vector [97], Bahdanau et. al. [98] introduced the attention mechanism with encoderdecoder architecture. In encoder-decoder architecture, a neural network called an encoder is used to encode the training input to a representation. After that, another network called a decoder is used to decode the representation to get the output. In attention-based networks, the decoder can essentially attend/focus on a specific region of the input/source sequence directly. This allows the model to learn the corresponding mapping of relevant words in different sentences for natural language processing.\n\nDiving further into attention, Vaswani et. al. [32] proposed to remove the whole sequence/recurrence bit from the model. Removing recurrence would enable models to be trained in parallel on massive datasets in significantly less time. They fundamentally only used a type of attention to process the whole sequence in parallel with the help of positional encoding. This model was called Transformers [32] which would employ self-attention, which allows the model to learn representations by looking at the input sequence itself. This Transformer model [32] actually revolutionized the NLP research as we know it today.\n\nWith the help of self-attention in transformers, Transformers encoders [32] learn unidirectional representations of words with the help of context. BERT [9] learns representation looking at both forward and backward section inspired by Cloze procedure [99]. To learn the word-word interaction, BERT masks a word and uses context around it to predict the masked word. The intuition is that the model will be able to predict masked words once it learns about the syntax and semantics of the words. It learns more general sentence-level representation with the help of proxy tasks like Next Sentence Prediction (NSP). The representations that it learns were so rich that when applied to multiple downstream tasks ranging from machine translation, Question Answering, etc. BERT works in the form of transfer learning, i.e., pre-training with massive datasets and",
            "score": 0.5450889677865609,
            "section_title": "Attention, Transformers, and BERT",
            "char_start_offset": 64201,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1456,
                    "end": 1460,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1595,
                    "end": 1599,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1776,
                    "end": 1780,
                    "matchedPaperCorpusId": "206666846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339111328125
        },
        {
            "corpus_id": "248784489",
            "title": "Turkish abstractive text summarization using pretrained sequence-to-sequence models",
            "text": "BERT (Devlin et al. 2019) is a bidirectional transformer network pretrained on a large corpus with two pretraining objectives: masked language modeling and next sentence prediction. It closely follows the original transformer network (Vaswani et al. 2017) with the major improvement being the bidirectional self-attention mechanism. The authors have released several multilingual pretrained models that support a wide variety of languages including Turkish. In addition to the multilingual models, monolingual models have been pretrained by the community (Virtanen et al. 2019;Polignano et al. 2019;Kuratov and Arkhipov 2019;Chan, Schweter, and M\u00f6ller 2020;Schweter 2020). Tokenization is an important aspect for these models since the input tokens are directly determined by the tokenization method and accordingly might impact the models' performance (Bostrom and Durrett 2020;Zhang et al. 2020). Most of the released models follow the original BERT model and were pretrained using the WordPiece (Wu et al. 2016) tokenization method. \n\nUnlike sequence-to-sequence models which are composed of two parts, an encoder and a decoder, BERT works as an encoder-only model. Figure 1 shows a high-level view of a sequenceto-sequence transformer encoder-decoder model. The encoder transformer layers usually contain bidirectional connections which closely resemble the BERT model, whereas the decoder layers contain unidirectional (left to right) connections. Although BERT is an encoder-only model, it is possible to utilize pretrained checkpoints so that a sequence-to-sequence model can be constructed by initializing both the encoder and the decoder parts with pretrained model checkpoints (Rothe et al. 2020). This procedure is known as warm-starting an encoder-decoder model.",
            "score": 0.5449889198750841,
            "section_title": "BERT2BERT",
            "char_start_offset": 12827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1774
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 24,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 234,
                    "end": 255,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 577,
                    "end": 599,
                    "matchedPaperCorpusId": "204914950"
                },
                {
                    "start": 625,
                    "end": 657,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 853,
                    "end": 879,
                    "matchedPaperCorpusId": "215416175"
                },
                {
                    "start": 879,
                    "end": 897,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1687,
                    "end": 1706,
                    "matchedPaperCorpusId": "198967997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2139892578125
        },
        {
            "corpus_id": "259047693",
            "title": "Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
            "text": "Encoder-decoder models [47] refer to sequence-to-sequence models, utilizing both components of the transformer architecture [48]. The encoder's attention layers can access all words in the input sentence at each stage, while the decoder's attention layers can only access the words preceding a given word in the input. Sequence-to-sequence models such as BART [49], T5 (Text-to-Text Transfer Transformer) [50], and TreeGen [51] are well-suited for tasks that involve generating new text based on an input, such as code generation, code refinement, defect detection, and clone detection, for AI-assisted programming tasks. \n\nEncoder-only models, also known as autoencoders, use only an encoder network to transform input data into a compressed representation. They are commonly used in unsupervised learning tasks such as dimensionality reduction and anomaly detection in NLP tasks. In the past, code embedding approaches could be utilized to obtain the representation from the input data such as Neural Network Language Model [52], Code2Vec [53], ELMo [54], TextRank [55], and GGNN [56]. For AI-assisted programming tasks, they are used for understanding tasks to learn useful representations with the The encoder considers all words in a sentence, while the decoder works sequentially. Once the initial words are predicted, they are used to generate subsequent words. The attention layers in the encoder consider all the words in a sentence, while the decoder works sequentially and can only focus on the words it has already translated. \n\nBERT [57] and RoBERTa [44] of data in an unsupervised manner, which can be used as features for downstream tasks such as code translation and code summarization. \n\nDecoder-only models, also known as autoregressive models, are a type of neural network architecture used in natural language processing tasks such as GPT-2 [58], GPT-3 [59], GPT-J [60], Reformer [61], and GPT-Neo [62], which use the decoder to predict the next token output given all previous tokens.",
            "score": 0.5448312494470665,
            "section_title": "Encoder-only Understanding",
            "char_start_offset": 8664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 423,
                    "end": 427,
                    "matchedPaperCorpusId": "208248351"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "1326925"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1067,
                    "end": 1071,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "3495200"
                },
                {
                    "start": 1546,
                    "end": 1550,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1248779296875
        },
        {
            "corpus_id": "263729712",
            "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization",
            "text": "For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers. In addition, a language model head layer follows the decoder blocks, converting the last decoder block's output vectors to logit vectors. As a result, the following steps have been taken to enable an encoder to function as a decoder: First, we alter the self-attention layers to operate unidirectionally, similar to the decoder, and initialize them with the weights from the encoder's self-attention layer. A cross-attention layer is then added between the self-attention layer and the two feed-forward layers. As suggested by [6], we randomize the initialization of this layer's weights, which are subsequently trained while finetuning the model on the summarization task. Finally, we add a language model head layer on top of the last block of the decoder and initialize it with the weights of the encoder's word embeddings. Table 3 highlights the necessary adjustments for this approach. It is worth mentioning that the hidden size of the encoder and decoder in warm-started models must match in order for them to communicate and perform dot products on their respective vectors.",
            "score": 0.5444023891336941,
            "section_title": "2) DECODERS",
            "char_start_offset": 21434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "198967997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4833984375
        },
        {
            "corpus_id": "221702979",
            "title": "Current Limitations of Language Models: What You Need is Retrieval",
            "text": "We consider some of the successful recent language model approaches to improve the performancecomputes trade-off of language model (Kaplan et al., 2020) and classify them for the ease of our argument. Each category tends to be orthogonal to each other, so that they can often be used in combination for further gain. Fig. 1 summarizes our classification. There are four major categories: non-causal models, extension of batch length with efficient attention, memory and retrieval. We note that batch length in this paper is defined to be the length of a minibatch across timestep dimension. We avoid calling it context length to be more specific. In this paper, the default modality of our consideration is text unless specified otherwise. Also, memory and retriever are defined to possess a certain property addressed later in this section, so their usage may differ from the usual one. Efficient attention also refers to non-attention alternatives with better complexity, such as variants of convolution, for the convenience of our argument. \n\nNon-causal models in this paper are defined to be the model that predicts a token using future information. The example includes various BERT variants, including BART and T5 (Devlin et al., 2018;Raffel et al., 2019;Yang et al., 2019;Liu et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Lewis et al., 2019;Hendrycks et al., 2020). On the other hand, causal models include the original Transformer encoder-decoder (Vaswani et al., 2017) and GPT-2/3. Though we usually consider the decoder-only model as the latter for a causal model, it should be clear from the context whether a causal model refers to the former, the latter or both.",
            "score": 0.5431058380721782,
            "section_title": "Classification of Recent Language Model Approaches",
            "char_start_offset": 53,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1683
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1566162109375
        },
        {
            "corpus_id": "212703302",
            "title": "Ultra-Short-Term Photovoltaic Power Prediction Based on Self-Attention Mechanism and Multi-Task Learning",
            "text": "In a traditional Encoder-Decoder network, when the input sequence is very long, it is difficult to get all the necessary information [30]. Fortunately, attention mechanism breaks the restriction of traditional Encoder-Decoder structure that the decoder only relies on a vector with a fixed length. The attention mechanism is an efficient way to obtain pertinent information. On the one hand, it allows the decoder to query the most relevant information at every step of decoding; on the other hand, it shortens the distance of information flow greatly. The decoder can obtain all the information needed in only one step at any moment [31]. Given these advantages, the attention mechanism has been applied in various models. \n\nGehring et al. [32] proposed the ConvSeq2Seq model, using a convolutional neural network instead of a recurrent neural network as the encoder and decoder. Between the encoder and the decoder, the attention mechanism was still used to transmit the information. Vaswani et al. [33] proposed the Transformer model, which abandoned the traditional method using recurrent neural networks or convolutional neural networks as the encoder and decoder, completely used the attention mechanism to transfer information between different network layers, which is called self-attention and achieved good results in the NLP field. \n\nIn this paper, we quote the self-attention mechanism in the Transformer model to extract features of photovoltaic power generation data. Different from the commonly used attention mechanism, the self-attention mechanism can be considered as a new idea for processing sequence data. Studies have shown that using self-attention instead of CNN or RNN to solve problems related to sequence data not only achieved better results, but also overcame the problem that RNN cannot be calculated in parallel, so it speeds up the operation speed [33]. \n\nThe self-attention mechanism is a method of calculating relevance. Its working principle can be observed in Figure 1. It can be understood as using a matrix called Query as a query object, a matrix called Key as a matching object, and a matrix named Value as a target.",
            "score": 0.5430669647583173,
            "section_title": "II. METHOD A. SELF-ATTENTION MECHANISM",
            "char_start_offset": 5447,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1880,
                    "end": 1884,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306884765625
        },
        {
            "corpus_id": "276558335",
            "title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning",
            "text": "Large Language Models (LLMs) learn statistical patterns in text from large corpora, enabling them to predict the likelihood of word (or token) sequences in context. They often rely on transformer architectures, which use self-attention to capture token dependencies [Vaswani et al., 2017]. Transformerbased LLMs include encoder-only models that mask part of the input and learn to predict the missing portion (useful for text understanding), decoder-only models that generate text by predicting the next token in a sequence (often used for text generation), and encoder-decoder models that encode input into a latent representation and then decode it (common for translation tasks). \n\nVision Language Models (VLMs) are multimodal, processing both visual and textual data, often relying on transformers. They can be categorized into encoder-decoder models that convert images and/or text into latent embeddings before generating output (used for tasks like captioning), dualencoder models that embed images and text separately into a shared latent space (used for similarity matching and retrieval), and single encoder models that encode images and text jointly (used for tasks like visual question answering).",
            "score": 0.5428023557244246,
            "section_title": "Language Models",
            "char_start_offset": 6171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1209
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 288,
                    "matchedPaperCorpusId": "271746329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1751708984375
        },
        {
            "corpus_id": "276287237",
            "title": "Mapping the Landscape of Generative AI in Network Monitoring and Management",
            "text": "The encoder comprises a stack of identical layers, each with two sub-layers, namely a multi-head self-attention and a position-wise fully connected feed-forward network. The decoder is similar to the encoder but includes an additional sub-layer that performs masked multi-head attention over the encoder's output. It also modifies the self-attention sub-layer to prevent positions from attending to subsequent positions, ensuring the auto-regressive property. Both encoder and decoder are designed with residual connections for each sub-layer followed by layer normalization. \n\nGoing into detail, the Transformer uses multi-head attention to allow the model to learn information from different representation subspaces jointly. In fact, instead of having a single attention function, the model linearly projects queries, keys, and values multiple times with different learned projections and performs the attention function in parallel. This process enhances the model's ability to focus on different parts of the input sequence. Moreover, since the Transformer lacks the inherent sequential order provided by recurrence, it introduces positional encodings to inject information on the position of tokens in the sequence. These encodings are added to the input embeddings at the bottom of the encoder and decoder stacks, enabling the model to understand the sequence order. \n\nThe Transformer is usually leveraged for sequence-tosequence tasks, i.e., when the input and the output are both sequences. Examples of applications are translation (from one language to another), summarization (condensation of documents), and text generation (based on given prompts). Notable architectures that fall into this category are XLNet, T5, Gemini, Mistral, and Zephyr. \n\nEncoder-Only (EO): Compared to FED, EO models only leverage the encoder unit of the FED architecture (as depicted in Figure 3b). EO is designed to model bidirectional relationships between tokens in an input sequence, generating either a vector representation for each token or a single vector summarizing the entire sentence. This architecture is wellsuited for tasks focused on text understanding and analysis rather than generation.",
            "score": 0.5425634837630882,
            "section_title": "C. Recent Advancements on GenAI",
            "char_start_offset": 27222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2119140625
        },
        {
            "corpus_id": "269981935",
            "title": "Bitune: Bidirectional Instruction-Tuning",
            "text": "Our approach shares similarities with the concept of \"prefix language modeling\", which enables a decoder-only model to handle bidirectional context within a prefix (instruction) while maintaining causal generation for the output sequence.The prefix-LM architecture was introduced by Liu et al.\n\n(2018) and further explored and popularized by Raffel et al. (2020).In their work on T5, Raffel et al. (2020) pretrained the prefix-LM architecture alongside other architectures, such as encoder-decoder and decoder-only models, demonstrating that prefix-LM outperforms decoder-only models on both training objectives: denoising and language modeling.\n\nThe prefix-LM approach has been used in UniLM (Dong et al., 2019), which trains a single transformer on three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction.UniLM employs a shared Transformer network and utilizes specific self-attention masks to control the context that predictions are conditioned on, where the sequence-to-sequence task is equivalent to the prefix-LM approach.\n\nAdditionally, UL2 (Tay et al., 2023) introduces a pretraining objective called \"Mixture of Denoisers\", which combines various denoising strategies, including the prefix-LM approach.Lastly, XLNet Yang et al. ( 2019) also allows for non-causal word ordering by allowing random permutations to be used with a next-token prediction objective.\n\nAll these works focused on the model pretraining.As for the utilization of pretrained causal language models, Springer et al. (2024) show in their work that simply repeating the input to these models improves the quality of token embeddings for text-retrieval.This work addresses the limitation that token embeddings in autoregressive models cannot contain information from tokens appearing later in the input.By repeating the input twice, the early tokens are allowed to encode information Table 6: Qualitative results on Llama3-8B.We show a response for LoRA and Bitune.\n\n[GSM8K] Question:\n\nJanet's ducks lay 16 eggs per day.",
            "score": 0.5425250196271598,
            "section_title": "Related Work",
            "char_start_offset": 19262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 293
                },
                {
                    "start": 295,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 645
                },
                {
                    "start": 647,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1080
                },
                {
                    "start": 1082,
                    "end": 1263
                },
                {
                    "start": 1263,
                    "end": 1420
                },
                {
                    "start": 1422,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 1832
                },
                {
                    "start": 1832,
                    "end": 1955
                },
                {
                    "start": 1955,
                    "end": 1994
                },
                {
                    "start": 1996,
                    "end": 2013
                },
                {
                    "start": 2015,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 362,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 384,
                    "end": 404,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299560546875
        },
        {
            "corpus_id": "258461308",
            "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
            "text": "Denoising with Encoder-Decoder Framework Unlike DiffuSeq (Gong et al., 2022) using encoderonly Transformer architecture, we propose using an encoder-decoder Transformers architecture to model the input and output text sequences. For z 0 \u03b8 (z t , w x , t), we use the encoder to process the input sequences w x and use the decoder to model the noisy output sequence z t . Following the previous work (Li et al., 2022), we inject time step information t by adding time step embedding to z t . Using the encoder-decoder architecture has computational convenience during generation because the input sequences w x only require one forward computation through the encoder network during the whole reverse process. Considering the reverse process requires thousands of iterations to generate the output sequences of high quality, the saving of computational resources can be significant. \n\nDuring training and generation, the function z 0 \u03b8 generates denoised samples at the sequence level. Therefore making predictions from the denoising function z 0 \u03b8 resembles the non-autoregressive natural language generation. In this regard, we use a decoder with full attention matrices instead of causal attention matrices to model z t at the sequence level.",
            "score": 0.5424285630825837,
            "section_title": "Diffusion Model",
            "char_start_offset": 10872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1244
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2181396484375
        },
        {
            "corpus_id": "248239682",
            "title": "DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks",
            "text": "In section 3.1, we conclude that Transformer with the causal attention masks is naturally sensitive to word order. Since the position information is inevitable for BERT, we propose to enhance existing BERT model based on causal attention masks.\n\nIn this paper, we intend to add the causal attention masks into all or some hidden layers of BERT. In this way, the specific layers with such masks are sensitive to word order by design, which can enhance the position encodings ability of BERT. Such framework can further result in better language understanding performances, e.g., in pretrained language modeling, casual attention masks were added on all 12 layers of GPT (Radford and Narasimhan, 2018). However, comparing with BERT (Devlin et al., 2019), we observe that GPT lags behind BERT on almost all downstream tasks. 3 This is because self-attention mechanism with such masks only consider one-side information flow, it cannot process the input sentence comprehensively and has a high risk of language information loss. Therefore, we can conjecture that it is not effective to use the causal attention masks in all hidden layers. There is a strong need to maintain a balance between the gain of position encoding ability and the loss of language information.\n\nIn order to determine which layer(s) should add casual attention masks, we refer to the BERTology work (Jawahar et al., 2019) that conduct comprehensive experiments to analyze and interpret the information captured by each layer of BERT. The experimental results indicate that the lower layers of BERT capture rich language structure information. The position information is also a common structure information, so that we propose to add the causal attention masks into the lower layers (e.g., the first two layers 4 ) to improve the position encoding ability of BERT. We denote our model as DecBERT. There are two versions of our model, DecBERT-Same and DecBERT-Diff. All of them are 12-layer base size models.\n\n\u2022 DecBERT-Same: This model has a similar structure as BERT (see Figure 1(a)), but we use the causal attention masks to convert the first two Encoder layers into two Decoder layers with the same direction (from left to right). So the 12-layer model has 10 Encoder layers and 2 Decoder layers, which is shown in",
            "score": 0.5416721024887496,
            "section_title": "Our DecBERT Model",
            "char_start_offset": 8778,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 730,
                    "end": 751,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1368,
                    "end": 1390,
                    "matchedPaperCorpusId": "195477534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.387939453125
        },
        {
            "corpus_id": "267101122",
            "title": "What is the Consumer Attitude toward Healthcare Services? A Transfer Learning Approach for Detecting Emotions from Consumer Feedback",
            "text": "BERT is a prominent model in the transformer encoders-only category, which utilized bidirectional transformer encoders for masked language modeling and bidirectional context understanding [Devlin et al. 2018]. On the other hand, transformer encoder-decoder frameworks, represented by models such as T5 (Text-to-Text Transfer Transformer) [Raffel et al. 2020] and ERNIE 3.0 [Sun et al. 2021], have been aimed at pre-training sequence-to-sequence generation models, with T5 framing all NLP tasks as text-to-text problems and ERNIE 3.0 outstanding in capturing bidirectional context understanding and generating contextually rich word embeddings. \n\nIn addition to the architectural differences, there is a distinction between shortsequence and long-sequence models. Short-sequence models such as BERT [Devlin et al. 2018], BioBERT [lee et al. 2020], or ClinicalBERT [Alsentzer et al. 2019] have been designed to efficiently process relatively small input sequences with standard transformer architectures. On the other hand, long-sequence models such as Longformer [Beltagy et al. 2020] and BigBird [Zaheer et al. 2020] have been tailored to handle much longer input sequences, such as lengthy documents or narratives, by incorporating novel attention mechanisms to efficiently process and understand long texts. \n\nThese PLMs have been used in many tasks, for instance, to classify lifestyle factors such as physical activity and excessive diet from clinical texts [Shen et al. 2021], to identify medication mentions in clinical notes [Sch\u00e4fer et al. 2023], to extract oncologic outcomes [Araki et al. 2023], to detect clinical NER, classify biomedical text and predict disease diagnosis [Ni et al. 2021], to categorize sentiments (positive or negative) [Punith et al. 2021] or to classify texts into clinical specialties in different languages [Pomares-Quimbaya et al. 2021], but little work can be found on emotion detection.",
            "score": 0.5414895668133748,
            "section_title": "Introduction and background",
            "char_start_offset": 5561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 207,
                    "matchedPaperCorpusId": "220635321"
                },
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 373,
                    "end": 390,
                    "matchedPaperCorpusId": "235731579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1265869140625
        },
        {
            "corpus_id": "276249737",
            "title": "Towards the Development of Balanced Synthetic Data for Correcting Grammatical Errors in Arabic: An Approach Based on Error Tagging Model and Synthetic Data Generating Model",
            "text": "A state-of-the-art model for natural language processing (NLP) tasks, Text-to-Text Transfer Transformer (T5) [28] can be applied to a variety of NLP tasks by treating them as text-to-text tasks. In addition to classifying, summarizing, and translating, the model uses a unified architecture and loss function. T5 utilizes a transformer architecture that uses encoder-decoder structures to produce coherent and fluent results. This architecture is composed of two parts: The encoder generates a contextual representation of the input sequence. In producing each entry of the output, the self-attention mechanism uses a fully visible attention mask to attend to any entry of the input. The decoder generates the output sequence from the encoder's output. This technique uses both causal masking and encoder-decoder attention to maintain autoregressive properties while at the same time attending to the encoder's output. Multiple benchmarks have been run on the model, and it performs better when scaled to various sizes, with the largest variant, T5-11B, containing 11 billion parameters. A powerful dataset known as the Colossal Clean Crawled Corpus (C4) was used to pre-train the model, making it able to learn from a wide range of texts. Although T5 performed well on several tasks, including CNN/Daily Mail summarization benchmarks, it performed poorly on some translation tasks, perhaps due to its reliance on an English-only dataset. mT5 [29] is an enhancement of T5 that utilizes GeGLU nonlinearities and scales both feed-forward dimensions and model dimensions in larger models. The encoder-decoder model supports generative tasks such as abstractive summarization and dialog, which is different from encoder-only models such as XLM-R. As a result of training more than 250,000 words, the mT5 model has several sizes, such as Small (300M parameters), Base (580M), Large (1.2B), XL (3.7B), and XXL (13B).",
            "score": 0.5412542817743896,
            "section_title": "Text-to-Text Transformer",
            "char_start_offset": 13312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262451171875
        },
        {
            "corpus_id": "257616068",
            "title": "Bilaterally Slimmable Transformer for Elastic and Efficient Visual Question Answering",
            "text": "Depending on the composition strategies of the Transformer layers above, existing Transformer architectures can be categorized into three classes, namely encoders [41][74], decoders [75][76], and encoderdecoders [1][77]. \n\nTaking a sequence of input tokens, the original Transformer [1] adopts an encoder-decoder architecture. The encoder is composed of a cascade of Transformer layers in depth to obtain the bidirectional representations by jointly conditioning on both the left and right contexts, and the decoder takes the representations from the last encoder layer as input to guide the learning of unidirectional representations by conditioning only on the left context. After that, pure encoder architectures (e.g., BERT [41]) and pure decoder architectures (e.g., GPT [75]) are introduced and integrated with the self-supervised pretraining paradigm, which has been used in a wide range of NLP tasks.",
            "score": 0.5410193064465108,
            "section_title": "A. Preliminaries",
            "char_start_offset": 15748,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 908
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 728,
                    "end": 732,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19873046875
        },
        {
            "corpus_id": "254854634",
            "title": "Mu2SLAM: Multitask, Multilingual Speech and Language Models",
            "text": "The recent rapid developments in NLP have witnessed the tremendous success of moving towards unified text models for both understanding and generation tasks across hundreds of languages, evolving into numerous pre-trained models from encoder-only models focusing on text understand- ing (Devlin et al., 2019;Devlin, 2018), to decoder-only models (Radford et al., 2018;Chowdhery et al., 2022) and encoder-decoder models (Song et al., 2019;Lewis et al., 2019;Raffel et al., 2020;Xue et al., 2020) for both understanding and generation. The speech pre-training methods have shown a similar trend towards unified models from the dominant encoder-only models (Baevski et al., 2020;Hsu et al., 2021;Babu et al., 2021;Bapna et al., 2021;2022) to generative models on cross-modal speech and text data, exemplified by a couple of recent trails such as decoder-only models (Borsos et al., 2022) and encoder-decoder models (Ao et al., 2021;Chen et al., 2022;Sainath et al., 2022;Zhou et al., 2022;Zhang et al., 2022b;Tang et al., 2022). \n\nAlthough these works have achieved impressive performance, they only consider partial aspects of the unified models in speech and text. First, except for SLAM and mSLAM (Bapna et al., 2021;2022), most of them merely focus on speech-related tasks by taking text data as auxiliary inputs while ignoring evaluations on text-related benchmarks, which leaves us unknown to gauge the effect of interference and capacity dilution. Second, there are few studies investigating multilingual modeling with both speech and text (Bapna et al., 2022;Chen et al., 2022), which limits them in leveraging cross-lingual transfer to enrich the speech and text joint representations.",
            "score": 0.5409174954512305,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1691
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 308,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 419,
                    "end": 438,
                    "matchedPaperCorpusId": "146808476"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1006,
                    "end": 1024,
                    "matchedPaperCorpusId": "248119033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039703369140625
        },
        {
            "corpus_id": "227254536",
            "title": "Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks",
            "text": "Perplexity on WikiText-103 BLEU on WMT17 en-de Average Accuracy on GLUE Figure 1: Attention pruning maintains performance and reduced attention computations. Pruning can often enable more efficient and interpretable models with only a modest decrease in performance. \n\nto remove weak connections between input tokens. Following Frankle and Carbin (2019), we then retrain these models, enforcing sparseness through masking, and we demonstrate that attention mechanisms incorporate extraneous connections between the input tokens: we obtain comparable performance while using sparse attention patterns for NLP tasks such as language and sequenceto-sequence (seq2seq) modelling, as well as prediction on GLUE tasks. Figure 1 summarizes the impact of using our pruning method on standard NLP tasks. \n\nThese global sparseness patterns could help improve both interpretability and inference-time computational efficiency for widely-used attention mod-arXiv:2012.02030v3 [cs.CL] 17 May 2024 els. Our contributions are as follows: \n\n\u2022 We present a novel pruning method for attention patterns, focusing on the model's computational graph, not attention weights, and showcase both theoretical and empirical gains. \n\n\u2022 Our data-informed, global method prunes attention while retaining information and closely preserving original results. \n\n\u2022 As an application-agnostic method, we investigate pruning impacts on language and seq2seq modeling, including GLUE task predictions. \n\n\u2022 In seq2seq experiments, we examine attention pruning in encoder self-attention, decoder self-attention, and encoder-decoder attention, highlighting key differences.",
            "score": 0.5408066047862338,
            "section_title": "Degradation Suppression",
            "char_start_offset": 1518,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1632
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 353,
                    "matchedPaperCorpusId": "53388625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2216796875
        },
        {
            "corpus_id": "19152001",
            "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding",
            "text": "In addition, computing attention only requires matrix multiplication, which is highly parallelizable compared to the sequential computation of RNN and two-dimensional convolution of CNN. \n\nIn a very recent work (Vaswani et al. 2017), an attention mechanism is solely used to construct a sequence to sequence (seq2seq) model that achieves a new state-of-theart quality score in the neural machine translation (NMT) task. The seq2seq model, \"Transformer\", has an encoderdecoder structure that is only composed of stacked attention networks, without using either recurrence or convolution. The proposed attention, \"multi-head attention\", projects the input sequence to multiple subspaces, applies scaled dotproduct attention (Rush, Chopra, and Weston 2015) to its representation in each subspace, and lastly concatenates their output. By doing this, it can combine different attentions from multiple subspaces. This mechanism is used in Transformer to compute both the self-context fusion features inside the encoder/decoder, and the bottleneck features between the encoder and decoder. \n\nThe attention mechanism has more flexibility in sequence length than RNN/CNN, and is more task/data-driven when modeling dependencies. Unlike sequential models, its computation can be easily and significantly accelerated by existing distributed/parallel computing schemes. However, to the best of our knowledge, a neural net entirely based on attention has not been designed for other NLP tasks except NMT, especially those that cannot be cast into a seq2seq problem. \n\nCompared to RNN, a disadvantage of most attention mechanisms is that the temporal order information is lost, which however might be important to the task. This explains why positional encoding is applied to each sequence before being processed by attention in Transformer. How to model order information within attention is still an open problem. \n\nThe goal of this paper is to develop a unified RNN/CNNfree attention network that can be generally utilized to learn the sentence encoding model for different NLP tasks, such as natural language inference, sentiment classification, document classification and sentence relatedness. We focus on the sentence encoding model because it is the basic module of most DNNs used in the NLP literature.",
            "score": 0.5406522865987303,
            "section_title": "Introduction",
            "char_start_offset": 1891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 232,
                    "matchedPaperCorpusId": "11440692"
                },
                {
                    "start": 722,
                    "end": 753,
                    "matchedPaperCorpusId": "1918428"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305908203125
        },
        {
            "corpus_id": "268041362",
            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
            "text": "Unlike decoder-only and encoder-decoder LLMs that utilize autoregressive regression, the encoder-only LLMs emphasize comprehension of input content and generation of task-specific outputs. \n\n2.2.1 BERT Series. BERT [47], short for Bidirectional Encoder Representations from Transformers, is a language model based on the transformer architecture, known for its significant improvement over previous state-of-the-art models. \n\nThe brief illustration for BERT series is shown in Figure 5.  BERT. BERT [47] namely a bidirectional Transformer-based encoder, is a LLM introduced by the Google AI Language team [48]. It incorporates masked language modeling, allowing pre-training to capture interactions between left and right context words. Recent advancements, such as extended training duration, parameter tying across layers, and span masking instead of individual words, have demonstrated improved performance. Notably, BERT's auto-regressive predictions limit its effectiveness for generation tasks. \n\nRoBERTa. RoBERTa [49], namely Robustly optimized BERT [47] approach, improves upon BERT [47] through straightforward modifications, such as longer training with larger batches, removal of the next sentence prediction objective, and dynamic changes to the masking pattern, and employs a more extensive byte-level Byte Pair Encoding (BPE) vocabulary. However, unlike BERT, RoBERTa streamlines its training process by omitting the Next Sentence Prediction (NSP) task and focusing solely on optimizing the Masked Language Model (MLM) task. This approach enhances the model's ability to learn bidirectional contextual information in language. \n\n2.2.2 UNiLM. UNILM [50] is a unified pre-training model jointly optimized for multiple language modeling objectives with shared parameters, covering bidirectional, unidirectional, and sequence-to-sequence language models. The model undergoes pre-training using three types of language modeling tasks: unidirectional, bidirectional, and sequence-tosequence prediction. Unified modeling is achieved through a shared Transformer network, incorporating specific self-attention masks to control the contextual conditions for predictions.",
            "score": 0.5403393727253435,
            "section_title": "Encoder-only Transformer Architecture",
            "char_start_offset": 12669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 1662,
                    "end": 1666,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "276725490",
            "title": "Retrieval Backward Attention without Additional Training: Enhance Embeddings of Large Language Models via Repetition",
            "text": "Research shows that decoder-only models, pre-trained with next-token prediction, achieve superior zero-shot generalization performance across various tasks (Wang et al., 2022), while its performance on NLU is still not good as that of bidirectional models, some prompt methods can be implemented to improve the embedding quality (Jiang et al., 2023;Liu et al., 2024), in this paper, we also focus on improving the embedding quality of decoder-only models from the perspective of the input text by repetition and backward attention and we call it ReBA embedding (Figure 1). \n\nRepetition: We focus on improving the performance of decoder-only models in a zero-shot setting. Research shows that repeating input sentences can provide additional contextual information, significantly boosting model performance. For instance, (Jelassi et al., 2024) found that text repetition allows Transformer models to outperform state-space models like S4 (Gu et al., 2021) and Mamba (Gu & Dao, 2023). Similarly, (Arora et al., 2024;Springer et al., 2025) demonstrated quality improvements in language models through repeated contexts. While repetition enhances decoder-only models, their performance still lags behind bidirectional models (see Table 2 and Figure 3). To address this gap, we propose a simple backward attention mechanism to further improve context encoding. Although our method remains inferior to bidirectional models, it achieves substantial improvements over simple repetition and classical embeddings in zero-shot scenarios, offering valuable insights for future research.",
            "score": 0.5400725515839826,
            "section_title": "Introduction",
            "char_start_offset": 1448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1575
                }
            ],
            "ref_mentions": [
                {
                    "start": 821,
                    "end": 843,
                    "matchedPaperCorpusId": "267406617"
                },
                {
                    "start": 1015,
                    "end": 1037,
                    "matchedPaperCorpusId": "267897400"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2841796875
        },
        {
            "corpus_id": "212747696",
            "title": "Deliberation Model Based Two-Pass End-To-End Speech Recognition",
            "text": "Additional Encoder Fig. 1. Diagram of the deliberation model with an optional additional encoder (dashed box).\n\ntiple hypotheses {y i r }, where i = 1, ..., H and H is the number of hypotheses, and in this scenario we encode each hypothesis y i r separately using the same bidirectional encoder, and then concatenate their outputs in time to form h b . We keep the audio encoder unidirectional due to latency considerations. Then, two attention layers are followed to attend to acoustic encoding and first-pass hypothesis encoding separately. The two context vectors, c b and ce, are concatenated as inputs to a LAS decoder.\n\nThere are two major differences between our model and the LAS rescoring [10]. First, the deliberation model attends to both e and yr, while [10] only attends to the acoustic embedding, e. Second, our deliberation model encodes yr bidirectionally, while [10] only relies on unidirectional encoding e for decoding. [10] shows that the incompatibility between an RNN-T encoder and a LAS decoder leads to a gap between the rescoring model and LASonly model. To help adaptation, we introduce a 2-layer LSTM as an additional encoder (dashed box in Fig. 1 to indicate optional) to further encode e. We show in Sect. 4 that additional encoder layers improve both deliberation and LAS rescoring models.",
            "score": 0.539545329917847,
            "section_title": "Deliberation Decoder",
            "char_start_offset": 15085,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 698,
                    "end": 702,
                    "matchedPaperCorpusId": "201667597"
                },
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "201667597"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "201667597"
                },
                {
                    "start": 939,
                    "end": 943,
                    "matchedPaperCorpusId": "201667597"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1343994140625
        },
        {
            "corpus_id": "273811349",
            "title": "Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula",
            "text": "Encoder-Decoder models encode a prefix sequence into a single vector (or set of hidden states, for cross-attention) to generate a suffix from (Kalchbrenner and Blunsom, 2013;Cho et al., 2014;Sutskever et al., 2014;Raffel et al., 2020). The prefix-LM architecture, introduced in T5 and UniLM (Raffel et al., 2020;Dong et al., 2019), simplifies this two-stage approach by instead only using self-attention layers with specific masking to allow for bidirectional processing in the prefix and causality in the suffix regions. We adapt this prefix-LM approach for recurrent models, which allows us to transfer information between the prefix and suffix regions for every layer, in contrast to the single-vector bottleneck with Encoder-Decoders. \n\nThis adaptation ensures our bidirectional SSMs maintain computational and parameter efficiency, enabling fair comparisons with unidirectional models. \n\nTo adapt prefix-LM to our models, we first split the recurrent state into forward and reverse components. The forward components are processed without modification, enabling our bidirectional layers to transmit information from the prefix to the suffix via the forward state dimensions. This contrasts with the bidirectional Encoder layers in Encoder-Decoder models, which are constrained to operate only within the prefix. The reverse components are modified in the suffix region to maintain causality; specifically, we zero out the forget gate (A t ) dimensions. This approach prevents the state from propagating information backward in causal areas. A mathematical description follows2 , and an additional example is included in Appendix Section A.4. \n\nCompared to causal SSMs, our bidirectional SSMs trained with Birdie show greatly enhanced capabilities on question-answering retrieval with information-dense Wikipedia articles in SQuAD V2, shown in Figure 3. As we allocate half of the state for the reverse direction, we conservatively only allow half of Birdie's layers to be bidirectional.",
            "score": 0.5393762419527737,
            "section_title": "Bidirectional Processing",
            "char_start_offset": 12929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1991
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 174,
                    "matchedPaperCorpusId": "12639289"
                },
                {
                    "start": 191,
                    "end": 214,
                    "matchedPaperCorpusId": "250920512"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3212890625
        },
        {
            "corpus_id": "261214487",
            "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
            "text": "We use the competitive baselines that demonstrate the downstream task results for comparison. Among them, BART (Lewis et al., 2020) is a standard full-attention PLM for sequence generation. Compared with BART, PEGASUS (Zhang et al., 2020) has a tailored pre-training objective for abstractive text summarization. LED (Longformer Encoder-Decoder) (Beltagy et al., 2020) uses the sparse attention-based encoder and full-attention decoder. Before pre-training, its parameters are initialized from BART. BIGBIRD (Zaheer et al., 2020), for an encoder-decoder setup, also introduces their specified sparse attention mechanism only at the encoder side. PRIMERA based on LED introduces a task-specific per-training objective for multi-document summarization. SLED (Ivgi et al., 2023) processes long sequences via short-context PLMs. The origin long sequence is partitioned into overlapping chunks. HEPOS (Huang et al., 2021) proposes head-wise positional strides to effectively pinpoint salient information from the source. Memorizing Transformers (Wu et al., 2022) employs a trainable attention gate to moderate between the standard crossattention and attention over retrieved keys from a datastore. Unlimiformer (Bertsch et al., 2023) uses the k-nearest-neighbor KNN index to choose full-length input to reduce computation overhead. \n\nHiMAP (Fabbri et al., 2019a)",
            "score": 0.5393232019212642,
            "section_title": "C Introduction for Baselines",
            "char_start_offset": 29226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1357
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 130,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 508,
                    "end": 529,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "251224058"
                },
                {
                    "start": 896,
                    "end": 916,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 1040,
                    "end": 1057,
                    "matchedPaperCorpusId": "247519194"
                },
                {
                    "start": 1335,
                    "end": 1357,
                    "matchedPaperCorpusId": "174799390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11260986328125
        },
        {
            "corpus_id": "267320237",
            "title": "Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending",
            "text": "Large Language Models (LLMs) are built upon the Transformer architecture [53], leveraging attention mechanisms to enhance language comprehension. LLMs can be broadly categorized into three primary families, each distinguished by its architecture: \n\n\u2022 Encoder-only, widely employed for language comprehension tasks such as text classification, named entity recognition, and extractive question answering. The most famous example is BERT [13], which will be explained in more detail below. \n\n\u2022 Decoder-only, designed for generative tasks, exemplified by the well-known GPT models [39,8]. It is employed in various tasks, including question answering [36], text summarization [6], and programming code generation [10]. \n\n\u2022 Encoder-decoder models, suited for tasks demanding both language understanding and generation, such as language translation or text summarization. The most influential models are BART [26] and T5 [40]. \n\nThe selection of the appropriate architecture hinges on the specific requirements of the intended task. Whether it be the nuanced comprehension of language, creative text generation, or the synthesis of both, the versatility of LLMs offers a tailored solution for diverse applications. \n\nWe will focus on BERT (Bidirectional Encoder Representations from Transformer), which is a Transformer-based language model introduced by Google researchers in 2018 [13]. BERT's architecture consists of a stack of encoders from the Transformer model. The bidirectional nature of BERT is key, as it considers both the left and right context of each word, enhancing its ability to understand context-dependent meanings and to be effective in language understanding tasks. Numerous studies have consistently shown that BERT is the most effective linguistic model for various of these tasks [23,47]. Notably, BERT has 340 million parameters, while the widely recognized GPT-3 model has 175 billion, making BERT 514 times smaller than GPT-3 [8]. Given this significant size difference, BERT can be operated on standard home equipment for model inference, which greatly simplifies its use in practical scenarios. In contrast, GPT, built with a Transformer decoder stack, not only demands much more powerful equipment but is suited for language generation tasks.",
            "score": 0.5392739106093593,
            "section_title": "Basics of LLM architectures and BERT",
            "char_start_offset": 12742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 77,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1377,
                    "end": 1381,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1799,
                    "end": 1803,
                    "matchedPaperCorpusId": "237395997"
                },
                {
                    "start": 1803,
                    "end": 1806,
                    "matchedPaperCorpusId": "214606011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278076171875
        },
        {
            "corpus_id": "208006155",
            "title": "English to Hindi Multi-modal Neural Machine Translation and Hindi Image Captioning",
            "text": "Literature survey mainly focused on multimodal based NMT works, where multimodal informa-tion (text and image) integrating into the attentionbased encoder-decoder architecture.(Huang et al., 2016), proposed a model using attention based NMT, where regional and global visual features are attached in parallel with multiple encoding threads and each thread is followed by the text sequence.They obtained BLEU score 36.5, which outperformed the text-only baseline model BLEU score 34.5.(Calixto and Liu, 2017) used bidirectional recurrent neural network (RNN) with gated recurrent unit (GRU) in the encoding phase instead of single-layer unidirectional LSTM in (Huang et al., 2016) and also, used image features separately either as a word in the source sentence or directly for encoder or decoder initialization unlike word only in (Huang et al., 2016), achieved BLEU score 38.5, 43.9 in English to German and German to English translation respectively.(Calixto et al., 2017), introduced two independent attention mechanisms over source language words and visual features in a single decoder RNN, which significantly improve over the models used in (Huang et al., 2016), obtained BLEU score 39.0, 43.2 in English to German and German to English translation respectively.(Dutta Chowdhury et al., 2018), investigated multimodel NMT following settings of (Calixto and Liu, 2017) for Hindi to English translation and acquired BLEU score 24.2.",
            "score": 0.538958147675469,
            "section_title": "Related Works",
            "char_start_offset": 1702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1437
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "11391667"
                },
                {
                    "start": 659,
                    "end": 679,
                    "matchedPaperCorpusId": "11391667"
                },
                {
                    "start": 831,
                    "end": 851,
                    "matchedPaperCorpusId": "11391667"
                },
                {
                    "start": 1148,
                    "end": 1168,
                    "matchedPaperCorpusId": "11391667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16259765625
        },
        {
            "corpus_id": "266902921",
            "title": "Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis",
            "text": "In line with existing literature on LLM-based recommender systems, the bidirectional property of the encoderdecoder architecture allows LLMs to easily customize encoders and decoders towards recommendation (e.g., dual-encoder considering both ids and text [79]), and conveniently adapt to multiple recommendation tasks [9,21,57,79,133]. A few LLMs use the encoder-decoder architecture, and the typical one is the series of T5 and its variant Flan-T5 [10]. \n\n\u2022 The prefix decoder architecture is also the decoder-only architectures, which is known as non-causal decoder. It can bidirectionally encode the prefix tokens like the encoder-decoder architecture, and perform unidirectional attention on the generated tokens like the causal decoder architecture. One of the representative LLMs based on the prefix decoder architecture is ChatGLM [129] and its variants, and researchers have attempted to explore the recommendation performance of ChatGLM as one of the benchmarks in related work [69]. \n\n\u2022 The causal decoder architecture has been widely adopted in various LLMs, and the series of GPT [4] and LLaMA [95] are the most representative models. It uses the unidirectional attention mask, and only decoders are deployed to process both the input and output tokens. Due to the popularity of the causal decoder architecture, most LLM-based recommender systems employ this framework to adapt to different recommendation tasks such as click-through rate predictions, sequential recommendation and conversational recommender systems. In general, open-source models have multiple parameter sizes to choose from, and larger parameter sizes generally mean better capabilities [95,140]. But meanwhile, the corresponding computational and spatial complexity will also increase. Considering the memory and efficiency issues for experiments, researchers in the field of LLM-based recommender systems generally use LLMs with parameters no more than 10B, while the performance of LLMs with larger parameters remains to be further explored in the field of recommender systems [40]. \n\n4.1.5 Context Length. Another property closely related to user needs is the length of the input context.",
            "score": 0.5389505523538829,
            "section_title": "Tuning Strategies.",
            "char_start_offset": 47878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 2068
                },
                {
                    "start": 2071,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "247749019"
                },
                {
                    "start": 1093,
                    "end": 1096,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.446533203125
        },
        {
            "corpus_id": "274822200",
            "title": "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models",
            "text": "Encoder-decoder architectures are composed of two main components: an encoder that is typically responsible for extracting features from inputs using input embedding modules as discussed above, and a decoder that generates the output (e.g., text or multi-modal predictions) auto-regressive. Feature fusion between the encoder and decoder is usually achieved by cross-attention layers in the decoder. This structure excels in tasks requiring a detailed understanding of the input modalities, such as image captioning and visual reasoning, due to its ability to explicitly encode multi-modal information prior to generation [33,48]. Representative models include Flamingo [1] and OFA [46]. Decoder-only. Decoder-only architectures, in contrast, rely on a unified transformer framework where both the input modalities (vision and text) and the output sequences are processed in the same auto-regressive decoder. In these models, visual features are first embedded into token-like representations (via vision processors), which are then concatenated with textual tokens and passed through the decoder. Multi-modal feature fusion occurs naturally through the self-attention mechanism, allowing the decoder to model dependencies between visual and textual inputs during token generation. Decoder-only architectures are more flexible and scalable, making them well-suited for tasks like instruction following, multi-modal question answering, and open-ended generation. Examples of decoder-only models include GPT-4V [49] and LLaVA [28].",
            "score": 0.5377734100583799,
            "section_title": "A. Vision Language Model",
            "char_start_offset": 41055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1529
                }
            ],
            "ref_mentions": [
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "235694621"
                },
                {
                    "start": 670,
                    "end": 673,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1524,
                    "end": 1528,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.206787109375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Large language models (LLMs) pretrained on unstructured text data have been shown to be capable of performing a wide variety of text processing tasks without additional training. This ability has been referred to as zero-shot generalization since these models are typically pretrained with a self-supervised objective that is not specific to a downstream task. Zero-shot generalization is particularly useful because it does not require any additional data or training in order to enable the model to perform a given task. As such, there has been an explosion of work on developing LLMs and training techniques that produce strong zero-shot generalization [Brown et al., 2020, Wang and Komatsuzaki, 2021, Du et al., 2021, Lin et al., 2021, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. One recent line of work [Sanh et al., 2021, Wei et al., 2021, Xu et al., 2022] has demonstrated that adding an explicit multitask finetuning stage on an ensemble of prompted tasks after pretraining can significantly boost the zero-shot capabilities of LLMs. \n\nModern LLMs are based on the Transformer architecture [Vaswani et al., 2017]. While the original Transformer included a separate encoder that processes input text and a decoder that generates target text, most recent LLMs are causal decoder-only (CD) models trained to autoregressively predict a text sequence [Liu et al., 2018, Radford et al., 2018, Al-Rfou et al., 2019]. In contrast with this trend, Raffel et al. [2020] has shown that encoder-decoder (ED) models outperform decoder-only LLMs for transfer learning (i.e. where a pretrained model is finetuned on a single downstream task). Non-causal decoders (ND) [Liu et al., 2018, Dong et al., 2019] use a modified attention mask to bridge the gap between decoder-only and encoder-decoder models. However, they have seen limited adoption.",
            "score": 0.5377424290452243,
            "section_title": "Introduction",
            "char_start_offset": 561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1859
                }
            ],
            "ref_mentions": [
                {
                    "start": 1120,
                    "end": 1142,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1376,
                    "end": 1393,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1393,
                    "end": 1415,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1415,
                    "end": 1438,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 1469,
                    "end": 1489,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1683,
                    "end": 1700,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1700,
                    "end": 1720,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23095703125
        },
        {
            "corpus_id": "254125438",
            "title": "Word Alignment in the Era of Deep Learning: A Tutorial",
            "text": "It turns out that attention is a highly effective and influential approach, beyond machine translation and to the NLP field as a whole. We now summarize follow-up work in the field. Luong, Pham, and Manning (2015) proposes several modifications to the original attention mechanism. First, they use a unidirectional encoder, instead of bidirectional. Second, to calculate attention, they consider the decoder state at t, instead of t \u2212 1, allowing us to directly use h (e) t without the extra RNN layer. Finally, they introduce several different attention functions, the simplest (and most effective) of which is dotproduct attention: \n\nThe main takeaway here is that regardless of differences in formulation, attention is quite powerful as a general mechanism. \n\nAttention solves one issue with RNN-based encoder-decoder models, but another major issue remains. RNNs are slow, during both training and inference, because generating a hidden state at time step t requires us to have generated all prior time steps 0, ..., t \u2212 1. Vaswani et al. (2017) propose the transformer, which removes recurrence entirely from the language model, in favor of only using attention 7 . \n\nThe key idea of the transformer is the idea of self-attention, which turns attention in on itself. The attention described above relates words between two different sequences. Self-attention operates in much the same way, except we now consider a single sequence s, learning correlations between each word s i with the prior words s 0 , ..., s i\u22121 . A transformer thus uses three instances of attention: source self-attention, target self-attention, and source-target attention. Further contributions of the transformer can be found in Appendix B.2. \n\nToday, the transformer architecture is the dominant approach to NLP tasks. Notable follow-up work includes BERT (Devlin et al. 2019) and GPT (Radford et al. 2018), among many others.",
            "score": 0.5374237476968158,
            "section_title": "Attention Functions and Transformers.",
            "char_start_offset": 27035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 213,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 1028,
                    "end": 1049,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1837,
                    "end": 1856,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443359375
        },
        {
            "corpus_id": "278024019",
            "title": "Knowledge Graphs and Their Reciprocal Relationship with Large Language Models",
            "text": "Encoder-Decoder Transformers (Seq2Seq Models) \n\nTransforms input sequences into structured or translated outputs, ideal for summarization, translation, and KG-to-text tasks. \n\n[ [65][66][67][68][69][70][71][72][73][74][75][76][77][78][79][80][81] Autoregressive Transformers (Decoder-Only) \n\nA family of autoregressive language models, designed to generate coherent and contextually relevant text. \n\n[ [82][83][84][85][86] Bidirectional Transformers (Encoder-Only) \n\nA transformer-based model pre-trained for understanding the context of words in both directions. [87,88] Symbolic-Neural Hybrids/Others \n\nIntegrates neural networks with rule-based or symbolic components, often used in domain-specific or low-resource applications. [89,90] We were also able to identify various applications of these studies. Notable applications include recommendation systems generation [91]. Another key application is language model generation, such as the Retrieve-and-Discriminate Prompter (RD-P) framework, which is formed to enhance the performance and overall ability and reliability in performing knowledge-intensive question-answering tasks [20]. Additionally, these studies contribute to general NLP applications such as the Patent Response Large Language Model (PRLLM) and Patent Precedents Knowledge Graph (PPNet), which is developed as a specialized NLP system for understanding and generating responses in the patent domain [58]. Most of the input types of the models were text-based. While video and image inputs appear rarely, their inclusions signify a growing potential for multimodal integration. Figure 3 shows the frequency of each LLM architecture across the reviewed papers, with Hybrid/Retrieval-Augmented Models being the most prevalent, followed by Seq2Seq and decoder-only models.",
            "score": 0.5360073908870476,
            "section_title": "Model Implemented Theme Papers",
            "char_start_offset": 22393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "267770712"
                },
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "263830580"
                },
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "269758450"
                },
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "269149448"
                },
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "270639102"
                },
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "271407677"
                },
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "259165563"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "273694976"
                },
                {
                    "start": 214,
                    "end": 218,
                    "matchedPaperCorpusId": "268249177"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "273605462"
                },
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "265043562"
                },
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "264183044"
                },
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "269951081"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "268847841"
                },
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "271770767"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "263132518"
                },
                {
                    "start": 406,
                    "end": 410,
                    "matchedPaperCorpusId": "265150432"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "272247064"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "268700069"
                },
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "275758336"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "271534128"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "271908910"
                },
                {
                    "start": 1135,
                    "end": 1139,
                    "matchedPaperCorpusId": "273499906"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "271769604"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1104736328125
        },
        {
            "corpus_id": "261945180",
            "title": "MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation",
            "text": "Transformer-based pre-trained models can be categorized into three main types by their network architecture: encoder-only, decoder-only, and encoder-decoder models. Encoder-only frameworks leverage a multi-layer bidirectional transformer encoder to effectively learn contextualized representations, primarily designed for understanding tasks, such as BERT [25]. Decoder-only models employ a unidirectional transformer decoder to effectively handle longer text sequences, making it particularly suitable for generative tasks, such as GPT [26]. Conventional encoderdecoder models combine the encoder and decoder components, resulting in a stronger ability to learn and generate sequences based on specific conditions [31]. \n\nThe standard objective for pre-training language models (LMs) is to improve the left-to-right prediction of the next word (i.e., causal language modeling) on large-scale unlabeled corpora. Besides, denoising objectives is a popular alternative to the standard LM objective, which aims to train LMs to reconstruct original data from corrupted or noisy input [32]. One of the most widely used denoising objectives is masked language modeling (MLM). The original masking strategy, first introduced in the BERT model, consists of randomly selecting 15% of the input tokens based on a uniform distribution. Furthermore, various sophisticated masking strategies have been proposed to introduce more diverse information and prior knowledge for LMs, such as Whole Word Masking [28], Contiguous Spans Masking [29], PMI-Masking [33], N-gram Masking [34,35], Entity Masking [36]. Other noising functions like deletion, infilling, permutation, and rotation have also improved PLMs for various downstream tasks [37]. For further details on other auxiliary training objectives, please refer to the referenced publication [32]. \n\nResearchers have proposed more efficient frameworks and objectives to enable a pre-training framework to address diverse downstream tasks.",
            "score": 0.5358696588308678,
            "section_title": "Pre-training framework and objective",
            "char_start_offset": 7022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1835
                },
                {
                    "start": 1838,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1080,
                    "end": 1084,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 1492,
                    "end": 1496,
                    "matchedPaperCorpusId": "195068911"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 1541,
                    "end": 1545,
                    "matchedPaperCorpusId": "222134068"
                },
                {
                    "start": 1562,
                    "end": 1566,
                    "matchedPaperCorpusId": "225062100"
                },
                {
                    "start": 1566,
                    "end": 1569,
                    "matchedPaperCorpusId": "216641856"
                },
                {
                    "start": 1721,
                    "end": 1725,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1830,
                    "end": 1834,
                    "matchedPaperCorpusId": "236493269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "253055386",
            "title": "Attacking neural machine translations via hybrid attention learning",
            "text": "Attenion was first derived from human intuition based on the human activities, later adapted to machine translation for automatic token alignment (Hu, 2019). Attention mechanism, a simple method that can be used for encoding sequence data based on the importance score each element is assigned, has been widely applied to and attained significant improvement in various tasks in natural language processing, including sentiment classification, text summarization, question answering, dependency parsing, etc. In this section, we will introduce some related work about attention mechanism in NLP. \n\nThe traditional machine translation models (Kalchbrenner & Blunsom, 2013) are constructed by an encoder-decoder architecture, both of which are recurrent neural networks. An input sequence of source tokens is first fed into the encoder, with which the tokens will be transferred to the hidden representations, and then the decoder will utilize these hidden representations from the encoders as the initial input and output a sequence of dependent tokens. Such an encoder-decoder framework had achieved highest performance compared to purely statistical machine translation models. However, this architecture suffers from two serious drawbacks. First, RNN is forgetful, meaning that old information cleaned up after being propagated over multiple time steps. Second, there is no explicit word alignment during decoding and therefore focus is scattered across the entire sequence. To this end, the concept of attention was first introduced for an encoder-decoder structured NMT by Bahdanau (2015), and has become popular in the NMT community as an essential component of sequence-to-sequence models. Bahdanau provided such an attention mechanism to model word alignments between input and output sequence, which is an essential aspect of structured output tasks such as translation or text summarization. Based on Bahdanau's attention, Luong (2015) proposed two attention models, namely local and global, in context of machine translation tasks. The global attention model is similar to Bahdanau's attention while the local attention is computed with hidden states from the output of the encoder. Luong's attention achieved a better performance than Bahdanau's attention and provided a way of transparentizing the NMTs.",
            "score": 0.5356738836106338,
            "section_title": "Attention in NMT",
            "char_start_offset": 8441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 156,
                    "matchedPaperCorpusId": "53305352"
                },
                {
                    "start": 641,
                    "end": 671,
                    "matchedPaperCorpusId": "12639289"
                },
                {
                    "start": 1932,
                    "end": 1944,
                    "matchedPaperCorpusId": "1998416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.347900390625
        },
        {
            "corpus_id": "264468067",
            "title": "Exploring ChatGPT Capabilities and Limitations: A Survey",
            "text": "When Transformers architecture was first designed in [12] as shown in Figure 3, they were applied in machine translation, where an input sequence of an initial language is transformed to the output sequence of the target language. The Transformer architecture followed an encoder-decoder model, where the encoders map a discrete representation of the input sequence (i.e., words, characters, sub-words) to a continuous representation denoted as an embedding vector (i.e., a vector of continuous values). The decoder takes embeddings as input and generates an output sequence of elements one at a time. As the transformer is an autoregressive generative model, it predicts the probability distribution of the next element in the output sequence, given the previous sequence, which can be seen as a special case of Hidden Markov Chains (HMMs). However, HMMs cannot capture long-term dependencies bidirectionally as transformers do. \n\nUnlike other models that use both an encoder and a decoder or an encoder only, like the Bert model family from Google [13], ChatGPT relies only on pure decoder architecture, illustrated in Figure 4, as defined in the first GPT paper [14]. Specifically, the underlying GPT model applies unidirectional attention using a language masking strategy to process the input sequences token-wise. The decoder is trained to take the first token of the input sequence as a start token and then generate subsequent output tokens based on the input sequence and previously generated tokens. This architecture represents the standard model for language modeling to generate the sequence of tokens with the highest likelihood given a context (i.e., input sequence and previously generated tokens). ChatGPT architecture does not rely on an encoder because the GPT models are trained on a large corpus of textual datasets, using unsupervised learning, to predict the next sequence of words, given a context. Therefore, these models are trained to generate text rather than mapping an input to an output, as in a typical encoder-decoder architecture. As such, the text embedding is directly fed into the self-attention modules to learn the complex relationship between these models in a cascade of self-attention layers.",
            "score": 0.5355142980718113,
            "section_title": "2) ENCODER-DECODER ARCHITECTURE",
            "char_start_offset": 15805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "265158147",
            "title": "To Transformers and Beyond: Large Language Models for the Genome",
            "text": "Transformer models are not always encoder-decoders. Sometimes, transformer models can be used just as encoders, or just as decoders. In NLP, the Bidirectional Encoder Representations from Transformers (BERT) 3 and the Generative Pre-trained Transformer (GPT) 4 are two distinct frameworks for implementing transformers. BERT 3 is an encoder-only transformer framework with 12 transformer layers, each with 12 attention heads, usually pre-trained using a Masked Language Modelling (MLM) pretext task. MLM is a pretext training task where the model is pre-trained to predict a word that's been masked out of a sentence. The GPT framework 4 is decoder-only, and is pre-trained with an Autoregressive Language Modelling (ALM) pretext task. ALM is a pretext training task where the model is pre-trained to predict the next word in a sequence given the previous words. In NLP, GPT models generate text unidirectionally and sequentially, predicting the next word based on the previous one. BERT, in contrast, is designed for bidirectional representation, processing words from both left-to-right and right-to-left. In genomics, a GPT-style model would use preceding nucleotide information to make a prediction about subsequent nucleotides in a sequence. A BERT-style model could predict missing nucleotide information within a sequence using both preceding and succeeding nucleotides. \n\nBERT-based and GPT-based models work best for different tasks 3,6 . Encoder-only models, like BERT, are useful in the cases where final predictions have high accuracy when based off only an embedding, or feature representation of the inputted sequence. This is oftentimes true for classification tasks. BERT-based models pre-trained with MLM are useful for understanding genomic sequences where the overall context (both upstream and downstream) is important, such as identifying genomic features or classifying sequences where directionality is not critical. In genomic modelling, the BERT framework has already been applied.",
            "score": 0.5354308458835675,
            "section_title": "Encoder-only and Decoder-only Transformers",
            "char_start_offset": 21626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 260,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29296875
        },
        {
            "corpus_id": "162169061",
            "title": "Sample Efficient Text Summarization Using a Single Pre-Trained Transformer",
            "text": "In this section, we describe the neural architectures and methods used for training a language model and fine-tuning it for text summarization. \n\nLanguage Model Pre-training. In this study, we train a Transformer (Vaswani et al., 2017) based language model. Unlike ELMo (Peters et al., 2018), which trains LMs in both directions, or BERT (Devlin et al., 2018), which trains a bidirectional word imputation model, we train a unidirectional LM (Radford et al., 2019). This is necessary for initializing an auto-regressive decoder. \n\nEncoder-Decoder Baselines. We first combine pre-training with standard sequence-to-sequence (Sutskever et al., 2014) models. These consist of a Transformer encoder network that reads the article, a Transformer decoder network that generates the summary, and an encoder-decoder attention mechanism (Bahdanau et al., 2015) that allows the decoder to attend to encoder states dur- ing generation, shown in Figure 2. Both the encoder and decoder use the same network architecture as the Transformer LM, making it easy to use the pre-trained weights. We compare three ways of incorporating weights from a pre-trained LM, proposed by Ramachandran et al. (2017): (1) pretraining the encoder only, (2) pre-training the decoder only, and (3) pre-training both. In (3), the encoder-decoder attention parameters are the only ones randomly initialized. When fine-tuning these models on summarization data, we follow recent work (Howard and Ruder, 2018;Radford et al., 2018;Devlin et al., 2018) and fine-tune all of the pre-trained as well as non-pre-trained parameters. \n\nTransformer LM. We can simplify the encoderdecoder model by casting summarization as a language modeling task (Liu et al., 2018).",
            "score": 0.5350688526312413,
            "section_title": "Methods",
            "char_start_offset": 3106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 146,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1720
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 235,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 270,
                    "end": 291,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 623,
                    "end": 647,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 828,
                    "end": 851,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1159,
                    "end": 1185,
                    "matchedPaperCorpusId": "3488076"
                },
                {
                    "start": 1447,
                    "end": 1471,
                    "matchedPaperCorpusId": "40100965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.315673828125
        },
        {
            "corpus_id": "216144667",
            "title": "On Sparsifying Encoder Outputs in Sequence-to-Sequence Models",
            "text": "Neural sequence-to-sequence (Seq2Seq) models have dominated various text generation tasks, including machine translation (Vaswani et al.,1 Source code is available at https://github.com/ bzhangGo/zero. source word estimated on the English-German WMT14 test set. For each (source sentence, translation) pair, we extract the attention matrices from all encoder-decoder attention sublayers in Transformer and average them over different (8) heads and (6) layers. The attention value for each source word is summed over all target words in the translation. Higher attention weights suggest larger impacts on translation. Around 49.7% source words get attention weights of less than 0.6, compared to the mean value of 1.03. 2017) and abstractive document summarization Liu and Lapata, 2019). These models generally follow the encoder-decoder paradigm, where the encoder interprets source context and converts source words into vector representations such that the decoder has sufficient information to predict the target sequence. Early Seq2Seq models (Sutskever et al., 2014;Cho et al., 2014) provided only the last and/or first encoder states to the decoder. In contrast, modern approaches rely on the attention mechanism (Bahdanau et al., 2015) and implicitly make an assumption that information from all encoder outputs should flow to the decoder. 2 However, this assumption neglects the fact that a large portion of source words in machine translation receives just minor attention as shown in Figure 1, let alone in summarization where the input contains redundant expressions and large parts of text are not relevant to any plausible summary. Moreover, information content varies words (y-axis) over source words (x-axis) for the vanilla attention , the sparse attention (Correia et al., 2019) and our model. Darker color indicates larger attention weight, and the white blocks denote an attention weight of 0. The source words whose encoding is pruned by L0DROP (receiving zero weight) are highlighted in red. across words, for example, it is negatively correlated with event frequency (Shannon, 1948;Zipf, 1949). When moving from word-level to",
            "score": 0.5349448324898617,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 764,
                    "end": 785,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1047,
                    "end": 1071,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1071,
                    "end": 1088,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 1219,
                    "end": 1242,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1773,
                    "end": 1795,
                    "matchedPaperCorpusId": "202538495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1121826171875
        },
        {
            "corpus_id": "273374881",
            "title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
            "text": "For instance, the \u03b4 F KGL values for settings B3 and B4 remain below 1, indicating an insufficient distinction in readability. In terms of length control, both models achieve PCC values below 0.3, reflecting a weak correlation between the generated length and the target. \n\nIn contrast, encoder-decoder (ED) models B1 and B2 achieve results comparable to or even better than GPT-4, despite their weaker performance in the zero-shot setting. B2, in particular, demon-  strates strong control over length and keywords, while also showing significant improvements in readability and focus, the two implicit attributes. Under high readability requirements, the average FKGL of the generated summaries drops to below 11, corresponding to a high school reading level. \n\nFor focus control, while the F1 score of 0.75 indicates potential for further improvement, it still exceeds GPT-4's performance. \n\nLimitations of Decoder-Only Models in Compositional Controllable Summarization Our com-parisons of B1, B2 (encoder-decoder models) with B3, B4 (decoder-only models) reveal that LLaMA2 and Mistral perform significantly worse than Flan-T5 under fine-tuning. This disparity likely stems from architectural differences. Encoder-decoder architectures, such as Flan-T5, are specifically designed for sequence-to-sequence tasks, where the encoder processes the input sequence and the decoder simultaneously generates the output by attending to the encoder's representations. However, decoder-only models such as LLaMA2 and Mistral, handle both the source sequence and the target sequence within the same unidirectional crossattention mechanism (Fu et al., 2023). As the sequence length increases, the attention becomes increasingly diffused, which degrades the model's ability to capture important long-term dependencies. This degradation fundamentally limits tasks requiring long-range context handling, such as controllable summarization. \n\nTo further validate this hypothesis, we analyze the attention patterns of LLaMA2 as a representative example. Figure 3 demonstrates the relationship between attention scores and token distance, revealing a sharp decline in attention as distance increases.",
            "score": 0.5348870044819067,
            "section_title": "Experiments",
            "char_start_offset": 17219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19091796875
        },
        {
            "corpus_id": "258676667",
            "title": "When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust",
            "text": "The sequence-to-sequence (seq2seq) architecture was originally designed for neural machine translation (NMT) (Sutskever et al., 2014). The architecture consists of two components: an encoder and a decoder. The encoder encodes an input sequence into a fixed-length context vector V \u2208 R k and the decoder uses the vector V for decoding output tokens. \n\nThe architecture of seq2seq. \n\nIn Figure 2 the encoder receives an input sequence and encodes the sequence to create the vector V . The vector V is the input for the decoder that outputs the next tokens based on previous predicted tokens. The decoding stops when the decoder outputs the end-of-sequence token. Common recurrent neural networks (RNNs) Graves (2013) and their variants (LSTM (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014)) can be used for the encoder and decoder. \n\nThe seq2seq architecture can be applied to several NLP problems such as NMT (Sutskever et al., 2014), seq2seq learning (Sutskever et al., 2014), text summarization (Chopra et al., 2016), grammatical error correction (Raffel et al., 2020), incomplete utterance restoration (Inoue et al., 2022), etc. However, this architecture suffers in encoding long sequences that cause long-term dependency and the bottleneck of information when the whole input sequence is represented by only using the context vector in the final state of the encoder. It motivated the introduction of attention (Bahdanau et al., 2015) or pointer networks (Vinyals et al., 2015) (we review attention because it is used in Transformer). The core concept of attention is that during each step of the decoding process, the decoder selectively concentrates on a specific segment of the input sequence (Bahdanau et al., 2015). This selective focus is represented by a weight matrix, which encapsulates the attention distribution across input tokens.",
            "score": 0.5348643117912267,
            "section_title": "Seq2seq and attention",
            "char_start_offset": 4255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 133,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 740,
                    "end": 772,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 782,
                    "end": 800,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 921,
                    "end": 945,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 964,
                    "end": 987,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1009,
                    "end": 1030,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 1117,
                    "end": 1137,
                    "matchedPaperCorpusId": "248069359"
                },
                {
                    "start": 1428,
                    "end": 1451,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1713,
                    "end": 1736,
                    "matchedPaperCorpusId": "11212020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "258418307",
            "title": "FlowTransformer: A Transformer Framework for Flow-based Network Intrusion Detection Systems",
            "text": "This study compares four transformer architectures, namely shallow encoder-based, shallow decoder-based, deep encoderbased, and deep decoder-based transformers. The shallow models are based on the basic multi-head self-attention transformer architecture and comprise between 2 and 6 encoder or decoder blocks. Two specific deep transformer models, GPT 2.0 and unmasked BERT, are also considered. The difference between shallow and deep models lies primarily in their depth, number of attention heads, and internal size, while their core transformer block structure is the same. Although both GPT and BERT use scaled-dot-product attention, BERT's attention mechanism is bidirectional, considering tokens in both directions, unlike GPT, which only considers previous tokens. \n\n1) GPT 2.0 -Deep Decoder Transformer [5]: GPT 2.0 is a generative model that is trained on next-word prediction. Unlike traditional input-output models, it treats the input prompt as part of a sequence and generates an output by using each generated word as part of the context to predict the subsequent word. This approach allows GPT to use exclusively transformer decoder blocks, rather than the traditional encoderdecoder structure. The internal block structure is repeated, with smaller GPT 2.0 models having 12 blocks. The input sequence is passed through these decoder blocks one by one, with the output of each block being fed as input to the next block. By using a stack of decoder blocks, GPT can model the distribution of natural language more effectively than a traditional transformer model, because the decoder blocks learn to generate the next word based on a combination of the input sequence and the previously generated words, instead of using only the input sequence. GPT is an autoregressive model that considers tokens only to the left of the token it is generating, moving through a sequence one token at a time. \n\n2) BERT -Deep Encoder Transformer [6]: BERT, in contrast to GPT, is an encoder-only transformer model, consisting of repeated blocks of transformer encoders. This is because BERT was primarily designed for natural language understanding tasks, rather than text generation.",
            "score": 0.5348368802002884,
            "section_title": "B. Transformer Models",
            "char_start_offset": 22153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 1945,
                    "end": 1948,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431396484375
        },
        {
            "corpus_id": "267101122",
            "title": "What is the Consumer Attitude toward Healthcare Services? A Transfer Learning Approach for Detecting Emotions from Consumer Feedback",
            "text": "Recently, transformer models are being increasingly utilized for NLP tasks such as text classification, clinical concept extraction or named entity recognition (NER) [Qiu et al. 2020]. Over the last few years, extensive approaches have been developed to generate pre-training contextual representations, such as bidirectional encoder representations from transformers (BERT) [Devlin et al. 2018], robustly optimized BERT pretraining approach (RoBERTa) [Zhuang et al. 2021], ELMo [Cassani et al. 2017], or ULMFiT [Howard and Ruder 2018]. Most of the publicly available pre-trained language models (PLMs) are trained on general domain corpora such as Wikipedia; nevertheless, medical texts greatly differ from general text because of their domain-specific vocabulary. Consequently, the performance of general PLMs is limited in many tasks [Moradi et al. 2020]. To address this issue, several PLMs trained on medical corpora have been proposed such as Clinical-Longformer [Yikuan et al. 2023], CODER [Yuan et al. 2022], BioBERT [lee et al. 2020], SciBERT [Beltagy et al. 2020] or Bio_ClinicalBERT [Alsentzer et al. 2019], among others. \n\nThe PLMs in NLP have primarily evolved into three main types: transformer decoders-only, transformer encoders-only, and transformer encoder-decoders, each serving as a powerful framework for training PLMs that have been outperformed in various NLP tasks [Wang et al. 2022]. An example of a transformer decoder-only model is GPT (Generative Pre-trained Transformer), which used a unidirectional transformer decoder to generate text token by token [Radford et al. 2018]. BERT is a prominent model in the transformer encoders-only category, which utilized bidirectional transformer encoders for masked language modeling and bidirectional context understanding [Devlin et al. 2018].",
            "score": 0.5342029832723982,
            "section_title": "Introduction and background",
            "char_start_offset": 3957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 395,
                    "matchedPaperCorpusId": "220635321"
                },
                {
                    "start": 837,
                    "end": 856,
                    "matchedPaperCorpusId": "204787530"
                },
                {
                    "start": 969,
                    "end": 989,
                    "matchedPaperCorpusId": "254123526"
                },
                {
                    "start": 997,
                    "end": 1015,
                    "matchedPaperCorpusId": "226254376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09173583984375
        },
        {
            "corpus_id": "273482718",
            "title": "Large Language Models Are Overparameterized Text Encoders",
            "text": "In the past few years, the field of natural language processing (NLP) has seen a significant shift towards large-scale language models (LLMs). These models, due to a combination of their large size, extensive pre-training, and instruction-following ability, have achieved state-of-the-art performance on a wide range of NLP tasks, such as language modeling, text generation, and text understanding (Dubey et al., 2024;Brown et al., 2020;Jiang et al., 2023a). \n\nDespite their strong generative capabilities, decoder-only LLMs have seen comparatively little adoption for text embedding tasks until recently (BehnamGhader et al., 2024). Text embedding, which involves mapping a text sequence of varying length to a fixed-dimensional vector representation, is a fundamental task in NLP and is used as a building block for a wide range of downstream tasks, such as semantic textual similarity, information retrieval, and text classification. Further, it is a fundamental step required for retrieval-augmented generation, a well-known paradigm for improving the performance of LLMs in knowledge-intensive tasks (Lewis et al., 2020). Traditionally, text embedding models have been based on masked language models (MLMs) and bidirectional encoders, such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), typically adapted for text embedding tasks by following a multi-step training pipeline consisting of weakly-and fully-supervised contrastive training (Ni et al., 2022;Li et al., 2023;Xiao et al., 2024). \n\nDecoder-only LLMs, however, offer several advantages over their encoder-only counterparts. They are more sample-efficient during pre-training, leverage instruction-following capabilities for task generalization, and benefit from a rich and evolving research ecosystem (Clark et al., 2020;Asai et al., 2023;BehnamGhader et al., 2024). Further, the availability of high-performing public pre-trained LLMs and their continual development make it appealing to explore their use for text embedding tasks.",
            "score": 0.5340283028668357,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 437,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 605,
                    "end": 632,
                    "matchedPaperCorpusId": "269009682"
                },
                {
                    "start": 1105,
                    "end": 1125,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1254,
                    "end": 1275,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1283,
                    "end": 1304,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1456,
                    "end": 1473,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1489,
                    "end": 1507,
                    "matchedPaperCorpusId": "271114619"
                },
                {
                    "start": 1779,
                    "end": 1799,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 1799,
                    "end": 1817,
                    "matchedPaperCorpusId": "253581733"
                },
                {
                    "start": 1817,
                    "end": 1843,
                    "matchedPaperCorpusId": "269009682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1190185546875
        },
        {
            "corpus_id": "270214176",
            "title": "A Survey on Large Language Models for Code Generation",
            "text": "where x < represents the sequence of preceding tokens { 1 , . . .,   \u22121 } before x  in the input,  denotes the model parameters.The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x \u2264 is the source sequence input and x < denotes the target sequence autoregressively generated so far.During the inference phase, pre-trained LLMs that have been trained on largescale code corpus can generate code in a zero-shot manner without the need for fine-tuning.This is achieved through the technique of prompt engineering, which guides the model to produce the desired output11 [31,186].Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further [131,178].Denoising Autoencoding.In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART [6], CodeT5 [234], and its enhanced successor, CodeT5+ [232].Following T5 [189] and CodeT5 [234], the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths.This corrupted sequence serves as the input for the encoder.",
            "score": 0.5336583075322304,
            "section_title": "Pre-training",
            "char_start_offset": 42181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 1300,
                    "end": 1304,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1701,
                    "end": 1706,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 1744,
                    "end": 1749,
                    "matchedPaperCorpusId": "258685677"
                },
                {
                    "start": 1763,
                    "end": 1768,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1780,
                    "end": 1785,
                    "matchedPaperCorpusId": "237386541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.250244140625
        },
        {
            "corpus_id": "268063321",
            "title": "Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review",
            "text": "LLMs, such as BERT, GPT, Text-To-Text Transfer Transformer (T5) [45], and Llama, are versatile in NLP. BERT is an encoder-only model with bidirectional attention, GPT employs a transformer decoder for one-directional processing, and T5 transforms NLP problems into text generation tasks. Recent LLMs like GPT-3 [4], Language Model for Dialogue Applications(LaMDA) [52], Pathways Language Model (PaLM) [6], and Vicuna excel in understanding human-like textual knowledge, employing In-Context Learning (ICL) [12] for context-based responses.",
            "score": 0.5335524814271415,
            "section_title": "Large Language Models (LLMs)",
            "char_start_offset": 2535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 539
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1954345703125
        },
        {
            "corpus_id": "274454618",
            "title": "MAGECODE: Machine-Generated Code Detection Method Using Large Language Models",
            "text": "CodeT5+ [6] is a new family of open code large language models for code understanding and generation tasks. Developed by Salesforce AI Research, CodeT5+ models are aimed at overcoming two major constraints of existing code LLMs: architecture flexibility and restricted collection of pretraining tasks. Current code LLMs can only work in certain designs, like encoder-only or decoder-only. CodeT5+ models, on the other hand, have a flexible architecture that can work in encoder-only, decoder-only, or combined encoder-decoder modes to dynamically adapt to a wide range of downstream applications. Additionally, CodeT5+ models include a variety of pre-training tasks, such as span denoising, causal language modeling (CLM), contrastive learning, and text-code matching tasks, to fix the problem of limited pretraining tasks. This allows CodeT5+ models to bridge the gap between the pre-training and fine-tuning stages, as well as surpass existing code LLMs in aligning with the complexities of different downstream code tasks. \n\nAt the heart of CodeT5+ models lies the Transformer architecture, created by Google to handle natural language processing tasks. The Transformer architecture [11] was developed to tackle the challenges faced by Recurrent Neural Networks (RNNs) in capturing relationships between distant words in a phrase and their slow training speed. The Transformer architecture follows the encoder-decoder architecture of RNNs, where the encoder encodes an input sequence of symbols (which represents, for example, words in a sentence) to a sequence of continuous representation, and the decoder produces a sequence of symbols from the continuous representation. However, instead of using the recurrence mechanism in RNNs, both the Transformer encoder and decoder employ the Multi-head self-attention mechanism, which allows the models to understand global dependencies between words in a phrase or sentence. A self-attention function takes an input embedding X = [x 1 , x 2 , . . . , x N ] \u2208 R D\u00d7N , each embedding x i of dimension D \u00d7 1 represents a word or token in the input sentence. Then, three following quantities, named Value, Key, and Query, are given as",
            "score": 0.5330411086710188,
            "section_title": "B. CODET5+ MODELS AND THE TRANSFORMER ARCHITECTURE",
            "char_start_offset": 7598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "264408158",
            "title": "Design of a Modified Transformer Architecture Based on Relative Position Coding",
            "text": "In addition, when processing long input sequences, the fixed-length semantic vector struggled to retain critical feature information, leading to reduced accuracy. To overcome these issues, attention mechanisms were introduced [14]. \n\nThe concept of attention mechanisms was initially proposed by Bahdanau et al. for machine translation and later improved by Luong et al. [15,16]. Drawing inspiration from human selective attention, attention mechanisms mimic the human process of rapidly scanning and focusing on relevant information while disregarding irrelevant details. In the context of deep learning, attention mechanisms act as a resource allocation mechanism [17][18][19]. They dynamically redistribute the weight of information based on its importance, ensuring that critical information is given higher weight, while less important information is assigned lower weight. This feature extraction and sequential data analysis capability has found applications in various fields, including language modeling and image processing [20,21]. \n\nAttention mechanism in the decoding process [22], each output not only depends on the fixed-size semantic vector encoded by the encoder, but also depends on the hidden layer state of the previous output unit and the corresponding hidden layer state of the current output unit in the decoding process. Attention is introduced into the Seq2Seq model to solve the problem that the original RNN often loses part of the input sequence information, and the accuracy of the model is improved. In the specific translation task [23,24], the decoding phase is to translate one word by one word in the time series. When decoding one word, it will not have the same association with all the words in the source sequence. In the decoder phase, the selected reference contributes the most to the semantic vector of the current sequence word, rather than uniformly referring to all the semantic vectors. \n\nThe introduction of attention mechanisms into Seq2Seq models aimed to address limitations in retaining input sequence information and improve model accuracy. During the decoding phase, rather than uniformly considering all input semantic vectors, attention mechanisms enable the model to selectively focus on the most relevant reference for the current sequence word. Prior to this development, the most effective language models were based on Seq2Seq architecture with LSTM for modeling.",
            "score": 0.5327110259271105,
            "section_title": "Introduction",
            "char_start_offset": 2206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1933
                },
                {
                    "start": 1936,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2424
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "219472430"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "1921173"
                },
                {
                    "start": 666,
                    "end": 670,
                    "matchedPaperCorpusId": "247368810"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "250372733"
                },
                {
                    "start": 674,
                    "end": 678,
                    "matchedPaperCorpusId": "249480572"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "matchedPaperCorpusId": "237596895"
                },
                {
                    "start": 1038,
                    "end": 1041,
                    "matchedPaperCorpusId": "231875037"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "246984992"
                },
                {
                    "start": 1564,
                    "end": 1568,
                    "matchedPaperCorpusId": "213550456"
                },
                {
                    "start": 1568,
                    "end": 1571,
                    "matchedPaperCorpusId": "221305796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2744140625
        },
        {
            "corpus_id": "247625205",
            "title": "What Language Model to Train if You Have One Million GPU Hours?",
            "text": "In this paper, we base all models on a decoder-only Transformer pretrained with an autoregressive language modeling objective. This is a popular choice for large language models (Brown et al., 2020;Rae et al., 2021;Thoppilan et al., 2022), possibly because it lends itself to zero-shot application to many downstream tasks (Radford et al., 2019). Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, Liu et al. (2018); Dong et al. (2019)). \n\nOur decision is motivated by the findings of Wang et al. (2022), which showed that decoderonly models combined with an autoregressive language modeling objective provide the best zeroshot generalization abilities immediately after pretraining. Although multitask finetuning (Sanh et al., 2021;Wei et al., 2021) will instead favor an encoder-decoder with span corruption for best zero-shot generalization, Wang et al. (2022) found a compromise between these two practices. Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning. Accordingly, we follow their recommendation, and train an autoregressive decoder-only model first which we will later consider adapting and finetuning.",
            "score": 0.5322144355285201,
            "section_title": "Architecture and Pretraining Objective",
            "char_start_offset": 2960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 198,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 556,
                    "end": 573,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 575,
                    "end": 593,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1688232421875
        },
        {
            "corpus_id": "274597951",
            "title": "KITE-DDI: A Knowledge Graph Integrated Transformer Model for Accurately Predicting Drug-Drug Interaction Events From Drug SMILES and Biomedical Knowledge Graph",
            "text": "The transformer architecture was first introduced in the paper \"Attention is all you need\" [40]. It is a sequence-tosequence model, meaning it takes a sequence as an input and outputs another. It started out as a natural language processing (NLP) algorithm but soon expanded to other tasks like computer vision, bioinformatics, and drug discovery. One of the major limitations of the transformer architecture is that it requires a large amount of data to be trained effectively and achieve optimal performance. One of the major components of the proposed algorithm in this work involves a transformer block but the architecture has been modified along with the training regime to adopt it for low data setting. \n\nThe original transformer consisted of an encoder and a decoder. The encoder is responsible for taking the input sequence and converting it into a latent vector. The decoder, on the other hand, takes the previous token of the sequence along with the latent vectors coming from the encoder to generate the output sequence. Depending on the context of the application, several variants of the original transformer model were later proposed which could be divided into three major categories [41]: \n\n\u2022 Encoder Only: This type of architecture only consists of the encoder block and the decoder is absent [42,43,44,45]. These are typically applied to tasks like sequence labeling, image classification, sequence ranking etc. \u2022 Encoder-Decoder: The complete transformer architecture consists of both an encoder and a decoder module [46,47]. This is commonly used in applications like neural machine translation and sequence-to-sequence tasks. \u2022 Decoder Only: These models only consist of a decoder module [48,49,50]. Applications of these types of models include sequence generation, language modeling, etc. \n\nThe transformer module used in this work is of the encoder-only variant. It consists of three encoders that take the tokenized input sequence and convert it into latent vectors. All the encoder modules consist of trainable weights which generate various features of the input sequence, that are then summed up to be fed into the encoder layers. The token embeddings contain information about the type of token that each sequence base consists of.",
            "score": 0.5319454224150876,
            "section_title": "Transformer",
            "char_start_offset": 15802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 95,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1316,
                    "end": 1319,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1542,
                    "end": 1545,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1715,
                    "end": 1718,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1124267578125
        },
        {
            "corpus_id": "278202626",
            "title": "Contrastive learning and mixture of experts enables precise vector embeddings in biological databases",
            "text": "GPT (Generative Pretrained Transformer) models, such as OpenAI's GPT series (GPT-3, GPT-4, etc.), are designed for generative tasks and use transformer decoders [36][37][38] . They employ causal (unidirectional) attention, meaning each token attends only to previous tokens in the sequence, enabling autoregressive generation during inference. This allows them to predict the next word in a sequence without direct access to future words. \n\nIn contrast, BERT models utilize transformer encoders with bidirectional attention, meaning they can attend to all tokens within an input simultaneously. This structure enables them to capture additional contextual dependencies, making them well-suited for tasks like text classification and sentence similarity 39 . Unlike GPT models, BERT is trained using a masked language modeling (MLM) objective, where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context.",
            "score": 0.5316062866523026,
            "section_title": "Transformer neural networks",
            "char_start_offset": 7341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 951
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "278129372",
            "title": "Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation",
            "text": "We systematically explore the impact of different pre-training strategies on the encoder and decoder, covering a variety of combinations from traditional Transformers to large pre-trained language models. For the image encoder, we use the CLIP-ViT model1 , which is widely used in image-text alignment tasks, to extract image features as visual input. Since CLIP has good migration capabilities on large-scale image-text paired data, we choose to freeze all its parameters in this experiment to avoid the unstable effect of image encoding on the training process. \n\nFor the text encoders and decoders, we divide the models into two categories according to their structural characteristics: \n\n(i) Encoder-Decoder structural models, including basic Transformer (three sizes: Small / Base / Large) (Vaswani et al., 2017), T5-small (Raffel et al., 2019) and mBART-large (Liu et al., 2020b). This type of structure has good bidirectional understanding (Encoder) and autoregressive generation (Decoder) capabilities, and is often used in classic machine translation scenarios. \n\n(ii) Decoder-only structural models, including lightweight large language models Qwen2.5-0.5B (Qwen et al., 2025) and LLaMA3.2-1B (Grattafiori et al., 2024). As representative general LLMs, they directly rely on context modeling capabilities for translation generation in the decoding stage and visual information is often incorporated as textual prompts, directly participating in context modeling and memory reviving. \n\nTable 1 summarizes the structural parameters of various encoders and decoders:",
            "score": 0.5311020071094348,
            "section_title": "Pre-trained Encoders and Decoders",
            "char_start_offset": 8676,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1573
                }
            ],
            "ref_mentions": [
                {
                    "start": 795,
                    "end": 817,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11395263671875
        },
        {
            "corpus_id": "271533495",
            "title": "Sentiment Analysis of Lithuanian Online Reviews Using Large Language Models",
            "text": "As mentioned in the Introduction section, the attention mechanism allowed models to weigh different words' relevance in the given input sequence differently. Models could finally capture complex relationships between words. This approach was initially applied as a translation mechanism [6]. After this article, further improvements involved removing LSTM parts, eliminating bottlenecks of encoder vectors, and enabling sequential processing. This work laid the foundation for models like BERT and GPT-4. Transfer learning, where a model is pre-trained on a large amount of data before being fine-tuned on a downstream task, has shown excellent results and has become a powerful technique in NLP. After introducing the BERT model, it took almost five years for transformer models to be widely applied in daily use. \n\nA. BERT BERT stands for \"Bidirectional Encoder Representations from Transformers\", an encoder-only Transformer [22]. By the name, this model uses bidirectional context for pertaining. This characteristic allows the model to learn the context of a word based on the entire input context (left and right to the word in a sequence). It was first introduced by the Google team in 2018, and since then, several model variations have been applied to solving NLP tasks [22]. The model's ability to learn bidirectional context is used during training when the model learns to predict masked words from the context in the sequence. The BERT model can be applied to various language tasks like classification, question answering, and entity recognition. Numerous model size variations and complexities exist depending on the NLP task and resources [23]. \n\nB. T5 T5 stands for \"Text-to-Text transfer Transformer\", an encoder-decoder model. The basic T5 model treats every text processing problem as a \"text-to-text\" problem [24]. This model architecture takes text as input and produces new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question-answering [24]. The main unsupervised training objective is training the model to predict sentinel tokens previously purposely dropped out of the text. This general-purpose model is widely used in various NLP tasks when converting them to textto-text problems [25].",
            "score": 0.5310673150224334,
            "section_title": "LLMs for text classification",
            "char_start_offset": 10166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1279,
                    "end": 1283,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1830,
                    "end": 1834,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 2044,
                    "end": 2048,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285400390625
        },
        {
            "corpus_id": "268723594",
            "title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers",
            "text": "We benchmark the performance of different models on three sizes: models with \u2248 110M parameters (e.g.bert-base-uncased); models with \u2248 330M parameters (e.g.bert-large-uncased) and models with > 700M parameters (e.g.T5-large and state-spaces/mamba-790m).\n\nFor encoder-only models, we choose BERT [Dev+19] and RoBERTa [Liu+19]; for decoder-only models, we include Pythia [Bid+23], OPT [Zha+22] and Mamba [GD23], and we opt for T5 [Raf+20] as the representative encoder-decoder models.The compared models are from different model families, varying in terms of pretraining objectives (e.g.causal language modeling, masked language modeling, denoising, next sentence prediction), information direction (bidirectional vs unidirectional), model structure (Attention vs Mamba) and Positional Encoding (no positional encoding, learned positional encoding, relative positional encoding and rotary positional encoding [Su+24]).An important note is that we do not include instruction-finetuned language models such as Flan-T5 models [Chu+22] for a fair comparison to other language model families.For a review of these models, refer to Table 1.",
            "score": 0.5306910374814297,
            "section_title": "Backbone Language Models",
            "char_start_offset": 10474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 100,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 252
                },
                {
                    "start": 254,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1131
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 376,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 427,
                    "end": 435,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 906,
                    "end": 913,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08477783203125
        },
        {
            "corpus_id": "263310443",
            "title": "Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors",
            "text": "The Transformer architecture was first introduced by Vaswani et al. [16] as a novel approach to sequence-to-sequence learning tasks, particularly in natural language processing. Transformers have since become a popular choice for various machine-learning applications, including language modeling, machine translation, and computer vision. The key innovation of the Transformer architecture is the self-attention mechanism, which allows the model to weigh different parts of the input sequence differently when making predictions. This mechanism enables Transformers to capture long-range dependencies and contextual information more effectively compared to traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Figure 2 presents the architecture of a Transformer, which typically consists of encoder blocks, decoder blocks, and other operations such as position embedding and layer normalization. Specifically, each encoder/decoder block consists of multi-head self-attention mechanisms followed by a position-wise feed-forward network. Many widely-received DNN models are based on Transformers. For example, the Bidirectional Encoder Representations from Transformers (BERT) [4] and the Generative Pre-trained Transformer (GPT) [14]. BERT is primarily an encoder from the Transformer architecture. GPT is both an encoder and a decoder, but during training, only the decoder portion is utilized. BERT is bidirectional, trying to understand the context on both sides of a word. GPT is unidirectional, predicting words based on the preceding context.",
            "score": 0.5305758542641039,
            "section_title": "Transformers",
            "char_start_offset": 9587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.351318359375
        },
        {
            "corpus_id": "269757218",
            "title": "Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM",
            "text": "Since the introduction of the transformer architecture [29], numerous transformer-based Large Language Models (LLMs) have emerged.While all these models utilize stacked attention layers to construct their networks, each exhibits unique characteristics.Broadly speaking, existing LLMs can be categorized into three groups: encoder-only, decoder-only, and encoder-decoder.The encoder-only LLMs are mostly used to extract features of input text and are used for different downstream tasks.The most representative one is the BERT [6] model.BERT adopts a bidirectional architecture and trains using a masked language model and next-sense prediction task, allowing it to extract representations of text based on the previous and following text.A popular optimized version of BERT is RoBERTa [13], which builds upon BERT's architecture and pre-training objectives, refining the training process to achieve improved performance.It removes the next sentence prediction objective and trains on more data and for longer epochs, leading to better representations.\n\nDecoder-only LLMs stem from the decoder component of the transformer architecture.Their key feature lies in masking future positions to ensure that predictions of the current token are based solely on preceding tokens.The most prominent example of decoder-only LLMs is the GPT (Generative Pre-trained Transformer) series [1,3,21,22], which introduces a large-scale autoregressive language model based on the Transformer architecture.These models have demonstrated the effectiveness of unsupervised pre-training followed by fine-tuning across various natural language processing tasks.Beyond their technical aspects, they have significantly advanced natural language understanding and generation tasks, including translation, question answering, code generation, and even creative writing.Notably, with the emergence  The training stages of our method.The upper part: we use the BERT and GPT-2 for the encoder and decoder of our text-to-text baseline.In this baseline, the BERT representation will be mapped into a text prompt which is used for reconstructing the original text using GPT-2.The lower part: we use a transformer fMRI encoder to extract the fMRI prompt and add a contrastive loss to align the fMRI prompt to the text prompt.",
            "score": 0.5299330063852606,
            "section_title": "RELATED WORK 2.1 Large language models",
            "char_start_offset": 4506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 130,
                    "end": 252
                },
                {
                    "start": 252,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 738
                },
                {
                    "start": 738,
                    "end": 920
                },
                {
                    "start": 920,
                    "end": 1051
                },
                {
                    "start": 1053,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1271
                },
                {
                    "start": 1271,
                    "end": 1486
                },
                {
                    "start": 1486,
                    "end": 1637
                },
                {
                    "start": 1637,
                    "end": 1841
                },
                {
                    "start": 1841,
                    "end": 1904
                },
                {
                    "start": 1904,
                    "end": 2003
                },
                {
                    "start": 2003,
                    "end": 2142
                },
                {
                    "start": 2142,
                    "end": 2290
                }
            ],
            "ref_mentions": [
                {
                    "start": 1377,
                    "end": 1379,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1382,
                    "end": 1385,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2269287109375
        },
        {
            "corpus_id": "273501862",
            "title": "Causality for Large Language Models",
            "text": "Traditional pre-trained language models are primarily categorized into encoder-only, decoder-only, and encoderdecoder architectures. Encoder-only models, like BERT [8], specialize in encoding input text into contextual representations for understanding tasks such as classification and entity recognition. Decoder-only models, such as GPT [10,11] and LLAMA [2,3], focus on generating text by predicting the next token in a sequence, making them ideal for language generation tasks. Encoder-decoder models, exemplified by T5 [9], integrate both encoding and decoding processes to handle sequence-to-sequence tasks like translation and summarization. \n\nCentral to these models is the concept of token embedding, which transforms discrete tokens into continuous vector representations, capturing semantic and syntactic relationships and enabling effective learning from textual data during pre-training. \n\nBERT (Bidirectional Encoder Representations from Transformers) [8]. BERT is a pre-trained language model designed to produce contextualized word embeddings by considering both the left and right context of a token within a sentence. Unlike traditional embedding methods like Word2Vec or GloVe, which generate static word embeddings, BERT creates dynamic embeddings that change depending on the context. Traditional models assign the same vector to a word regardless of context, while BERT achieves context-aware embeddings through its Transformer encoder architecture, which leverages self-attention to process all tokens in a sentence simultaneously. This allows BERT to capture token relationships in both directions, adjusting each token's representation based on its surrounding context. \n\nDuring training, BERT is optimized using two primary tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, 15% of the input tokens are randomly masked, and the model must predict these masked tokens based on their context, which is formalized by the cross-entropy loss: \n\nwhere M is the set of masked tokens, and x \\M represents the surrounding context. In NSP, the model predicts whether two sentences are consecutive in the original text, with the binary cross-entropy loss:",
            "score": 0.5298650857514954,
            "section_title": "Traditional Pre-trained Models",
            "char_start_offset": 26916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 343,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 524,
                    "end": 527,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "260887420",
            "title": "CausalLM is not optimal for in-context learning",
            "text": "However, recent work (Von Oswald et al., 2023) focusing on ICL of linear regression was able to show that a specifically designed parameter construction of a one-layer Linear Self-Attention (LSA) transformer can simulate a single step of gradient descent by using the in-context examples as training data. Moreover, a different recent study (Zhang et al., 2023) used gradient flow to prove that a randomly initialized LSA-transformer indeed converges to such a construction during training. \n\nSince the softmax attention of standard transformers is non-linear, its theoretical analysis becomes complicated even for a single layer. For this reason, theoretical approaches to analyze transformers have often resorted to the Linear Self-Attention (LSA) layer (Von Oswald et al., 2023;Zhang et al., 2023), which simply drops the softmax function from the attention, \n\nFurthermore, since each input z j can attend to all positions j \u2208 {1 . . . n}, this form of attention is categorized as full (or bidirectional) attention, and is typically used in the transformer encoder. \n\nOn the other hand, a (linear) transformer decoder uses the auto-regressive attention \n\nwhich restricts each token z j to attend only to previous positions (and itself) from {1 . . . j}. This restriction is due to the role of the decoder as a causal language model (causalLM) which predicts the next token in the context of the previously generated ones. \n\nThe original transformer involves both a full attention based encoder and an auto-regressive attention based decoder. However, prominent NLP research has often chosen either encoder-only (e.g. \n\nBERT (Devlin et al., 2018)) or decoder-only (e.g. GPT (Brown et al., 2020), PaLM (Chowdhery et al., 2022)) models according to the task at hand. This is partially for the purpose of halving the parameter sizes. \n\nAnother version of attention, between full and auto-regressive, followed from the observation that some tasks can benefit from a prefix sequence such as context or prompt.",
            "score": 0.5296711298993766,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 46,
                    "matchedPaperCorpusId": "254685643"
                },
                {
                    "start": 756,
                    "end": 781,
                    "matchedPaperCorpusId": "254685643"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3720703125
        },
        {
            "corpus_id": "266362350",
            "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions",
            "text": "Language models are probabilistic models of natural language that can generate the likelihood of word sequences so as to predict the probabilities of future tokens [119,283]. Large language models (LLMs) refer to the deep neural language models with billions of learnable parameters that are pretrained on an extremely large textual corpus to understand the distribution and structure of natural language [283]. Thanks to the efficiency of the Transformer architecture [219], almost all large language models employ it as the backbone. There are three types of language model design: encoder-only (e.g., BERT [102]), decoder-only (e.g., GPT [180]), and encoder-decoder (e.g., T5 [182]). Encoder-only models, specifically for BERT, use bidirectional attention to process token sequences and are pre-trained on masked token prediction and next-sentence classification tasks. This process can extract semantic embeddings for general purposes and enable the models to quickly adapt to diverse downstream tasks after fine-tuning. Decoder-only models, such as GPT, conduct text-to-text tasks based on the transformer decoder architecture. They are trained on the next token prediction tasks from left to right generation. Encoder-decoder models, such as T5, are trained on text-to-text tasks. Their encoders extract contextual representations from the input sequence, and their decoders use cross attention to map latent representations back to the text output space. In the context of LLMs, most models follow the decoder-only architecture as it simplifies the model and makes efficient inferences [232]. \n\nRecently, researchers have found that scaling pretrained language models' training data and parameter size often leads to a significant performance gain, a.k.a scaling law [101]. The large language models present emergent abilities [240], referring to the abilities that are not present in small models. Typically, there are three types of well-studied emergent abilities: in-context learning (ICL), instruction following, and step-by-step reasoning.",
            "score": 0.5290841603043451,
            "section_title": "Large Language Model",
            "char_start_offset": 17413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 614,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 679,
                    "end": 684,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1593,
                    "end": 1598,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359375
        },
        {
            "corpus_id": "277244688",
            "title": "CASE - Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement",
            "text": "Although this makes sense for decoders, it is sub-optimal when using LLMs for encoding a given sequence as evident from the poor performance of GPT models (Lin et al., 2024) compared to similar-sized BERT (Devlin et al., 2018) or T5 (Chung et al., 2024) models on various natural language understanding benchmarks (Wang et al., 2019). \n\nBehnamGhader et al. ( 2024) proposed a postprocessing method for obtaining text embeddings from decoder-only LLMs following three steps: (a) enable bidirectional attention to overcome the restrictions due to causal attention, (b) train the model to predict the masked next token using bidirectional attention, and (c) use unsupervised contrastive learning to compute better sequence representations. Finally, a pooling method is applied on the token embedding sequence to create a fixeddimensional embedding for the input text such as the embedding of the last token in the sequence (i.e. last token pooling) (Meng et al., 2024;Li et al., 2023) or the average over all token embeddings (i.e. mean pooling) (Wang et al., 2024). Moreover, Lee et al. ( 2024) proposed a latent attention layer where they compute the cross attention between the last hidden layer from the decoder and a trainable latent array to compute a weighted pooling mechanism.",
            "score": 0.5289495092406796,
            "section_title": "Related Work",
            "char_start_offset": 10244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1282
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 173,
                    "matchedPaperCorpusId": "273901397"
                },
                {
                    "start": 205,
                    "end": 226,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 233,
                    "end": 253,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 314,
                    "end": 333,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "242033930",
            "title": "Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation",
            "text": "One of the most recent and arguably substantial paradigm shifts since the initial advance of distributional semantics was sparked by the introduction of the transformer model by Vaswani et al. (2017). Its model architecture holds numerous advantages when applied to sequential data such as natural language. First, sequential data can be processed in parallel by transformer models, reducing the resources required to train such a model. Sequential information (i.e., the order of words) is preserved by a process termed positional encoding, which engrains each word in a sentence with its intended sequential position. As a consequence, larger and more competent language models can be trained. Second, and of central importance to the design, transformer models learn through a mechanism referred to as self-attention. In essence, self-attention refers to the concept of determining the relevance of a word in relation to the relevance of other words in the input sequence. We provide more details on how attention is computed in the next section of this article. In particular, these two features allow the transformer model to learn long-range dependencies better than LSTMs. \n\nSince the publication of Vaswani et al.'s (2017) paper, a plethora of transformer implementations have been released with various modifications. One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados & Pilehvar, 2018;Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation.",
            "score": 0.5289319305733715,
            "section_title": "Transformer Models and the Attention Mechanism",
            "char_start_offset": 18001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 199,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1715,
                    "end": 1736,
                    "matchedPaperCorpusId": "264847728"
                },
                {
                    "start": 1835,
                    "end": 1870,
                    "matchedPaperCorpusId": "13696533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52392578125
        },
        {
            "corpus_id": "262088801",
            "title": "Attentive Multi-Layer Perceptron for Non-autoregressive Generation",
            "text": "Given a source sequence X 1:m , conditional sequence generation targets to predict a target sequence Y 1:n by modeling the conditional probability p(Y |X). \n\nAutoregressive generation decomposes the probability p(Y |X) as: \n\nwhich is implemented as a typical encoder-decoder architecture shown in Fig. 1a. Although such decomposition is proved effective, it suffers from two main drawbacks: efficiency and exposure bias. On the one hand, the autoregressive decoding process, where each token depends on the previous predicted ones, prevents the model from fast inference in usage. On the other hand, teacher-forcing exposes ground truth tokens in network inputs during the training process, where the exposed tokens are unable to observe in inference. Such exposure creates an inconsistency between the training and inference, and harms the prediction quality. \n\nRecently, non-autoregressive generation, depicted as Fig. 1b, shows its capability of sequence modeling in terms of both efficiency and efficacy, which decomposes the conditional probability p(Y |X) via a Na\u00efve Bayes assumption: \n\nThe NAR decomposition enables parallel decoding for each token, and speeds up the inference process substantially. Although NAR generation is much faster than AR generation, its speed is still limited by the O n 2 + nm + m 2 time complexity of the multi-head softmax attention module. This is especially problematic in modeling long sequences. \n\nAttention Types in AR & NAR Models Although autoregressive and nonautoregressive models differ from each other in sequence generation paradigms, their underlying attention mechanisms in their architectures are also different. \n\nThe token-by-token generation of AR models requires a causal decoder that forces tokens to attend to only previous features. A typical causal decoder utilizes causal softmax attentions both in self-attention and cross-attention. The attention causality entails that during the computation, it is important to ensure that the query token does not attend to the context on its right side, just as the shaded blocks in Fig. 1a. In contrast, the NAR model, which allows for parallel generation of the output sequence and global contextualization using attention, employs a noncausal decoder in Fig. 1b.",
            "score": 0.5288751227174636,
            "section_title": "Background: Autoregressive and Non-Autoregressive Generation",
            "char_start_offset": 3179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "254044433",
            "title": "Arguments to Key Points Mapping with Prompt-based Learning",
            "text": "BERT Unidirectional pre-train architectures limit the choice of architectures during pre-training. For instance, utilizing left-to-right architecture like in OpenAI GPT (Radford et al., 2018), each token can only attend to previous tokens in the selfattention layer of the Transformer. Devlin et al. (2018) proposed BERT to alleviate the limitations of unidirectional architectures using a masked language model. The architecture of the model is a multi-layer bidirectional Transformer encoder. The model is pre-trained utilizing two unsupervised tasks: Masked Language Models and Next Sentence Prediction (NSP). In many downstream tasks as well as the argument-to-keypoint task, understanding the relationship between two sentences is critical. The BERT model is pre-trained for a binarized NSP task to train the model to understand sentence relationships, which makes the BERT model a good choice for the key point analysis task. \n\nBART BART is a PLM that combines Bidirectional and Auto-Regressive Transformers (Lewis et al., 2020). The denoising autoencoder is built using a sequence-to-sequence model and it can be applied to various downstream tasks. It uses a standard Transformer-based neural machine translation architecture with a bidirectional encoder and a left-to-right decoder. During the pre-training process, an arbitrary noise function is applied to the input text, and then a sequence-to-sequence model is responsible for reconstructing the original text. Section 3.3.2 describes Approach 2 in which our model generates an intermediary text. BART can be a reasonable choice for this task because it performs effectively in text generation (Yuan et al., 2021) and text summarization (Huang et al., 2020) tasks. \n\nT5 Raffel et al. (2020) proposed a unified text-totext Transformer-based model to explore the limitations of transfer learning using an encoder-decoder architecture. It comprises an encoder that maps the input words from the source language to an output representation.",
            "score": 0.5286889380082724,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 6943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1034,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1657,
                    "end": 1675,
                    "matchedPaperCorpusId": "235593404"
                },
                {
                    "start": 1700,
                    "end": 1720,
                    "matchedPaperCorpusId": "222272417"
                },
                {
                    "start": 1733,
                    "end": 1753,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24169921875
        },
        {
            "corpus_id": "276538080",
            "title": "Tiny Language Models for Automation and Control: Overview, Potential Applications, and Future Research Directions",
            "text": "These models combine both encoder and decoder components, enabling them to handle a wide range of tasks, including translation, summarization, and question answering. \n\nT5-Small: A compact version of the T5 (Text-To-Text Transfer Transformer) model developed by Google Research, designed to unify various NLP tasks within a text-to-text framework [5]. Despite its reduced parameter count, T5-Small retains the effectiveness of the larger T5 models through an encoder-decoder Transformer architecture and pretraining on a large, diverse dataset (C4). This design allows T5-Small to handle tasks like summarization, translation, and classification efficiently, making it suitable for applications on resource-constrained devices that benefit from a versatile, low-compute language model. CodeT5+: An adaptable encoder-decoder language model family designed for coderelated tasks, addressing the limitations of existing code LLMs that rely solely on encoder only or decoder-only architectures [113]. Unlike traditional models, CodeT5+ enables flexible module combinations tailored for different tasks, enhancing its versatility across applications. The model is pre-trained with a diverse set of objectives including span denoising, contrastive learning, text-code matching, and causal language modeling on both unimodal and bimodal multilingual code datasets, which mitigates performance issues in task-specific scenarios. By initializing with pre-trained, frozen LLMs, CodeT5+ scales efficiently, and instruction-tuning aligns the models with natural language prompts. Evaluated on over 20 benchmarks, CodeT5+ achieves state-of-the-art performance in code generation, completion, mathematical programming, and text-to-code retrieval, with the instruction-tuned 16 B model establishing new records on the HumanEval benchmark for code generation.",
            "score": 0.5285215241651879,
            "section_title": "Transformer-Based Encoder-Decoder Models",
            "char_start_offset": 40962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1843
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1339111328125
        },
        {
            "corpus_id": "229339991",
            "title": "Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text Summarization",
            "text": "ParsBERT [19] is a monolingual version of BERT language model [14] for the Persian language that adopts the base configuration of the BERT model (i.e. 12 hidden layers, hidden size of 768 with 12 attention heads). BERT is a transformer-based [21] language model with an encoderonly architecture that is shown in Fig. 1. In this architecture the input sequence {x 1 , x 2 , ..., x n } is mapped to a contextualized encoded sequence\n\nby going through a series of bi-directional self-attention blocks with two feed-forward layers in each block. The output sequence can then be mapped to a task-specific output class by adding a classification layer to the last hidden layer. BERT model achieves state-of-the-art performance on NLU tasks by mapping input sequences to output sequences with a priori known output lengths. However, since the output sequence dimension does not rely on the input, it is impractical to use BERT for text generation (summarization). In other words, any BERT-based model corresponds to the architecture of only the encoder part of transformer-based encoder-decoder models, which are mostly used for text generation. On the other hand, decoder-only models such as GPT-2 [22] can be used as a means of text generation. However, it has been shown that encoder-decoder structures can perform better for such a task [23]. As a result, we used ParsBERT to warm-start both encoder and decoder from an encoder-only checkpoint as mentioned in [20], to achieve a pre-trained encoderdecoder model (BERT2BERT or B2B) which can be finetuned for text summarization using the dataset introduced in section IV. In this architecture, the encoder layer is the same as the ParsBERT transformer layers. The decoder layers are also the same as that of ParsBERT, with a few changes. First, cross-attention layers are added between self-attention and feed-forward layers in order to condition the decoder on the contextualized encoded sequence (e.g., the output of the ParsBERT model).",
            "score": 0.5284071951069436,
            "section_title": "A. Sequence-to-Sequence ParsBERT",
            "char_start_offset": 5911,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1457,
                    "end": 1461,
                    "matchedPaperCorpusId": "198967997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2027587890625
        },
        {
            "corpus_id": "277130726",
            "title": "A Survey of Large Language Model for Drug Research and Development",
            "text": "The decoder-only architecture processes word sequences in a unidirectional manner, from left to right, using a self-attention mechanism. This architecture is primarily employed in Natural Language Generation (NLG) tasks, where it converts embedding vectors into text and produces contextually relevant responses. The most well-known representative of this architecture is the GPT series. Pathways Language Model (PaLM) [19] and Large Language Model Meta AI (LLaMA) [110] are two additional noteworthy models that are built on the decoder-only architecture. Transformer technology is rapidly advancing, and new model architectures are frequently created by merging with other models to expand their applicability, as shown in Figure 4. For example, Conformer [39] combines a Convolutional Neural Network (CNN) with a self-attention mechanism to address long dependency challenges and improve local feature extraction. Tacotron [121] and Tacotron-2 [100], designed for text-to-speech (TTS) synthesis, follow the encoder-decoder architecture but differ from conventional Seq2Seq models by incorporating a Post-net component to enhance output quality. Similarly, models like Vision Transformer (ViT) [28] and Visual Attention Network (VAN) [137] combine CNNs with transformers to leverage both technologies. The Receptance Weighted Key Value (RWKV) architecture [67] is an innovative model that integrates the Recurrent Neural Networks (RNNs) with transformers, further broadening the scope of applications. The UNiversal Image-TExt Representation (UNITER) model [17] uses the ResNet model to extract regional picture characteristics, which are subsequently supplied into the transformer encoder. While UNITER's design stems from the encoder-decoder architecture, it places a strong emphasis on the encoder component, effectively handling both image and text data for multi-modal tasks. These models enhance processing efficiency by optimizing and adjusting their architectures. Transformers and LLMs have demonstrated the ability to overcome bottlenecks in drug discovery processes, such as retrosynthetic planning and chemical space exploration, revolutionizing ML approaches in chemistry and medicine, and accelerating the research and development process.",
            "score": 0.5283702252509485,
            "section_title": "2) DECODER-ONLY",
            "char_start_offset": 27548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 419,
                    "end": 423,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "218674528"
                },
                {
                    "start": 947,
                    "end": 952,
                    "matchedPaperCorpusId": "206742911"
                },
                {
                    "start": 1559,
                    "end": 1563,
                    "matchedPaperCorpusId": "216080982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25830078125
        },
        {
            "corpus_id": "257687801",
            "title": "CoBIT: A Contrastive Bi-directional Image-Text Generation Model",
            "text": "As shown in Fig. 2, CoBIT is composed of one image unicoder, one text unicoder and one cross-attention decoder. We term them unicoders because they can act as either encoders or decoders, depending on role they play for each task. The incorporation of text/image unicoder is inspired by [14,3,64], which demonstrated that one Transformer model can perform both bidirectional encoding for understanding tasks and autoregressive decoding for generation tasks. In our scenario, compared with plain image/text encoders, unicoders in decoding mode can take advantage of the common knowledge shared with encoding to produce unimodal autoregressive features as a decent prior for crossmodal generative objective. Experimental ablation also validates that unicoders boost both T2I generation and multimodal understanding. Image Unicoder. Recently, Vision Transformers (ViT) [15,45,27]  for image feature encoding. As decoders, Transformers are used in autoregressive image token generation [36,16,58]. \n\nWe combine these two functionalities into a single image unicoder. The image unicoder has two working modes: (1) \n\nIn the encoding mode, following ViT, each 2D patch in the raw image is projected into a feature vector by a trainable linear projection layer. Then, the sequence of projected features is input into cascaded Transformer layers to obtain the encoded image features, where the attention mask is bidirectional. \n\n(2) In the decoding mode, firstly, the input processing is different. As described in Sec. 3.1, we tokenize the raw image into image tokens and initialize an embedding layer where token embeddings are indexed. Then, the same Transformer layers in encoding mode are reused in decoding mode to process the features; however, to guarantee the causal decoding ability, we use causal conv-shaped attention mask [36,58,9] instead. Overall, the two modes share the Transformer layers' parameters, and only differ in input processing and attention masks.",
            "score": 0.5282069988510595,
            "section_title": "Architecture",
            "char_start_offset": 8704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 291,
                    "end": 293,
                    "matchedPaperCorpusId": "211572655"
                },
                {
                    "start": 293,
                    "end": 296,
                    "matchedPaperCorpusId": "202734445"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 873,
                    "end": 876,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 986,
                    "end": 989,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 1826,
                    "end": 1830,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.409423828125
        },
        {
            "corpus_id": "271051319",
            "title": "Looking into Black Box Code Language Models",
            "text": "The Transformer architecture [42] employs interconnected attention blocks and feed-forward layers.The attention block [3] facilitates the model's ability to weigh the significance of individual tokens in a sequence, thus capturing longrange dependencies across the input sequence.Concurrently, the feed-forward layers enable the model to retain crucial information derived from the training data [15].The transformerbased LMs are trained using extensive text data in a selfsupervised manner.Their substantial parameter space, often reaching billions or even trillions, gives them an impressive ability to absorb broad semantic and syntactic knowledge and strong memorization skills.These models have achieved state-of-the-art performance for various NLP tasks, and the utilization of transformer-based LMs has emerged as a highly promising research direction in NLP [9], [34], [36], [38], [42].\n\nTransformer-based LMs have three variations in their architecture.Table I illustrates these architectures.Encoder-decoder models, such as T5 [38], adhere to the original transformer architecture, with both encoder and decoder stacks.They formulate tasks by framing them as text-to-text problems, enabling unified training and inference.Encoder models, such as Bidirectional Encoder Representations from Transformers (BERT) [8], utilize the encoder stack and adopt a masked language modeling objective during training.They leverage bidirectional context understanding to comprehend text effectively.Decoder models, such as Generative Pre-trained Transformer (GPT) [36], leverage the decoder stack.They are trained to predict the next tokens based on preceding ones, they excel in language generation tasks.Due to the simplicity of the decoder architecture and the prevalence of text generation tasks, decoder models have become a de facto standard for various language modeling tasks.",
            "score": 0.5279792224471038,
            "section_title": "A. Transformer-based Language Models",
            "char_start_offset": 10595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 894
                },
                {
                    "start": 896,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1002
                },
                {
                    "start": 1002,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1494
                },
                {
                    "start": 1494,
                    "end": 1592
                },
                {
                    "start": 1592,
                    "end": 1701
                },
                {
                    "start": 1701,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 883,
                    "end": 887,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1037,
                    "end": 1041,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1559,
                    "end": 1563,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21533203125
        },
        {
            "corpus_id": "277634038",
            "title": "EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture",
            "text": "The original transformer architecture, introduced by Vaswani et al. [1], was designed for sequence-to-sequence tasks such as language translation, specially English-to-French and English-to-German. It consists of two main components: an encoder and a decoder. The encoder is responsible for processing and understanding the input text (source language). It converts the input sequence into a continuous representation, or embedding, which captures the relevant information from the source text. This embedding is then passed to the decoder for further processing. The decoder takes the continuous representation from the encoder along with the previously generated translated tokens, and generates the output sequence (target language). It uses the encoder's output and the autoregressive property of the model to sequentially generate the translated text, predicting one token at a time based on the context provided by the encoder's embeddings and prior decoded tokens. Together, the encoder and decoder form the basis of the transformer architecture, which enables effective handling of complex sequence-to-sequence tasks by leveraging self-attention and cross-attention mechanisms for capturing dependencies across tokens in the input and output sequences. \n\nRecent advancements in encoder-decoder transformer models, such as BART [27] and T5 [28,29], have significantly enhanced performance in various NLP tasks. These models combine innovative techniques, pre-training objectives, and architectural modifications to leverage the strengths of both the encoder and the decoder components. Encoder-decoder models are particularly effective for tasks involving input and output sequences with varying lengths or structures. They excel in applications where it is crucial to model the relationships between elements in both sequences. Common use cases include text translation, summarization, question answering, and text generation. By capturing contextual information in the input sequence and generating a coherent output, these models have become central to a variety of NLP tasks.",
            "score": 0.52758296813703,
            "section_title": "Encoder-Decoder architecture",
            "char_start_offset": 10773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 71,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1335,
                    "end": 1339,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1347,
                    "end": 1351,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1351,
                    "end": 1354,
                    "matchedPaperCorpusId": "252780443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361083984375
        },
        {
            "corpus_id": "51682556",
            "title": "Notes on Deep Learning for NLP",
            "text": "While different models can be used as the encoder, in NMT the decoder is usually a unidirectional RNN because this model is naturally adapted to the sequential nature of the generation task, and is usually deep (stacking). The decoder generates each word of the target sentence one step at a time. \n\nKey idea. Making the decoder use only the last annotation h Tx produced by the encoder to generate output forces the encoder to fit as much information as possible into h Tx . Since h Tx is a single fixed-size vector, its capacity is limited, so some information is lost. On the other hand, the attention mechanism allows the decoder to consider the entire sequence h 1 , . . . , h Tx of annotations produced by the encoder at each step of the generation process. As a result, the encoder is able to keep much more information by distributing it among all its annotations, knowing that the decoder will be able to decide later on which vectors it should pay attention to. \n\nMore precisely, the target sentence y = (y 1 , . . . , y Ty ) is generated one word y t at a time based on the distribution: P y t |{y 1 , ..., y t\u22121 }, c t = softmax W s ht (16) where ht , the attentional hidden state, is computed as: \n\nh t is the hidden state of the decoder (hidden state of the top layer, when the decoder is a stacking RNN) and provides information about the previously generated target words {y 1 , ..., y t\u22121 }, c t is the source context vector, and ; is concatenation. W s and W c are matrices of trainable parameters. Biases are not shown for simplicity. As shown in Fig. 11, the context vector c t can be computed in two ways: globally and locally. We describe each approach in the next two subsections. \n\nA note on beam search. Trying all possible combinations of words in the vocabulary to find the target sentence with highest joint probability is intractable.",
            "score": 0.5271353534742005,
            "section_title": "Decoder",
            "char_start_offset": 32532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1863
                }
            ],
            "ref_mentions": [
                {
                    "start": 1148,
                    "end": 1152,
                    "matchedPaperCorpusId": "14542261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "235421816",
            "title": "Pre-Trained Models: Past, Present and Future",
            "text": "We give an example in Figure 6, where the self-attention accurately captures the referential relationships between \"Jack\" and \"he\", generating the highest attention score.\n\n(2) Masked self-attention is used in the decoder, whose attention matrix satisfies A ij = 0, i > j.This attention is beneficial to autoregressive language modeling.In the decoding phase, the selfattention is similar to the encoding, except that it only decodes one representation from left to right at one time.Since each step of the decoding phase only consults the previously decoded results, we thus require to add the masking function into the self-attention.\n\n(3) Cross-attention is also used in the decoder, which uses the output of the previous decoder block as Q as well as the output of the encoder as K and V.Such a procedure is essentially an aggregation of the information of the whole input sequence, and it will be applied to all the words to generate in the decoding phase.Taking advantage of the input context is of great significance to some seq2seq tasks such as machine translation and text summarization.\n\nFor more details of Transformer, please refer to its original paper (Vaswani et al., 2017) and the survey paper (Lin et al., 2021).Due to the prominent nature, Transformer gradually becomes a standard neural structure for natural language understanding and generation.Moreover, it also serves as the backbone neural structure for the subsequently derived PTMs.Next, we will introduce two landmarks that completely open the door towards the era of large-scale self-supervised PTMs, GPT and BERT.In general, GPT is good at natural language generation, while BERT focuses more on natural language understanding.BERT GPT",
            "score": 0.5269821146479612,
            "section_title": "Transformer",
            "char_start_offset": 26556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 173,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 636
                },
                {
                    "start": 638,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1097
                },
                {
                    "start": 1099,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1367
                },
                {
                    "start": 1367,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1593
                },
                {
                    "start": 1593,
                    "end": 1707
                },
                {
                    "start": 1707,
                    "end": 1715
                }
            ],
            "ref_mentions": [
                {
                    "start": 1167,
                    "end": 1189,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1424560546875
        },
        {
            "corpus_id": "248052909",
            "title": "Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning",
            "text": "modeling tasks. In particular, popular architectures such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) use pre-training for downstream language generation tasks. BERT uses pre-training on unlabeled text for extracting deep bidirectional representations by jointly conditioning on both left and right context, and is trained using masked language modeling and next sentence prediction. It has become the state-of-the-art in several NLU tasks. Fine-tuning pre-trained models like BERT for downstream tasks has become a popular approach in NLP (Xia et al., 2020b). Another transformer-based model such as GPT (Generative Pre-Training) (Radford & Narasimhan, 2018) and its variants also utilizes pre-training for both NLG and NLU tasks with generative pre-training of a language model on a diverse corpus of unlabeled text.\n\nThe Unified pre-trained Language Model (UniLM) (Dong et al., 2019) architecture integrates multiple language modeling strategies into a joint framework, i.e., it involves a standard unidirectional language modelling loss coupled with both a bidirectional and a sequence-to-sequence language modeling task. The combination of different language model objectives makes its learning strategy special, allowing UniLM to achieve high performance for both natural language understanding and natural language generation tasks -including abstractive summarization -as well. The BART  model formulation also offers a modification to BERT by putting special emphasis on natural text generation as well. BART combines a bidirectional encoder over corrupted texts and an autoregressive decoder. The model is trained by first perturbing the input sequences with some noising function (e.g. token deletion and masking), then trying to reconstruct the original text in a sequence-to-sequence manner. Multilingual extensions to BERT-based models have been proposed in the literature, such as mBART  and mT5 (Xue et al., 2021).",
            "score": 0.5267406346676707,
            "section_title": "Models",
            "char_start_offset": 49628,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 145,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 584,
                    "end": 603,
                    "matchedPaperCorpusId": "222124957"
                },
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1955,
                    "end": 1973,
                    "matchedPaperCorpusId": "225040574"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.157470703125
        }
    ],
    "quotes": {
        "cost": 0.27069,
        "quotes": [
            {
                "idx": 0,
                "key": "[211003689 | Zhou et al. | 2020 | Citations: 3]",
                "snippets": "Pseudo-bidirectional Attention Mask While the proposed approach is versatile and applicable to RNN-based seq2seq models, in this paper, we focus on the transformer model [Vaswani et al., 2017] which yields state-of-the-art performance in most natural language generation tasks. The pseudo-bidirectional decoding copies the representation of tokens from the N +1 th position to the end of the input sequence in the corresponding encoder layer to the decoder when predicting the N th output token. This is easy to implement for auto-regressive decoding during inference. However, a straightforward way to make the model behaves consistently during training and inference requires us to change the decoder input dynamically for each decoding step instead of feeding the entire output sequence into the decoder and use a causal attention mask to ensure the causality of the decoder. This would hinder the transformer model from being trained in parallel, thus makes the training much less efficient.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Pseudo Future Context Modeling",
                        "pdf_hash": "",
                        "start": 432,
                        "end": 1427,
                        "sentence_offsets": [
                            {
                                "start": 432,
                                "end": 709
                            },
                            {
                                "start": 710,
                                "end": 927
                            },
                            {
                                "start": 928,
                                "end": 1000
                            },
                            {
                                "start": 1001,
                                "end": 1310
                            },
                            {
                                "start": 1311,
                                "end": 1427
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Pseudo-bidirectional Attention Mask While the proposed approach is versatile and applicable to RNN-based seq2seq models, in this paper, we focus on the transformer model [Vaswani et al., 2017] which yields state-of-the-art performance in most natural language generation tasks. The pseudo-bidirectional decoding copies the representation of tokens from the N +1 th position to the end of the input sequence in the corresponding encoder layer to the decoder when predicting the N th output token. This is easy to implement for auto-regressive decoding during inference. However, a straightforward way to make the model behaves consistently during training and inference requires us to change the decoder input dynamically for each decoding step instead of feeding the entire output sequence into the decoder and use a causal attention mask to ensure the causality of the decoder. This would hinder the transformer model from being trained in parallel, thus makes the training much less efficient."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[242033930 | Hommel et al. | 2021 | Citations: 21]",
                "snippets": "One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; (Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados et al., 2018)Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13696533 | Camacho-Collados et al. | 2018 | Citations: 340]": "\n \n \nOver the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality. \n \n \n",
                    "[264847728 | Manning et al. | 2008 | Citations: 1581]": "Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures."
                },
                "metadata": [
                    {
                        "section_title": "Transformer Models and the Attention Mechanism",
                        "pdf_hash": "",
                        "start": 1327,
                        "end": 2239,
                        "sentence_offsets": [
                            {
                                "start": 1327,
                                "end": 1415
                            },
                            {
                                "start": 1416,
                                "end": 1546
                            },
                            {
                                "start": 1547,
                                "end": 1666
                            },
                            {
                                "start": 1667,
                                "end": 1737
                            },
                            {
                                "start": 1738,
                                "end": 1913
                            },
                            {
                                "start": 1914,
                                "end": 2057
                            },
                            {
                                "start": 2058,
                                "end": 2239
                            }
                        ],
                        "ref_mentions": [
                            "264847728",
                            "13696533"
                        ],
                        "quote": "One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; (Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados et al., 2018)Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[244119798 | Li et al. | 2021 | Citations: 11]",
                "snippets": "Typically, the encoder-decoder based models generate text starting from the beginning, and the decoder adopts a causal mask to force the state of each decoder time step s t i only attend to the state from the previous time steps, s t|t\u2264t i , to avoid seeing tokens \"from the future\". We consider this type of attention as unidirectional. In our task, we have the input prefix as the written text. Tokens in the prefix should be visible to each other. Therefore, we adopt the causal with prefix mask: bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Decoder Self-attention Visibility",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 607,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 283
                            },
                            {
                                "start": 284,
                                "end": 337
                            },
                            {
                                "start": 338,
                                "end": 396
                            },
                            {
                                "start": 397,
                                "end": 450
                            },
                            {
                                "start": 451,
                                "end": 607
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Typically, the encoder-decoder based models generate text starting from the beginning, and the decoder adopts a causal mask to force the state of each decoder time step s t i only attend to the state from the previous time steps, s t|t\u2264t i , to avoid seeing tokens \"from the future\". We consider this type of attention as unidirectional. In our task, we have the input prefix as the written text. Tokens in the prefix should be visible to each other. Therefore, we adopt the causal with prefix mask: bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[245385261 | Laskar et al. | 2021 | Citations: 45]",
                "snippets": "(i) The Bidirectional Self-Attention Mechanism: In the original BERTSUM architecture, the bidirectional self-attention mechanism (Devlin et al., 2019) is utilized by the BERT encoder to generate the encoded representation of the input text. In the bidirectional self-attention mechanism, when a pair of sentences are combined together and given as input to the BERT encoder, both sentences will give attention to each other. Thus, when we utilize the bidirectional self-attention mechanism (see Figure 2a) in the PreQFAS model, both the query and the document will not only give attention to themselves, but also they will give attention to each other to provide the encoded representation of the concatenated input.\n\n(ii) The Query-Document Attention Mechanism: Dong et al. (2019) proposed the sequence-tosequence language modeling objective for text sequences that are consisted of two segments. In such text sequences, each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[147704286 | Dong et al. | 2019 | Citations: 1560]": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "The PreQFAS model for the SD-QFAS task",
                        "pdf_hash": "",
                        "start": 42,
                        "end": 1264,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 50
                            },
                            {
                                "start": 53,
                                "end": 293
                            },
                            {
                                "start": 294,
                                "end": 477
                            },
                            {
                                "start": 478,
                                "end": 769
                            },
                            {
                                "start": 772,
                                "end": 951
                            },
                            {
                                "start": 952,
                                "end": 1276
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "147704286"
                        ],
                        "quote": "(i) The Bidirectional Self-Attention Mechanism: In the original BERTSUM architecture, the bidirectional self-attention mechanism (Devlin et al., 2019) is utilized by the BERT encoder to generate the encoded representation of the input text. In the bidirectional self-attention mechanism, when a pair of sentences are combined together and given as input to the BERT encoder, both sentences will give attention to each other. Thus, when we utilize the bidirectional self-attention mechanism (see Figure 2a) in the PreQFAS model, both the query and the document will not only give attention to themselves, but also they will give attention to each other to provide the encoded representation of the concatenated input.\n\n(ii) The Query-Document Attention Mechanism: Dong et al. (2019) proposed the sequence-tosequence language modeling objective for text sequences that are consisted of two segments. In such text sequences, each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[248377166 | Chaffin et al. | 2022 | Citations: 4]",
                "snippets": "By default, attention layers as defined in (Vaswani et al., 2017) are bidirectional: every token can attend to tokens at every position. When it comes to discrimination, models based on such bidirectional attention are commonly used since \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than [...] a left-to-right model\" (Devlin et al., 2019). However, while it brings some capacity to the model, it also makes it non auto-regressive: when a token is added at the end of a sequence, every hidden states need to be re-computed. \n\nOne way to train a transformer based LM for text generation is to use unidirectional attention masks [29]. In this unidirectional setting, any extra token added at the end of a sequence does not change the already calculated hidden states, since previous tokens do not attend to it. Thus, starting from an already classified sequence  1: \u22121 , classifying  1: only requires to compute  attention scores, rather than the whole set of  2 scores per self-attention layer, as it would be required in the bidirectional setting.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Choosing the Right Teammate",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 327,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 118
                            },
                            {
                                "start": 119,
                                "end": 359
                            }
                        ],
                        "ref_mentions": [
                            "13756489"
                        ],
                        "quote": "By default, attention layers as defined in (Vaswani et al., 2017) are bidirectional: every token can attend to tokens at every position. When it comes to discrimination, models based on such bidirectional attention are commonly used since \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than ["
                    },
                    {
                        "section_title": "Choosing the Right Teammate",
                        "pdf_hash": "",
                        "start": 330,
                        "end": 1066,
                        "sentence_offsets": [
                            {
                                "start": 119,
                                "end": 359
                            },
                            {
                                "start": 360,
                                "end": 542
                            },
                            {
                                "start": 545,
                                "end": 651
                            },
                            {
                                "start": 652,
                                "end": 827
                            },
                            {
                                "start": 828,
                                "end": 1066
                            }
                        ],
                        "ref_mentions": [
                            "52967399"
                        ],
                        "quote": "] a left-to-right model\" (Devlin et al., 2019). However, while it brings some capacity to the model, it also makes it non auto-regressive: when a token is added at the end of a sequence, every hidden states need to be re-computed. \n\nOne way to train a transformer based LM for text generation is to use unidirectional attention masks [29]. In this unidirectional setting, any extra token added at the end of a sequence does not change the already calculated hidden states, since previous tokens do not attend to it. Thus, starting from an already classified sequence  1: \u22121 , classifying  1: only requires to compute  attention scores, rather than the whole set of  2 scores per self-attention layer, as it would be required in the bidirectional setting."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[249626024 | Hao et al. | 2022 | Citations: 99]",
                "snippets": "Non-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available.\n\nAs shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Non-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available.\n\nAs shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[258461112 | Jain et al. | 2022 | Citations: 16]",
                "snippets": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D.1 Bridge the Gap on Discriminative Tasks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 273,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 273
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[258461229 | Nijkamp et al. | 2023 | Citations: 169]",
                "snippets": "For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2019). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                "metadata": [
                    {
                        "section_title": "COMPONENTS: ARCHITECTURE, OBJECTIVE, SAMPLING, DATA",
                        "pdf_hash": "",
                        "start": 247,
                        "end": 1702,
                        "sentence_offsets": [
                            {
                                "start": 247,
                                "end": 278
                            },
                            {
                                "start": 279,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 573
                            },
                            {
                                "start": 574,
                                "end": 905
                            },
                            {
                                "start": 906,
                                "end": 1017
                            },
                            {
                                "start": 1018,
                                "end": 1098
                            },
                            {
                                "start": 1099,
                                "end": 1130
                            },
                            {
                                "start": 1131,
                                "end": 1259
                            },
                            {
                                "start": 1260,
                                "end": 1294
                            },
                            {
                                "start": 1295,
                                "end": 1408
                            },
                            {
                                "start": 1409,
                                "end": 1543
                            },
                            {
                                "start": 1544,
                                "end": 1702
                            }
                        ],
                        "ref_mentions": [
                            "204838007"
                        ],
                        "quote": "For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2019). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[261064777 | Zheng et al. | 2023 | Citations: 76]",
                "snippets": "This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture...Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b).\n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                "metadata": [
                    {
                        "section_title": "Model Architecture",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 280,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 79
                            },
                            {
                                "start": 80,
                                "end": 281
                            }
                        ],
                        "ref_mentions": [
                            "160025533",
                            "218971783"
                        ],
                        "quote": "This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture"
                    },
                    {
                        "section_title": "Model Architecture",
                        "pdf_hash": "",
                        "start": 443,
                        "end": 1687,
                        "sentence_offsets": [
                            {
                                "start": 282,
                                "end": 608
                            },
                            {
                                "start": 609,
                                "end": 888
                            },
                            {
                                "start": 889,
                                "end": 1067
                            },
                            {
                                "start": 1070,
                                "end": 1190
                            },
                            {
                                "start": 1191,
                                "end": 1252
                            },
                            {
                                "start": 1253,
                                "end": 1396
                            },
                            {
                                "start": 1397,
                                "end": 1552
                            },
                            {
                                "start": 1553,
                                "end": 1716
                            }
                        ],
                        "ref_mentions": [
                            "218971783"
                        ],
                        "quote": "Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b).\n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[262088801 | Jiang et al. | 2023 | Citations: 0]",
                "snippets": "The token-by-token generation of AR models requires a causal decoder that forces tokens to attend to only previous features. A typical causal decoder utilizes causal softmax attentions both in self-attention and cross-attention. The attention causality entails that during the computation, it is important to ensure that the query token does not attend to the context on its right side, just as the shaded blocks in Fig. 1a. In contrast, the NAR model, which allows for parallel generation of the output sequence and global contextualization using attention, employs a noncausal decoder in Fig. 1b.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Background: Autoregressive and Non-Autoregressive Generation",
                        "pdf_hash": "",
                        "start": 1668,
                        "end": 2266,
                        "sentence_offsets": [
                            {
                                "start": 1668,
                                "end": 1792
                            },
                            {
                                "start": 1793,
                                "end": 1896
                            },
                            {
                                "start": 1897,
                                "end": 2092
                            },
                            {
                                "start": 2093,
                                "end": 2266
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The token-by-token generation of AR models requires a causal decoder that forces tokens to attend to only previous features. A typical causal decoder utilizes causal softmax attentions both in self-attention and cross-attention. The attention causality entails that during the computation, it is important to ensure that the query token does not attend to the context on its right side, just as the shaded blocks in Fig. 1a. In contrast, the NAR model, which allows for parallel generation of the output sequence and global contextualization using attention, employs a noncausal decoder in Fig. 1b."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[263829839 | Saha et al. | 2023 | Citations: 54]",
                "snippets": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named...In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[252780443 | Tay et al. | 2022 | Citations: 313]": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B."
                },
                "metadata": [
                    {
                        "section_title": "A. Preliminaries",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 852,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 165
                            },
                            {
                                "start": 166,
                                "end": 342
                            },
                            {
                                "start": 343,
                                "end": 465
                            },
                            {
                                "start": 466,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 762
                            },
                            {
                                "start": 763,
                                "end": 853
                            }
                        ],
                        "ref_mentions": [
                            "204838007",
                            "252780443"
                        ],
                        "quote": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named"
                    },
                    {
                        "section_title": "A. Preliminaries",
                        "pdf_hash": "",
                        "start": 1719,
                        "end": 2271,
                        "sentence_offsets": [
                            {
                                "start": 1719,
                                "end": 1846
                            },
                            {
                                "start": 1847,
                                "end": 1942
                            },
                            {
                                "start": 1943,
                                "end": 1998
                            },
                            {
                                "start": 1999,
                                "end": 2140
                            },
                            {
                                "start": 2141,
                                "end": 2270
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[265150001 | Lv et al. | 2023 | Citations: 21]",
                "snippets": "Converting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask, as doing so would introduce positional information that the model has never encountered during training, in which stage a query vector is only allowed to calculate the inner product with its preceding key vectors. This is evident in Eq.4: the relative position n \u2212 m is always non-positive during training but is positive when q m needs to attend to k >m.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Extending Causal Attention to",
                        "pdf_hash": "",
                        "start": 13,
                        "end": 550,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 14
                            },
                            {
                                "start": 17,
                                "end": 139
                            },
                            {
                                "start": 140,
                                "end": 412
                            },
                            {
                                "start": 413,
                                "end": 555
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Converting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask, as doing so would introduce positional information that the model has never encountered during training, in which stage a query vector is only allowed to calculate the inner product with its preceding key vectors. This is evident in Eq.4: the relative position n \u2212 m is always non-positive during training but is positive when q m needs to attend to k >m."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[266755678 | Liu et al. | 2024 | Citations: 74]",
                "snippets": "The Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices.\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices.\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[267312283 | Park et al. | 2024 | Citations: 16]",
                "snippets": "Encoder-only Transformers (a) [25,(Devlin et al., 2019)83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,(Dolan et al., 2005), natural language inference [140](Williams et al., 2017), and question answering (Rajpurkar et al., 2018)(Rajpurkar et al., 2016). On the other hand, decoder-only Transformers (b) (Brown et al., 2020)124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[11816014 | Rajpurkar et al. | 2016 | Citations: 8174]": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[3432876 | Williams et al. | 2017 | Citations: 4497]": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
                    "[47018994 | Rajpurkar et al. | 2018 | Citations: 2854]": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Transformer Architecture",
                        "pdf_hash": "",
                        "start": 1165,
                        "end": 1850,
                        "sentence_offsets": [
                            {
                                "start": 1165,
                                "end": 1336
                            },
                            {
                                "start": 1337,
                                "end": 1529
                            },
                            {
                                "start": 1530,
                                "end": 1725
                            },
                            {
                                "start": 1726,
                                "end": 1850
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "16639476",
                            "3432876",
                            "47018994",
                            "11816014",
                            "218971783"
                        ],
                        "quote": "Encoder-only Transformers (a) [25,(Devlin et al., 2019)83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,(Dolan et al., 2005), natural language inference [140](Williams et al., 2017), and question answering (Rajpurkar et al., 2018)(Rajpurkar et al., 2016). On the other hand, decoder-only Transformers (b) (Brown et al., 2020)124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[267701011 | Cascella et al. | 2024 | Citations: 83]",
                "snippets": "Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Development of LLMs for Chatbots and Enhanced human-like Interaction",
                        "pdf_hash": "",
                        "start": 325,
                        "end": 1323,
                        "sentence_offsets": [
                            {
                                "start": 325,
                                "end": 467
                            },
                            {
                                "start": 468,
                                "end": 602
                            },
                            {
                                "start": 603,
                                "end": 756
                            },
                            {
                                "start": 757,
                                "end": 903
                            },
                            {
                                "start": 904,
                                "end": 1130
                            },
                            {
                                "start": 1131,
                                "end": 1323
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[267759571 | Lin et al. | 2024 | Citations: 2]",
                "snippets": "\u2022 Full attention (Devlin et al., 2019)(Feng et al., 2020). This bidirectional attention operates between every pair of tokens in the sequence. Notably, both subsequent and preceding tokens can assimilate information from each other. The robust information encoding capability afforded by full attention designates the model as a transformer-encoder, specialized for tasks involving understanding. \u2022 Masking attention (Brown et al., 2020)[3]. In this directional attention model, interactions occur exclusively from left to right within the token sequence. Consequently, only the subsequent token can integrate information from preceding tokens, creating a unidirectional flow. Masking attention aligns with standard causal language modeling and is denoted as a transformerdecoder, tailored for tasks centered around generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211171605 | Feng et al. | 2020 | Citations: 2668]": "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both \u201cbimodal\u201d data of NL-PL pairs and \u201cunimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Transformer Architecture",
                        "pdf_hash": "",
                        "start": 257,
                        "end": 1031,
                        "sentence_offsets": [
                            {
                                "start": 257,
                                "end": 280
                            },
                            {
                                "start": 281,
                                "end": 364
                            },
                            {
                                "start": 365,
                                "end": 454
                            },
                            {
                                "start": 455,
                                "end": 618
                            },
                            {
                                "start": 619,
                                "end": 645
                            },
                            {
                                "start": 646,
                                "end": 759
                            },
                            {
                                "start": 760,
                                "end": 880
                            },
                            {
                                "start": 881,
                                "end": 1031
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "211171605",
                            "218971783"
                        ],
                        "quote": "\u2022 Full attention (Devlin et al., 2019)(Feng et al., 2020). This bidirectional attention operates between every pair of tokens in the sequence. Notably, both subsequent and preceding tokens can assimilate information from each other. The robust information encoding capability afforded by full attention designates the model as a transformer-encoder, specialized for tasks involving understanding. \u2022 Masking attention (Brown et al., 2020)[3]. In this directional attention model, interactions occur exclusively from left to right within the token sequence. Consequently, only the subsequent token can integrate information from preceding tokens, creating a unidirectional flow. Masking attention aligns with standard causal language modeling and is denoted as a transformerdecoder, tailored for tasks centered around generation."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[268157336 | Patil et al. | 2024 | Citations: 80]",
                "snippets": "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks...As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Encoder-Decoder-Based Model",
                        "pdf_hash": "",
                        "start": 418,
                        "end": 983,
                        "sentence_offsets": [
                            {
                                "start": 418,
                                "end": 575
                            },
                            {
                                "start": 576,
                                "end": 725
                            },
                            {
                                "start": 726,
                                "end": 792
                            },
                            {
                                "start": 793,
                                "end": 984
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks"
                    },
                    {
                        "section_title": "Prefix (Non-Causal) Language Model",
                        "pdf_hash": "",
                        "start": 1183,
                        "end": 1799,
                        "sentence_offsets": [
                            {
                                "start": 1183,
                                "end": 1329
                            },
                            {
                                "start": 1330,
                                "end": 1436
                            },
                            {
                                "start": 1437,
                                "end": 1595
                            },
                            {
                                "start": 1596,
                                "end": 1798
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[268247581 | Pei et al. | 2024 | Citations: 18]",
                "snippets": "The standard Transformer (Vaswani et al., 2017) and its variants (Lewis et al., 2019), (Raffel et al., 2019) adopt the encoder-decoder framework (Figure 6d),where the encoder processes and contextualize the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention. Models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) are with this architecture and demonstrating its effectiveness across a broad spectrum of applications. In biomolecule and text cross modeling scenario, this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models. This enriched interaction allows for a deeper understanding of the input sequences. Subsequently, the encoder-decoder attention mechanism empowers the decoder to generate outputs tailored for specific biological tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
                },
                "metadata": [
                    {
                        "section_title": "Encoder-decoder",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 896,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 283
                            },
                            {
                                "start": 284,
                                "end": 426
                            },
                            {
                                "start": 427,
                                "end": 677
                            },
                            {
                                "start": 678,
                                "end": 761
                            },
                            {
                                "start": 762,
                                "end": 896
                            }
                        ],
                        "ref_mentions": [
                            "13756489",
                            "204960716",
                            "204838007",
                            "204960716",
                            "204838007"
                        ],
                        "quote": "The standard Transformer (Vaswani et al., 2017) and its variants (Lewis et al., 2019), (Raffel et al., 2019) adopt the encoder-decoder framework (Figure 6d),where the encoder processes and contextualize the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention. Models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) are with this architecture and demonstrating its effectiveness across a broad spectrum of applications. In biomolecule and text cross modeling scenario, this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models. This enriched interaction allows for a deeper understanding of the input sequences. Subsequently, the encoder-decoder attention mechanism empowers the decoder to generate outputs tailored for specific biological tasks."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[269009682 | BehnamGhader et al. | 2024 | Citations: 241]",
                "snippets": "We speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position i is influenced solely by the representations of preceding tokens at positions 0, 1, . . . , i \u2212 1. Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 852,
                        "end": 1491,
                        "sentence_offsets": [
                            {
                                "start": 852,
                                "end": 1070
                            },
                            {
                                "start": 1071,
                                "end": 1281
                            },
                            {
                                "start": 1282,
                                "end": 1290
                            },
                            {
                                "start": 1291,
                                "end": 1491
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position i is influenced solely by the representations of preceding tokens at positions 0, 1, . . . , i \u2212 1. Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[269981935 | Kopiczko et al. | 2024 | Citations: 3]",
                "snippets": "But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2019) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020)Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 993,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 172
                            },
                            {
                                "start": 172,
                                "end": 560
                            },
                            {
                                "start": 560,
                                "end": 826
                            },
                            {
                                "start": 826,
                                "end": 993
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "204838007",
                            "218869575"
                        ],
                        "quote": "But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2019) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020)Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[270521959 | Gao et al. | 2024 | Citations: 0]",
                "snippets": "The encoder-decoder architecture [25](Raffel et al., 2019)[27] consists of two stacks of Transformer blocks to construct encoder and decoder separately.The encoder encodes the input sequence into a common history and the decoder generates the target sequence based on the common history in an autoregressive way.The prefix decoder architecture (Chowdhery et al., 2022)[29] performs bidirectional attention over the prefix tokens and unidirectional attention on generated tokens.The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input through a unidirectional attention mask.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[247951931 | Chowdhery et al. | 2022 | Citations: 6293]": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "Generative Language Models",
                        "pdf_hash": "",
                        "start": 442,
                        "end": 1015,
                        "sentence_offsets": [
                            {
                                "start": 442,
                                "end": 577
                            },
                            {
                                "start": 577,
                                "end": 737
                            },
                            {
                                "start": 737,
                                "end": 882
                            },
                            {
                                "start": 882,
                                "end": 1015
                            }
                        ],
                        "ref_mentions": [
                            "204838007",
                            "247951931"
                        ],
                        "quote": "The encoder-decoder architecture [25](Raffel et al., 2019)[27] consists of two stacks of Transformer blocks to construct encoder and decoder separately.The encoder encodes the input sequence into a common history and the decoder generates the target sequence based on the common history in an autoregressive way.The prefix decoder architecture (Chowdhery et al., 2022)[29] performs bidirectional attention over the prefix tokens and unidirectional attention on generated tokens.The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input through a unidirectional attention mask."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[270702559 | Yin et al. | 2024 | Citations: 12]",
                "snippets": "The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[270703043 | Posada et al. | 2024 | Citations: 0]",
                "snippets": "Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Pre-trained language models",
                        "pdf_hash": "",
                        "start": 1198,
                        "end": 2208,
                        "sentence_offsets": [
                            {
                                "start": 1147,
                                "end": 1259
                            },
                            {
                                "start": 1260,
                                "end": 1322
                            },
                            {
                                "start": 1323,
                                "end": 1563
                            },
                            {
                                "start": 1564,
                                "end": 1642
                            },
                            {
                                "start": 1643,
                                "end": 1689
                            },
                            {
                                "start": 1690,
                                "end": 1772
                            },
                            {
                                "start": 1773,
                                "end": 1870
                            },
                            {
                                "start": 1871,
                                "end": 1926
                            },
                            {
                                "start": 1927,
                                "end": 2097
                            },
                            {
                                "start": 2098,
                                "end": 2208
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[270711050 | Dam et al. | 2024 | Citations: 62]",
                "snippets": "The GPT-series models (Brown et al., 2020), (269498086) use an autoregressive or causal decoder architecture with a one-way attention mask that lets each input token think about only the elements that came before it and itself while it is being processed (see Fig. 8). This makes the processing more like how a conversation would naturally flow. Both input and output tokens undergo similar processing within this framework. The non-causal or prefix decoder architecture (Zhang et al., 2022) performs bidirectional attention on prefix tokens, i.e., it considers both preceding and subsequent tokens. While autoregressively predicting the output tokens utilizing the same set of parameters used in encoding, it performs unidirectional attention (Liu et al., 2021), (Dong et al., 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[147704286 | Dong et al. | 2019 | Citations: 1560]": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[236493269 | Liu et al. | 2021 | Citations: 3992]": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub \u201cprompt-based learning.\u201d Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x\u2032 that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x\u0302, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia\u2013Pretrain including constantly updated survey and paperlist.",
                    "[246441975 | Zhang et al. | 2022 | Citations: 23]": "Natural language understanding and generation models follow one of the two dominant architectural paradigms: language models (LMs) that process concatenated sequences in a single stack of layers, and encoder-decoder models (EncDec) that utilize separate layer stacks for input and output processing. In machine translation, EncDec has long been the favoured approach, but with few studies investigating the performance of LMs. In this work, we thoroughly examine the role of several architectural design choices on the performance of LMs on bilingual, (massively) multilingual and zero-shot translation tasks, under systematic variations of data conditions and model sizes. Our results show that: (i) Different LMs have different scaling properties, where architectural differences often have a significant impact on model performance at small scales, but the performance gap narrows as the number of parameters increases, (ii) Several design choices, including causal masking and language-modeling objectives for the source sequence, have detrimental effects on translation quality, and (iii) When paired with full-visible masking for source sequences, LMs could perform on par with EncDec on supervised bilingual and multilingual translation tasks, and improve greatly on zero-shot directions by facilitating the reduction of off-target translations."
                },
                "metadata": [
                    {
                        "section_title": "B. Large Language Models (LLMs)",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 715,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 244
                            },
                            {
                                "start": 245,
                                "end": 321
                            },
                            {
                                "start": 322,
                                "end": 400
                            },
                            {
                                "start": 401,
                                "end": 559
                            },
                            {
                                "start": 560,
                                "end": 715
                            }
                        ],
                        "ref_mentions": [
                            "218971783",
                            "269498086",
                            "246441975",
                            "236493269",
                            "147704286"
                        ],
                        "quote": "The GPT-series models (Brown et al., 2020), (269498086) use an autoregressive or causal decoder architecture with a one-way attention mask that lets each input token think about only the elements that came before it and itself while it is being processed (see Fig. 8). This makes the processing more like how a conversation would naturally flow. Both input and output tokens undergo similar processing within this framework. The non-causal or prefix decoder architecture (Zhang et al., 2022) performs bidirectional attention on prefix tokens, i.e., it considers both preceding and subsequent tokens. While autoregressively predicting the output tokens utilizing the same set of parameters used in encoding, it performs unidirectional attention (Liu et al., 2021), (Dong et al., 2019)."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[270832367 | Busto-Castineira et al. | 2024 | Citations: 1]",
                "snippets": "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder...Even though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], [22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[209832341 | Wu | 2020 | Citations: 4]": "We describe a method for training accurate Transformer machine-translation models to run inference using 8-bit integer (INT8) hardware matrix multipliers, as opposed to the more costly single-precision floating-point (FP32) hardware. Unlike previous work, which converted only 85 Transformer matrix multiplications to INT8, leaving 48 out of 133 of them in FP32 because of unacceptable accuracy loss, we convert them all to INT8 without compromising accuracy. Tested on the newstest2014 English-to-German translation task, our INT8 Transformer Base and Transformer Big models yield BLEU scores that are 99.3\u2013100% relative to those of the corresponding FP32 models. Our approach converts all matrix-multiplication tensors from an existing FP32 model into INT8 tensors by automatically making range-precision trade-offs during training. To demonstrate the robustness of this approach, we also include results from INT6 Transformer models.",
                    "[219463386 | Chen et al. | 2020 | Citations: 23]": "Source input information plays a very important role in the Transformer-based translation system. In practice, word embedding and positional embedding of each word are added as the input representation. Then self-attention networks are used to encode the global dependencies in the input representation to generate a source representation. However, this processing on the source representation only adopts a single source feature and excludes richer and more diverse features such as recurrence features, local features, and syntactic features, which results in tedious representation and thereby hinders the further translation performance improvement. In this paper, we introduce a simple and efficient method to encode more diverse source features into the input representation simultaneously, and thereby learning an effective source representation by self-attention networks. In particular, the proposed grouped strategy is only applied to the input representation layer, to keep the diversity of translation information and the efficiency of the self-attention networks at the same time. Experimental results show that our approach improves the translation performance over the state-of-the-art baselines of Transformer in regard to WMT14 English-to-German and NIST Chinese-to-English machine translation tasks.",
                    "[234785837 | Nguyen et al. | 2021 | Citations: 22]": "Transformer is a neural machine translation model which revolutionizes machine translation. Compared with traditional statistical machine translation models and other neural machine translation models, the recently proposed transformer model radically and fundamentally changes machine translation with its self-attention and cross-attention mechanisms. These mechanisms effectively model token alignments between source and target sentences. It has been reported that the transformer model provides accurate posterior alignments. In this work, we empirically prove the reverse effect, showing that prior alignments help transformer models produce better translations. Experiment results on Vietnamese-English news translation task show not only the positive effect of manually annotated alignments on transformer models but also the surprising outperformance of statistically constructed alignments reinforced with the flexibility of token-type selection over manual alignments in improving transformer models. Statistically constructed word-to-lemma alignments are used to train a word-to-word transformer model. The novel hybrid transformer model improves the baseline transformer model and transformer model trained with manual alignments by 2.53 and 0.79 BLEU, respectively. In addition to BLEU score, we make limited human judgment on translation results. Strong correlation between human and machine judgment confirms our findings.",
                    "[265629619 | Zeng et al. | 2021 | Citations: 8]": "Machine reading comprehension (MRC) is a challenging natural language processing (NLP) task. It has a wide application potential in the fields of question answering robots, human-computer interactions in mobile virtual reality systems, etc. Recently, the emergence of pretrained models (PTMs) has brought this research field into a new era, in which the training objective plays a key role. The masked language model (MLM) is a self-supervised training objective widely used in various PTMs. With the development of training objectives, many variants of MLM have been proposed, such as whole word masking, entity masking, phrase masking, and span masking. In different MLMs, the length of the masked tokens is different. Similarly, in different machine reading comprehension tasks, the length of the answer is also different, and the answer is often a word, phrase, or sentence. Thus, in MRC tasks with different answer lengths, whether the length of MLM is related to performance is a question worth studying. If this hypothesis is true, it can guide us on how to pretrain the MLM with a relatively suitable mask length distribution for MRC tasks. In this paper, we try to uncover how much of MLM\u2019s success in the machine reading comprehension tasks comes from the correlation between masking length distribution and answer length in the MRC dataset. In order to address this issue, herein, (1) we propose four MRC tasks with different answer length distributions, namely, the short span extraction task, long span extraction task, short multiple-choice cloze task, and long multiple-choice cloze task; (2) four Chinese MRC datasets are created for these tasks; (3) we also have pretrained four masked language models according to the answer length distributions of these datasets; and (4) ablation experiments are conducted on the datasets to verify our hypothesis. The experimental results demonstrate that our hypothesis is true. On four different machine reading comprehension datasets, the performance of the model with correlation length distribution surpasses the model without correlation."
                },
                "metadata": [
                    {
                        "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
                        "pdf_hash": "",
                        "start": 739,
                        "end": 1011,
                        "sentence_offsets": [
                            {
                                "start": 739,
                                "end": 1012
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder"
                    },
                    {
                        "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
                        "pdf_hash": "",
                        "start": 796,
                        "end": 1712,
                        "sentence_offsets": [
                            {
                                "start": 739,
                                "end": 1012
                            },
                            {
                                "start": 1015,
                                "end": 1129
                            },
                            {
                                "start": 1132,
                                "end": 1211
                            },
                            {
                                "start": 1212,
                                "end": 1355
                            },
                            {
                                "start": 1356,
                                "end": 1441
                            },
                            {
                                "start": 1442,
                                "end": 1514
                            },
                            {
                                "start": 1515,
                                "end": 1700
                            },
                            {
                                "start": 1701,
                                "end": 1932
                            }
                        ],
                        "ref_mentions": [
                            "219463386",
                            "209832341",
                            "234785837",
                            "265629619"
                        ],
                        "quote": "Even though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], [22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31]."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[271600495 | Lu et al. | 2024 | Citations: 0]",
                "snippets": "Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021). Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[147704286 | Dong et al. | 2019 | Citations: 1560]": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[247519241 | Du et al. | 2021 | Citations: 1554]": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25\u00d7 parameters of BERT Large , demonstrating its generalizability to different downstream tasks."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 580,
                        "end": 1818,
                        "sentence_offsets": [
                            {
                                "start": 580,
                                "end": 923
                            },
                            {
                                "start": 924,
                                "end": 1052
                            },
                            {
                                "start": 1053,
                                "end": 1264
                            },
                            {
                                "start": 1265,
                                "end": 1387
                            },
                            {
                                "start": 1388,
                                "end": 1582
                            },
                            {
                                "start": 1583,
                                "end": 1701
                            },
                            {
                                "start": 1702,
                                "end": 1818
                            }
                        ],
                        "ref_mentions": [
                            "204838007",
                            "204960716",
                            "147704286",
                            "218971783",
                            "247519241",
                            "218971783",
                            "247519241"
                        ],
                        "quote": "Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021). Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[272423598 | Gao et al. | 2024 | Citations: 2]",
                "snippets": "In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Frame-wise Block Causal Attention Mask",
                        "pdf_hash": "",
                        "start": 126,
                        "end": 616,
                        "sentence_offsets": [
                            {
                                "start": 126,
                                "end": 429
                            },
                            {
                                "start": 430,
                                "end": 616
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[272831970 | Li et al. | 2024 | Citations: 37]",
                "snippets": "The attention mechanism in LLM-based embedding models is typically unidirectional, aligned with the next-token prediction task fundamental to their pre-training (Touvron et al., 2023). \n\nHowever, recent studies indicate that unidirectional attention may limit the model's capacity for representation learning. Evidence suggests that bidirectional attention is more effective at integrating contextual information, resulting in improved performance on certain tasks. For example, LLM2Vec (BehnamGhader et al., 2024) introduces an additional training phase with a masked token prediction task, preconditioning the model for bidirectional attention. Approaches such as NV-Embed (Lee et al., 2024a) and GritLM (Campos et al., 2016) replace unidirectional attention with bidirectional attention during the embedding training phase, often employing mean pooling or more sophisticated latent attention layers to obtain representations for queries and passages. \n\nDespite these advances, we argue that incorporating bidirectional attention during embedding finetuning creates a mismatch with the model's pre-training design, potentially undermining its incontext learning and generative properties.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1289517 | Campos et al. | 2016 | Citations: 2745]": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community."
                },
                "metadata": [
                    {
                        "section_title": "REPRESENTATION METHOD",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1195,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 184
                            },
                            {
                                "start": 187,
                                "end": 309
                            },
                            {
                                "start": 310,
                                "end": 465
                            },
                            {
                                "start": 466,
                                "end": 646
                            },
                            {
                                "start": 647,
                                "end": 958
                            },
                            {
                                "start": 961,
                                "end": 1195
                            }
                        ],
                        "ref_mentions": [
                            "1289517"
                        ],
                        "quote": "The attention mechanism in LLM-based embedding models is typically unidirectional, aligned with the next-token prediction task fundamental to their pre-training (Touvron et al., 2023). \n\nHowever, recent studies indicate that unidirectional attention may limit the model's capacity for representation learning. Evidence suggests that bidirectional attention is more effective at integrating contextual information, resulting in improved performance on certain tasks. For example, LLM2Vec (BehnamGhader et al., 2024) introduces an additional training phase with a masked token prediction task, preconditioning the model for bidirectional attention. Approaches such as NV-Embed (Lee et al., 2024a) and GritLM (Campos et al., 2016) replace unidirectional attention with bidirectional attention during the embedding training phase, often employing mean pooling or more sophisticated latent attention layers to obtain representations for queries and passages. \n\nDespite these advances, we argue that incorporating bidirectional attention during embedding finetuning creates a mismatch with the model's pre-training design, potentially undermining its incontext learning and generative properties."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273532366 | Liu et al. | 2024 | Citations: 1]",
                "snippets": "Decoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models.\n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token.\n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Decoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models.\n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token.\n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[273877976 | Karagodin et al. | 2024 | Citations: 7]",
                "snippets": "Causal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1563,
                        "end": 2518,
                        "sentence_offsets": [
                            {
                                "start": 1563,
                                "end": 1746
                            },
                            {
                                "start": 1747,
                                "end": 1924
                            },
                            {
                                "start": 1925,
                                "end": 2124
                            },
                            {
                                "start": 2125,
                                "end": 2278
                            },
                            {
                                "start": 2279,
                                "end": 2518
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Causal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[274305758 | Yang et al. | 2024 | Citations: 0]",
                "snippets": "GPT is a one-way or causal language model that predicts the next word in a sequence by considering only the preceding words. In contrast, BERT is a bidirectional language model, which processes both forward and backward context simultaneously to improve feature extraction and representation capacity. Both models are based on the Transformer (Vaswani et al., 2017) architecture, which consists of two main components: the encoder and decoder. Transformer introduces a self-attention mechanism. For each position in the sequence, the self-attention mechanism computes the query, the key and value vectors, and the attention weight, and uses the attention weight to weigh the sum of the value vectors to get the final output representation. In the Transformer encoder architecture, each encoding layer's multi-head self-attention sublayer will interact with each position in the input sequence with all other positions in the sequence, so that the selfattention layer when encoding a word will consider the entire sentence's words when encoding a word. However, in the decoder, the self-attention layer uses a mask matrix so that each position in the sequence can only see the sequence before it and the positions behind it will be hidden. GPT employs the Transformer decoder, which uses masked self-attention to only attend to past tokens in the sequence, making it suitable for tasks like text generation. On the other hand, BERT utilizes the Transformer encoder, which allows it to attend to all tokens in the sequence, both before and after each word, making it ideal for tasks that require deep understanding of context, such as question answering and text classification.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                },
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 645,
                        "end": 2303,
                        "sentence_offsets": [
                            {
                                "start": 645,
                                "end": 769
                            },
                            {
                                "start": 770,
                                "end": 946
                            },
                            {
                                "start": 947,
                                "end": 1070
                            },
                            {
                                "start": 1071,
                                "end": 1121
                            },
                            {
                                "start": 1122,
                                "end": 1366
                            },
                            {
                                "start": 1367,
                                "end": 1678
                            },
                            {
                                "start": 1679,
                                "end": 1865
                            },
                            {
                                "start": 1866,
                                "end": 2033
                            },
                            {
                                "start": 2034,
                                "end": 2303
                            }
                        ],
                        "ref_mentions": [
                            "13756489"
                        ],
                        "quote": "GPT is a one-way or causal language model that predicts the next word in a sequence by considering only the preceding words. In contrast, BERT is a bidirectional language model, which processes both forward and backward context simultaneously to improve feature extraction and representation capacity. Both models are based on the Transformer (Vaswani et al., 2017) architecture, which consists of two main components: the encoder and decoder. Transformer introduces a self-attention mechanism. For each position in the sequence, the self-attention mechanism computes the query, the key and value vectors, and the attention weight, and uses the attention weight to weigh the sum of the value vectors to get the final output representation. In the Transformer encoder architecture, each encoding layer's multi-head self-attention sublayer will interact with each position in the input sequence with all other positions in the sequence, so that the selfattention layer when encoding a word will consider the entire sentence's words when encoding a word. However, in the decoder, the self-attention layer uses a mask matrix so that each position in the sequence can only see the sequence before it and the positions behind it will be hidden. GPT employs the Transformer decoder, which uses masked self-attention to only attend to past tokens in the sequence, making it suitable for tasks like text generation. On the other hand, BERT utilizes the Transformer encoder, which allows it to attend to all tokens in the sequence, both before and after each word, making it ideal for tasks that require deep understanding of context, such as question answering and text classification."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[274992300 | Katz et al. | 2024 | Citations: 1]",
                "snippets": "Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI.\n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approximately twice the number of parameters compared to decoder-only models with equivalent capacity.\n\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI.\n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approximately twice the number of parameters compared to decoder-only models with equivalent capacity.\n\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[275544523 | Khosla et al. | 2025 | Citations: 1]",
                "snippets": "Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details)...During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Modifying Attention",
                        "pdf_hash": "",
                        "start": 117,
                        "end": 974,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 132
                            },
                            {
                                "start": 133,
                                "end": 232
                            },
                            {
                                "start": 233,
                                "end": 391
                            },
                            {
                                "start": 392,
                                "end": 478
                            },
                            {
                                "start": 479,
                                "end": 491
                            },
                            {
                                "start": 492,
                                "end": 652
                            },
                            {
                                "start": 653,
                                "end": 781
                            },
                            {
                                "start": 782,
                                "end": 975
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details)"
                    },
                    {
                        "section_title": "Modifying Attention",
                        "pdf_hash": "",
                        "start": 1085,
                        "end": 1349,
                        "sentence_offsets": [
                            {
                                "start": 1085,
                                "end": 1348
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[276423946 | Busto-Castineira et al. | 2025 | Citations: 0]",
                "snippets": "Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder.\n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems.\n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51].\n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[208224776 | Park et al. | 2019 | Citations: 12]": "In this paper, we propose a novel data augmentation method with respect to the target context of the data via self-supervised learning. Instead of looking for the exact synonyms of masked words, the proposed method finds words that can replace the original words considering the context. For self-supervised learning, we can employ the masked language model (MLM), which masks a specific word within a sentence and obtains the original word. The MLM learns the context of a sentence through asymmetrical inputs and outputs. However, without using the existing MLM, we propose a label-masked language model (LMLM) that can include label information for the mask tokens used in the MLM to effectively use the MLM in data with label information. The augmentation method performs self-supervised learning using LMLM and then implements data augmentation through the trained model. We demonstrate that our proposed method improves the classification accuracy of recurrent neural networks and convolutional neural network-based classifiers through several experiments for text classification benchmark datasets, including the Stanford Sentiment Treebank-5 (SST5), the Stanford Sentiment Treebank-2 (SST2), the subjectivity (Subj), the Multi-Perspective Question Answering (MPQA), the Movie Reviews (MR), and the Text Retrieval Conference (TREC) datasets. In addition, since the proposed method does not use external data, it can eliminate the time spent collecting external data, or pre-training using external data.",
                    "[211069634 | Shen et al. | 2020 | Citations: 73]": "We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.",
                    "[212657570 | Yu et al. | 2020 | Citations: 34]": "We develop a chatbot using deep bidirectional transformer (BERT) models to handle client questions in financial investment customer service. The bot can recognize 381 intents, decides when to say I don\u2019t know, and escalate escalation/uncertain questions to human operators. Our main novel contribution is the discussion about the uncertainty measure for BERT, where three different approaches are systematically compared with real problems. We investigated two uncertainty metrics, information entropy and variance of dropout sampling, in BERT, followed by mixed-integer programming to optimize decision thresholds. Another novel contribution is the usage of BERT as a language model in automatic spelling correction. Inputs with accidental spelling errors can significantly decrease intent classification performance. The proposed approach combines probabilities from masked language model and word edit distances to find the best corrections for misspelled words. The chatbot and the entire conversational AI system are developed using open-source tools and deployed within our company\u2019s intranet. The proposed approach can be useful for industries seeking similar in-house solutions in their specific business domains. We share all our code and a sample chatbot built on a public data set on GitHub.",
                    "[218487230 | Kaneko et al. | 2020 | Citations: 146]": "This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods. Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.",
                    "[231645376 | Kawara et al. | 2021 | Citations: 17]": "The difference in word orders between source and target languages is a serious hurdle for machine translation. Preordering methods, which reorder the words in a source sentence before translation to obtain a similar word ordering with a target language, significantly improve the quality in statistical machine translation. While the information on the preordering position improved the translation quality in recurrent neural network-based models, questions such as how to use preordering information and whether it is helpful for the Transformer model remain unaddressed. In this article, we successfully employed preordering techniques in the Transformer-based neural machine translation. Specifically, we proposed a novel <italic>preordering encoding</italic> that exploits the reordering information of the source and target sentences as positional encoding in the Transformer model. Experimental results on ASPEC Japanese\u2013English and WMT 2015 English\u2013German, English\u2013Czech, and English\u2013Russian translation tasks confirmed that the proposed method significantly improved the translation quality evaluated by the BLEU scores of the Transformer model by <inline-formula><tex-math notation=\"LaTeX\">${\\text{1.34}}$</tex-math></inline-formula> points in the Japanese\u2013to\u2013English task, <inline-formula><tex-math notation=\"LaTeX\">${\\text{2.19}}$</tex-math></inline-formula> points in the English\u2013to\u2013German task, <inline-formula><tex-math notation=\"LaTeX\">${\\text{0.15}}$</tex-math></inline-formula> points in the Czech\u2013to\u2013English task, and <inline-formula><tex-math notation=\"LaTeX\">${\\text {1.48}}$</tex-math></inline-formula> points in the English\u2013to\u2013Russian task.",
                    "[231715684 | Chen et al. | 2021 | Citations: 70]": "Very deep transformers outperform conventional bi-directional long short-term memory networks for automatic speech recognition (ASR) by a significant margin. However, being autoregressive models, their computational complexity is still a prohibitive factor in their deployment into production systems. To amend this problem, we study two different non-autoregressive transformer structures for ASR: Audio-Conditional Masked Language Model (A-CMLM) and Audio-Factorized Masked Language Model (A-FMLM). When training these frameworks, the decoder input tokens are randomly replaced by special mask tokens. Then, the network is optimized to predict the masked tokens by taking both the unmasked context tokens and the input speech into consideration. During inference, we start from all masked tokens and the network iteratively predicts missing tokens based on partial results. A new decoding strategy is proposed as an example, which starts from the most confident predictions to the rest. Results on Mandarin (AISHELL), Japanese (CSJ), English (LibriSpeech) benchmarks show promising results to train such a non-autoregressive network for ASR. Especially in AISHELL, the proposed method outperformed the Kaldi ASR system and matched the performance of the state-of-the-art autoregressive transformer with $7\\times$ speedup.",
                    "[234785837 | Nguyen et al. | 2021 | Citations: 22]": "Transformer is a neural machine translation model which revolutionizes machine translation. Compared with traditional statistical machine translation models and other neural machine translation models, the recently proposed transformer model radically and fundamentally changes machine translation with its self-attention and cross-attention mechanisms. These mechanisms effectively model token alignments between source and target sentences. It has been reported that the transformer model provides accurate posterior alignments. In this work, we empirically prove the reverse effect, showing that prior alignments help transformer models produce better translations. Experiment results on Vietnamese-English news translation task show not only the positive effect of manually annotated alignments on transformer models but also the surprising outperformance of statistically constructed alignments reinforced with the flexibility of token-type selection over manual alignments in improving transformer models. Statistically constructed word-to-lemma alignments are used to train a word-to-word transformer model. The novel hybrid transformer model improves the baseline transformer model and transformer model trained with manual alignments by 2.53 and 0.79 BLEU, respectively. In addition to BLEU score, we make limited human judgment on translation results. Strong correlation between human and machine judgment confirms our findings.",
                    "[247126308 | Wang et al. | 2022 | Citations: 21]": "Spoken Language Understanding (SLU) aims to interpret the meanings of human speeches in order to support various human-machine interaction systems. A key technique for SLU is Automatic Speech Recognition (ASR), which transcribes speech signals into text contents. As the output texts of modern ASR systems unavoidably contain errors, mainstream SLU models either trained or tested on texts transcribed by ASR systems would not be sufficiently error robust. We present ARoBERT, an ASR Robust BERT model, which can be fine-tuned to solve a variety of SLU tasks with noisy inputs. To guarantee the robustness of ARoBERT, during pretraining, we decrease the fluctuations of language representations when some parts of the input texts are replaced by homophones or synophones. Specifically, we propose two novel self-supervised pre-training tasks for ARoBERT, namely Phonetically-aware Masked Language Modeling (PMLM) and ASR Model-adaptive Masked Language Modeling (AMMLM). The PMLM task explicitly fuses the knowledge of word phonetic similarities into the pre-training process, which forces homophones and synophones to share similar representations. In AMMLM, a data-driven algorithm is further introduced to mine typical ASR errors such that ARoBERT can tolerate ASR model errors. In the experiments, we evaluate ARoBERT over multiple datasets. The results show the superiority of ARoBERT, which consistently outperforms strong baselines. We have also shown that ARoBERT outperforms state-of-the-arts on a public benchmark. Currently, ARoBERT has been deployed in an online production system with significant improvements.",
                    "[265629619 | Zeng et al. | 2021 | Citations: 8]": "Machine reading comprehension (MRC) is a challenging natural language processing (NLP) task. It has a wide application potential in the fields of question answering robots, human-computer interactions in mobile virtual reality systems, etc. Recently, the emergence of pretrained models (PTMs) has brought this research field into a new era, in which the training objective plays a key role. The masked language model (MLM) is a self-supervised training objective widely used in various PTMs. With the development of training objectives, many variants of MLM have been proposed, such as whole word masking, entity masking, phrase masking, and span masking. In different MLMs, the length of the masked tokens is different. Similarly, in different machine reading comprehension tasks, the length of the answer is also different, and the answer is often a word, phrase, or sentence. Thus, in MRC tasks with different answer lengths, whether the length of MLM is related to performance is a question worth studying. If this hypothesis is true, it can guide us on how to pretrain the MLM with a relatively suitable mask length distribution for MRC tasks. In this paper, we try to uncover how much of MLM\u2019s success in the machine reading comprehension tasks comes from the correlation between masking length distribution and answer length in the MRC dataset. In order to address this issue, herein, (1) we propose four MRC tasks with different answer length distributions, namely, the short span extraction task, long span extraction task, short multiple-choice cloze task, and long multiple-choice cloze task; (2) four Chinese MRC datasets are created for these tasks; (3) we also have pretrained four masked language models according to the answer length distributions of these datasets; and (4) ablation experiments are conducted on the datasets to verify our hypothesis. The experimental results demonstrate that our hypothesis is true. On four different machine reading comprehension datasets, the performance of the model with correlation length distribution surpasses the model without correlation.",
                    "[267201220 | Wang et al. | 2024 | Citations: 3]": "This article presents a novel open-domain dialogue generation model emphasizing the differentiation of speakers in multi-turn conversations. Differing from prior work that treats the conversation history as a long text, we argue that capturing relative social relations among utterances (i.e., generated by either the same speaker or different persons) benefits the machine capturing fine-grained context information from a conversation history to improve context coherence in the generated response. Given that, we propose a Parallel Hierarchical Attentive Encoder-Decoder (PHAED) model that can effectively leverage conversation history by modeling each utterance with the awareness of its speaker and contextual associations with the same speaker's previous messages. Specifically, to distinguish the speaker roles over a multi-turn conversation (involving two speakers), we regard the utterances from one speaker as responses and those from the other as queries. After understanding queries via hierarchical encoder with inner-query and inter-query encodings, transformer-xl style decoder reuses the hidden states of previously generated responses to generate a new response. Our empirical results with three large-scale benchmarks show that PHAED significantly outperforms baseline models on both automatic and human evaluations. Furthermore, our ablation study shows that dialogue models with speaker tokens can generally decrease the possibility of generating non-coherent responses."
                },
                "metadata": [
                    {
                        "section_title": "Causality in generative transformer language models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1545,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 227
                            },
                            {
                                "start": 228,
                                "end": 458
                            },
                            {
                                "start": 461,
                                "end": 645
                            },
                            {
                                "start": 646,
                                "end": 777
                            },
                            {
                                "start": 778,
                                "end": 864
                            },
                            {
                                "start": 867,
                                "end": 947
                            },
                            {
                                "start": 948,
                                "end": 1146
                            },
                            {
                                "start": 1149,
                                "end": 1236
                            },
                            {
                                "start": 1237,
                                "end": 1312
                            },
                            {
                                "start": 1313,
                                "end": 1441
                            },
                            {
                                "start": 1442,
                                "end": 1548
                            }
                        ],
                        "ref_mentions": [
                            "13756489",
                            "231645376",
                            "234785837",
                            "231715684",
                            "247126308",
                            "218487230",
                            "208224776",
                            "212657570",
                            "267201220",
                            "265629619",
                            "211069634"
                        ],
                        "quote": "Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder.\n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems.\n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51].\n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[276771845 | Suganthan et al. | 2025 | Citations: 1]",
                "snippets": "Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is prevalent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                "metadata": [
                    {
                        "section_title": "Attention Masking",
                        "pdf_hash": "",
                        "start": 224,
                        "end": 1475,
                        "sentence_offsets": [
                            {
                                "start": 168,
                                "end": 283
                            },
                            {
                                "start": 286,
                                "end": 405
                            },
                            {
                                "start": 406,
                                "end": 592
                            },
                            {
                                "start": 595,
                                "end": 692
                            },
                            {
                                "start": 693,
                                "end": 801
                            },
                            {
                                "start": 802,
                                "end": 934
                            },
                            {
                                "start": 935,
                                "end": 1028
                            },
                            {
                                "start": 1031,
                                "end": 1137
                            },
                            {
                                "start": 1138,
                                "end": 1233
                            },
                            {
                                "start": 1234,
                                "end": 1347
                            },
                            {
                                "start": 1348,
                                "end": 1538
                            }
                        ],
                        "ref_mentions": [
                            "204838007"
                        ],
                        "quote": "Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is prevalent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[277349741 | Nie et al. | 2025 | Citations: 4]",
                "snippets": "\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                "metadata": [
                    {
                        "section_title": "Architecture",
                        "pdf_hash": "",
                        "start": 394,
                        "end": 1061,
                        "sentence_offsets": [
                            {
                                "start": 394,
                                "end": 411
                            },
                            {
                                "start": 412,
                                "end": 603
                            },
                            {
                                "start": 604,
                                "end": 665
                            },
                            {
                                "start": 666,
                                "end": 776
                            },
                            {
                                "start": 779,
                                "end": 800
                            },
                            {
                                "start": 801,
                                "end": 871
                            },
                            {
                                "start": 872,
                                "end": 994
                            },
                            {
                                "start": 995,
                                "end": 1061
                            }
                        ],
                        "ref_mentions": [
                            "160025533",
                            "218971783"
                        ],
                        "quote": "\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022)."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[277626724 | Zhang et al. | 2025 | Citations: 0]",
                "snippets": "1. Encoder has exactly the same architecture as the decoder-only model, but self-attention is switched from causal to bidirectional. We provide ablations in Section 6 that illustrate the critical effect of bidirectional attention on downstream performance.\n\n2. In each Decoder block, FFN and self-attention parts are identical to the corresponding parts in decoder-only models, and cross-attention has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Architecture",
                        "pdf_hash": "",
                        "start": 370,
                        "end": 882,
                        "sentence_offsets": [
                            {
                                "start": 253,
                                "end": 402
                            },
                            {
                                "start": 403,
                                "end": 446
                            },
                            {
                                "start": 449,
                                "end": 581
                            },
                            {
                                "start": 582,
                                "end": 705
                            },
                            {
                                "start": 708,
                                "end": 962
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "1. Encoder has exactly the same architecture as the decoder-only model, but self-attention is switched from causal to bidirectional. We provide ablations in Section 6 that illustrate the critical effect of bidirectional attention on downstream performance.\n\n2. In each Decoder block, FFN and self-attention parts are identical to the corresponding parts in decoder-only models, and cross-attention has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[278202626 | Hallee et al. | 2025 | Citations: 1]",
                "snippets": "GPT (Generative Pretrained Transformer) models, such as OpenAI's GPT series (GPT-3, GPT-4, etc.), are designed for generative tasks and use transformer decoders [36][37][38] . They employ causal (unidirectional) attention, meaning each token attends only to previous tokens in the sequence, enabling autoregressive generation during inference. This allows them to predict the next word in a sequence without direct access to future words. \n\nIn contrast, BERT models utilize transformer encoders with bidirectional attention, meaning they can attend to all tokens within an input simultaneously. This structure enables them to capture additional contextual dependencies, making them well-suited for tasks like text classification and sentence similarity 39 . Unlike GPT models, BERT is trained using a masked language modeling (MLM) objective, where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Transformer neural networks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 951,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            },
                            {
                                "start": 176,
                                "end": 343
                            },
                            {
                                "start": 344,
                                "end": 438
                            },
                            {
                                "start": 441,
                                "end": 594
                            },
                            {
                                "start": 595,
                                "end": 757
                            },
                            {
                                "start": 758,
                                "end": 951
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "GPT (Generative Pretrained Transformer) models, such as OpenAI's GPT series (GPT-3, GPT-4, etc.), are designed for generative tasks and use transformer decoders [36][37][38] . They employ causal (unidirectional) attention, meaning each token attends only to previous tokens in the sequence, enabling autoregressive generation during inference. This allows them to predict the next word in a sequence without direct access to future words. \n\nIn contrast, BERT models utilize transformer encoders with bidirectional attention, meaning they can attend to all tokens within an input simultaneously. This structure enables them to capture additional contextual dependencies, making them well-suited for tasks like text classification and sentence similarity 39 . Unlike GPT models, BERT is trained using a masked language modeling (MLM) objective, where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.043323,
        "cot": "After analyzing the user query and the provided quotes, I need to organize information about causal (unidirectional) and non-causal (bidirectional) attention mechanisms in different language model architectures.\n\nFirst, let me plan the key dimensions needed to address this query:\n\n1. \"Introduction\" - This section should provide background on attention mechanisms in language models, explaining the fundamental differences between causal and non-causal attention. I'll need to explain what these mechanisms are at a basic level before diving into their specific implementations.\n\n2. \"Causal vs. Non-Causal Attention Mechanisms\" - This dimension should focus on the technical differences between these two approaches, including how information flows and their mathematical/computational differences. This should be a synthesis to explain the core differences.\n\n3. \"Encoder-Only Models\" - This dimension should cover how non-causal (bidirectional) attention is implemented in encoder-only models like BERT, what advantages it provides, and what tasks these models excel at. This will be a synthesis of the information.\n\n4. \"Decoder-Only Models\" - This dimension needs to address how causal (unidirectional) attention works in traditional decoder-only models like GPT, as well as covering hybrid approaches like prefix LMs that combine both attention types. This will be a synthesis.\n\n5. \"Encoder-Decoder Models\" - This dimension should explain how these architectures implement both attention mechanisms (bidirectional in encoder, unidirectional in decoder), and how cross-attention connects them. This will be a synthesis.\n\n6. \"Suitability for NLP Tasks\" - This dimension should directly address which model architectures and attention mechanisms are most suited for different NLP applications, including text generation, understanding, and sequence-to-sequence tasks. I'll format this as a list to clearly differentiate the task types and corresponding model architectures.\n\nThis organization will allow me to comprehensively address all aspects of the user's query while maintaining a logical flow from fundamental concepts to specific applications.",
        "plan": {
            "Introduction (synthesis)": [],
            "Causal vs. Non-Causal Attention Mechanisms (synthesis)": [
                4,
                11,
                15,
                26,
                27,
                29,
                32,
                33,
                34
            ],
            "Encoder-Only Models (synthesis)": [
                13,
                14,
                22,
                30,
                31,
                37
            ],
            "Decoder-Only Models (synthesis)": [
                1,
                8,
                10,
                12,
                18,
                19,
                20,
                21,
                23,
                25,
                28,
                31,
                35
            ],
            "Encoder-Decoder Models (synthesis)": [
                0,
                2,
                3,
                5,
                6,
                9,
                10,
                16,
                17,
                21,
                24,
                28,
                36
            ],
            "Hybrid Attention Approaches (synthesis)": [
                5,
                7,
                8,
                25,
                32,
                34
            ],
            "Suitability for NLP Tasks (list)": [
                1,
                6,
                13,
                14,
                16,
                19,
                22,
                28,
                31
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction",
                "tldr": "Attention mechanisms enable language models to process contextual information, with causal attention looking only at past tokens and non-causal attention accessing the entire sequence. Different model architectures implement these attention mechanisms in distinct ways, affecting their capabilities and suitable applications. (LLM Memory)",
                "text": "\nAttention mechanisms are fundamental components of modern language models that determine how these models process and utilize contextual information. The distinction between causal (unidirectional) and non-causal (bidirectional) attention represents one of the most important architectural choices in natural language processing systems. Causal attention restricts a model to only attend to previous tokens in a sequence, creating a left-to-right information flow that mimics how humans generate text. In contrast, non-causal attention allows a model to access the entire sequence simultaneously, providing a comprehensive view of context from both directions. This fundamental difference in information access significantly impacts how language models function across different architectures\u2014encoder-only, decoder-only, and encoder-decoder frameworks\u2014each optimized for specific tasks in the NLP landscape. Understanding these differences is crucial for selecting the appropriate model architecture for tasks ranging from text generation and summarization to comprehension and translation. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Causal vs. Non-Causal Attention Mechanisms",
                "tldr": "Causal attention restricts information flow to previous tokens only, enabling autoregressive text generation while non-causal attention allows bidirectional information exchange between all tokens, facilitating comprehensive understanding of context. (11 sources)",
                "text": "\nCausal (unidirectional) and non-causal (bidirectional) attention mechanisms represent fundamentally different approaches to processing sequence information in language models. In causal attention, each token can only attend to itself and preceding tokens, creating a left-to-right information flow where \"subsequent tokens can 'see' preceding tokens, but preceding tokens cannot 'see' subsequent tokens\" <Paper corpusId=\"272423598\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This unidirectional design is essential for autoregressive text generation, as it \"ensures that during text aggressive generation, historical token information is not leaked\" <Paper corpusId=\"272423598\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\nThe causal mechanism is particularly valuable for sequence generation tasks because it \"preserves the correct temporal order\" <Paper corpusId=\"273877976\" paperTitle=\"(Karagodin et al., 2024)\" isShortName></Paper>. During inference, this allows the model to generate text one token at a time, using only previously generated tokens to inform the next prediction. This process is computationally efficient as \"each token is produced in a forward pass without needing to revisit previous steps\" <Paper corpusId=\"273877976\" paperTitle=\"(Karagodin et al., 2024)\" isShortName></Paper>.\n\nIn contrast, non-causal (bidirectional) attention allows each token to attend to all other tokens in the sequence, regardless of position. As described by Lin et al., \"both subsequent and preceding tokens can assimilate information from each other\" <Paper corpusId=\"267759571\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. This provides the model with a \"robust information encoding capability\" <Paper corpusId=\"267759571\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>, allowing it to develop a comprehensive understanding of the entire sequence. BERT exemplifies this approach, as it is \"designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers\" <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nThe architectural choice between causal and non-causal attention creates a fundamental trade-off. Bidirectional models are generally more powerful for understanding tasks because \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than a left-to-right model\" <Paper corpusId=\"248377166\" paperTitle=\"(Chaffin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. However, bidirectional models lack the autoregressive capability necessary for efficient text generation. When a new token is added to the sequence in a bidirectional model, \"every hidden states need to be re-computed\" <Paper corpusId=\"248377166\" paperTitle=\"(Chaffin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>, making them less suitable for generative tasks.\n\nConverting between these attention types is not trivial. As Lv et al. note, \"Converting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask\" <Paper corpusId=\"265150001\" paperTitle=\"(Lv et al., 2023)\" isShortName></Paper>. This is because doing so would introduce positional information patterns that the model was never exposed to during training.\n\nHybrid approaches have emerged to leverage the benefits of both attention types. PrefixLM, introduced in T5, represents one such approach that \"utilizes causal masking with a designated 'prefix' section\" <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. In this design, the prefix is processed bidirectionally while the remaining sequence uses causal attention, \"combining the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence\" <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>.\n\nOther hybrid approaches include the context-span method described by Khosla et al., where \"context tokens attend to all other context tokens\" bidirectionally, while \"span tokens are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves\" <Paper corpusId=\"275544523\" paperTitle=\"(Khosla et al., 2025)\" isShortName></Paper>. This configuration allows the model to operate in multiple modes: \"fully causal/unidirectional for open-ended text generation tasks, fully bidirectional representation learning tasks, or a combination of causal and bidirectional for text infilling\" <Paper corpusId=\"275544523\" paperTitle=\"(Khosla et al., 2025)\" isShortName></Paper>.\n\nThe choice between causal and non-causal attention mechanisms ultimately depends on the intended application. Causal attention is the foundation of \"decoder-only transformers are the most effective task-agnostic NLG systems\" <Paper corpusId=\"276423946\" paperTitle=\"(Busto-Castineira et al., 2025)\" isShortName></Paper>, while non-causal attention excels in understanding tasks and specialized generation scenarios like \"speech recognition, style transfer and grammar correction, textual data augmentation, and dialog systems\" <Paper corpusId=\"276423946\" paperTitle=\"(Busto-Castineira et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text."
                        ],
                        "paper": {
                            "corpus_id": 272423598,
                            "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
                            "authors": [
                                {
                                    "authorId": "2319805585",
                                    "name": "Mingze Gao"
                                },
                                {
                                    "authorId": "2302790279",
                                    "name": "Jingyu Liu"
                                },
                                {
                                    "authorId": "2302785092",
                                    "name": "Mingda Li"
                                },
                                {
                                    "authorId": "2319964588",
                                    "name": "Jiangtao Xie"
                                },
                                {
                                    "authorId": "2258682951",
                                    "name": "Qingbin Liu"
                                },
                                {
                                    "authorId": "2304448412",
                                    "name": "Bo Zhao"
                                },
                                {
                                    "authorId": "2302990371",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "2319814814",
                                    "name": "Hui Xiong"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.6767578125
                    },
                    {
                        "id": "(Karagodin et al., 2024)",
                        "snippets": [
                            "Causal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output."
                        ],
                        "paper": {
                            "corpus_id": 273877976,
                            "title": "Clustering in Causal Attention Masking",
                            "authors": [
                                {
                                    "authorId": "2329559804",
                                    "name": "Nikita Karagodin"
                                },
                                {
                                    "authorId": "2300093577",
                                    "name": "Yury Polyanskiy"
                                },
                                {
                                    "authorId": "2275197338",
                                    "name": "Philippe Rigollet"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7
                        },
                        "score": 0.60400390625
                    },
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "\u2022 Full attention (Devlin et al., 2019)(Feng et al., 2020). This bidirectional attention operates between every pair of tokens in the sequence. Notably, both subsequent and preceding tokens can assimilate information from each other. The robust information encoding capability afforded by full attention designates the model as a transformer-encoder, specialized for tasks involving understanding. \u2022 Masking attention (Brown et al., 2020)[3]. In this directional attention model, interactions occur exclusively from left to right within the token sequence. Consequently, only the subsequent token can integrate information from preceding tokens, creating a unidirectional flow. Masking attention aligns with standard causal language modeling and is denoted as a transformerdecoder, tailored for tasks centered around generation."
                        ],
                        "paper": {
                            "corpus_id": 267759571,
                            "title": "Scaling Laws Behind Code Understanding Model",
                            "authors": [
                                {
                                    "authorId": "2211900186",
                                    "name": "Jiayi Lin"
                                },
                                {
                                    "authorId": "2260535450",
                                    "name": "Hande Dong"
                                },
                                {
                                    "authorId": "2260831632",
                                    "name": "Yutao Xie"
                                },
                                {
                                    "authorId": "2261093743",
                                    "name": "Lei Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.5048828125
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chaffin et al., 2022)",
                        "snippets": [
                            "By default, attention layers as defined in (Vaswani et al., 2017) are bidirectional: every token can attend to tokens at every position. When it comes to discrimination, models based on such bidirectional attention are commonly used since \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than [",
                            "] a left-to-right model\" (Devlin et al., 2019). However, while it brings some capacity to the model, it also makes it non auto-regressive: when a token is added at the end of a sequence, every hidden states need to be re-computed. \n\nOne way to train a transformer based LM for text generation is to use unidirectional attention masks [29]. In this unidirectional setting, any extra token added at the end of a sequence does not change the already calculated hidden states, since previous tokens do not attend to it. Thus, starting from an already classified sequence  1: \u22121 , classifying  1: only requires to compute  attention scores, rather than the whole set of  2 scores per self-attention layer, as it would be required in the bidirectional setting."
                        ],
                        "paper": {
                            "corpus_id": 248377166,
                            "title": "Which Discriminator for Cooperative Text Generation?",
                            "authors": [
                                {
                                    "authorId": "2129106958",
                                    "name": "Antoine Chaffin"
                                },
                                {
                                    "authorId": "90745780",
                                    "name": "Thomas Scialom"
                                },
                                {
                                    "authorId": "1782552",
                                    "name": "S. Lamprier"
                                },
                                {
                                    "authorId": "1767493",
                                    "name": "Jacopo Staiano"
                                },
                                {
                                    "authorId": "1703777",
                                    "name": "Benjamin Piwowarski"
                                },
                                {
                                    "authorId": "1801242",
                                    "name": "Ewa Kijak"
                                },
                                {
                                    "authorId": "1735666",
                                    "name": "V. Claveau"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 4
                        },
                        "score": 0.5556640625
                    },
                    {
                        "id": "(Vaswani et al., 2017)",
                        "snippets": [
                            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                        ],
                        "paper": {
                            "corpus_id": 13756489,
                            "title": "Attention is All you Need",
                            "authors": [
                                {
                                    "authorId": "40348417",
                                    "name": "Ashish Vaswani"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3877127",
                                    "name": "Niki Parmar"
                                },
                                {
                                    "authorId": "39328010",
                                    "name": "Jakob Uszkoreit"
                                },
                                {
                                    "authorId": "145024664",
                                    "name": "Llion Jones"
                                },
                                {
                                    "authorId": "19177000",
                                    "name": "Aidan N. Gomez"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "3443442",
                                    "name": "I. Polosukhin"
                                }
                            ],
                            "year": 2017,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 132444
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lv et al., 2023)",
                        "snippets": [
                            "Converting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask, as doing so would introduce positional information that the model has never encountered during training, in which stage a query vector is only allowed to calculate the inner product with its preceding key vectors. This is evident in Eq.4: the relative position n \u2212 m is always non-positive during training but is positive when q m needs to attend to k >m."
                        ],
                        "paper": {
                            "corpus_id": 265150001,
                            "title": "An Analysis and Mitigation of the Reversal Curse",
                            "authors": [
                                {
                                    "authorId": "66261602",
                                    "name": "Ang Lv"
                                },
                                {
                                    "authorId": "2266486204",
                                    "name": "Kaiyi Zhang"
                                },
                                {
                                    "authorId": "1889683",
                                    "name": "Shufang Xie"
                                },
                                {
                                    "authorId": "2071635049",
                                    "name": "Quan Tu"
                                },
                                {
                                    "authorId": "2266420263",
                                    "name": "Yuhan Chen"
                                },
                                {
                                    "authorId": "2263887786",
                                    "name": "Ji-Rong Wen"
                                },
                                {
                                    "authorId": "2172312251",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 21
                        },
                        "score": 0.5771484375
                    },
                    {
                        "id": "(Suganthan et al., 2025)",
                        "snippets": [
                            "Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is prevalent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."
                        ],
                        "paper": {
                            "corpus_id": 276771845,
                            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                            "authors": [
                                {
                                    "authorId": "1658871094",
                                    "name": "P. Suganthan"
                                },
                                {
                                    "authorId": "2165469946",
                                    "name": "Fedor Moiseev"
                                },
                                {
                                    "authorId": "2348489099",
                                    "name": "Le Yan"
                                },
                                {
                                    "authorId": "2261361394",
                                    "name": "Junru Wu"
                                },
                                {
                                    "authorId": "2348507846",
                                    "name": "Jianmo Ni"
                                },
                                {
                                    "authorId": "2348488953",
                                    "name": "Jay Han"
                                },
                                {
                                    "authorId": "1954563",
                                    "name": "I. Zitouni"
                                },
                                {
                                    "authorId": "1727837",
                                    "name": "Enrique Alfonseca"
                                },
                                {
                                    "authorId": "2348422460",
                                    "name": "Xuanhui Wang"
                                },
                                {
                                    "authorId": "2349772191",
                                    "name": "Zhe Dong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.82861328125
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khosla et al., 2025)",
                        "snippets": [
                            "Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details)",
                            "During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling."
                        ],
                        "paper": {
                            "corpus_id": 275544523,
                            "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
                            "authors": [
                                {
                                    "authorId": "2056070459",
                                    "name": "Savya Khosla"
                                },
                                {
                                    "authorId": "2218199485",
                                    "name": "Aditi Tiwari"
                                },
                                {
                                    "authorId": "33315685",
                                    "name": "Kushal Kafle"
                                },
                                {
                                    "authorId": "2297849207",
                                    "name": "Simon Jenni"
                                },
                                {
                                    "authorId": "2341050386",
                                    "name": "Handong Zhao"
                                },
                                {
                                    "authorId": "2288642908",
                                    "name": "John P. Collomosse"
                                },
                                {
                                    "authorId": "2288445861",
                                    "name": "Jing Shi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.76513671875
                    },
                    {
                        "id": "(Busto-Castineira et al., 2025)",
                        "snippets": [
                            "Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder.\n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems.\n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51].\n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way."
                        ],
                        "paper": {
                            "corpus_id": 276423946,
                            "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
                            "authors": [
                                {
                                    "authorId": "2222734467",
                                    "name": "Andrea Busto-Casti\u00f1eira"
                                },
                                {
                                    "authorId": "1405165681",
                                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                                },
                                {
                                    "authorId": "2326130687",
                                    "name": "Francisco de Arriba-P\u00e9rez"
                                },
                                {
                                    "authorId": "1395988865",
                                    "name": "F. Gonz\u00e1lez-Casta\u00f1o"
                                }
                            ],
                            "year": 2025,
                            "venue": "Pattern Recognition Letters",
                            "n_citations": 0
                        },
                        "score": 0.6884765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Encoder-Only Models",
                "tldr": "Encoder-only models like BERT use bidirectional attention to understand context from both directions simultaneously, making them excellent for comprehension tasks like classification, question answering, and natural language inference, but less efficient for text generation. (10 sources)",
                "text": "\nEncoder-only models represent a major architectural approach in the landscape of language models, with BERT (Bidirectional Encoder Representations from Transformers) being the most prominent example. These models are distinguished by their use of bidirectional self-attention mechanisms, which allow each token to attend to all other tokens in the sequence regardless of position <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. This bidirectional processing enables the models to \"capture contextual information bidirectionally, fostering a comprehensive understanding of language semantics\" <Paper corpusId=\"267701011\" paperTitle=\"(Cascella et al., 2024)\" isShortName></Paper>.\n\nThe architecture of encoder-only models is based on the Transformer encoder, which \"allows it to attend to all tokens in the sequence, both before and after each word\" <Paper corpusId=\"274305758\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. During processing, the input is tokenized, and some tokens may be masked. These tokens are then \"fed into Transformer blocks with self-attention to obtain contextualized output embeddings\" <Paper corpusId=\"270703043\" paperTitle=\"(Posada et al., 2024)\" isShortName></Paper>. The resulting embeddings can be processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads.\n\nThe bidirectional nature of encoder-only models makes them particularly well-suited for \"natural language understanding (NLU) tasks including sentence similarity, natural language inference, and question answering\" <Paper corpusId=\"267312283\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper> <Paper corpusId=\"3432876\" paperTitle=\"(Williams et al., 2017)\" isShortName></Paper> <Paper corpusId=\"47018994\" paperTitle=\"(Rajpurkar et al., 2018)\" isShortName></Paper> <Paper corpusId=\"11816014\" paperTitle=\"(Rajpurkar et al., 2016)\" isShortName></Paper>. These models \"excel in applications such as classification and question answering\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>, where the ability to process context from both directions simultaneously provides significant advantages for understanding the meaning and relationships between elements in text.\n\nHowever, encoder-only models face limitations when it comes to text generation tasks. While they can theoretically generate text autoregressively, this process is inefficient because \"each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>. Unlike decoder-only models that can use a key-value (KV) cache to efficiently generate multiple tokens during inference, \"BERT's bidirectional design prevents such optimization,\" making it \"impractical for token-by-token decoding\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>.\n\nThe training objective for encoder-only models like BERT typically involves masked language modeling (MLM), \"where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context\" <Paper corpusId=\"278202626\" paperTitle=\"(Hallee et al., 2025)\" isShortName></Paper>. This approach helps the model develop a deep understanding of language semantics and contextual relationships, further enhancing its capabilities for comprehension tasks.\n\nIn summary, encoder-only models are primarily designed for \"discriminative tasks\" <Paper corpusId=\"270703043\" paperTitle=\"(Posada et al., 2024)\" isShortName></Paper> that require a thorough understanding of text, leveraging their bidirectional attention mechanisms to build rich contextual representations. While highly effective for understanding tasks, they are less suitable for the efficient generation of text when compared to their decoder-only counterparts.",
                "citations": [
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cascella et al., 2024)",
                        "snippets": [
                            "Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial."
                        ],
                        "paper": {
                            "corpus_id": 267701011,
                            "title": "The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives",
                            "authors": [
                                {
                                    "authorId": "2265474928",
                                    "name": "M. Cascella"
                                },
                                {
                                    "authorId": "2265474397",
                                    "name": "Federico Semeraro"
                                },
                                {
                                    "authorId": "4239573",
                                    "name": "J. Montomoli"
                                },
                                {
                                    "authorId": "8663537",
                                    "name": "Valentina Bellini"
                                },
                                {
                                    "authorId": "2239310513",
                                    "name": "Ornella Piazza"
                                },
                                {
                                    "authorId": "2198023579",
                                    "name": "E. Bignami"
                                }
                            ],
                            "year": 2024,
                            "venue": "J. Medical Syst.",
                            "n_citations": 83
                        },
                        "score": 0.51123046875
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "GPT is a one-way or causal language model that predicts the next word in a sequence by considering only the preceding words. In contrast, BERT is a bidirectional language model, which processes both forward and backward context simultaneously to improve feature extraction and representation capacity. Both models are based on the Transformer (Vaswani et al., 2017) architecture, which consists of two main components: the encoder and decoder. Transformer introduces a self-attention mechanism. For each position in the sequence, the self-attention mechanism computes the query, the key and value vectors, and the attention weight, and uses the attention weight to weigh the sum of the value vectors to get the final output representation. In the Transformer encoder architecture, each encoding layer's multi-head self-attention sublayer will interact with each position in the input sequence with all other positions in the sequence, so that the selfattention layer when encoding a word will consider the entire sentence's words when encoding a word. However, in the decoder, the self-attention layer uses a mask matrix so that each position in the sequence can only see the sequence before it and the positions behind it will be hidden. GPT employs the Transformer decoder, which uses masked self-attention to only attend to past tokens in the sequence, making it suitable for tasks like text generation. On the other hand, BERT utilizes the Transformer encoder, which allows it to attend to all tokens in the sequence, both before and after each word, making it ideal for tasks that require deep understanding of context, such as question answering and text classification."
                        ],
                        "paper": {
                            "corpus_id": 274305758,
                            "title": "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?",
                            "authors": [
                                {
                                    "authorId": "2332727631",
                                    "name": "Lewen Yang"
                                },
                                {
                                    "authorId": "2332813122",
                                    "name": "Xuanyu Zhou"
                                },
                                {
                                    "authorId": "2332600501",
                                    "name": "Juao Fan"
                                },
                                {
                                    "authorId": "2332733783",
                                    "name": "Xinyi Xie"
                                },
                                {
                                    "authorId": "2332541087",
                                    "name": "Shengxin Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 2nd International Conference on Foundation and Large Language Models (FLLM)",
                            "n_citations": 0
                        },
                        "score": 0.701171875
                    },
                    {
                        "id": "(Posada et al., 2024)",
                        "snippets": [
                            "Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively."
                        ],
                        "paper": {
                            "corpus_id": 270703043,
                            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
                            "authors": [
                                {
                                    "authorId": "2308036602",
                                    "name": "Andrea Posada"
                                },
                                {
                                    "authorId": "2091163163",
                                    "name": "D. Rueckert"
                                },
                                {
                                    "authorId": "2126502908",
                                    "name": "Felix Meissen"
                                },
                                {
                                    "authorId": "1399796227",
                                    "name": "Philip M\u00fcller"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.5234375
                    },
                    {
                        "id": "(Park et al., 2024)",
                        "snippets": [
                            "Encoder-only Transformers (a) [25,(Devlin et al., 2019)83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,(Dolan et al., 2005), natural language inference [140](Williams et al., 2017), and question answering (Rajpurkar et al., 2018)(Rajpurkar et al., 2016). On the other hand, decoder-only Transformers (b) (Brown et al., 2020)124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration."
                        ],
                        "paper": {
                            "corpus_id": 267312283,
                            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
                            "authors": [
                                {
                                    "authorId": "2108418112",
                                    "name": "Seungcheol Park"
                                },
                                {
                                    "authorId": "2281832669",
                                    "name": "Jaehyeon Choi"
                                },
                                {
                                    "authorId": "2281792832",
                                    "name": "Sojin Lee"
                                },
                                {
                                    "authorId": "2281746333",
                                    "name": "U. Kang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 16
                        },
                        "score": 0.52587890625
                    },
                    {
                        "id": "(Williams et al., 2017)",
                        "snippets": [
                            "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement."
                        ],
                        "paper": {
                            "corpus_id": 3432876,
                            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
                            "authors": [
                                {
                                    "authorId": "81840293",
                                    "name": "Adina Williams"
                                },
                                {
                                    "authorId": "10666396",
                                    "name": "Nikita Nangia"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                }
                            ],
                            "year": 2017,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 4497
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rajpurkar et al., 2018)",
                        "snippets": [
                            "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
                        ],
                        "paper": {
                            "corpus_id": 47018994,
                            "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD",
                            "authors": [
                                {
                                    "authorId": "2706258",
                                    "name": "Pranav Rajpurkar"
                                },
                                {
                                    "authorId": "3422908",
                                    "name": "Robin Jia"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                }
                            ],
                            "year": 2018,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 2854
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rajpurkar et al., 2016)",
                        "snippets": [
                            "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"
                        ],
                        "paper": {
                            "corpus_id": 11816014,
                            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                            "authors": [
                                {
                                    "authorId": "2706258",
                                    "name": "Pranav Rajpurkar"
                                },
                                {
                                    "authorId": "2151810148",
                                    "name": "Jian Zhang"
                                },
                                {
                                    "authorId": "2787620",
                                    "name": "Konstantin Lopyrev"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                }
                            ],
                            "year": 2016,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 8174
                        },
                        "score": 0
                    },
                    {
                        "id": "(Katz et al., 2024)",
                        "snippets": [
                            "Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI.\n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approximately twice the number of parameters compared to decoder-only models with equivalent capacity.\n\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design."
                        ],
                        "paper": {
                            "corpus_id": 274992300,
                            "title": "Segment-Based Attention Masking for GPTs",
                            "authors": [
                                {
                                    "authorId": "121254633",
                                    "name": "Shahar Katz"
                                },
                                {
                                    "authorId": "2186740854",
                                    "name": "Liran Ringel"
                                },
                                {
                                    "authorId": "2335566528",
                                    "name": "Yaniv Romano"
                                },
                                {
                                    "authorId": "2284763723",
                                    "name": "Lior Wolf"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.61474609375
                    },
                    {
                        "id": "(Hallee et al., 2025)",
                        "snippets": [
                            "GPT (Generative Pretrained Transformer) models, such as OpenAI's GPT series (GPT-3, GPT-4, etc.), are designed for generative tasks and use transformer decoders [36][37][38] . They employ causal (unidirectional) attention, meaning each token attends only to previous tokens in the sequence, enabling autoregressive generation during inference. This allows them to predict the next word in a sequence without direct access to future words. \n\nIn contrast, BERT models utilize transformer encoders with bidirectional attention, meaning they can attend to all tokens within an input simultaneously. This structure enables them to capture additional contextual dependencies, making them well-suited for tasks like text classification and sentence similarity 39 . Unlike GPT models, BERT is trained using a masked language modeling (MLM) objective, where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context."
                        ],
                        "paper": {
                            "corpus_id": 278202626,
                            "title": "Contrastive learning and mixture of experts enables precise vector embeddings in biological databases",
                            "authors": [
                                {
                                    "authorId": "2143308913",
                                    "name": "Logan Hallee"
                                },
                                {
                                    "authorId": "2281747985",
                                    "name": "Rohan Kapur"
                                },
                                {
                                    "authorId": "2281850189",
                                    "name": "Arjun Patel"
                                },
                                {
                                    "authorId": "4561626",
                                    "name": "Jason P. Gleghorn"
                                },
                                {
                                    "authorId": "2281747963",
                                    "name": "Bohdan B. Khomtchouk"
                                }
                            ],
                            "year": 2025,
                            "venue": "Scientific Reports",
                            "n_citations": 1
                        },
                        "score": 0.70068359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Decoder-Only Models",
                "tldr": "Decoder-only models use either causal attention (allowing tokens to only attend to previous tokens) or prefix decoder attention (combining bidirectional attention for prefixes with causal attention for generation), making them particularly effective for text generation tasks while maintaining computational efficiency during inference. (10 sources)",
                "text": "\nDecoder-only models have emerged as the dominant architecture for modern large language models, with two main variants: causal decoder and prefix decoder architectures <Paper corpusId=\"242033930\" paperTitle=\"(Hommel et al., 2021)\" isShortName></Paper> <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. The distinction between these variants lies primarily in their attention mechanisms, which determine how information flows through the model.\n\nThe causal decoder architecture, exemplified by the GPT series of models, employs a unidirectional attention mechanism that restricts each token to attend only to itself and preceding tokens <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>. This design ensures an autoregressive generation process with a strictly left-to-right information flow <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. The unidirectional constraint is implemented through a specific attention mask that prevents tokens from accessing information from future positions <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This architecture has become the standard for most modern large language models, including PaLM, LLaMA, OPT, and BLOOM <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>.\n\nThe prefix decoder architecture represents a hybrid approach that combines elements of both causal and non-causal attention. In this design, bidirectional attention is applied to the prefix tokens (allowing them to attend to both preceding and succeeding tokens), while unidirectional attention is maintained for the generated tokens <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This arrangement enables \"flexible and controlled generation, conditioned on both the prefix and the generated tokens\" <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. Notable models using this architecture include U-PaLM and GLM-130B <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277349741\" paperTitle=\"(Nie et al., 2025)\" isShortName></Paper>.\n\nThe popularity of decoder-only models for text generation stems from their efficiency in autoregressive processing. Unlike encoder-only models that would need to recompute all hidden states when generating new tokens, decoder-only models can use a key-value (KV) cache for efficient token-by-token generation <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>. This makes them \"highly efficient for token-by-token generation\" during inference <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>.\n\nHowever, the causal attention mechanism in standard decoder-only models does create limitations for certain applications. The restricted information flow means that \"the representation of each word is restricted to depend solely on the words that came before,\" which can be suboptimal for tasks requiring rich contextual understanding <Paper corpusId=\"269981935\" paperTitle=\"(Kopiczko et al., 2024)\" isShortName></Paper>. This limitation has led some researchers to suggest that \"the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations\" <Paper corpusId=\"269009682\" paperTitle=\"(BehnamGhader et al., 2024)\" isShortName></Paper>.\n\nDespite these limitations, decoder-only architectures\u2014particularly causal decoders\u2014have become the dominant choice for large language models focused on text generation <Paper corpusId=\"269981935\" paperTitle=\"(Kopiczko et al., 2024)\" isShortName></Paper>. Their ability to efficiently generate coherent text in an autoregressive manner makes them well-suited for a wide range of applications, while prefix decoders offer a compromise that provides some bidirectional context understanding while maintaining generation capabilities <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Hommel et al., 2021)",
                        "snippets": [
                            "One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; (Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados et al., 2018)Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation."
                        ],
                        "paper": {
                            "corpus_id": 242033930,
                            "title": "Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation",
                            "authors": [
                                {
                                    "authorId": "2136597350",
                                    "name": "Bj\u00f6rn E. Hommel"
                                },
                                {
                                    "authorId": "2138754315",
                                    "name": "Franz-Josef M. Wollang"
                                },
                                {
                                    "authorId": "2137277509",
                                    "name": "Veronika Kotova"
                                },
                                {
                                    "authorId": "50999175",
                                    "name": "H. Zacher"
                                },
                                {
                                    "authorId": "5278036",
                                    "name": "S. Schmukle"
                                }
                            ],
                            "year": 2021,
                            "venue": "Psychometrika",
                            "n_citations": 21
                        },
                        "score": 0.52392578125
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture",
                            "Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b).\n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a)."
                        ],
                        "paper": {
                            "corpus_id": 261064777,
                            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
                            "authors": [
                                {
                                    "authorId": "2148256392",
                                    "name": "Zibin Zheng"
                                },
                                {
                                    "authorId": "2115304",
                                    "name": "Kai-Chun Ning"
                                },
                                {
                                    "authorId": "2254800142",
                                    "name": "Jiachi Chen"
                                },
                                {
                                    "authorId": "2214155529",
                                    "name": "Yanlin Wang"
                                },
                                {
                                    "authorId": "2274095496",
                                    "name": "Wenqing Chen"
                                },
                                {
                                    "authorId": "2217902484",
                                    "name": "Lianghong Guo"
                                },
                                {
                                    "authorId": "2233023641",
                                    "name": "Weicheng Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Empirical Software Engineering",
                            "n_citations": 76
                        },
                        "score": 0.52880859375
                    },
                    {
                        "id": "(Brown et al., 2020)",
                        "snippets": [
                            "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                        ],
                        "paper": {
                            "corpus_id": 218971783,
                            "title": "Language Models are Few-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "2056658938",
                                    "name": "Benjamin Mann"
                                },
                                {
                                    "authorId": "39849748",
                                    "name": "Nick Ryder"
                                },
                                {
                                    "authorId": "2065894334",
                                    "name": "Melanie Subbiah"
                                },
                                {
                                    "authorId": "152724169",
                                    "name": "J. Kaplan"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "2072676",
                                    "name": "Arvind Neelakantan"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "103143311",
                                    "name": "T. Henighan"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "2052152920",
                                    "name": "Daniel M. Ziegler"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2059411355",
                                    "name": "Clemens Winter"
                                },
                                {
                                    "authorId": "144239765",
                                    "name": "Christopher Hesse"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "2064673055",
                                    "name": "Eric Sigler"
                                },
                                {
                                    "authorId": "1380985420",
                                    "name": "Ma-teusz Litwin"
                                },
                                {
                                    "authorId": "145565184",
                                    "name": "Scott Gray"
                                },
                                {
                                    "authorId": "1490681878",
                                    "name": "Benjamin Chess"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "133740015",
                                    "name": "Christopher Berner"
                                },
                                {
                                    "authorId": "52238703",
                                    "name": "Sam McCandlish"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 42437
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."
                        ],
                        "paper": {
                            "corpus_id": 270702559,
                            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                            "authors": [
                                {
                                    "authorId": "2265383225",
                                    "name": "Kai Yin"
                                },
                                {
                                    "authorId": "2308073678",
                                    "name": "Chengkai Liu"
                                },
                                {
                                    "authorId": "2258714985",
                                    "name": "Ali Mostafavi"
                                },
                                {
                                    "authorId": "2308068627",
                                    "name": "Xia Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.6611328125
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices.\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."
                        ],
                        "paper": {
                            "corpus_id": 266755678,
                            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                            "authors": [
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "2155082967",
                                    "name": "Haoyang He"
                                },
                                {
                                    "authorId": "2184719751",
                                    "name": "Tianle Han"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2257433902",
                                    "name": "Jiaming Tian"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2110238778",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2277869261",
                                    "name": "Xiaohui Gao"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2211904452",
                                    "name": "Shaochen Xu"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257586495",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "49104946",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 74
                        },
                        "score": 0.732421875
                    },
                    {
                        "id": "(Nie et al., 2025)",
                        "snippets": [
                            "\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 277349741,
                            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
                            "authors": [
                                {
                                    "authorId": "94168461",
                                    "name": "Tong Nie"
                                },
                                {
                                    "authorId": "2028643500",
                                    "name": "Jiangming Sun"
                                },
                                {
                                    "authorId": "2277421553",
                                    "name": "Wei Ma"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.63818359375
                    },
                    {
                        "id": "(Katz et al., 2024)",
                        "snippets": [
                            "Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI.\n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approximately twice the number of parameters compared to decoder-only models with equivalent capacity.\n\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design."
                        ],
                        "paper": {
                            "corpus_id": 274992300,
                            "title": "Segment-Based Attention Masking for GPTs",
                            "authors": [
                                {
                                    "authorId": "121254633",
                                    "name": "Shahar Katz"
                                },
                                {
                                    "authorId": "2186740854",
                                    "name": "Liran Ringel"
                                },
                                {
                                    "authorId": "2335566528",
                                    "name": "Yaniv Romano"
                                },
                                {
                                    "authorId": "2284763723",
                                    "name": "Lior Wolf"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.61474609375
                    },
                    {
                        "id": "(Kopiczko et al., 2024)",
                        "snippets": [
                            "But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2019) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020)Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling."
                        ],
                        "paper": {
                            "corpus_id": 269981935,
                            "title": "Bitune: Bidirectional Instruction-Tuning",
                            "authors": [
                                {
                                    "authorId": "2258964424",
                                    "name": "D. J. Kopiczko"
                                },
                                {
                                    "authorId": "83133279",
                                    "name": "Tijmen Blankevoort"
                                },
                                {
                                    "authorId": "2258963625",
                                    "name": "Yuki Markus Asano"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.76611328125
                    },
                    {
                        "id": "(BehnamGhader et al., 2024)",
                        "snippets": [
                            "We speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position i is influenced solely by the representations of preceding tokens at positions 0, 1, . . . , i \u2212 1. Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence."
                        ],
                        "paper": {
                            "corpus_id": 269009682,
                            "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
                            "authors": [
                                {
                                    "authorId": "2101317786",
                                    "name": "Parishad BehnamGhader"
                                },
                                {
                                    "authorId": "1666183192",
                                    "name": "Vaibhav Adlakha"
                                },
                                {
                                    "authorId": "2269460274",
                                    "name": "Marius Mosbach"
                                },
                                {
                                    "authorId": "3335364",
                                    "name": "Dzmitry Bahdanau"
                                },
                                {
                                    "authorId": "2748188",
                                    "name": "Nicolas Chapados"
                                },
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 241
                        },
                        "score": 0.51220703125
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named",
                            "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."
                        ],
                        "paper": {
                            "corpus_id": 263829839,
                            "title": "LLM for SoC Security: A Paradigm Shift",
                            "authors": [
                                {
                                    "authorId": "2256992493",
                                    "name": "Dipayan Saha"
                                },
                                {
                                    "authorId": "2114625129",
                                    "name": "Shams Tarek"
                                },
                                {
                                    "authorId": "2256991081",
                                    "name": "Katayoon Yahyaei"
                                },
                                {
                                    "authorId": "2231854143",
                                    "name": "Sujan Kumar Saha"
                                },
                                {
                                    "authorId": "2257235852",
                                    "name": "Jingbo Zhou"
                                },
                                {
                                    "authorId": "145954982",
                                    "name": "M. Tehranipoor"
                                },
                                {
                                    "authorId": "1997019",
                                    "name": "Farimah Farahmandi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 54
                        },
                        "score": 0.62548828125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Encoder-Decoder Models",
                "tldr": "Encoder-decoder models combine bidirectional attention in the encoder with causal attention in the decoder, while introducing cross-attention to connect the two components, making them particularly suitable for sequence-to-sequence tasks like translation and summarization. (16 sources)",
                "text": "\nEncoder-decoder models represent a fundamental architecture in the language model landscape, combining elements of both encoder-only and decoder-only approaches while introducing unique mechanisms for information flow. The architecture consists of two distinct components: \"the encoder processes and contextualizes the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention\" <Paper corpusId=\"268247581\" paperTitle=\"(Pei et al., 2024)\" isShortName></Paper>.\n\nThe attention mechanisms in encoder-decoder models follow a specific pattern. The encoder employs bidirectional attention, allowing each token to attend to all other tokens in the input sequence, which enables comprehensive context understanding. In contrast, \"the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multi-head attention block that receives non-causal information from the encoder\" <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper>. This arrangement is reflected in the masking patterns: \"fully visible masking is used in the encoder and causal masking is used in the decoder\" <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nThe cross-attention mechanism is a distinctive feature that connects the encoder and decoder components. It \"allows the decoder to access the fully processed encoder output and is responsible for connecting input tokens to target tokens\" <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. The cross-attention enables the decoder to generate content that is directly informed by the encoder's bidirectional understanding of the input sequence. As Zhang et al. describe, cross-attention \"has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder\" <Paper corpusId=\"277626724\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.\n\nThe bidirectional nature of the encoder's attention is crucial for model performance. Compared to the causal attention mechanism used in decoder-only models, \"the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations\" <Paper corpusId=\"258461112\" paperTitle=\"(Jain et al., 2022)\" isShortName></Paper>. Specifically, in cross-domain applications, \"this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models\" <Paper corpusId=\"268247581\" paperTitle=\"(Pei et al., 2024)\" isShortName></Paper>.\n\nFor the decoder component, the standard approach uses causal attention to ensure autoregressive generation. As Liu et al. explain, \"For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token\" <Paper corpusId=\"273532366\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper>. This unidirectional constraint is necessary because, during generation, \"the decoder adopts a causal mask to force the state of each decoder time step only attend to the state from the previous time steps, to avoid seeing tokens 'from the future'\" <Paper corpusId=\"244119798\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nHowever, variations in the decoder's attention mechanism have been explored. Researchers have developed alternative approaches such as the \"causal with prefix mask\" where \"bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens\" <Paper corpusId=\"244119798\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. Similarly, the query-document attention mechanism proposed by Dong et al. implements a specialized attention pattern where \"each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment\" <Paper corpusId=\"245385261\" paperTitle=\"(Laskar et al., 2021)\" isShortName></Paper> <Paper corpusId=\"147704286\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper>.\n\nEncoder-decoder models excel in sequence-to-sequence tasks where there is \"a direct and complex transformation between inputs and outputs\" <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. This makes them particularly effective for applications like machine translation <Paper corpusId=\"209832341\" paperTitle=\"(Wu, 2020)\" isShortName></Paper> <Paper corpusId=\"219463386\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper> <Paper corpusId=\"234785837\" paperTitle=\"(Nguyen et al., 2021)\" isShortName></Paper>, text summarization, and question answering <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. Notable examples of encoder-decoder models include BART <Paper corpusId=\"204960716\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>, T5 <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>, and UL2 <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>, which have demonstrated strong performance across a range of tasks.\n\nThe encoder-decoder architecture represents a middle ground between encoder-only and decoder-only approaches. While non-causal encoding provides comprehensive context understanding, the causal decoding supports efficient autoregressive text generation. This combination makes encoder-decoder models particularly well-suited for conditional generation tasks, where the output is directly dependent on understanding the input context.",
                "citations": [
                    {
                        "id": "(Pei et al., 2024)",
                        "snippets": [
                            "The standard Transformer (Vaswani et al., 2017) and its variants (Lewis et al., 2019), (Raffel et al., 2019) adopt the encoder-decoder framework (Figure 6d),where the encoder processes and contextualize the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention. Models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) are with this architecture and demonstrating its effectiveness across a broad spectrum of applications. In biomolecule and text cross modeling scenario, this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models. This enriched interaction allows for a deeper understanding of the input sequences. Subsequently, the encoder-decoder attention mechanism empowers the decoder to generate outputs tailored for specific biological tasks."
                        ],
                        "paper": {
                            "corpus_id": 268247581,
                            "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2171652249",
                                    "name": "Qizhi Pei"
                                },
                                {
                                    "authorId": "47767791",
                                    "name": "Lijun Wu"
                                },
                                {
                                    "authorId": "1944690382",
                                    "name": "Kaiyuan Gao"
                                },
                                {
                                    "authorId": "151068900",
                                    "name": "Jinhua Zhu"
                                },
                                {
                                    "authorId": "2290062348",
                                    "name": "Yue Wang"
                                },
                                {
                                    "authorId": "2290024261",
                                    "name": "Zun Wang"
                                },
                                {
                                    "authorId": "2267250090",
                                    "name": "Tao Qin"
                                },
                                {
                                    "authorId": "2257028545",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.5068359375
                    },
                    {
                        "id": "(Busto-Castineira et al., 2024)",
                        "snippets": [
                            "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder",
                            "Even though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], [22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31]."
                        ],
                        "paper": {
                            "corpus_id": 270832367,
                            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
                            "authors": [
                                {
                                    "authorId": "2222734467",
                                    "name": "Andrea Busto-Casti\u00f1eira"
                                },
                                {
                                    "authorId": "2323809078",
                                    "name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o"
                                },
                                {
                                    "authorId": "1405165681",
                                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                                },
                                {
                                    "authorId": "2034282614",
                                    "name": "Francisco de Arriba-P\u00e9rez"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.63671875
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks",
                            "As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.853515625
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "1. Encoder has exactly the same architecture as the decoder-only model, but self-attention is switched from causal to bidirectional. We provide ablations in Section 6 that illustrate the critical effect of bidirectional attention on downstream performance.\n\n2. In each Decoder block, FFN and self-attention parts are identical to the corresponding parts in decoder-only models, and cross-attention has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder."
                        ],
                        "paper": {
                            "corpus_id": 277626724,
                            "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
                            "authors": [
                                {
                                    "authorId": "2354284757",
                                    "name": "Biao Zhang"
                                },
                                {
                                    "authorId": "2165469946",
                                    "name": "Fedor Moiseev"
                                },
                                {
                                    "authorId": "2343748926",
                                    "name": "Joshua Ainslie"
                                },
                                {
                                    "authorId": "1658871094",
                                    "name": "P. Suganthan"
                                },
                                {
                                    "authorId": "2352024723",
                                    "name": "Min Ma"
                                },
                                {
                                    "authorId": "9692128",
                                    "name": "Surya Bhupatiraju"
                                },
                                {
                                    "authorId": "2275184616",
                                    "name": "Federico Lebron"
                                },
                                {
                                    "authorId": "2273534960",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "2349772191",
                                    "name": "Zhe Dong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.57080078125
                    },
                    {
                        "id": "(Jain et al., 2022)",
                        "snippets": [
                            "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations."
                        ],
                        "paper": {
                            "corpus_id": 258461112,
                            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
                            "authors": [
                                {
                                    "authorId": "2146677401",
                                    "name": "Nihal Jain"
                                },
                                {
                                    "authorId": "2358258",
                                    "name": "Dejiao Zhang"
                                },
                                {
                                    "authorId": "38123220",
                                    "name": "Wasi Uddin Ahmad"
                                },
                                {
                                    "authorId": "50219006",
                                    "name": "Zijian Wang"
                                },
                                {
                                    "authorId": "144647318",
                                    "name": "Feng Nan"
                                },
                                {
                                    "authorId": "2187045812",
                                    "name": "Xiaopeng Li"
                                },
                                {
                                    "authorId": "144745483",
                                    "name": "Ming Tan"
                                },
                                {
                                    "authorId": "1701451",
                                    "name": "Ramesh Nallapati"
                                },
                                {
                                    "authorId": "31631000",
                                    "name": "Baishakhi Ray"
                                },
                                {
                                    "authorId": "50339091",
                                    "name": "Parminder Bhatia"
                                },
                                {
                                    "authorId": "47646605",
                                    "name": "Xiaofei Ma"
                                },
                                {
                                    "authorId": "144028698",
                                    "name": "Bing Xiang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 16
                        },
                        "score": 0.55224609375
                    },
                    {
                        "id": "(Liu et al._1, 2024)",
                        "snippets": [
                            "Decoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models.\n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token.\n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure."
                        ],
                        "paper": {
                            "corpus_id": 273532366,
                            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
                            "authors": [
                                {
                                    "authorId": "2327246234",
                                    "name": "Junhua Liu"
                                },
                                {
                                    "authorId": "2325881142",
                                    "name": "Bin Fu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.5791015625
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Typically, the encoder-decoder based models generate text starting from the beginning, and the decoder adopts a causal mask to force the state of each decoder time step s t i only attend to the state from the previous time steps, s t|t\u2264t i , to avoid seeing tokens \"from the future\". We consider this type of attention as unidirectional. In our task, we have the input prefix as the written text. Tokens in the prefix should be visible to each other. Therefore, we adopt the causal with prefix mask: bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens."
                        ],
                        "paper": {
                            "corpus_id": 244119798,
                            "title": "TWT: Table with Written Text for Controlled Data-to-Text Generation",
                            "authors": [
                                {
                                    "authorId": "2841012",
                                    "name": "Tongliang Li"
                                },
                                {
                                    "authorId": "2153681740",
                                    "name": "Lei Fang"
                                },
                                {
                                    "authorId": "153249455",
                                    "name": "Jian-Guang Lou"
                                },
                                {
                                    "authorId": "1707275",
                                    "name": "Zhoujun Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.57763671875
                    },
                    {
                        "id": "(Laskar et al., 2021)",
                        "snippets": [
                            "(i) The Bidirectional Self-Attention Mechanism: In the original BERTSUM architecture, the bidirectional self-attention mechanism (Devlin et al., 2019) is utilized by the BERT encoder to generate the encoded representation of the input text. In the bidirectional self-attention mechanism, when a pair of sentences are combined together and given as input to the BERT encoder, both sentences will give attention to each other. Thus, when we utilize the bidirectional self-attention mechanism (see Figure 2a) in the PreQFAS model, both the query and the document will not only give attention to themselves, but also they will give attention to each other to provide the encoded representation of the concatenated input.\n\n(ii) The Query-Document Attention Mechanism: Dong et al. (2019) proposed the sequence-tosequence language modeling objective for text sequences that are consisted of two segments. In such text sequences, each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment."
                        ],
                        "paper": {
                            "corpus_id": 245385261,
                            "title": "Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization",
                            "authors": [
                                {
                                    "authorId": "46437970",
                                    "name": "Md Tahmid Rahman Laskar"
                                },
                                {
                                    "authorId": "2939577",
                                    "name": "Enamul Hoque"
                                },
                                {
                                    "authorId": "1683391",
                                    "name": "J. Huang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computational Linguistics",
                            "n_citations": 45
                        },
                        "score": 0.642578125
                    },
                    {
                        "id": "(Dong et al., 2019)",
                        "snippets": [
                            "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 147704286,
                            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "145307652",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "144610884",
                                    "name": "Nan Yang"
                                },
                                {
                                    "authorId": "51456429",
                                    "name": "Wenhui Wang"
                                },
                                {
                                    "authorId": "49807919",
                                    "name": "Furu Wei"
                                },
                                {
                                    "authorId": "46522098",
                                    "name": "Xiaodong Liu"
                                },
                                {
                                    "authorId": "72682749",
                                    "name": "Yu Wang"
                                },
                                {
                                    "authorId": "1800422",
                                    "name": "Jianfeng Gao"
                                },
                                {
                                    "authorId": "143849609",
                                    "name": "M. Zhou"
                                },
                                {
                                    "authorId": "145058181",
                                    "name": "H. Hon"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1560
                        },
                        "score": 0
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named",
                            "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."
                        ],
                        "paper": {
                            "corpus_id": 263829839,
                            "title": "LLM for SoC Security: A Paradigm Shift",
                            "authors": [
                                {
                                    "authorId": "2256992493",
                                    "name": "Dipayan Saha"
                                },
                                {
                                    "authorId": "2114625129",
                                    "name": "Shams Tarek"
                                },
                                {
                                    "authorId": "2256991081",
                                    "name": "Katayoon Yahyaei"
                                },
                                {
                                    "authorId": "2231854143",
                                    "name": "Sujan Kumar Saha"
                                },
                                {
                                    "authorId": "2257235852",
                                    "name": "Jingbo Zhou"
                                },
                                {
                                    "authorId": "145954982",
                                    "name": "M. Tehranipoor"
                                },
                                {
                                    "authorId": "1997019",
                                    "name": "Farimah Farahmandi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 54
                        },
                        "score": 0.62548828125
                    },
                    {
                        "id": "(Wu, 2020)",
                        "snippets": [
                            "We describe a method for training accurate Transformer machine-translation models to run inference using 8-bit integer (INT8) hardware matrix multipliers, as opposed to the more costly single-precision floating-point (FP32) hardware. Unlike previous work, which converted only 85 Transformer matrix multiplications to INT8, leaving 48 out of 133 of them in FP32 because of unacceptable accuracy loss, we convert them all to INT8 without compromising accuracy. Tested on the newstest2014 English-to-German translation task, our INT8 Transformer Base and Transformer Big models yield BLEU scores that are 99.3\u2013100% relative to those of the corresponding FP32 models. Our approach converts all matrix-multiplication tensors from an existing FP32 model into INT8 tensors by automatically making range-precision trade-offs during training. To demonstrate the robustness of this approach, we also include results from INT6 Transformer models."
                        ],
                        "paper": {
                            "corpus_id": 209832341,
                            "title": "Learning Accurate Integer Transformer Machine-Translation Models",
                            "authors": [
                                {
                                    "authorId": "11591799",
                                    "name": "Ephrem Wu"
                                }
                            ],
                            "year": 2020,
                            "venue": "SN Computer Science",
                            "n_citations": 4
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2020)",
                        "snippets": [
                            "Source input information plays a very important role in the Transformer-based translation system. In practice, word embedding and positional embedding of each word are added as the input representation. Then self-attention networks are used to encode the global dependencies in the input representation to generate a source representation. However, this processing on the source representation only adopts a single source feature and excludes richer and more diverse features such as recurrence features, local features, and syntactic features, which results in tedious representation and thereby hinders the further translation performance improvement. In this paper, we introduce a simple and efficient method to encode more diverse source features into the input representation simultaneously, and thereby learning an effective source representation by self-attention networks. In particular, the proposed grouped strategy is only applied to the input representation layer, to keep the diversity of translation information and the efficiency of the self-attention networks at the same time. Experimental results show that our approach improves the translation performance over the state-of-the-art baselines of Transformer in regard to WMT14 English-to-German and NIST Chinese-to-English machine translation tasks."
                        ],
                        "paper": {
                            "corpus_id": 219463386,
                            "title": "Towards More Diverse Input Representation for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2849740",
                                    "name": "Kehai Chen"
                                },
                                {
                                    "authorId": "108085542",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "1802277",
                                    "name": "M. Utiyama"
                                },
                                {
                                    "authorId": "1698363",
                                    "name": "E. Sumita"
                                },
                                {
                                    "authorId": "145382463",
                                    "name": "T. Zhao"
                                },
                                {
                                    "authorId": "2105775",
                                    "name": "Muyun Yang"
                                },
                                {
                                    "authorId": "47941144",
                                    "name": "Hai Zhao"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                            "n_citations": 23
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nguyen et al., 2021)",
                        "snippets": [
                            "Transformer is a neural machine translation model which revolutionizes machine translation. Compared with traditional statistical machine translation models and other neural machine translation models, the recently proposed transformer model radically and fundamentally changes machine translation with its self-attention and cross-attention mechanisms. These mechanisms effectively model token alignments between source and target sentences. It has been reported that the transformer model provides accurate posterior alignments. In this work, we empirically prove the reverse effect, showing that prior alignments help transformer models produce better translations. Experiment results on Vietnamese-English news translation task show not only the positive effect of manually annotated alignments on transformer models but also the surprising outperformance of statistically constructed alignments reinforced with the flexibility of token-type selection over manual alignments in improving transformer models. Statistically constructed word-to-lemma alignments are used to train a word-to-word transformer model. The novel hybrid transformer model improves the baseline transformer model and transformer model trained with manual alignments by 2.53 and 0.79 BLEU, respectively. In addition to BLEU score, we make limited human judgment on translation results. Strong correlation between human and machine judgment confirms our findings."
                        ],
                        "paper": {
                            "corpus_id": 234785837,
                            "title": "Improving Transformer-Based Neural Machine Translation with Prior Alignments",
                            "authors": [
                                {
                                    "authorId": "33775047",
                                    "name": "Thien Nguyen"
                                },
                                {
                                    "authorId": "2151126066",
                                    "name": "Lam Nguyen"
                                },
                                {
                                    "authorId": "153460631",
                                    "name": "Phuoc Tran"
                                },
                                {
                                    "authorId": "122619186",
                                    "name": "Huu Nguyen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Complex",
                            "n_citations": 22
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lewis et al., 2019)",
                        "snippets": [
                            "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
                        ],
                        "paper": {
                            "corpus_id": 204960716,
                            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
                            "authors": [
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "11323179",
                                    "name": "Yinhan Liu"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "2320509",
                                    "name": "Marjan Ghazvininejad"
                                },
                                {
                                    "authorId": "113947684",
                                    "name": "Abdel-rahman Mohamed"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1759422",
                                    "name": "Veselin Stoyanov"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 10856
                        },
                        "score": 0
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tay et al., 2022)",
                        "snippets": [
                            "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B."
                        ],
                        "paper": {
                            "corpus_id": 252780443,
                            "title": "UL2: Unifying Language Learning Paradigms",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "3226635",
                                    "name": "Mostafa Dehghani"
                                },
                                {
                                    "authorId": "2057663102",
                                    "name": "Vinh Q. Tran"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "2119725651",
                                    "name": "Dara Bahri"
                                },
                                {
                                    "authorId": "32303439",
                                    "name": "Tal Schuster"
                                },
                                {
                                    "authorId": "2115689465",
                                    "name": "H. Zheng"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "2815290",
                                    "name": "N. Houlsby"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 313
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Hybrid Attention Approaches",
                "tldr": "Hybrid attention approaches combine causal and non-causal mechanisms to leverage their complementary strengths, with models like PrefixLM implementing bidirectional attention for prefix tokens and causal attention for generation, while others like context-span models allow different attention patterns for different parts of the sequence. (8 sources)",
                "text": "\nHybrid attention approaches have emerged as innovative solutions that aim to combine the strengths of both causal and non-causal attention mechanisms. These approaches seek to balance the comprehensive contextual understanding provided by bidirectional attention with the efficient autoregressive generation capabilities of unidirectional attention.\n\nThe PrefixLM architecture, introduced in T5, represents one of the most prominent hybrid approaches. In this design, the input sequence is divided into a prefix section and a subsequent generation section. The prefix is processed with bidirectional attention, \"allowing the model to attend to all tokens within it,\" while \"the remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix\" <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. This configuration \"combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence\" <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>.\n\nNijkamp et al. describe a similar approach where \"for the prefix p = (x\u2081, ..., x\u2098) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x\u2098\u208a\u2081, ..., x\u2099), each token can only attend to previous tokens, which amounts to uni-directional decoder representations\" <Paper corpusId=\"258461229\" paperTitle=\"(Nijkamp et al., 2023)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. This unification of bidirectional and unidirectional attention aims to \"achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks\" <Paper corpusId=\"258461229\" paperTitle=\"(Nijkamp et al., 2023)\" isShortName></Paper>.\n\nIn the decoder-only model space, the prefix decoder architecture represents a specialized implementation of hybrid attention. Similar to the causal decoder, it consists of decoder layers, but with a key distinction in its attention mechanism: \"The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens\" <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. Models implementing this approach include U-PaLM and GLM-130B <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247519241\" paperTitle=\"(Du et al., 2021)\" isShortName></Paper>.\n\nLu et al. further clarify the attention masking distinction in prefix decoders, noting that while \"Causal Mask employs unidirectional attention on prefix sequences,\" the \"Prefix Mask applies bidirectional attention\" <Paper corpusId=\"271600495\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This differentiation in masking patterns allows prefix decoders to process dialogue history and current queries with comprehensive context awareness while maintaining efficient autoregressive generation for responses.\n\nAnother innovative hybrid approach is the context-span method described by Khosla et al. In this design, \"context tokens attend to all other context tokens in the sequence,\" implementing fully bidirectional attention within the context portion. Meanwhile, \"span tokens are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves\" <Paper corpusId=\"275544523\" paperTitle=\"(Khosla et al., 2025)\" isShortName></Paper>. This configuration enables the model to operate in multiple modes: \"fully causal/unidirectional for open-ended text generation tasks, fully bidirectional representation learning tasks, or a combination of causal and bidirectional for text infilling\" <Paper corpusId=\"275544523\" paperTitle=\"(Khosla et al., 2025)\" isShortName></Paper>.\n\nThe semi-causal language model proposed by Hao et al. offers yet another hybrid approach. This architecture features \"a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder\" <Paper corpusId=\"249626024\" paperTitle=\"(Hao et al., 2022)\" isShortName></Paper>. In this configuration, \"the model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders\" <Paper corpusId=\"249626024\" paperTitle=\"(Hao et al., 2022)\" isShortName></Paper>. The authors argue that \"non-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities\" <Paper corpusId=\"249626024\" paperTitle=\"(Hao et al., 2022)\" isShortName></Paper>, making this hybrid approach particularly valuable for transfer learning scenarios.\n\nHybrid attention approaches continue to evolve, with researchers exploring various combinations of attention patterns to optimize the trade-off between comprehensive context understanding and efficient generation. These approaches demonstrate that the strict dichotomy between causal and non-causal attention can be transcended through architectural innovations that selectively apply different attention mechanisms to different parts of the input sequence.",
                "citations": [
                    {
                        "id": "(Suganthan et al., 2025)",
                        "snippets": [
                            "Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is prevalent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."
                        ],
                        "paper": {
                            "corpus_id": 276771845,
                            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                            "authors": [
                                {
                                    "authorId": "1658871094",
                                    "name": "P. Suganthan"
                                },
                                {
                                    "authorId": "2165469946",
                                    "name": "Fedor Moiseev"
                                },
                                {
                                    "authorId": "2348489099",
                                    "name": "Le Yan"
                                },
                                {
                                    "authorId": "2261361394",
                                    "name": "Junru Wu"
                                },
                                {
                                    "authorId": "2348507846",
                                    "name": "Jianmo Ni"
                                },
                                {
                                    "authorId": "2348488953",
                                    "name": "Jay Han"
                                },
                                {
                                    "authorId": "1954563",
                                    "name": "I. Zitouni"
                                },
                                {
                                    "authorId": "1727837",
                                    "name": "Enrique Alfonseca"
                                },
                                {
                                    "authorId": "2348422460",
                                    "name": "Xuanhui Wang"
                                },
                                {
                                    "authorId": "2349772191",
                                    "name": "Zhe Dong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.82861328125
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nijkamp et al., 2023)",
                        "snippets": [
                            "For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2019). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks."
                        ],
                        "paper": {
                            "corpus_id": 258461229,
                            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
                            "authors": [
                                {
                                    "authorId": "2043490",
                                    "name": "Erik Nijkamp"
                                },
                                {
                                    "authorId": "50376014",
                                    "name": "Hiroaki Hayashi"
                                },
                                {
                                    "authorId": "2054594326",
                                    "name": "Caiming Xiong"
                                },
                                {
                                    "authorId": "1702137",
                                    "name": "S. Savarese"
                                },
                                {
                                    "authorId": "2118860628",
                                    "name": "Yingbo Zhou"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 169
                        },
                        "score": 0.5146484375
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture",
                            "Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b).\n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a)."
                        ],
                        "paper": {
                            "corpus_id": 261064777,
                            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
                            "authors": [
                                {
                                    "authorId": "2148256392",
                                    "name": "Zibin Zheng"
                                },
                                {
                                    "authorId": "2115304",
                                    "name": "Kai-Chun Ning"
                                },
                                {
                                    "authorId": "2254800142",
                                    "name": "Jiachi Chen"
                                },
                                {
                                    "authorId": "2214155529",
                                    "name": "Yanlin Wang"
                                },
                                {
                                    "authorId": "2274095496",
                                    "name": "Wenqing Chen"
                                },
                                {
                                    "authorId": "2217902484",
                                    "name": "Lianghong Guo"
                                },
                                {
                                    "authorId": "2233023641",
                                    "name": "Weicheng Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Empirical Software Engineering",
                            "n_citations": 76
                        },
                        "score": 0.52880859375
                    },
                    {
                        "id": "(Du et al., 2021)",
                        "snippets": [
                            "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25\u00d7 parameters of BERT Large , demonstrating its generalizability to different downstream tasks."
                        ],
                        "paper": {
                            "corpus_id": 247519241,
                            "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
                            "authors": [
                                {
                                    "authorId": "66395694",
                                    "name": "Zhengxiao Du"
                                },
                                {
                                    "authorId": "5606742",
                                    "name": "Yujie Qian"
                                },
                                {
                                    "authorId": "2111312892",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "145573466",
                                    "name": "Ming Ding"
                                },
                                {
                                    "authorId": "40125294",
                                    "name": "J. Qiu"
                                },
                                {
                                    "authorId": "2109512754",
                                    "name": "Zhilin Yang"
                                },
                                {
                                    "authorId": "2109541439",
                                    "name": "Jie Tang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1554
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021). Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention."
                        ],
                        "paper": {
                            "corpus_id": 271600495,
                            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                            "authors": [
                                {
                                    "authorId": "2314473248",
                                    "name": "Mingcong Lu"
                                },
                                {
                                    "authorId": "2314649002",
                                    "name": "Jiangcai Zhu"
                                },
                                {
                                    "authorId": "2314113733",
                                    "name": "Wang Hao"
                                },
                                {
                                    "authorId": "2314323587",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2314311430",
                                    "name": "Shusheng Zhang"
                                },
                                {
                                    "authorId": "2314110211",
                                    "name": "Kailai Shao"
                                },
                                {
                                    "authorId": "2314192630",
                                    "name": "Chao Chen"
                                },
                                {
                                    "authorId": "2314343132",
                                    "name": "Nan Li"
                                },
                                {
                                    "authorId": "2324104105",
                                    "name": "Feng Wang"
                                },
                                {
                                    "authorId": "2324103820",
                                    "name": "Xin Lu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.505859375
                    },
                    {
                        "id": "(Khosla et al., 2025)",
                        "snippets": [
                            "Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details)",
                            "During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling."
                        ],
                        "paper": {
                            "corpus_id": 275544523,
                            "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
                            "authors": [
                                {
                                    "authorId": "2056070459",
                                    "name": "Savya Khosla"
                                },
                                {
                                    "authorId": "2218199485",
                                    "name": "Aditi Tiwari"
                                },
                                {
                                    "authorId": "33315685",
                                    "name": "Kushal Kafle"
                                },
                                {
                                    "authorId": "2297849207",
                                    "name": "Simon Jenni"
                                },
                                {
                                    "authorId": "2341050386",
                                    "name": "Handong Zhao"
                                },
                                {
                                    "authorId": "2288642908",
                                    "name": "John P. Collomosse"
                                },
                                {
                                    "authorId": "2288445861",
                                    "name": "Jing Shi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.76513671875
                    },
                    {
                        "id": "(Hao et al., 2022)",
                        "snippets": [
                            "Non-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available.\n\nAs shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders."
                        ],
                        "paper": {
                            "corpus_id": 249626024,
                            "title": "Language Models are General-Purpose Interfaces",
                            "authors": [
                                {
                                    "authorId": "34128716",
                                    "name": "Y. Hao"
                                },
                                {
                                    "authorId": "2187079427",
                                    "name": "Haoyu Song"
                                },
                                {
                                    "authorId": "145307652",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "3110003",
                                    "name": "Shaohan Huang"
                                },
                                {
                                    "authorId": "46221722",
                                    "name": "Zewen Chi"
                                },
                                {
                                    "authorId": "51456429",
                                    "name": "Wenhui Wang"
                                },
                                {
                                    "authorId": "2118866998",
                                    "name": "Shuming Ma"
                                },
                                {
                                    "authorId": "49807919",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 99
                        },
                        "score": 0.7265625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Suitability for NLP Tasks",
                "tldr": "Different attention mechanisms are suited for specific NLP tasks, with bidirectional attention excelling in understanding tasks like classification and question answering, while unidirectional attention is optimal for autoregressive text generation, and hybrid approaches balancing the strengths of both. (13 sources)",
                "text": "\n## Text Understanding Tasks\n- **Classification**: Bidirectional attention mechanisms found in encoder-only models like BERT are particularly well-suited for text classification tasks, allowing the model to leverage context from both directions <Paper corpusId=\"242033930\" paperTitle=\"(Hommel et al., 2021)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n- **Natural Language Inference**: Encoder-only models with bidirectional attention excel at natural language inference, where understanding relationships between sentences is crucial <Paper corpusId=\"267312283\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper> <Paper corpusId=\"3432876\" paperTitle=\"(Williams et al., 2017)\" isShortName></Paper>.\n- **Question Answering**: Bidirectional models are particularly effective for question answering tasks, as they can process the entire context to locate precise answers <Paper corpusId=\"267312283\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper> <Paper corpusId=\"11816014\" paperTitle=\"(Rajpurkar et al., 2016)\" isShortName></Paper> <Paper corpusId=\"47018994\" paperTitle=\"(Rajpurkar et al., 2018)\" isShortName></Paper>.\n- **Sentence Similarity**: Tasks involving measuring semantic similarity between texts benefit from the comprehensive contextual representations provided by bidirectional attention models <Paper corpusId=\"267312283\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>.\n\n## Text Generation Tasks\n- **Open-ended Text Generation**: Decoder-only models with causal (unidirectional) attention are dominant for autoregressive text generation, as their left-to-right processing matches the natural flow of text generation <Paper corpusId=\"267701011\" paperTitle=\"(Cascella et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270703043\" paperTitle=\"(Posada et al., 2024)\" isShortName></Paper>.\n- **Conversational AI**: The unidirectional structure of decoder models makes them \"particularly effective for autoregressive tasks, such as conversational AI\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>.\n- **Generative Large Language Models**: Causal decoder architectures have become the standard for modern large language models focused on text generation, including GPT, Llama, and similar models <Paper corpusId=\"273532366\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"269981935\" paperTitle=\"(Kopiczko et al., 2024)\" isShortName></Paper>.\n\n## Sequence-to-Sequence Tasks\n- **Machine Translation**: Encoder-decoder models with bidirectional encoding and causal decoding excel at translation tasks, where understanding the source language and generating target language are both critical <Paper corpusId=\"242033930\" paperTitle=\"(Hommel et al., 2021)\" isShortName></Paper> <Paper corpusId=\"267701011\" paperTitle=\"(Cascella et al., 2024)\" isShortName></Paper>.\n- **Summarization**: The encoder-decoder architecture is well-suited for text summarization, leveraging bidirectional understanding of the source text and causal generation of the summary <Paper corpusId=\"267701011\" paperTitle=\"(Cascella et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n- **Question Answering with Generation**: For question answering tasks that require generating answers rather than extracting them, encoder-decoder models offer advantages by combining understanding and generation capabilities <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\n## Task-Architecture Alignment Considerations\n- **Natural Language Understanding (NLU)**: \"Autoencoding models learn bidirectional contextualized representation suited for NLU tasks\" <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>, making encoder-only and encoder components of encoder-decoder models optimal for understanding-focused applications.\n- **Natural Language Generation (NLG)**: \"Autoregressive models learn to generate the next token and hence are suited for NLG tasks\" <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>, making decoder-only and decoder components of encoder-decoder models better for generation tasks.\n- **Performance vs. Parameter Efficiency**: While encoder-decoder models offer strong performance on sequence-to-sequence tasks, they \"require approximately twice the number of parameters compared to decoder-only models with equivalent capacity\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>, presenting a trade-off between capability and efficiency.\n- **Bidirectional Context Benefits**: \"The bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations\" <Paper corpusId=\"258461112\" paperTitle=\"(Jain et al., 2022)\" isShortName></Paper>, which is particularly valuable for tasks requiring deep understanding of text.",
                "citations": [
                    {
                        "id": "(Hommel et al., 2021)",
                        "snippets": [
                            "One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; (Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados et al., 2018)Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation."
                        ],
                        "paper": {
                            "corpus_id": 242033930,
                            "title": "Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation",
                            "authors": [
                                {
                                    "authorId": "2136597350",
                                    "name": "Bj\u00f6rn E. Hommel"
                                },
                                {
                                    "authorId": "2138754315",
                                    "name": "Franz-Josef M. Wollang"
                                },
                                {
                                    "authorId": "2137277509",
                                    "name": "Veronika Kotova"
                                },
                                {
                                    "authorId": "50999175",
                                    "name": "H. Zacher"
                                },
                                {
                                    "authorId": "5278036",
                                    "name": "S. Schmukle"
                                }
                            ],
                            "year": 2021,
                            "venue": "Psychometrika",
                            "n_citations": 21
                        },
                        "score": 0.52392578125
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Park et al., 2024)",
                        "snippets": [
                            "Encoder-only Transformers (a) [25,(Devlin et al., 2019)83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,(Dolan et al., 2005), natural language inference [140](Williams et al., 2017), and question answering (Rajpurkar et al., 2018)(Rajpurkar et al., 2016). On the other hand, decoder-only Transformers (b) (Brown et al., 2020)124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration."
                        ],
                        "paper": {
                            "corpus_id": 267312283,
                            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
                            "authors": [
                                {
                                    "authorId": "2108418112",
                                    "name": "Seungcheol Park"
                                },
                                {
                                    "authorId": "2281832669",
                                    "name": "Jaehyeon Choi"
                                },
                                {
                                    "authorId": "2281792832",
                                    "name": "Sojin Lee"
                                },
                                {
                                    "authorId": "2281746333",
                                    "name": "U. Kang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 16
                        },
                        "score": 0.52587890625
                    },
                    {
                        "id": "(Williams et al., 2017)",
                        "snippets": [
                            "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement."
                        ],
                        "paper": {
                            "corpus_id": 3432876,
                            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
                            "authors": [
                                {
                                    "authorId": "81840293",
                                    "name": "Adina Williams"
                                },
                                {
                                    "authorId": "10666396",
                                    "name": "Nikita Nangia"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                }
                            ],
                            "year": 2017,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 4497
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rajpurkar et al., 2016)",
                        "snippets": [
                            "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"
                        ],
                        "paper": {
                            "corpus_id": 11816014,
                            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                            "authors": [
                                {
                                    "authorId": "2706258",
                                    "name": "Pranav Rajpurkar"
                                },
                                {
                                    "authorId": "2151810148",
                                    "name": "Jian Zhang"
                                },
                                {
                                    "authorId": "2787620",
                                    "name": "Konstantin Lopyrev"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                }
                            ],
                            "year": 2016,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 8174
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rajpurkar et al., 2018)",
                        "snippets": [
                            "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
                        ],
                        "paper": {
                            "corpus_id": 47018994,
                            "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD",
                            "authors": [
                                {
                                    "authorId": "2706258",
                                    "name": "Pranav Rajpurkar"
                                },
                                {
                                    "authorId": "3422908",
                                    "name": "Robin Jia"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                }
                            ],
                            "year": 2018,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 2854
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cascella et al., 2024)",
                        "snippets": [
                            "Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial."
                        ],
                        "paper": {
                            "corpus_id": 267701011,
                            "title": "The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives",
                            "authors": [
                                {
                                    "authorId": "2265474928",
                                    "name": "M. Cascella"
                                },
                                {
                                    "authorId": "2265474397",
                                    "name": "Federico Semeraro"
                                },
                                {
                                    "authorId": "4239573",
                                    "name": "J. Montomoli"
                                },
                                {
                                    "authorId": "8663537",
                                    "name": "Valentina Bellini"
                                },
                                {
                                    "authorId": "2239310513",
                                    "name": "Ornella Piazza"
                                },
                                {
                                    "authorId": "2198023579",
                                    "name": "E. Bignami"
                                }
                            ],
                            "year": 2024,
                            "venue": "J. Medical Syst.",
                            "n_citations": 83
                        },
                        "score": 0.51123046875
                    },
                    {
                        "id": "(Posada et al., 2024)",
                        "snippets": [
                            "Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively."
                        ],
                        "paper": {
                            "corpus_id": 270703043,
                            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
                            "authors": [
                                {
                                    "authorId": "2308036602",
                                    "name": "Andrea Posada"
                                },
                                {
                                    "authorId": "2091163163",
                                    "name": "D. Rueckert"
                                },
                                {
                                    "authorId": "2126502908",
                                    "name": "Felix Meissen"
                                },
                                {
                                    "authorId": "1399796227",
                                    "name": "Philip M\u00fcller"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.5234375
                    },
                    {
                        "id": "(Katz et al., 2024)",
                        "snippets": [
                            "Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI.\n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approximately twice the number of parameters compared to decoder-only models with equivalent capacity.\n\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design."
                        ],
                        "paper": {
                            "corpus_id": 274992300,
                            "title": "Segment-Based Attention Masking for GPTs",
                            "authors": [
                                {
                                    "authorId": "121254633",
                                    "name": "Shahar Katz"
                                },
                                {
                                    "authorId": "2186740854",
                                    "name": "Liran Ringel"
                                },
                                {
                                    "authorId": "2335566528",
                                    "name": "Yaniv Romano"
                                },
                                {
                                    "authorId": "2284763723",
                                    "name": "Lior Wolf"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.61474609375
                    },
                    {
                        "id": "(Liu et al._1, 2024)",
                        "snippets": [
                            "Decoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models.\n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token.\n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure."
                        ],
                        "paper": {
                            "corpus_id": 273532366,
                            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
                            "authors": [
                                {
                                    "authorId": "2327246234",
                                    "name": "Junhua Liu"
                                },
                                {
                                    "authorId": "2325881142",
                                    "name": "Bin Fu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.5791015625
                    },
                    {
                        "id": "(Kopiczko et al., 2024)",
                        "snippets": [
                            "But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2019) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020)Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling."
                        ],
                        "paper": {
                            "corpus_id": 269981935,
                            "title": "Bitune: Bidirectional Instruction-Tuning",
                            "authors": [
                                {
                                    "authorId": "2258964424",
                                    "name": "D. J. Kopiczko"
                                },
                                {
                                    "authorId": "83133279",
                                    "name": "Tijmen Blankevoort"
                                },
                                {
                                    "authorId": "2258963625",
                                    "name": "Yuki Markus Asano"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.76611328125
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks",
                            "As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.853515625
                    },
                    {
                        "id": "(Jain et al., 2022)",
                        "snippets": [
                            "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations."
                        ],
                        "paper": {
                            "corpus_id": 258461112,
                            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
                            "authors": [
                                {
                                    "authorId": "2146677401",
                                    "name": "Nihal Jain"
                                },
                                {
                                    "authorId": "2358258",
                                    "name": "Dejiao Zhang"
                                },
                                {
                                    "authorId": "38123220",
                                    "name": "Wasi Uddin Ahmad"
                                },
                                {
                                    "authorId": "50219006",
                                    "name": "Zijian Wang"
                                },
                                {
                                    "authorId": "144647318",
                                    "name": "Feng Nan"
                                },
                                {
                                    "authorId": "2187045812",
                                    "name": "Xiaopeng Li"
                                },
                                {
                                    "authorId": "144745483",
                                    "name": "Ming Tan"
                                },
                                {
                                    "authorId": "1701451",
                                    "name": "Ramesh Nallapati"
                                },
                                {
                                    "authorId": "31631000",
                                    "name": "Baishakhi Ray"
                                },
                                {
                                    "authorId": "50339091",
                                    "name": "Parminder Bhatia"
                                },
                                {
                                    "authorId": "47646605",
                                    "name": "Xiaofei Ma"
                                },
                                {
                                    "authorId": "144028698",
                                    "name": "Bing Xiang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 16
                        },
                        "score": 0.55224609375
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.28791
    }
}