{
    "query": "anyone fine tuning RAG or work this",
    "user_id": "lib_user",
    "task_id": "9b1806f2-06fe-4c4a-a776-c95ee04c1dbf",
    "timestamp": "2025-06-23T21:36:39.750636",
    "n_retrieval": 256,
    "n_retrieved": 220,
    "n_candidates": 28,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.34067400000000003,
    "decomposed_query": {
        "rewritten_query": "Fine tuning RAG.",
        "keyword_query": "fine tuning RAG retrieval augmented generation",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009498,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "venue": "",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362089035",
                    "name": "Zhan Peng Lee"
                },
                {
                    "authorId": "2362188632",
                    "name": "Andre Lin"
                },
                {
                    "authorId": "2363425126",
                    "name": "Calvin Tan"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.",
            "corpus_id": 278714952,
            "sentences": [
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "We con-structed a diverse dataset covering legal documents, scientific literature, books, and web data, each paired with a plausible but fictitious counterpart. We then fine-tune instruction-tuned LLMs, specifically Meta's Llama 3.1-8B-Instruct (Grattafiori et al., 2024), on this dataset using two prompt variants: a Baseline format and a Structured XML variant. This setup allows us to assess generalization and prompt sensitivity. To our knowledge, Finetune-RAG provides the first RAG dataset of its kind, as existing RAG finetuning datasets implicitly assume perfect information retrieval, and mostly focus only the LLM's ability to extract coherent answers from relevant chunks. \n\nOur key insight is that LLMs struggle to identify contextual clues that are obvious to the human eye, such as financial reports from a similarly named company or outdated information based on dates indicated by document metadata. Through fine-tuning models with a controlled mixture of true and false context placed alongside, we teach them to ground their answers exclusively in the reliable information provided. \n\nWe evaluated the effectiveness of Finetune-RAG using Bench-RAG, a custom benchmarking suite we have created that leverages GPT-4o (OpenAI, 2024) as an automated judge to assess the accuracy, relevance, helpfulness and depth of the LLM response. Our results show that Finetune-RAG substantially improves factual correctness while maintaining output quality across other dimensions, demonstrating that generation-time defenses are a viable complement to improved retrieval. \n\nOur contributions are as follows: \n\n\u2022 Fine-tuning Approach. We propose a novel fine-tuning strategy for RAG systems that teaches models to ignore misleading context and generate answers based solely on factual input. \n\n\u2022 Training Dataset. We release a curated, multi-domain dataset designed for hallucination resistance training, with both factual and fictitious content. \u2022 Evaluation Setup. We benchmark the effectiveness of our approach using GPT-4o-based evaluations and show significant gains in factual accuracy without compromising helpfulness or relevance. \n\n\u2022 Open-source release. We make our code, models, dataset, and evaluation framework publicly available to facilitate further research. They can be accessed in our open-source repository4 and dataset5 .",
                    "score": 0.1515673494698344,
                    "section_title": "Introduction",
                    "char_start_offset": 1761,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 683
                        },
                        {
                            "start": 686,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1100
                        },
                        {
                            "start": 1103,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1574
                        },
                        {
                            "start": 1577,
                            "end": 1610
                        },
                        {
                            "start": 1613,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1793
                        },
                        {
                            "start": 1796,
                            "end": 1815
                        },
                        {
                            "start": 1816,
                            "end": 1948
                        },
                        {
                            "start": 1949,
                            "end": 1968
                        },
                        {
                            "start": 1969,
                            "end": 2140
                        },
                        {
                            "start": 2143,
                            "end": 2165
                        },
                        {
                            "start": 2166,
                            "end": 2276
                        },
                        {
                            "start": 2277,
                            "end": 2343
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.876953125
                },
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
                    "score": 0.166830777478359,
                    "section_title": "Methodology",
                    "char_start_offset": 8999,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 632
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8095703125
                },
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "Our results show that Finetune-RAG significantly improves a model's ability to resist hallucinations in a RAG setting, even when the prompt includes both correct and misleading context. Fine-tuning with dual-context examples leads to consistent improvements in factual accuracy, while preserving helpfulness, relevance, and depth.",
                    "score": 0.16666905827244902,
                    "section_title": "Discussion",
                    "char_start_offset": 19342,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 330
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52880859375
                }
            ],
            "relevance_judgement": 0.876953125,
            "relevance_judgment_input_expanded": "# Title: Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation\n# Venue: \n# Authors: Zhan Peng Lee, Andre Lin, Calvin Tan\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.\n## Introduction\nWe con-structed a diverse dataset covering legal documents, scientific literature, books, and web data, each paired with a plausible but fictitious counterpart. We then fine-tune instruction-tuned LLMs, specifically Meta's Llama 3.1-8B-Instruct (Grattafiori et al., 2024), on this dataset using two prompt variants: a Baseline format and a Structured XML variant. This setup allows us to assess generalization and prompt sensitivity. To our knowledge, Finetune-RAG provides the first RAG dataset of its kind, as existing RAG finetuning datasets implicitly assume perfect information retrieval, and mostly focus only the LLM's ability to extract coherent answers from relevant chunks. \n\nOur key insight is that LLMs struggle to identify contextual clues that are obvious to the human eye, such as financial reports from a similarly named company or outdated information based on dates indicated by document metadata. Through fine-tuning models with a controlled mixture of true and false context placed alongside, we teach them to ground their answers exclusively in the reliable information provided. \n\nWe evaluated the effectiveness of Finetune-RAG using Bench-RAG, a custom benchmarking suite we have created that leverages GPT-4o (OpenAI, 2024) as an automated judge to assess the accuracy, relevance, helpfulness and depth of the LLM response. Our results show that Finetune-RAG substantially improves factual correctness while maintaining output quality across other dimensions, demonstrating that generation-time defenses are a viable complement to improved retrieval. \n\nOur contributions are as follows: \n\n\u2022 Fine-tuning Approach. We propose a novel fine-tuning strategy for RAG systems that teaches models to ignore misleading context and generate answers based solely on factual input. \n\n\u2022 Training Dataset. We release a curated, multi-domain dataset designed for hallucination resistance training, with both factual and fictitious content. \u2022 Evaluation Setup. We benchmark the effectiveness of our approach using GPT-4o-based evaluations and show significant gains in factual accuracy without compromising helpfulness or relevance. \n\n\u2022 Open-source release. We make our code, models, dataset, and evaluation framework publicly available to facilitate further research. They can be accessed in our open-source repository4 and dataset5 .\n\n## Methodology\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.\n\n## Discussion\nOur results show that Finetune-RAG significantly improves a model's ability to resist hallucinations in a RAG setting, even when the prompt includes both correct and misleading context. Fine-tuning with dual-context examples leads to consistent improvements in factual accuracy, while preserving helpfulness, relevance, and depth.",
            "reference_string": "[278714952 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 14,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220669262",
                    "name": "Qingfei Zhao"
                },
                {
                    "authorId": "2314784069",
                    "name": "Ruobing Wang"
                },
                {
                    "authorId": "83546711",
                    "name": "Yukuo Cen"
                },
                {
                    "authorId": "2325728623",
                    "name": "Daren Zha"
                },
                {
                    "authorId": "2327738081",
                    "name": "Shicheng Tan"
                },
                {
                    "authorId": "2243402027",
                    "name": "Yuxiao Dong"
                },
                {
                    "authorId": "2327297599",
                    "name": "Jie Tang"
                }
            ],
            "abstract": "Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the \u201clost in the middle\u201d issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG\u2019s understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system\u2019s components and fine-tuning strategies.Data and code are available at [https://github.com/QingFei1/LongRAG](https://github.com/QingFei1/LongRAG).",
            "corpus_id": 273532096,
            "sentences": [
                {
                    "corpus_id": "273532096",
                    "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
                    "text": "Fine-tuning has gradually become a popular strategy (Ke et al., 2024) for enhancing the capabilities of components of RAG. Existing works include fine-tuning retrieval-related components to achieve better retrieval outcomes (Yan et al., 2024), fine-tuning generators for more personalized outputs (Zhang et al., 2024b), and employing collaborative fine-tuning (Lin et al., 2023). Additionally, Zhou et al. (2023) discovered that fine-tuning LLMs with a limited quantity of high-quality data significantly enhances the performance of LLMs. This finding provides a robust theoretical basis for collaboratively fine-tuning multiple components within advanced RAG methodologies at a minimal data expense.",
                    "score": 0.1665354471763399,
                    "section_title": "Domain-Specific Fine-Tuning for RAG",
                    "char_start_offset": 8022,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 700
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 394,
                            "end": 412,
                            "matchedPaperCorpusId": "258822910"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82763671875
                }
            ],
            "relevance_judgement": 0.82763671875,
            "relevance_judgment_input_expanded": "# Title: LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, Jie Tang\n## Abstract\nLong-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the \u201clost in the middle\u201d issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG\u2019s understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system\u2019s components and fine-tuning strategies.Data and code are available at [https://github.com/QingFei1/LongRAG](https://github.com/QingFei1/LongRAG).\n## Domain-Specific Fine-Tuning for RAG\nFine-tuning has gradually become a popular strategy (Ke et al., 2024) for enhancing the capabilities of components of RAG. Existing works include fine-tuning retrieval-related components to achieve better retrieval outcomes (Yan et al., 2024), fine-tuning generators for more personalized outputs (Zhang et al., 2024b), and employing collaborative fine-tuning (Lin et al., 2023). Additionally, Zhou et al. (2023) discovered that fine-tuning LLMs with a limited quantity of high-quality data significantly enhances the performance of LLMs. This finding provides a robust theoretical basis for collaboratively fine-tuning multiple components within advanced RAG methodologies at a minimal data expense.",
            "reference_string": "[273532096 | Zhao et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 61,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273537815",
                    "name": "Xiaohua Wang"
                },
                {
                    "authorId": "2308276345",
                    "name": "Zhenghua Wang"
                },
                {
                    "authorId": "2292070745",
                    "name": "Xuan Gao"
                },
                {
                    "authorId": "2308226671",
                    "name": "Feiran Zhang"
                },
                {
                    "authorId": "2308043953",
                    "name": "Yixin Wu"
                },
                {
                    "authorId": "2308044030",
                    "name": "Zhibo Xu"
                },
                {
                    "authorId": "2308036711",
                    "name": "Tianyuan Shi"
                },
                {
                    "authorId": "2309182278",
                    "name": "Zhengyuan Wang"
                },
                {
                    "authorId": "2309656885",
                    "name": "Shizheng Li"
                },
                {
                    "authorId": "2309176521",
                    "name": "Qi Qian"
                },
                {
                    "authorId": "2292032843",
                    "name": "Ruicheng Yin"
                },
                {
                    "authorId": "2220896023",
                    "name": "Changze Lv"
                },
                {
                    "authorId": "2257315404",
                    "name": "Xiaoqing Zheng"
                },
                {
                    "authorId": "2257129987",
                    "name": "Xuanjing Huang"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.",
            "corpus_id": 270870251,
            "sentences": [
                {
                    "corpus_id": "270870251",
                    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                    "text": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
                    "score": 0.15864571328242938,
                    "section_title": "Retriever and Generator Fine-tuning",
                    "char_start_offset": 7670,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 94,
                            "end": 242
                        },
                        {
                            "start": 242,
                            "end": 345
                        },
                        {
                            "start": 345,
                            "end": 551
                        },
                        {
                            "start": 553,
                            "end": 739
                        },
                        {
                            "start": 739,
                            "end": 910
                        },
                        {
                            "start": 910,
                            "end": 1037
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 181,
                            "end": 185,
                            "matchedPaperCorpusId": "258865283"
                        },
                        {
                            "start": 658,
                            "end": 660,
                            "matchedPaperCorpusId": "250340214"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80517578125
                }
            ],
            "relevance_judgement": 0.80517578125,
            "relevance_judgment_input_expanded": "# Title: Searching for Best Practices in Retrieval-Augmented Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang\n## Abstract\nRetrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.\n## Retriever and Generator Fine-tuning\nFine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
            "reference_string": "[270870251 | Wang et al. | 2024 | Citations: 61]"
        },
        {
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 10,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279813822",
                    "name": "Liang Zhang"
                },
                {
                    "authorId": "2279831793",
                    "name": "Katherine Jijo"
                },
                {
                    "authorId": "2282528163",
                    "name": "Spurthi Setty"
                },
                {
                    "authorId": "2279830841",
                    "name": "Eden Chung"
                },
                {
                    "authorId": "2282539958",
                    "name": "Fatima Javid"
                },
                {
                    "authorId": "2279830757",
                    "name": "Natan Vidra"
                },
                {
                    "authorId": "2279838243",
                    "name": "Thomas Clifford"
                }
            ],
            "abstract": "Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
            "corpus_id": 267412954,
            "sentences": [
                {
                    "corpus_id": "267412954",
                    "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
                    "text": "For next steps, one thing we can do is more testing to tune different model parameters to see if it would enhance performance. Only chunk size and chunk overlap were adjusted in RAG, but most of the training parameters were kept at the default for the training process. Testing out different embedding models, or even fine tuning the embedding model for the finance specific domain would be a method to try in the near future. Additionally, while supervised fine tuning (SFT) was the primary technique used in this paper, it is also possible to in corporate other methods such as unsupervised fine tuning and reinforcement learning with human feedback into these models to further improve performance. We are also looking to test out more methods to improve retreival algorithms in addition to FLARE and HyDE to help find the best and optimal chunk of text. These include implementing a re-ranking algorithm, such as Cohere's that will rearrange the document chunks based on relevant instead of just similarity. 13 Since we are working with a variety of financial documents with various different and complex sections, metadata annotations could provide some value to further filter document chunks for RAG. In summary, we will continue to strive for more ways to make these models and the fine tuning process faster and more efficient while maintaining and improving model performance.",
                    "score": 0.1387902285918625,
                    "section_title": "Next Steps",
                    "char_start_offset": 23054,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1386
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7958984375
                }
            ],
            "relevance_judgement": 0.7958984375,
            "relevance_judgment_input_expanded": "# Title: Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately\n# Venue: arXiv.org\n# Authors: Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid, Natan Vidra, Thomas Clifford\n## Abstract\nLarge Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.\n## Next Steps\nFor next steps, one thing we can do is more testing to tune different model parameters to see if it would enhance performance. Only chunk size and chunk overlap were adjusted in RAG, but most of the training parameters were kept at the default for the training process. Testing out different embedding models, or even fine tuning the embedding model for the finance specific domain would be a method to try in the near future. Additionally, while supervised fine tuning (SFT) was the primary technique used in this paper, it is also possible to in corporate other methods such as unsupervised fine tuning and reinforcement learning with human feedback into these models to further improve performance. We are also looking to test out more methods to improve retreival algorithms in addition to FLARE and HyDE to help find the best and optimal chunk of text. These include implementing a re-ranking algorithm, such as Cohere's that will rearrange the document chunks based on relevant instead of just similarity. 13 Since we are working with a variety of financial documents with various different and complex sections, metadata annotations could provide some value to further filter document chunks for RAG. In summary, we will continue to strive for more ways to make these models and the fine tuning process faster and more efficient while maintaining and improving model performance.",
            "reference_string": "[267412954 | Zhang et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
            "corpus_id": 271571401,
            "sentences": [
                {
                    "corpus_id": "271571401",
                    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "text": "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.",
                    "score": 0.18491696089551102,
                    "section_title": "E. Tuning Pattern",
                    "char_start_offset": 55399,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 768
                        },
                        {
                            "start": 771,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 920
                        },
                        {
                            "start": 921,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1617
                        },
                        {
                            "start": 1620,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1748
                        },
                        {
                            "start": 1749,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1924
                        },
                        {
                            "start": 1925,
                            "end": 1940
                        },
                        {
                            "start": 1941,
                            "end": 2144
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7880859375
                },
                {
                    "corpus_id": "271571401",
                    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "text": "When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model. \u2022 RL from LLM/human feedback. Reinforcement learning based on feedback from the final generated answers. In addition to using human evaluations, powerful LLMs can also serve as an evaluative judge. 3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the Fig. 16. Generator fine-tuning pattern, The main methods include SFT, distillation and RL from LLM/human feedback. Fig. 17. Dual fine-tuning pattern. In this mode, both the retriever and generator participate in fine-tuning, and their preferences will be aligned. likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
                    "score": 0.1643630500093324,
                    "section_title": "E. Tuning Pattern",
                    "char_start_offset": 57340,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1290
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5732421875
                }
            ],
            "relevance_judgement": 0.7880859375,
            "relevance_judgment_input_expanded": "# Title: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n## Abstract\nRetrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.\n## E. Tuning Pattern\nRAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.\n...\nWhen using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model. \u2022 RL from LLM/human feedback. Reinforcement learning based on feedback from the final generated answers. In addition to using human evaluations, powerful LLMs can also serve as an evaluative judge. 3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the Fig. 16. Generator fine-tuning pattern, The main methods include SFT, distillation and RL from LLM/human feedback. Fig. 17. Dual fine-tuning pattern. In this mode, both the retriever and generator participate in fine-tuning, and their preferences will be aligned. likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
            "reference_string": "[271571401 | Gao et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108795448",
                    "name": "Mingtian Zhang"
                },
                {
                    "authorId": "2284682723",
                    "name": "Shawn Lan"
                },
                {
                    "authorId": "2067492948",
                    "name": "Peter Hayes"
                },
                {
                    "authorId": "2282542157",
                    "name": "David Barber"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.",
            "corpus_id": 267750557,
            "sentences": [
                {
                    "corpus_id": "267750557",
                    "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
                    "text": "In this work, we have introduced Mafin, a novel methodology for fine-tuning black-box embedding models, thereby addressing a significant gap in the field of Retrieval Augmented Generation (RAG). \n\nRecognizing the need for enhanced performance in black-box embedding models, especially when applied to new documents or within specific domains, Mafin effectively meets this challenge by augmenting a black-box model with a small tunable embedding model thus significantly boosting its performance while only requiring a minimal fine-tuning cost. This method leverages both the powerful language representation provided by large pre-trained models and the benefits of fine-tuning with a small embedding model. The low fine-tuning cost of the small models enables its use for large-scale, customized online fine-tuning tailored to each company and individual, promising to be a performance-effective and cost-efficient framework for the RAG infrastructure. \n\nWe have demonstrated Mafin's excellence in fine-tuning embedding models for text retrieval tasks within the RAG framework. Future work will explore Mafin's potential across a broader range of fields. We aim to test and validate our fine-tuning methodology's effectiveness in tasks such as classification and clustering, thereby further expanding the applicability and impact of our approach. \n\nhttp://arxiv.org/ps/2402.12177v4",
                    "score": 0.1384442519572369,
                    "section_title": "Conclusion",
                    "char_start_offset": 23095,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 197,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 952
                        },
                        {
                            "start": 955,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1346
                        },
                        {
                            "start": 1349,
                            "end": 1381
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7607421875
                }
            ],
            "relevance_judgement": 0.7607421875,
            "relevance_judgment_input_expanded": "# Title: Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning\n# Venue: arXiv.org\n# Authors: Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber\n## Abstract\nRetrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.\n## Conclusion\nIn this work, we have introduced Mafin, a novel methodology for fine-tuning black-box embedding models, thereby addressing a significant gap in the field of Retrieval Augmented Generation (RAG). \n\nRecognizing the need for enhanced performance in black-box embedding models, especially when applied to new documents or within specific domains, Mafin effectively meets this challenge by augmenting a black-box model with a small tunable embedding model thus significantly boosting its performance while only requiring a minimal fine-tuning cost. This method leverages both the powerful language representation provided by large pre-trained models and the benefits of fine-tuning with a small embedding model. The low fine-tuning cost of the small models enables its use for large-scale, customized online fine-tuning tailored to each company and individual, promising to be a performance-effective and cost-efficient framework for the RAG infrastructure. \n\nWe have demonstrated Mafin's excellence in fine-tuning embedding models for text retrieval tasks within the RAG framework. Future work will explore Mafin's potential across a broader range of fields. We aim to test and validate our fine-tuning methodology's effectiveness in tasks such as classification and clustering, thereby further expanding the applicability and impact of our approach. \n\nhttp://arxiv.org/ps/2402.12177v4",
            "reference_string": "[267750557 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 80,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2025.3550145",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3550145?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3550145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349663965",
                    "name": "Mahd Hindi"
                },
                {
                    "authorId": "2349670786",
                    "name": "Linda Mohammed"
                },
                {
                    "authorId": "2349663960",
                    "name": "Ommama Maaz"
                },
                {
                    "authorId": "3360346",
                    "name": "Abdulmalik Alwarafy"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is a promising solution that can enhance the capabilities of large language model (LLM) applications in critical domains, including legal technology, by retrieving knowledge from external databases. Implementing RAG pipelines requires careful attention to the techniques and methods implemented in the different stages of the RAG process. However, robust RAG can enhance LLM generation with faithfulness and few hallucinations in responses. In this paper, we discuss the application of RAG in the legal domain. First, we present an overview of the main RAG methods, stages, techniques, and applications in the legal domain. We then briefly discuss the different information retrieval models, processes, and applied methods in current legal RAG solutions. Then, we explain the different quantitative and qualitative evaluation metrics. We also describe several emerging datasets and benchmarks. We then discuss and assess the ethical and privacy considerations for legal RAG and summarize various challenges, and propose a challenge scale based on RAG failure points and control over external knowledge. Finally, we provide insights into promising future research to leverage RAG efficiently and effectively in the legal field.",
            "corpus_id": 276939617,
            "sentences": [
                {
                    "corpus_id": "276939617",
                    "title": "Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey",
                    "text": "Fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from the For example, HyPA-RAG fine-tunes its distilBERT model on legal corpora and CASEGPT adopts a fine-tuned version of Legal-BERT to achieve enhanced retrieval performance [39], [49]. In addition, CamemBERT is fully fine-tuned on the long-form LQA (LLeQA) dataset to improve its ability to handle complex legal queries [21]. Table 4 shows the best Transformers-based retrievers, of which four models are enhanced by fine-tuning processes.",
                    "score": 0.1371084870438863,
                    "section_title": "3) FINE-TUNING RETRIEVAL MODELS",
                    "char_start_offset": 27999,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 593
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 474,
                            "end": 478,
                            "matchedPaperCorpusId": "263310713"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7265625
                }
            ],
            "relevance_judgement": 0.7265625,
            "relevance_judgment_input_expanded": "# Title: Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey\n# Venue: IEEE Access\n# Authors: Mahd Hindi, Linda Mohammed, Ommama Maaz, Abdulmalik Alwarafy\n## Abstract\nRetrieval-Augmented Generation (RAG) is a promising solution that can enhance the capabilities of large language model (LLM) applications in critical domains, including legal technology, by retrieving knowledge from external databases. Implementing RAG pipelines requires careful attention to the techniques and methods implemented in the different stages of the RAG process. However, robust RAG can enhance LLM generation with faithfulness and few hallucinations in responses. In this paper, we discuss the application of RAG in the legal domain. First, we present an overview of the main RAG methods, stages, techniques, and applications in the legal domain. We then briefly discuss the different information retrieval models, processes, and applied methods in current legal RAG solutions. Then, we explain the different quantitative and qualitative evaluation metrics. We also describe several emerging datasets and benchmarks. We then discuss and assess the ethical and privacy considerations for legal RAG and summarize various challenges, and propose a challenge scale based on RAG failure points and control over external knowledge. Finally, we provide insights into promising future research to leverage RAG efficiently and effectively in the legal field.\n## 3) FINE-TUNING RETRIEVAL MODELS\nFine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from the For example, HyPA-RAG fine-tunes its distilBERT model on legal corpora and CASEGPT adopts a fine-tuned version of Legal-BERT to achieve enhanced retrieval performance [39], [49]. In addition, CamemBERT is fully fine-tuned on the long-form LQA (LLeQA) dataset to improve its ability to handle complex legal queries [21]. Table 4 shows the best Transformers-based retrievers, of which four models are enhanced by fine-tuning processes.",
            "reference_string": "[276939617 | Hindi et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.02411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2353386625",
                    "name": "Alexandre Misrahi"
                },
                {
                    "authorId": "2258716783",
                    "name": "Nadezhda Chirkova"
                },
                {
                    "authorId": "2342407524",
                    "name": "Maxime Louis"
                },
                {
                    "authorId": "2841761",
                    "name": "Vassilina Nikoulina"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but multi-domain applications face challenges like lack of diverse benchmarks and poor out-of-domain generalization. The first contribution of this work is to introduce a diverse benchmark comprising a variety of question-answering tasks from 8 sources and covering 13 domains. Our second contribution consists in systematically testing out-of-domain generalization for typical RAG tuning strategies. While our findings reveal that standard fine-tuning fails to generalize effectively, we show that sequence-level distillation with teacher-generated labels improves out-of-domain performance by providing more coherent supervision. Our findings highlight key strategies for improving multi-domain RAG robustness.",
            "corpus_id": 277510500,
            "sentences": [
                {
                    "corpus_id": "277510500",
                    "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation",
                    "text": "Recent works have demonstrated that it is beneficial to finetune an LLM to encourage it to better use retrieved context [21,22,32,37]. It has been shown to improve RAG performances in-domain, and we investigate whether this holds for out-of-domain RAG generalization. To do so, we run supervised fine-tuning on the MultiQA dataset9 : a dataset consisting of 450k general domain questions and answers, described in Appendix Table 5. Its associated document collection consists of Wikipedia [30] and MSMARCO [2] documents. Each supervised fine-tuning sample then consists of a prompt (taken from [32]) with 5 retrieved documents, the question and its answer. Models are trained with LoRA [12]. After training, we evaluate the RAG-adapted models on the multi-domain benchmark. \n\nTraining hyper-parameters are given in Appendix B.",
                    "score": 0.14063117619650303,
                    "section_title": "LLM Finetuning for RAG",
                    "char_start_offset": 10052,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 826
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70556640625
                }
            ],
            "relevance_judgement": 0.70556640625,
            "relevance_judgment_input_expanded": "# Title: Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation\n# Venue: arXiv.org\n# Authors: Alexandre Misrahi, Nadezhda Chirkova, Maxime Louis, Vassilina Nikoulina\n## Abstract\nRetrieval-Augmented Generation (RAG) enhances LLM factuality, but multi-domain applications face challenges like lack of diverse benchmarks and poor out-of-domain generalization. The first contribution of this work is to introduce a diverse benchmark comprising a variety of question-answering tasks from 8 sources and covering 13 domains. Our second contribution consists in systematically testing out-of-domain generalization for typical RAG tuning strategies. While our findings reveal that standard fine-tuning fails to generalize effectively, we show that sequence-level distillation with teacher-generated labels improves out-of-domain performance by providing more coherent supervision. Our findings highlight key strategies for improving multi-domain RAG robustness.\n## LLM Finetuning for RAG\nRecent works have demonstrated that it is beneficial to finetune an LLM to encourage it to better use retrieved context [21,22,32,37]. It has been shown to improve RAG performances in-domain, and we investigate whether this holds for out-of-domain RAG generalization. To do so, we run supervised fine-tuning on the MultiQA dataset9 : a dataset consisting of 450k general domain questions and answers, described in Appendix Table 5. Its associated document collection consists of Wikipedia [30] and MSMARCO [2] documents. Each supervised fine-tuning sample then consists of a prompt (taken from [32]) with 5 retrieved documents, the question and its answer. Models are trained with LoRA [12]. After training, we evaluate the RAG-adapted models on the multi-domain benchmark. \n\nTraining hyper-parameters are given in Appendix B.",
            "reference_string": "[277510500 | Misrahi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279752649",
                    "name": "Scott Barnett"
                },
                {
                    "authorId": "2279020735",
                    "name": "Zach Brannelly"
                },
                {
                    "authorId": "2266469333",
                    "name": "Stefanus Kurniawan"
                },
                {
                    "authorId": "2307101480",
                    "name": "Sheng Wong"
                }
            ],
            "abstract": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "corpus_id": 270560495,
            "sentences": [
                {
                    "corpus_id": "270560495",
                    "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
                    "text": "This process involves training existing pre-trained LLMs on domain-specific curated data, thereby enhancing their answering capabilities by adjusting the weights of the model's parameters.Fine-tuning involves optimizing the model's performance for the target task by adjusting and updating its weights based on domain-specific data during training.This process allows the model to learn task-specific information, thereby improving its ability to generate accurate and relevant responses.Fine-tuning is essential for adapting pre-trained LLMs to new tasks or domains without requiring complete retraining from scratch, which results in improved cost efficiency and reduced computational overhead.\n\nIn this study, we examine the effects of fine-tuning LLMs within a RAG pipeline on their question-answering performance.Initially conducted on private and proprietary datasets from a telecommunications company consisting of customer inquiries to the support team, we replicated the experiment using publicly available datasets.This research seeks to answer two research questions: Firstly, how do fine-tuned models compare to its baseline counterparts in a RAG pipeline?Secondly, does the size of the training dataset have an impact on the effectiveness of fine-tuning?By addressing these questions, we aim to contribute to a better understanding of how fine-tuning affects the question-answering abilities of RAG-integrated LLMs across various domains.",
                    "score": 0.14136569889205197,
                    "section_title": "Introduction",
                    "char_start_offset": 2321,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 188,
                            "end": 348
                        },
                        {
                            "start": 348,
                            "end": 488
                        },
                        {
                            "start": 488,
                            "end": 696
                        },
                        {
                            "start": 698,
                            "end": 818
                        },
                        {
                            "start": 818,
                            "end": 1025
                        },
                        {
                            "start": 1025,
                            "end": 1168
                        },
                        {
                            "start": 1168,
                            "end": 1267
                        },
                        {
                            "start": 1267,
                            "end": 1451
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.689453125
                }
            ],
            "relevance_judgement": 0.689453125,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models\n# Venue: arXiv.org\n# Authors: Scott Barnett, Zach Brannelly, Stefanus Kurniawan, Sheng Wong\n## Abstract\nLarge Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.\n## Introduction\nThis process involves training existing pre-trained LLMs on domain-specific curated data, thereby enhancing their answering capabilities by adjusting the weights of the model's parameters.Fine-tuning involves optimizing the model's performance for the target task by adjusting and updating its weights based on domain-specific data during training.This process allows the model to learn task-specific information, thereby improving its ability to generate accurate and relevant responses.Fine-tuning is essential for adapting pre-trained LLMs to new tasks or domains without requiring complete retraining from scratch, which results in improved cost efficiency and reduced computational overhead.\n\nIn this study, we examine the effects of fine-tuning LLMs within a RAG pipeline on their question-answering performance.Initially conducted on private and proprietary datasets from a telecommunications company consisting of customer inquiries to the support team, we replicated the experiment using publicly available datasets.This research seeks to answer two research questions: Firstly, how do fine-tuned models compare to its baseline counterparts in a RAG pipeline?Secondly, does the size of the training dataset have an impact on the effectiveness of fine-tuning?By addressing these questions, we aim to contribute to a better understanding of how fine-tuning affects the question-answering abilities of RAG-integrated LLMs across various domains.",
            "reference_string": "[270560495 | Barnett et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349833035",
                    "name": "Jiale Wei"
                },
                {
                    "authorId": "2351464722",
                    "name": "Shuchi Wu"
                },
                {
                    "authorId": "2351243441",
                    "name": "Ruochen Liu"
                },
                {
                    "authorId": "2308276200",
                    "name": "Xiang Ying"
                },
                {
                    "authorId": "2308276676",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "2351054847",
                    "name": "Fangbo Tao"
                }
            ],
            "abstract": "Memory, additional information beyond the training of large language models (LLMs), is crucial to various real-world applications, such as personal assistant. The two mainstream solutions to incorporate memory into the generation process are long-context LLMs and retrieval-augmented generation (RAG). In this paper, we first systematically compare these two types of solutions on three renovated/new datasets and show that (1) long-context solutions, although more expensive, shall be easier to capture the big picture and better answer queries which require considering the memory as a whole; and (2) when the queries concern specific information, RAG solutions shall be more competitive especially when the keywords can be explicitly matched. Therefore, we propose a novel method RAG-Tuned-LLM which fine-tunes a relative small (e.g., 7B) LLM using the data generated following the RAG principles, so it can combine the advantages of both solutions. Extensive experiments on three datasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG methods across a wide range of query types.",
            "corpus_id": 277150553,
            "sentences": [
                {
                    "corpus_id": "277150553",
                    "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
                    "text": "To incrementally expand the knowledge of a pretrained LLM or to align it with human preferences, fine-tuning stands as one of the most prevalent approaches, encompassing methods such as supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), and direct preference optimization (DPO) (Rafailov et al., 2024). Despite their effectiveness, these techniques are notably annotationheavy and computationally intensive, rendering the fine-tuning of an LLM using these methods prohibitively costly. To circumvent the extensive computational demands of full fine-tuning, which can reach into tens of billions, numerous parameterefficient fine-tuning (PEFT) methods have been explored, including BitFit (Zaken et al., 2021), adapter (Houlsby et al., 2019), andLora (Hu et al., 2021). In this paper, we primarily employ a LoRA to fine-tune a RAG-Tuned-LLM. Methodologically, RAFT (Zhang et al., 2024b) is the closest to our approach, as it explores the potential integration of RAG and fine-tuning. However, there are two fundamental differences between our work and RAFT: First, the model we train is not intended for use in the generation stage of RAG, making our objectives fundamentally different; Second, our training data does not include deliberately introduced noise, which distinguishes our approach significantly in terms of methodology.",
                    "score": 0.15640968066394403,
                    "section_title": "Fine-Tuning LLMs",
                    "char_start_offset": 21885,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 892
                        },
                        {
                            "start": 893,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1383
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 266,
                            "end": 287,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 330,
                            "end": 353,
                            "matchedPaperCorpusId": "258959321"
                        },
                        {
                            "start": 770,
                            "end": 797,
                            "matchedPaperCorpusId": "59599816"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68505859375
                },
                {
                    "corpus_id": "277150553",
                    "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
                    "text": "In this paper, we validate RAG's fine-grained retrieval abilities and the global abstraction strengths of LLM-native solutions. However, RAG lacks holistic understanding, and long-context models tend to lose key information over extended contexts. We integrate these strengths of both RAG and LLM-native solutions by fine-tuning an LLM within an RAG framework for data generation. This work is the first to explore LLM and RAG integration within a unified framework, bridging open-domain and domain-specific query-answering tasks. Our RAG-Tuned LLM, equipped with LLMnative memory, outperforms both standard RAG methods and long-context LLMs across diverse datasets, demonstrating superior performance in handling hierarchical queries. \n\nFuture Work. Building on this study, several future directions are worth exploring to further validate and enhance our proposed method. First, we plan to extend RAG-Tuned-LLM to more diverse datasets and domains, enabling us to evaluate its generalizability across different tasks, including complex challenges like multi-hop reasoning and multi-modal query-answering. This will provide a clearer understanding of RAG-Tuned-LLM's effectiveness in both open-domain and domain-specific contexts. Additionally, we will experiment with various foundational models (e.g., the Llama series (Touvron et al., 2023a,b)), evaluating RAG-Tuned-LLM's adaptability to different architectures and model scales. This will highlight the tradeoffs between model size, computational efficiency, and performance when combining RAG and LLMnative methods.",
                    "score": 0.143342741966782,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 23299,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 735
                        },
                        {
                            "start": 738,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 873
                        },
                        {
                            "start": 874,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1572
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.48291015625
                }
            ],
            "relevance_judgement": 0.68505859375,
            "relevance_judgment_input_expanded": "# Title: Tuning LLMs by RAG Principles: Towards LLM-native Memory\n# Venue: arXiv.org\n# Authors: Jiale Wei, Shuchi Wu, Ruochen Liu, Xiang Ying, Jingbo Shang, Fangbo Tao\n## Abstract\nMemory, additional information beyond the training of large language models (LLMs), is crucial to various real-world applications, such as personal assistant. The two mainstream solutions to incorporate memory into the generation process are long-context LLMs and retrieval-augmented generation (RAG). In this paper, we first systematically compare these two types of solutions on three renovated/new datasets and show that (1) long-context solutions, although more expensive, shall be easier to capture the big picture and better answer queries which require considering the memory as a whole; and (2) when the queries concern specific information, RAG solutions shall be more competitive especially when the keywords can be explicitly matched. Therefore, we propose a novel method RAG-Tuned-LLM which fine-tunes a relative small (e.g., 7B) LLM using the data generated following the RAG principles, so it can combine the advantages of both solutions. Extensive experiments on three datasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG methods across a wide range of query types.\n## Fine-Tuning LLMs\nTo incrementally expand the knowledge of a pretrained LLM or to align it with human preferences, fine-tuning stands as one of the most prevalent approaches, encompassing methods such as supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), and direct preference optimization (DPO) (Rafailov et al., 2024). Despite their effectiveness, these techniques are notably annotationheavy and computationally intensive, rendering the fine-tuning of an LLM using these methods prohibitively costly. To circumvent the extensive computational demands of full fine-tuning, which can reach into tens of billions, numerous parameterefficient fine-tuning (PEFT) methods have been explored, including BitFit (Zaken et al., 2021), adapter (Houlsby et al., 2019), andLora (Hu et al., 2021). In this paper, we primarily employ a LoRA to fine-tune a RAG-Tuned-LLM. Methodologically, RAFT (Zhang et al., 2024b) is the closest to our approach, as it explores the potential integration of RAG and fine-tuning. However, there are two fundamental differences between our work and RAFT: First, the model we train is not intended for use in the generation stage of RAG, making our objectives fundamentally different; Second, our training data does not include deliberately introduced noise, which distinguishes our approach significantly in terms of methodology.\n\n## Conclusion and Future Work\nIn this paper, we validate RAG's fine-grained retrieval abilities and the global abstraction strengths of LLM-native solutions. However, RAG lacks holistic understanding, and long-context models tend to lose key information over extended contexts. We integrate these strengths of both RAG and LLM-native solutions by fine-tuning an LLM within an RAG framework for data generation. This work is the first to explore LLM and RAG integration within a unified framework, bridging open-domain and domain-specific query-answering tasks. Our RAG-Tuned LLM, equipped with LLMnative memory, outperforms both standard RAG methods and long-context LLMs across diverse datasets, demonstrating superior performance in handling hierarchical queries. \n\nFuture Work. Building on this study, several future directions are worth exploring to further validate and enhance our proposed method. First, we plan to extend RAG-Tuned-LLM to more diverse datasets and domains, enabling us to evaluate its generalizability across different tasks, including complex challenges like multi-hop reasoning and multi-modal query-answering. This will provide a clearer understanding of RAG-Tuned-LLM's effectiveness in both open-domain and domain-specific contexts. Additionally, we will experiment with various foundational models (e.g., the Llama series (Touvron et al., 2023a,b)), evaluating RAG-Tuned-LLM's adaptability to different architectures and model scales. This will highlight the tradeoffs between model size, computational efficiency, and performance when combining RAG and LLMnative methods.",
            "reference_string": "[277150553 | Wei et al. | 2025 | Citations: 0]"
        },
        {
            "title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation",
            "venue": "",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314784069",
                    "name": "Ruobing Wang"
                },
                {
                    "authorId": "2325728623",
                    "name": "Daren Zha"
                },
                {
                    "authorId": "2314785970",
                    "name": "Shi Yu"
                },
                {
                    "authorId": "2220669262",
                    "name": "Qingfei Zhao"
                },
                {
                    "authorId": "2311726706",
                    "name": "Yuxuan Chen"
                },
                {
                    "authorId": "2310129232",
                    "name": "Yixuan Wang"
                },
                {
                    "authorId": "2267033597",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2277242040",
                    "name": "Yukun Yan"
                },
                {
                    "authorId": "49047064",
                    "name": "Zhenghao Liu"
                },
                {
                    "authorId": "2284728053",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2266886975",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) mitigates factual errors and hallucinations in Large Language Models (LLMs) for question-answering (QA) by incorporating external knowledge. However, existing adaptive RAG methods rely on LLMs to predict retrieval timing and directly use retrieved information for generation, often failing to reflect real information needs and fully leverage retrieved knowledge. We develop DeepNote, an adaptive RAG framework that achieves in-depth and robust exploration of knowledge sources through note-centric adaptive retrieval. DeepNote employs notes as carriers for refining and accumulating knowledge. During in-depth exploration, it uses these notes to determine retrieval timing, formulate retrieval queries, and iteratively assess knowledge growth, ultimately leveraging the best note for answer generation. Extensive experiments and analyses demonstrate that DeepNote significantly outperforms all baselines (+10.2% to +20.1%) and exhibits the ability to gather knowledge with both high density and quality. Additionally, DPO further improves the performance of DeepNote. The code and data are available at https://github.com/thunlp/DeepNote.",
            "corpus_id": 273323657,
            "sentences": [
                {
                    "corpus_id": "273323657",
                    "title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation",
                    "text": "Fine-tuning is widely used to improve the capabilities of LLM-augmented components in RAG systems (de Luis Balaguer et al., 2024). Early methods of fine-tuning to enhance LLM-based components in RAG primarily focused on training the retriever and the generator (Ke et al., 2024;Lin et al., 2024). Recent RAG methods have shifted toward modular designs (Gao et al., 2023b). Particularly in complex QA tasks, adaptive RAG often requires base models to follow intricate instructions (Yin et al., 2023a;Xu et al., 2024) to enable the functionality of diverse components (Asai et al., 2024). Classic alignment training methods include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). However, SFT lacks negative feedback and is prone to overfitting. Recently, Rafailov et al. proposed a more efficient reinforcement learning algorithm, direct preference optimization (DPO), which aligns response preferences and enhances the model's instruction-following ability by learning the differences between positive and negative sample pairs. In our work, we focus on using DPO to enhance the model's capability in multiple processes.",
                    "score": 0.15007998214881818,
                    "section_title": "Fine-Tuning for RAG",
                    "char_start_offset": 5440,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1156
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 261,
                            "end": 278,
                            "matchedPaperCorpusId": "266998785"
                        },
                        {
                            "start": 278,
                            "end": 295,
                            "matchedPaperCorpusId": "263605962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67529296875
                }
            ],
            "relevance_judgement": 0.67529296875,
            "relevance_judgment_input_expanded": "# Title: DeepNote: Note-Centric Deep Retrieval-Augmented Generation\n# Venue: \n# Authors: Ruobing Wang, Daren Zha, Shi Yu, Qingfei Zhao, Yuxuan Chen, Yixuan Wang, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, Maosong Sun\n## Abstract\nRetrieval-Augmented Generation (RAG) mitigates factual errors and hallucinations in Large Language Models (LLMs) for question-answering (QA) by incorporating external knowledge. However, existing adaptive RAG methods rely on LLMs to predict retrieval timing and directly use retrieved information for generation, often failing to reflect real information needs and fully leverage retrieved knowledge. We develop DeepNote, an adaptive RAG framework that achieves in-depth and robust exploration of knowledge sources through note-centric adaptive retrieval. DeepNote employs notes as carriers for refining and accumulating knowledge. During in-depth exploration, it uses these notes to determine retrieval timing, formulate retrieval queries, and iteratively assess knowledge growth, ultimately leveraging the best note for answer generation. Extensive experiments and analyses demonstrate that DeepNote significantly outperforms all baselines (+10.2% to +20.1%) and exhibits the ability to gather knowledge with both high density and quality. Additionally, DPO further improves the performance of DeepNote. The code and data are available at https://github.com/thunlp/DeepNote.\n## Fine-Tuning for RAG\nFine-tuning is widely used to improve the capabilities of LLM-augmented components in RAG systems (de Luis Balaguer et al., 2024). Early methods of fine-tuning to enhance LLM-based components in RAG primarily focused on training the retriever and the generator (Ke et al., 2024;Lin et al., 2024). Recent RAG methods have shifted toward modular designs (Gao et al., 2023b). Particularly in complex QA tasks, adaptive RAG often requires base models to follow intricate instructions (Yin et al., 2023a;Xu et al., 2024) to enable the functionality of diverse components (Asai et al., 2024). Classic alignment training methods include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). However, SFT lacks negative feedback and is prone to overfitting. Recently, Rafailov et al. proposed a more efficient reinforcement learning algorithm, direct preference optimization (DPO), which aligns response preferences and enhances the model's instruction-following ability by learning the differences between positive and negative sample pairs. In our work, we focus on using DPO to enhance the model's capability in multiple processes.",
            "reference_string": "[273323657 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.02545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296788981",
                    "name": "Daniel Fleischer"
                },
                {
                    "authorId": "2077591838",
                    "name": "Moshe Berchansky"
                },
                {
                    "authorId": "2134755",
                    "name": "Moshe Wasserblat"
                },
                {
                    "authorId": "2477428",
                    "name": "Peter Izsak"
                }
            ],
            "abstract": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.",
            "corpus_id": 271710111,
            "sentences": [
                {
                    "corpus_id": "271710111",
                    "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
                    "text": "We introduced RAG FOUNDRY, an open-source library dedicated to the task of RAG-augmentation of LLMs, namely fine-tuning LLMs to become better at RAG settings. The library is designed to serve as an end-to-end experimentation environment, enabling users to quickly prototype and experiment with different RAG techniques. We demonstrated the usefulness of the library by augmenting two models with RAG configurations, evaluating on three Q&A datasets and showing the benefit of RAG techniques, as well as of using multi-aspect metrics relevant for RAG systems evaluation.",
                    "score": 0.1538227580982834,
                    "section_title": "Conclusion",
                    "char_start_offset": 15878,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 569
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65576171875
                },
                {
                    "corpus_id": "271710111",
                    "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
                    "text": "We explore several techniques for RAG augmentation, and use RAG FOUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and a CoT scheme which guides the model to use the retrieved context, explain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To complement CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt. All prompt templates are included in appendix A.1.",
                    "score": 0.1416248627825137,
                    "section_title": "RAG Augmentation Techniques",
                    "char_start_offset": 11783,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 553
                        },
                        {
                            "start": 554,
                            "end": 605
                        },
                        {
                            "start": 606,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 972
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.4306640625
                }
            ],
            "relevance_judgement": 0.65576171875,
            "relevance_judgment_input_expanded": "# Title: RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation\n# Venue: arXiv.org\n# Authors: Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak\n## Abstract\nImplementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.\n## RAG Augmentation Techniques\nWe explore several techniques for RAG augmentation, and use RAG FOUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and a CoT scheme which guides the model to use the retrieved context, explain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To complement CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt. All prompt templates are included in appendix A.1.\n\n## Conclusion\nWe introduced RAG FOUNDRY, an open-source library dedicated to the task of RAG-augmentation of LLMs, namely fine-tuning LLMs to become better at RAG settings. The library is designed to serve as an end-to-end experimentation environment, enabling users to quickly prototype and experiment with different RAG techniques. We demonstrated the usefulness of the library by augmenting two models with RAG configurations, evaluating on three Q&A datasets and showing the benefit of RAG techniques, as well as of using multi-aspect metrics relevant for RAG systems evaluation.",
            "reference_string": "[271710111 | Fleischer et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 60,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.04666, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359632364",
                    "name": "Mohammad Aqib"
                },
                {
                    "authorId": "2359630758",
                    "name": "Mohd Hamza"
                },
                {
                    "authorId": "2335471239",
                    "name": "Qipei Mei"
                },
                {
                    "authorId": "2273503155",
                    "name": "Y. Chui"
                }
            ],
            "abstract": "Building codes are regulations that establish standards for the design, construction, and safety of buildings to ensure structural integrity, fire protection, and accessibility. They are often extensive, complex, and subject to frequent updates, making manual querying challenging and time-consuming. Key difficulties include navigating large volumes of text, interpreting technical language, and identifying relevant clauses across different sections. A potential solution is to build a Question-Answering (QA) system that answers user queries based on building codes. Among the various methods for building a QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG consists of two components: a retriever and a language model. This study focuses on identifying a suitable retriever method for building codes and optimizing the generational capability of the language model using fine-tuning techniques. We conducted a detailed evaluation of various retrieval methods by performing the retrieval on the National Building Code of Canada (NBCC) and explored the impact of domain-specific fine-tuning on several language models using the dataset derived from NBCC. Our analysis included a comparative assessment of different retrievers and the performance of both pre-trained and fine-tuned models to determine the efficacy and domain-specific adaptation of language models using fine-tuning on the NBCC dataset. Experimental results showed that Elasticsearch proved to be the most robust retriever among all. The findings also indicate that fine-tuning language models on an NBCC-specific dataset can enhance their ability to generate contextually relevant responses. When combined with context retrieved by a powerful retriever like Elasticsearch, this improvement in LLM performance can optimize the RAG system, enabling it to better navigate the complexities of the NBCC.",
            "corpus_id": 278394304,
            "sentences": [
                {
                    "corpus_id": "278394304",
                    "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes",
                    "text": "Although LLMs have been very popular lately, domain-specific tasks are still an issue for them [27]. The major problem is that models generate wrong answers when they are queried outside of their training data on occasions, a condition called hallucination [28]. RAG can withstand such drastic issues caused by hallucination and is widely utilized to enhance the quality and efficiency of generated content nowadays [29]. RAG makes use of the retrieval methods and generation capabilities of LLMs to produce contextually full and relevant responses to the questions asked from external knowledge databases [30]. In a RAG framework, retrievers play a critical role by extracting the relevant context from a large document in response to a query. This retrieved context is then passed to the LLM, which generates the final answer. The effectiveness of a RAG system relies heavily on the quality of the retrieval process. \n\nFine-tuning is another technique for adapting pre-trained LLMs to new tasks and reducing hallucinations by continuing training on new data [31]. Fine-tuning involves training pre-trained models on task-specific datasets, which helps in making the model capable enough to perform better on the desired task. It has been found that fine-tuning the LLM in an RAG system can help improve its performance, as the LLM is responsible for text generation utilizing the retrieved information [32]. However, full fine-tuning, which initializes the model with pre-trained weights and updates all of them, becomes impractical when dealing with models having billions of parameters [33]. Great efforts are put into avoiding training all parameters of the model and promoting an efficient fine-tuning process [34]. There are several techniques that are utilized to fine-tune models efficiently and can resolve the issue of heavy computational dependency that arises due to full fine-tuning. One of the most popular techniques for fine-tuning language models is parameter-efficient fine-tuning (PEFT), which efficiently adapts language models to specific downstream tasks while minimizing the constraints of dependency on computational resources and storage requirements.",
                    "score": 0.14397895780695333,
                    "section_title": "Introduction",
                    "char_start_offset": 4752,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 828
                        },
                        {
                            "start": 829,
                            "end": 918
                        },
                        {
                            "start": 921,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1721
                        },
                        {
                            "start": 1722,
                            "end": 1897
                        },
                        {
                            "start": 1898,
                            "end": 2177
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1404,
                            "end": 1408,
                            "matchedPaperCorpusId": "268033066"
                        },
                        {
                            "start": 1590,
                            "end": 1594,
                            "matchedPaperCorpusId": "257316425"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64306640625
                }
            ],
            "relevance_judgement": 0.64306640625,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes\n# Venue: arXiv.org\n# Authors: Mohammad Aqib, Mohd Hamza, Qipei Mei, Y. Chui\n## Abstract\nBuilding codes are regulations that establish standards for the design, construction, and safety of buildings to ensure structural integrity, fire protection, and accessibility. They are often extensive, complex, and subject to frequent updates, making manual querying challenging and time-consuming. Key difficulties include navigating large volumes of text, interpreting technical language, and identifying relevant clauses across different sections. A potential solution is to build a Question-Answering (QA) system that answers user queries based on building codes. Among the various methods for building a QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG consists of two components: a retriever and a language model. This study focuses on identifying a suitable retriever method for building codes and optimizing the generational capability of the language model using fine-tuning techniques. We conducted a detailed evaluation of various retrieval methods by performing the retrieval on the National Building Code of Canada (NBCC) and explored the impact of domain-specific fine-tuning on several language models using the dataset derived from NBCC. Our analysis included a comparative assessment of different retrievers and the performance of both pre-trained and fine-tuned models to determine the efficacy and domain-specific adaptation of language models using fine-tuning on the NBCC dataset. Experimental results showed that Elasticsearch proved to be the most robust retriever among all. The findings also indicate that fine-tuning language models on an NBCC-specific dataset can enhance their ability to generate contextually relevant responses. When combined with context retrieved by a powerful retriever like Elasticsearch, this improvement in LLM performance can optimize the RAG system, enabling it to better navigate the complexities of the NBCC.\n## Introduction\nAlthough LLMs have been very popular lately, domain-specific tasks are still an issue for them [27]. The major problem is that models generate wrong answers when they are queried outside of their training data on occasions, a condition called hallucination [28]. RAG can withstand such drastic issues caused by hallucination and is widely utilized to enhance the quality and efficiency of generated content nowadays [29]. RAG makes use of the retrieval methods and generation capabilities of LLMs to produce contextually full and relevant responses to the questions asked from external knowledge databases [30]. In a RAG framework, retrievers play a critical role by extracting the relevant context from a large document in response to a query. This retrieved context is then passed to the LLM, which generates the final answer. The effectiveness of a RAG system relies heavily on the quality of the retrieval process. \n\nFine-tuning is another technique for adapting pre-trained LLMs to new tasks and reducing hallucinations by continuing training on new data [31]. Fine-tuning involves training pre-trained models on task-specific datasets, which helps in making the model capable enough to perform better on the desired task. It has been found that fine-tuning the LLM in an RAG system can help improve its performance, as the LLM is responsible for text generation utilizing the retrieved information [32]. However, full fine-tuning, which initializes the model with pre-trained weights and updates all of them, becomes impractical when dealing with models having billions of parameters [33]. Great efforts are put into avoiding training all parameters of the model and promoting an efficient fine-tuning process [34]. There are several techniques that are utilized to fine-tune models efficiently and can resolve the issue of heavy computational dependency that arises due to full fine-tuning. One of the most popular techniques for fine-tuning language models is parameter-efficient fine-tuning (PEFT), which efficiently adapts language models to specific downstream tasks while minimizing the constraints of dependency on computational resources and storage requirements.",
            "reference_string": "[278394304 | Aqib et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 40,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.15191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350956983",
                    "name": "Sejong Kim"
                },
                {
                    "authorId": "2350857179",
                    "name": "Hyunseo Song"
                },
                {
                    "authorId": "2351317331",
                    "name": "Hyunwoo Seo"
                },
                {
                    "authorId": "2350867590",
                    "name": "Hyunjun Kim"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR.",
            "corpus_id": 277113527,
            "sentences": [
                {
                    "corpus_id": "277113527",
                    "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
                    "text": "2.1 EMBEDDER FINE-TUNING Fine-tuning, the process of adapting a pre-trained model to domain-specific tasks using typically smaller datasets, has been widely explored across various applications. While embedding models exhibit strong zero-shot performance on general benchmarks such as MTEB (Muennighoff et al., 2023;Zhang et al., 2017), recent studies have demonstrated that even modestly sized models can benefit substantially from fine-tuning when applied to domain-specific tasks. For instance, finetuning embedders on specialized datasets has led to notable improvements in areas such as medical question answering Sengupta et al. (2024) and financial question answering Anderson et al. (2024). In the finance domain, prior research on embedders has underscored several inherent challenges: domain-specific vocabulary and semantic patterns, the complexity of multi-hop queries, and multimodal data (e.g. text, tables, and time-series) Tang & Yang (2024); Kim et al. (2024); Xie et al. (2024). These challenges necessitate tailored fine-tuning strategies that can effectively capture the nuanced information contained in financial documents. Within the framework of Retrieval-Augmented Generation (RAG), embedding models are primarily tasked with Information Retrieval (IR), where the semantic similarity between a query and a corpus is assessed and ranked. A prevalent strategy for enhancing this process is contrastive learning or contrastive fine-tuning-which relies on constructing triplets (query, relevant corpus, irrelevant corpus) to form positive and negative training pairs Karpukhin et al. (2020). Despite the effectiveness of contrastive learning in embedders Lu et al. (2024), there remains a notable gap in the literature regarding the impact of embedder fine-tuning on RAG systems, particularly within the finance domain Setty et al. (2024). By addressing this gap, our work aims to explore and quantify the benefits of embedder fine-tuning in RAG applications, thereby contributing to the broader understanding of domain-adaptive IR.",
                    "score": 0.14586653288160292,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 2115,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 907
                        },
                        {
                            "start": 908,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1611
                        },
                        {
                            "start": 1612,
                            "end": 1859
                        },
                        {
                            "start": 1860,
                            "end": 2052
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 290,
                            "end": 316,
                            "matchedPaperCorpusId": "252907685"
                        },
                        {
                            "start": 316,
                            "end": 335,
                            "matchedPaperCorpusId": "206595438"
                        },
                        {
                            "start": 619,
                            "end": 641,
                            "matchedPaperCorpusId": "266999115"
                        },
                        {
                            "start": 675,
                            "end": 697,
                            "matchedPaperCorpusId": "273901204"
                        },
                        {
                            "start": 1587,
                            "end": 1610,
                            "matchedPaperCorpusId": "215737187"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6181640625
                }
            ],
            "relevance_judgement": 0.6181640625,
            "relevance_judgment_input_expanded": "# Title: Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems\n# Venue: arXiv.org\n# Authors: Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR.\n## RELATED WORK\n2.1 EMBEDDER FINE-TUNING Fine-tuning, the process of adapting a pre-trained model to domain-specific tasks using typically smaller datasets, has been widely explored across various applications. While embedding models exhibit strong zero-shot performance on general benchmarks such as MTEB (Muennighoff et al., 2023;Zhang et al., 2017), recent studies have demonstrated that even modestly sized models can benefit substantially from fine-tuning when applied to domain-specific tasks. For instance, finetuning embedders on specialized datasets has led to notable improvements in areas such as medical question answering Sengupta et al. (2024) and financial question answering Anderson et al. (2024). In the finance domain, prior research on embedders has underscored several inherent challenges: domain-specific vocabulary and semantic patterns, the complexity of multi-hop queries, and multimodal data (e.g. text, tables, and time-series) Tang & Yang (2024); Kim et al. (2024); Xie et al. (2024). These challenges necessitate tailored fine-tuning strategies that can effectively capture the nuanced information contained in financial documents. Within the framework of Retrieval-Augmented Generation (RAG), embedding models are primarily tasked with Information Retrieval (IR), where the semantic similarity between a query and a corpus is assessed and ranked. A prevalent strategy for enhancing this process is contrastive learning or contrastive fine-tuning-which relies on constructing triplets (query, relevant corpus, irrelevant corpus) to form positive and negative training pairs Karpukhin et al. (2020). Despite the effectiveness of contrastive learning in embedders Lu et al. (2024), there remains a notable gap in the literature regarding the impact of embedder fine-tuning on RAG systems, particularly within the finance domain Setty et al. (2024). By addressing this gap, our work aims to explore and quantify the benefits of embedder fine-tuning in RAG applications, thereby contributing to the broader understanding of domain-adaptive IR.",
            "reference_string": "[277113527 | Kim et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2322992640",
                    "name": "Isaac Chung"
                },
                {
                    "authorId": "2322982756",
                    "name": "Phat Vo"
                },
                {
                    "authorId": "2322991957",
                    "name": "Arman Kizilkale"
                },
                {
                    "authorId": "2322982549",
                    "name": "Aaron Reite"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
            "corpus_id": 272911196,
            "sentences": [
                {
                    "corpus_id": "272911196",
                    "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                    "text": "We release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
                    "score": 0.15761382098972154,
                    "section_title": "Introduction",
                    "char_start_offset": 4339,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 91,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 422
                        },
                        {
                            "start": 425,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1753
                        },
                        {
                            "start": 1756,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 2090
                        },
                        {
                            "start": 2091,
                            "end": 2235
                        },
                        {
                            "start": 2236,
                            "end": 2383
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6171875
                }
            ],
            "relevance_judgement": 0.6171875,
            "relevance_judgment_input_expanded": "# Title: Efficient In-Domain Question Answering for Resource-Constrained Environments\n# Venue: arXiv.org\n# Authors: Isaac Chung, Phat Vo, Arman Kizilkale, Aaron Reite\n## Abstract\nRetrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.\n## Introduction\nWe release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
            "reference_string": "[272911196 | Chung et al. | 2024 | Citations: 0]"
        },
        {
            "title": "ARAGOG: Advanced RAG Output Grading",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01037, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294361167",
                    "name": "Matouvs Eibich"
                },
                {
                    "authorId": "2294361283",
                    "name": "Shivay Nagpal"
                },
                {
                    "authorId": "2294362877",
                    "name": "Alexander Fred-Ojala"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We welcome the community to further this exploratory study in RAG systems.",
            "corpus_id": 268819923,
            "sentences": [
                {
                    "corpus_id": "268819923",
                    "title": "ARAGOG: Advanced RAG Output Grading",
                    "text": "\u2022 Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs.\n\n\u2022 Experiment replication across diverse datasets: To ensure the robustness and generalizability of our findings, it is imperative for future research to replicate our experiments using a variety of datasets.Conducting these experiments across multiple datasets is important to verify the applicability of our results and to identify any context-specific adjustments needed.\n\n\u2022 Auto-RAG: The idea of automatically optimizing RAG systems, akin to Auto-ML's approach in traditional machine learning, presents a significant opportunity for future exploration.Currently, selecting the optimal configuration of RAG components -e.g., chunking strategies, window sizes, and parameters within rerankers -relies on manual experimentation and intuition.An automated system could systematically explore a vast space of RAG configurations and select the very best model (Markr.AI, 2024).",
                    "score": 0.14557398015919568,
                    "section_title": "Conclusion",
                    "char_start_offset": 24731,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 250
                        },
                        {
                            "start": 250,
                            "end": 384
                        },
                        {
                            "start": 384,
                            "end": 505
                        },
                        {
                            "start": 507,
                            "end": 714
                        },
                        {
                            "start": 714,
                            "end": 880
                        },
                        {
                            "start": 882,
                            "end": 1062
                        },
                        {
                            "start": 1062,
                            "end": 1249
                        },
                        {
                            "start": 1249,
                            "end": 1381
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58154296875
                }
            ],
            "relevance_judgement": 0.58154296875,
            "relevance_judgment_input_expanded": "# Title: ARAGOG: Advanced RAG Output Grading\n# Venue: arXiv.org\n# Authors: Matouvs Eibich, Shivay Nagpal, Alexander Fred-Ojala\n## Abstract\nRetrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We welcome the community to further this exploratory study in RAG systems.\n## Conclusion\n\u2022 Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs.\n\n\u2022 Experiment replication across diverse datasets: To ensure the robustness and generalizability of our findings, it is imperative for future research to replicate our experiments using a variety of datasets.Conducting these experiments across multiple datasets is important to verify the applicability of our results and to identify any context-specific adjustments needed.\n\n\u2022 Auto-RAG: The idea of automatically optimizing RAG systems, akin to Auto-ML's approach in traditional machine learning, presents a significant opportunity for future exploration.Currently, selecting the optimal configuration of RAG components -e.g., chunking strategies, window sizes, and parameters within rerankers -relies on manual experimentation and intuition.An automated system could systematically explore a vast space of RAG configurations and select the very best model (Markr.AI, 2024).",
            "reference_string": "[268819923 | Eibich et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Synergizing RAG and Reasoning: A Systematic Review",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 87,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2291322497",
                    "name": "Yijie Zhong"
                },
                {
                    "authorId": "2275171009",
                    "name": "Yuxi Bi"
                },
                {
                    "authorId": "2356716546",
                    "name": "Ming Xue"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Recent breakthroughs in large language models (LLMs), particularly in reasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to unprecedented levels. By synergizing retrieval mechanisms with advanced reasoning, LLMs can now tackle increasingly complex problems. This paper presents a systematic review of the collaborative interplay between RAG and reasoning, clearly defining\"reasoning\"within the RAG context. It construct a comprehensive taxonomy encompassing multi-dimensional collaborative objectives, representative paradigms, and technical implementations, and analyze the bidirectional synergy methods. Additionally, we critically evaluate current limitations in RAG assessment, including the absence of intermediate supervision for multi-step reasoning and practical challenges related to cost-risk trade-offs. To bridge theory and practice, we provide practical guidelines tailored to diverse real-world applications. Finally, we identify promising research directions, such as graph-based knowledge integration, hybrid model collaboration, and RL-driven optimization. Overall, this work presents a theoretical framework and practical foundation to advance RAG systems in academia and industry, fostering the next generation of RAG solutions.",
            "corpus_id": 277994112,
            "sentences": [
                {
                    "corpus_id": "277994112",
                    "title": "Synergizing RAG and Reasoning: A Systematic Review",
                    "text": "Tuning-Based. The tuning-based approach improves the integration of RAG and reasoning by optimizing model parameters to internalize the retrieval-augmented chain-ofthought mechanism within LLMs. Current research mainly targets three goals: retrieval pathway optimization, structured generation enhancement, and collaborative training with external modules. \n\nFor retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy. \n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning. \n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically.",
                    "score": 0.1398961831249341,
                    "section_title": "5.2.2",
                    "char_start_offset": 63935,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 13
                        },
                        {
                            "start": 14,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 356
                        },
                        {
                            "start": 359,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1119
                        },
                        {
                            "start": 1122,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 2065
                        },
                        {
                            "start": 2068,
                            "end": 2224
                        },
                        {
                            "start": 2225,
                            "end": 2315
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1183,
                            "end": 1186,
                            "matchedPaperCorpusId": "264288947"
                        },
                        {
                            "start": 1947,
                            "end": 1951,
                            "matchedPaperCorpusId": "258841328"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57958984375
                }
            ],
            "relevance_judgement": 0.57958984375,
            "relevance_judgment_input_expanded": "# Title: Synergizing RAG and Reasoning: A Systematic Review\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, Haofen Wang\n## Abstract\nRecent breakthroughs in large language models (LLMs), particularly in reasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to unprecedented levels. By synergizing retrieval mechanisms with advanced reasoning, LLMs can now tackle increasingly complex problems. This paper presents a systematic review of the collaborative interplay between RAG and reasoning, clearly defining\"reasoning\"within the RAG context. It construct a comprehensive taxonomy encompassing multi-dimensional collaborative objectives, representative paradigms, and technical implementations, and analyze the bidirectional synergy methods. Additionally, we critically evaluate current limitations in RAG assessment, including the absence of intermediate supervision for multi-step reasoning and practical challenges related to cost-risk trade-offs. To bridge theory and practice, we provide practical guidelines tailored to diverse real-world applications. Finally, we identify promising research directions, such as graph-based knowledge integration, hybrid model collaboration, and RL-driven optimization. Overall, this work presents a theoretical framework and practical foundation to advance RAG systems in academia and industry, fostering the next generation of RAG solutions.\n## 5.2.2\nTuning-Based. The tuning-based approach improves the integration of RAG and reasoning by optimizing model parameters to internalize the retrieval-augmented chain-ofthought mechanism within LLMs. Current research mainly targets three goals: retrieval pathway optimization, structured generation enhancement, and collaborative training with external modules. \n\nFor retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy. \n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning. \n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically.",
            "reference_string": "[277994112 | Gao et al. | 2025 | Citations: 7]"
        },
        {
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 229,
            "citation_count": 1819,
            "influential_citation_count": 106,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.10997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2275341478",
                    "name": "Xinyu Gao"
                },
                {
                    "authorId": "2275191447",
                    "name": "Kangxiang Jia"
                },
                {
                    "authorId": "2275530552",
                    "name": "Jinliu Pan"
                },
                {
                    "authorId": "2275171009",
                    "name": "Yuxi Bi"
                },
                {
                    "authorId": "2276187454",
                    "name": "Yi Dai"
                },
                {
                    "authorId": "2275540959",
                    "name": "Jiawei Sun"
                },
                {
                    "authorId": "2258800561",
                    "name": "Qianyu Guo"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
            "corpus_id": 266359151,
            "sentences": [
                {
                    "corpus_id": "266359151",
                    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                    "text": "Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-toend joint training-and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels.",
                    "score": 0.1746718969290781,
                    "section_title": "C. Hybrid Approaches",
                    "char_start_offset": 56875,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 66,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 632
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5751953125
                }
            ],
            "relevance_judgement": 0.5751953125,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang\n## Abstract\nLarge Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\n## C. Hybrid Approaches\nCombining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-toend joint training-and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels.",
            "reference_string": "[266359151 | Gao et al. | 2023 | Citations: 1819]"
        },
        {
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 106,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324796381",
                    "name": "Thang Nguyen"
                },
                {
                    "authorId": "2324790937",
                    "name": "Peter Chin"
                },
                {
                    "authorId": "2324792268",
                    "name": "Yu-Wing Tai"
                }
            ],
            "abstract": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
            "corpus_id": 273186680,
            "sentences": [
                {
                    "corpus_id": "273186680",
                    "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
                    "text": "Fine-tuning \n\nFigure 1: Overview of our Reward-RAG. Given a query and its knowledge database, a retrieval model is used to retrieve the top-k relevant documents, which then are rated for the relevance by a CriticGPT. These \u27e8query, document\u27e9 pairs and their CriticGPTs' feedback are used to train a reward model, which is used to fine-tune the RAG retrieval to better align with human preferences. \n\nIn this section, we present our Reward-RAG. We first describe the dense retrieval problem in RAG in section 3.1, then present how we apply reinforcement learning to this problem in section 3.2. Fig. 2 illustrates the high-level design for Reward-RAG.",
                    "score": 0.13858380907663292,
                    "section_title": "Reward Model",
                    "char_start_offset": 8093,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 11
                        },
                        {
                            "start": 14,
                            "end": 51
                        },
                        {
                            "start": 52,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 396
                        },
                        {
                            "start": 399,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 649
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5732421875
                }
            ],
            "relevance_judgement": 0.5732421875,
            "relevance_judgment_input_expanded": "# Title: Reward-RAG: Enhancing RAG with Reward Driven Supervision\n# Venue: arXiv.org\n# Authors: Thang Nguyen, Peter Chin, Yu-Wing Tai\n## Abstract\nIn this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.\n## Reward Model\nFine-tuning \n\nFigure 1: Overview of our Reward-RAG. Given a query and its knowledge database, a retrieval model is used to retrieve the top-k relevant documents, which then are rated for the relevance by a CriticGPT. These \u27e8query, document\u27e9 pairs and their CriticGPTs' feedback are used to train a reward model, which is used to fine-tune the RAG retrieval to better align with human preferences. \n\nIn this section, we present our Reward-RAG. We first describe the dense retrieval problem in RAG in section 3.1, then present how we apply reinforcement learning to this problem in section 3.2. Fig. 2 illustrates the high-level design for Reward-RAG.",
            "reference_string": "[273186680 | Nguyen et al. | 2024 | Citations: 5]"
        },
        {
            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302798653",
                    "name": "Dian Jiao"
                },
                {
                    "authorId": "2303434387",
                    "name": "Li Cai"
                },
                {
                    "authorId": "2303044665",
                    "name": "Jingsheng Huang"
                },
                {
                    "authorId": "2108125912",
                    "name": "Wenqiao Zhang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches suffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultaneously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing generation quality. Finally, we demonstrate DuetRAG' s matches with expert human researchers on HotPot QA.",
            "corpus_id": 269983737,
            "sentences": [
                {
                    "corpus_id": "269983737",
                    "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
                    "score": 0.13747280483600285,
                    "section_title": "Related Work",
                    "char_start_offset": 2461,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 394
                        },
                        {
                            "start": 394,
                            "end": 446
                        },
                        {
                            "start": 446,
                            "end": 704
                        },
                        {
                            "start": 704,
                            "end": 890
                        },
                        {
                            "start": 890,
                            "end": 993
                        },
                        {
                            "start": 993,
                            "end": 1190
                        },
                        {
                            "start": 1190,
                            "end": 1340
                        },
                        {
                            "start": 1340,
                            "end": 1487
                        },
                        {
                            "start": 1487,
                            "end": 1656
                        },
                        {
                            "start": 1656,
                            "end": 1848
                        },
                        {
                            "start": 1848,
                            "end": 2094
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 242,
                            "end": 260,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 260,
                            "end": 282,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 723,
                            "end": 746,
                            "matchedPaperCorpusId": "244954723"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56884765625
                }
            ],
            "relevance_judgement": 0.56884765625,
            "relevance_judgment_input_expanded": "# Title: DuetRAG: Collaborative Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang\n## Abstract\nRetrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches suffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultaneously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing generation quality. Finally, we demonstrate DuetRAG' s matches with expert human researchers on HotPot QA.\n## Related Work\nRetrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
            "reference_string": "[269983737 | Jiao et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 22,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.05903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266237514",
                    "name": "Jennifer Dodgson"
                },
                {
                    "authorId": "2266281858",
                    "name": "Nanzheng Lin"
                },
                {
                    "authorId": "2266237540",
                    "name": "Julian Peh"
                },
                {
                    "authorId": "2266241068",
                    "name": "Akira Rafhael Janson Pattirane"
                },
                {
                    "authorId": "2047388958",
                    "name": "Alfath Daryl Alhajir"
                },
                {
                    "authorId": "2266238002",
                    "name": "Eko Ridho Dinarto"
                },
                {
                    "authorId": "2266365094",
                    "name": "Joseph Lim"
                },
                {
                    "authorId": "2266346433",
                    "name": "Syed Danyal Ahmad"
                }
            ],
            "abstract": "Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach out-performed both. The application of a soft prompt significantly improved the performance of each approach.",
            "corpus_id": 265128626,
            "sentences": [
                {
                    "corpus_id": "265128626",
                    "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",
                    "text": "While we found that RAG significantly out-performed fine-tuning as an LLM-improvement strategy, until comparatively recently they were too technically complex for the majority of users to implement. While creating a fine-tuned model was within the reach of even comparatively un-technical users thanks to the OpenAI fine-tuning API and related documentation, RAG remained the preserve of experts. The growing availability of RAG tools -first in the form of code libraries such as Langchain and LlamaIndex, and then via consumer-facing GUIs such as OpenAI's Chat with PDF tool, or the KIPLEY.AI platform used in this paper -has changed the cost-benefit ratio for non-expert users. \n\nThe fine-tuning exercise conducted for the purpose of this paper takes between 15 and 30 minutes to complete (depending on GPT API traffic), though it should be noted that both the cost and time required would increase were a user to iterate over multiple fine-tuning attempts, as is generally recommend. For comparison, generating the vector database used by the RAG process cost took around five minutes. \n\nFor a non-expert user, the baseline outputs of the RAG approach -whether in combination with a basic system prompt or alone -are significantly more accurate than those provided by the default fine-tuning settings, for a similar investment in time and effort. We thus recommend this approach for commercial users dipping a toe into the field.",
                    "score": 0.17649556279009848,
                    "section_title": "Conclusion",
                    "char_start_offset": 25302,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 679
                        },
                        {
                            "start": 682,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1088
                        },
                        {
                            "start": 1091,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1432
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53759765625
                }
            ],
            "relevance_judgement": 0.53759765625,
            "relevance_judgment_input_expanded": "# Title: Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users\n# Venue: arXiv.org\n# Authors: Jennifer Dodgson, Nanzheng Lin, Julian Peh, Akira Rafhael Janson Pattirane, Alfath Daryl Alhajir, Eko Ridho Dinarto, Joseph Lim, Syed Danyal Ahmad\n## Abstract\nResearch into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach out-performed both. The application of a soft prompt significantly improved the performance of each approach.\n## Conclusion\nWhile we found that RAG significantly out-performed fine-tuning as an LLM-improvement strategy, until comparatively recently they were too technically complex for the majority of users to implement. While creating a fine-tuned model was within the reach of even comparatively un-technical users thanks to the OpenAI fine-tuning API and related documentation, RAG remained the preserve of experts. The growing availability of RAG tools -first in the form of code libraries such as Langchain and LlamaIndex, and then via consumer-facing GUIs such as OpenAI's Chat with PDF tool, or the KIPLEY.AI platform used in this paper -has changed the cost-benefit ratio for non-expert users. \n\nThe fine-tuning exercise conducted for the purpose of this paper takes between 15 and 30 minutes to complete (depending on GPT API traffic), though it should be noted that both the cost and time required would increase were a user to iterate over multiple fine-tuning attempts, as is generally recommend. For comparison, generating the vector database used by the RAG process cost took around five minutes. \n\nFor a non-expert user, the baseline outputs of the RAG approach -whether in combination with a basic system prompt or alone -are significantly more accurate than those provided by the default fine-tuning settings, for a similar investment in time and effort. We thus recommend this approach for commercial users dipping a toe into the field.",
            "reference_string": "[265128626 | Dodgson et al. | 2023 | Citations: 8]"
        },
        {
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275628230",
                    "name": "Yiteng Tu"
                },
                {
                    "authorId": "2147219374",
                    "name": "Weihang Su"
                },
                {
                    "authorId": "2290870875",
                    "name": "Yujia Zhou"
                },
                {
                    "authorId": "2260835922",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2256982003",
                    "name": "Qingyao Ai"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "corpus_id": 275993994,
            "sentences": [],
            "relevance_judgement": 0.5341796875,
            "relevance_judgment_input_expanded": "# Title: RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects\n# Venue: arXiv.org\n# Authors: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai\n## Abstract\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.\n",
            "reference_string": "[275993994 | Tu et al. | 2025 | Citations: 6]"
        },
        {
            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 74,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2259265562",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "2253664013",
                    "name": "Wei Ping"
                },
                {
                    "authorId": "2256582287",
                    "name": "Zihan Liu"
                },
                {
                    "authorId": "2256656241",
                    "name": "Boxin Wang"
                },
                {
                    "authorId": "2287859963",
                    "name": "Jiaxuan You"
                },
                {
                    "authorId": "2256776233",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "1911755",
                    "name": "M. Shoeybi"
                },
                {
                    "authorId": "2264406909",
                    "name": "Bryan Catanzaro"
                }
            ],
            "abstract": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
            "corpus_id": 270878612,
            "sentences": [],
            "relevance_judgement": 0.5283203125,
            "relevance_judgment_input_expanded": "# Title: RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs\n# Venue: Neural Information Processing Systems\n# Authors: Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, M. Shoeybi, Bryan Catanzaro\n## Abstract\nLarge language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.\n",
            "reference_string": "[270878612 | Yu et al. | 2024 | Citations: 74]"
        },
        {
            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.11792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297189569",
                    "name": "Zooey Nguyen"
                },
                {
                    "authorId": "2297188041",
                    "name": "Anthony Annunziata"
                },
                {
                    "authorId": "69442223",
                    "name": "Vinh Luong"
                },
                {
                    "authorId": "2297188221",
                    "name": "Sang Dinh"
                },
                {
                    "authorId": "2297190249",
                    "name": "Quynh Le"
                },
                {
                    "authorId": "2297189614",
                    "name": "A. Ha"
                },
                {
                    "authorId": "2297189915",
                    "name": "Chanh Le"
                },
                {
                    "authorId": "2297189697",
                    "name": "Hong An Phan"
                },
                {
                    "authorId": "2058395065",
                    "name": "Shruti Raghavan"
                },
                {
                    "authorId": "2297324474",
                    "name": "Christopher Nguyen"
                }
            ],
            "abstract": "This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models. Additionally, employing reasoning iterations on top of RAG delivers an even bigger jump in performance, enabling the Q&A systems to get closer to human-expert quality. We discuss the implications of such findings, propose a structured technical design space capturing major technical components of Q&A AI, and provide recommendations for making high-impact technical choices for such components. We plan to follow up on this work with actionable guides for AI teams and further investigations into the impact of domain-specific augmentation in RAG and into agentic AI capabilities such as advanced planning and reasoning.",
            "corpus_id": 269214364,
            "sentences": [
                {
                    "corpus_id": "269214364",
                    "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
                    "text": "Our experimental results demonstrate several important findings:\n\nGains in accuracy from RAG with fine-tuning component models 1. RAG with fine-tuned retriever, fine-tuned generator, or full fine-tuning outperforms the generic RAG.Specifically, using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\n2. Fine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation.\n\nGains in accuracy from RAG with OODA Reasoning 1. Integrating iterative reasoning capabilities, such as OODA, with the RAG engine substantially enhances performance.Specifically, the Generic RAG with OODA Reasoning configuration achieves an accuracy increase of up to 50 percentage points across the FinanceBench dataset compared to the generic RAG baseline.\n\nFinanceBench dataset, the generic OODA configuration outperformed the fully fine-tuned RAG by a considerable margin of 20-25 percentage points.",
                    "score": 0.21211325864260117,
                    "section_title": "Key Findings",
                    "char_start_offset": 22603,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 64
                        },
                        {
                            "start": 66,
                            "end": 231
                        },
                        {
                            "start": 231,
                            "end": 359
                        },
                        {
                            "start": 361,
                            "end": 463
                        },
                        {
                            "start": 463,
                            "end": 615
                        },
                        {
                            "start": 617,
                            "end": 782
                        },
                        {
                            "start": 782,
                            "end": 975
                        },
                        {
                            "start": 977,
                            "end": 1120
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52392578125
                }
            ],
            "relevance_judgement": 0.52392578125,
            "relevance_judgment_input_expanded": "# Title: Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study\n# Venue: arXiv.org\n# Authors: Zooey Nguyen, Anthony Annunziata, Vinh Luong, Sang Dinh, Quynh Le, A. Ha, Chanh Le, Hong An Phan, Shruti Raghavan, Christopher Nguyen\n## Abstract\nThis paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models. Additionally, employing reasoning iterations on top of RAG delivers an even bigger jump in performance, enabling the Q&A systems to get closer to human-expert quality. We discuss the implications of such findings, propose a structured technical design space capturing major technical components of Q&A AI, and provide recommendations for making high-impact technical choices for such components. We plan to follow up on this work with actionable guides for AI teams and further investigations into the impact of domain-specific augmentation in RAG and into agentic AI capabilities such as advanced planning and reasoning.\n## Key Findings\nOur experimental results demonstrate several important findings:\n\nGains in accuracy from RAG with fine-tuning component models 1. RAG with fine-tuned retriever, fine-tuned generator, or full fine-tuning outperforms the generic RAG.Specifically, using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\n2. Fine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation.\n\nGains in accuracy from RAG with OODA Reasoning 1. Integrating iterative reasoning capabilities, such as OODA, with the RAG engine substantially enhances performance.Specifically, the Generic RAG with OODA Reasoning configuration achieves an accuracy increase of up to 50 percentage points across the FinanceBench dataset compared to the generic RAG baseline.\n\nFinanceBench dataset, the generic OODA configuration outperformed the fully fine-tuned RAG by a considerable margin of 20-25 percentage points.",
            "reference_string": "[269214364 | Nguyen et al. | 2024 | Citations: 4]"
        },
        {
            "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17888, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347252099",
                    "name": "Mingyan Wu"
                },
                {
                    "authorId": "2323176343",
                    "name": "Zhenghao Liu"
                },
                {
                    "authorId": "2277242040",
                    "name": "Yukun Yan"
                },
                {
                    "authorId": "2261354998",
                    "name": "Xinze Li"
                },
                {
                    "authorId": "2314785970",
                    "name": "Shi Yu"
                },
                {
                    "authorId": "1633538428",
                    "name": "Zheni Zeng"
                },
                {
                    "authorId": "2261295920",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2204644192",
                    "name": "Ge Yu"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.",
            "corpus_id": 276580741,
            "sentences": [
                {
                    "corpus_id": "276580741",
                    "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
                    "text": "Modular RAG systems (Gao et al., 2024;Xu et al., 2024c) focus on refining external knowledge through different modules implemented by LLMs, which have become a key trend in the RAG area. For instance, Self-RAG (Asai et al., 2024a) uses different tags for adaptive retrieval (Jiang et al., 2023) and self-reflection to refine knowledge. Some approaches also focus on reformulating queries to identify more useful documents for answering ques-tions (Yan et al., 2024;Trivedi et al., 2023). Yan et al. (2024) introduce a retrieval evaluator that acts as a judge to trigger query reformulation, search, and knowledge refinement actions to supply more accurate evidence for generation. \n\nTo further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023;Shi et al., 2024;Izacard and Grave, 2021). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2024) to jointly optimize the modules in a RAG system, aligning their data preferences.",
                    "score": 0.14550788887975255,
                    "section_title": "Related Work",
                    "char_start_offset": 5892,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 680
                        },
                        {
                            "start": 683,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1405
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 210,
                            "end": 230,
                            "matchedPaperCorpusId": "264288947"
                        },
                        {
                            "start": 274,
                            "end": 294,
                            "matchedPaperCorpusId": "258615731"
                        },
                        {
                            "start": 465,
                            "end": 486,
                            "matchedPaperCorpusId": "254877499"
                        },
                        {
                            "start": 979,
                            "end": 996,
                            "matchedPaperCorpusId": "258960666"
                        },
                        {
                            "start": 996,
                            "end": 1013,
                            "matchedPaperCorpusId": "256389797"
                        },
                        {
                            "start": 1013,
                            "end": 1037,
                            "matchedPaperCorpusId": "227746078"
                        },
                        {
                            "start": 1300,
                            "end": 1323,
                            "matchedPaperCorpusId": "258959321"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5234375
                }
            ],
            "relevance_judgement": 0.5234375,
            "relevance_judgment_input_expanded": "# Title: RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts\n# Venue: arXiv.org\n# Authors: Mingyan Wu, Zhenghao Liu, Yukun Yan, Xinze Li, Shi Yu, Zheni Zeng, Yu Gu, Ge Yu\n## Abstract\nRetrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.\n## Related Work\nModular RAG systems (Gao et al., 2024;Xu et al., 2024c) focus on refining external knowledge through different modules implemented by LLMs, which have become a key trend in the RAG area. For instance, Self-RAG (Asai et al., 2024a) uses different tags for adaptive retrieval (Jiang et al., 2023) and self-reflection to refine knowledge. Some approaches also focus on reformulating queries to identify more useful documents for answering ques-tions (Yan et al., 2024;Trivedi et al., 2023). Yan et al. (2024) introduce a retrieval evaluator that acts as a judge to trigger query reformulation, search, and knowledge refinement actions to supply more accurate evidence for generation. \n\nTo further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023;Shi et al., 2024;Izacard and Grave, 2021). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2024) to jointly optimize the modules in a RAG system, aligning their data preferences.",
            "reference_string": "[276580741 | Wu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327211339",
                    "name": "Salman Rakin"
                },
                {
                    "authorId": "2327211291",
                    "name": "Md. A.R. Shibly"
                },
                {
                    "authorId": "2327211491",
                    "name": "Zahin M. Hossain"
                },
                {
                    "authorId": "2327258807",
                    "name": "Zeeshan Khan"
                },
                {
                    "authorId": "2327212434",
                    "name": "Md. Mostofa Akbar"
                }
            ],
            "abstract": "While ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.",
            "corpus_id": 273532207,
            "sentences": [
                {
                    "corpus_id": "273532207",
                    "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
                    "text": "The RAG-Original model served as our baseline for evaluation, following the architecture proposed by Lewis et al. We indexed our HotelConvQA knowledge base and utilized this index to retrieve documents in response to queries. This model was evaluated without any kind of domain-specific fine-tuning, serving as a benchmark against which other models could be evaluated. \n\nRAG-Finetuned-QA : The RAG-Finetuned-QA model builds upon the baseline RAG-Original architecture by incorporating domain-specific fine-tuning on our hotel domain dataset. Like the RAG-Original model, it utilizes a Hotel Domain Knowledge Base, with dense vector representations indexed using the FAISS library. The loss function is designed to fine-tune both the generator and question encoder collectively. We have used ray as the distributed retriever with 4 retrieval workers as described in the original RAG paper. The model was trained for 20 epochs while the final checkpoint has been selected based on the highest validation accuracy. The purpose of this experiment was to assess the improvements gained from domain-specific training, comparing its performance against the baseline RAG-Original model.",
                    "score": 0.14101835054696354,
                    "section_title": "RAG-Original :",
                    "char_start_offset": 27197,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 369
                        },
                        {
                            "start": 372,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 889
                        },
                        {
                            "start": 890,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1179
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51220703125
                }
            ],
            "relevance_judgement": 0.51220703125,
            "relevance_judgment_input_expanded": "# Title: Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination\n# Venue: arXiv.org\n# Authors: Salman Rakin, Md. A.R. Shibly, Zahin M. Hossain, Zeeshan Khan, Md. Mostofa Akbar\n## Abstract\nWhile ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.\n## RAG-Original :\nThe RAG-Original model served as our baseline for evaluation, following the architecture proposed by Lewis et al. We indexed our HotelConvQA knowledge base and utilized this index to retrieve documents in response to queries. This model was evaluated without any kind of domain-specific fine-tuning, serving as a benchmark against which other models could be evaluated. \n\nRAG-Finetuned-QA : The RAG-Finetuned-QA model builds upon the baseline RAG-Original architecture by incorporating domain-specific fine-tuning on our hotel domain dataset. Like the RAG-Original model, it utilizes a Hotel Domain Knowledge Base, with dense vector representations indexed using the FAISS library. The loss function is designed to fine-tune both the generator and question encoder collectively. We have used ray as the distributed retriever with 4 retrieval workers as described in the original RAG paper. The model was trained for 20 epochs while the final checkpoint has been selected based on the highest validation accuracy. The purpose of this experiment was to assess the improvements gained from domain-specific training, comparing its performance against the baseline RAG-Original model.",
            "reference_string": "[273532207 | Rakin et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Meta Knowledge for Retrieval Augmented Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.09017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287923849",
                    "name": "Laurent Mombaerts"
                },
                {
                    "authorId": "2316430921",
                    "name": "Terry Ding"
                },
                {
                    "authorId": "2316476835",
                    "name": "Adi Banerjee"
                },
                {
                    "authorId": "2316429297",
                    "name": "Florian Felice"
                },
                {
                    "authorId": "2287922211",
                    "name": "Jonathan Taws"
                },
                {
                    "authorId": "3322455",
                    "name": "Tarik Borogovac"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) is a technique used to augment Large Language Models (LLMs) with contextually relevant, time-critical, or domain-specific information without altering the underlying model parameters. However, constructing RAG systems that can effectively synthesize information from large and diverse set of documents remains a significant challenge. We introduce a novel data-centric RAG workflow for LLMs, transforming the traditional retrieve-then-read system into a more advanced prepare-then-rewrite-then-retrieve-then-read framework, to achieve higher domain expert-level understanding of the knowledge base. Our methodology relies on generating metadata and synthetic Questions and Answers (QA) for each document, as well as introducing the new concept of Meta Knowledge Summary (MK Summary) for metadata-based clusters of documents. The proposed innovations enable personalized user-query augmentation and in-depth information retrieval across the knowledge base. Our research makes two significant contributions: using LLMs as evaluators and employing new comparative performance metrics, we demonstrate that (1) using augmented queries with synthetic question matching significantly outperforms traditional RAG pipelines that rely on document chunking (p<0.01), and (2) meta knowledge-augmented queries additionally significantly improve retrieval precision and recall, as well as the final answers breadth, depth, relevancy, and specificity. Our methodology is cost-effective, costing less than $20 per 2000 research papers using Claude 3 Haiku, and can be adapted with any fine-tuning of either the language or embedding models to further enhance the performance of end-to-end RAG pipelines.",
            "corpus_id": 271903789,
            "sentences": [
                {
                    "corpus_id": "271903789",
                    "title": "Meta Knowledge for Retrieval Augmented Large Language Models",
                    "text": "Our work integrates concepts from methodologies that generate QA from document collections for downstream fine-tuning of either LLMs or encoder models, with techniques leveraging query augmentation to boost the performance of retrievers in RAG pipelines. Below, we outline related work relevant to these two areas of RAG enhancement.",
                    "score": 0.14044956703136746,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 6525,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 333
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50927734375
                }
            ],
            "relevance_judgement": 0.50927734375,
            "relevance_judgment_input_expanded": "# Title: Meta Knowledge for Retrieval Augmented Large Language Models\n# Venue: arXiv.org\n# Authors: Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan Taws, Tarik Borogovac\n## Abstract\nRetrieval Augmented Generation (RAG) is a technique used to augment Large Language Models (LLMs) with contextually relevant, time-critical, or domain-specific information without altering the underlying model parameters. However, constructing RAG systems that can effectively synthesize information from large and diverse set of documents remains a significant challenge. We introduce a novel data-centric RAG workflow for LLMs, transforming the traditional retrieve-then-read system into a more advanced prepare-then-rewrite-then-retrieve-then-read framework, to achieve higher domain expert-level understanding of the knowledge base. Our methodology relies on generating metadata and synthetic Questions and Answers (QA) for each document, as well as introducing the new concept of Meta Knowledge Summary (MK Summary) for metadata-based clusters of documents. The proposed innovations enable personalized user-query augmentation and in-depth information retrieval across the knowledge base. Our research makes two significant contributions: using LLMs as evaluators and employing new comparative performance metrics, we demonstrate that (1) using augmented queries with synthetic question matching significantly outperforms traditional RAG pipelines that rely on document chunking (p<0.01), and (2) meta knowledge-augmented queries additionally significantly improve retrieval precision and recall, as well as the final answers breadth, depth, relevancy, and specificity. Our methodology is cost-effective, costing less than $20 per 2000 research papers using Claude 3 Haiku, and can be adapted with any fine-tuning of either the language or embedding models to further enhance the performance of end-to-end RAG pipelines.\n## RELATED WORK\nOur work integrates concepts from methodologies that generate QA from document collections for downstream fine-tuning of either LLMs or encoder models, with techniques leveraging query augmentation to boost the performance of retrievers in RAG pipelines. Below, we outline related work relevant to these two areas of RAG enhancement.",
            "reference_string": "[271903789 | Mombaerts et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Self-adaptive Multimodal Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325948461",
                    "name": "Wenjia Zhai"
                }
            ],
            "abstract": "Traditional Retrieval-Augmented Generation (RAG) methods are limited by their reliance on a fixed number of retrieved documents, often resulting in incomplete or noisy information that undermines task performance. Although recent adaptive approaches alleviated these problems, their application in intricate and real-world multimodal tasks remains limited. To address these, we propose a new approach called Self-adaptive Multimodal Retrieval-Augmented Generation (SAM-RAG), tailored specifically for multimodal contexts. SAM-RAG not only dynamically filters relevant documents based on the input query, including image captions when needed, but also verifies the quality of both the retrieved documents and the output. Extensive experimental results show that SAM-RAG surpasses existing state-of-the-art methods in both retrieval accuracy and response generation. By further ablation experiments and effectiveness analysis, SAM-RAG maintains high recall quality while improving overall task performance in multimodal RAG task. Our codes are available at https://github.com/SAM-RAG/SAM_RAG.",
            "corpus_id": 273350575,
            "sentences": [
                {
                    "corpus_id": "273350575",
                    "title": "Self-adaptive Multimodal Retrieval-Augmented Generation",
                    "text": "Table 2 (top) demonstrates that fine-tuning the retrieval model R or the VLM M improves multimodal RAG performance. However, finetuning M provides more substantial gains. This suggests that the baseline retrieval model R is already sufficiently optimized, leading to smaller performance improvements from finetuning. In contrast, optimizing M directly enhances the model's ability to process multimodal data. Notably, the fine-tuned RAG(R * + M * ) shows slightly lower performance than MuRAG, despite differences in the volume of training data. \n\n3. SAM-RAG vs. Conventional RAG: As shown in Table 2 (middle), SAM-RAG consistently outperforms conventional RAG methods. By dynamically retrieving documents until relevant information is found, SAM-RAG avoids the limitations of fixed retrieval strategies. Fine-tuning the retrieval model R speeds up document retrieval but does not significantly impact overall performance, as the strength of SAM-RAG lies in leveraging the fine-tuned M. The results indicate that fine-tuning M leads to the most notable performance improvements in SAM-RAG, especially for multimodal tasks. \n\n4. Effect of GPT Integration: Table 2 (bottom) highlights the substantial performance gains from integration of G into the SAM-RAG and conventional RAG frameworks. Models incorporating G outperform all other configurations, underscoring G's advanced reasoning and understanding abilities. SAM-RAG combined with G shows the most significant improvements, particularly in visual tasks, where it exceeds textual performance. This suggests that the SAM-RAG framework, when paired with G, achieves a deeper understanding of visual content than other approaches. To validate the effectiveness of each component of SAM-RAG, a series of ablation studies are performed that check the performance of the framework after removing each component, as shown in Table 3. The results indicate that, compared to conventional RAG, the introduction of distinct verification, particularly isRel and isSup , leads to notable improvements in both F1 and EM scores. Specifically, although introducing isRel only brings minimal improvement, the subsequent introduction of isUse and isSup makes a greater impact. When all verifications are combined (labeled \"with all\"), the EM score rises to the maximum value, reflecting an improvement of more than 20% compared to the performance of MuRAG.",
                    "score": 0.15331593629777032,
                    "section_title": "Performance Comparison with Baselines:",
                    "char_start_offset": 17677,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 408
                        },
                        {
                            "start": 409,
                            "end": 545
                        },
                        {
                            "start": 548,
                            "end": 669
                        },
                        {
                            "start": 670,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 1122
                        },
                        {
                            "start": 1125,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1413
                        },
                        {
                            "start": 1414,
                            "end": 1546
                        },
                        {
                            "start": 1547,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1880
                        },
                        {
                            "start": 1881,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2212
                        },
                        {
                            "start": 2213,
                            "end": 2392
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50390625
                }
            ],
            "relevance_judgement": 0.50390625,
            "relevance_judgment_input_expanded": "# Title: Self-adaptive Multimodal Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Wenjia Zhai\n## Abstract\nTraditional Retrieval-Augmented Generation (RAG) methods are limited by their reliance on a fixed number of retrieved documents, often resulting in incomplete or noisy information that undermines task performance. Although recent adaptive approaches alleviated these problems, their application in intricate and real-world multimodal tasks remains limited. To address these, we propose a new approach called Self-adaptive Multimodal Retrieval-Augmented Generation (SAM-RAG), tailored specifically for multimodal contexts. SAM-RAG not only dynamically filters relevant documents based on the input query, including image captions when needed, but also verifies the quality of both the retrieved documents and the output. Extensive experimental results show that SAM-RAG surpasses existing state-of-the-art methods in both retrieval accuracy and response generation. By further ablation experiments and effectiveness analysis, SAM-RAG maintains high recall quality while improving overall task performance in multimodal RAG task. Our codes are available at https://github.com/SAM-RAG/SAM_RAG.\n## Performance Comparison with Baselines:\nTable 2 (top) demonstrates that fine-tuning the retrieval model R or the VLM M improves multimodal RAG performance. However, finetuning M provides more substantial gains. This suggests that the baseline retrieval model R is already sufficiently optimized, leading to smaller performance improvements from finetuning. In contrast, optimizing M directly enhances the model's ability to process multimodal data. Notably, the fine-tuned RAG(R * + M * ) shows slightly lower performance than MuRAG, despite differences in the volume of training data. \n\n3. SAM-RAG vs. Conventional RAG: As shown in Table 2 (middle), SAM-RAG consistently outperforms conventional RAG methods. By dynamically retrieving documents until relevant information is found, SAM-RAG avoids the limitations of fixed retrieval strategies. Fine-tuning the retrieval model R speeds up document retrieval but does not significantly impact overall performance, as the strength of SAM-RAG lies in leveraging the fine-tuned M. The results indicate that fine-tuning M leads to the most notable performance improvements in SAM-RAG, especially for multimodal tasks. \n\n4. Effect of GPT Integration: Table 2 (bottom) highlights the substantial performance gains from integration of G into the SAM-RAG and conventional RAG frameworks. Models incorporating G outperform all other configurations, underscoring G's advanced reasoning and understanding abilities. SAM-RAG combined with G shows the most significant improvements, particularly in visual tasks, where it exceeds textual performance. This suggests that the SAM-RAG framework, when paired with G, achieves a deeper understanding of visual content than other approaches. To validate the effectiveness of each component of SAM-RAG, a series of ablation studies are performed that check the performance of the framework after removing each component, as shown in Table 3. The results indicate that, compared to conventional RAG, the introduction of distinct verification, particularly isRel and isSup , leads to notable improvements in both F1 and EM scores. Specifically, although introducing isRel only brings minimal improvement, the subsequent introduction of isUse and isSup makes a greater impact. When all verifications are combined (labeled \"with all\"), the EM score rises to the maximum value, reflecting an improvement of more than 20% compared to the performance of MuRAG.",
            "reference_string": "[273350575 | Zhai | 2024 | Citations: 0]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "269214364",
            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
            "text": "Our experimental results demonstrate several important findings:\n\nGains in accuracy from RAG with fine-tuning component models 1. RAG with fine-tuned retriever, fine-tuned generator, or full fine-tuning outperforms the generic RAG.Specifically, using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\n2. Fine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation.\n\nGains in accuracy from RAG with OODA Reasoning 1. Integrating iterative reasoning capabilities, such as OODA, with the RAG engine substantially enhances performance.Specifically, the Generic RAG with OODA Reasoning configuration achieves an accuracy increase of up to 50 percentage points across the FinanceBench dataset compared to the generic RAG baseline.\n\nFinanceBench dataset, the generic OODA configuration outperformed the fully fine-tuned RAG by a considerable margin of 20-25 percentage points.",
            "score": 0.21211325864260117,
            "section_title": "Key Findings",
            "char_start_offset": 22603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 66,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 359
                },
                {
                    "start": 361,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 615
                },
                {
                    "start": 617,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 975
                },
                {
                    "start": 977,
                    "end": 1120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52392578125
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "Baseline Mistral-7B-Instruct Establish the degree of pretraining and instruct-tuning knowledge within the model LoRA Finetuning\n\nFine-tuning with Low-Rank Adaptation NEFTune Finetuning Fine-tuning with Noise-Enhanced Fine-Tuning Instruct LLM + RAG (Raw)\n\nRetrieval-Augmented Generation with raw articles Instruct LLM + RAG (Extracted) RAG with auto-extracted data Best Combination\n\nBest fine-tuning + RAG approach",
            "score": 0.2108222154696769,
            "section_title": "Method Description",
            "char_start_offset": 45534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 129,
                    "end": 253
                },
                {
                    "start": 255,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2183837890625
        },
        {
            "corpus_id": "277510500",
            "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation",
            "text": "In Section 5, we fine-tune models on general domain RAG data, to see if it improves performances on the outof-domain RAG pipeline. Hyper-parameters are given in Table 6.",
            "score": 0.20441275938148873,
            "section_title": "B. Experimental details",
            "char_start_offset": 18252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.184814453125
        },
        {
            "corpus_id": "115677979",
            "title": "Melody Extraction Using Chroma-Level Note Tracking and Pitch Mapping",
            "text": "End for. \n\nThere are numerous pitch candidates calculated according to the above procedure. Fine tuning aims to track the dynamic changing due to frequency mistuning or singing voice. The frequency mistuned deviation is generally less than 20 cents, while the vibrato extent of western opera can be more than one semitone, but still rarely exceeds 1.5 semitones [24,25]. In this paper, fine tuning is accomplished using heuristics based on auditory streaming cues [23]. First, a fine tuned pitch is the one with the greatest salience within one semitone range of the coarse one obtained according to Equation (12), and its salience is preserved as the pitch salience, i.e., \n\nwhere \n\nIf there is one gap, where no pitch is found locating within one semitone range of the coarse one, then track from the left side of each gap. The pitch candidate whose frequency is within the 80 cents range from the left adjacent pitch is added to the contour, a weight is assigned to the candidate according to Equation (15). Repeat the procedure until no further candidate can be found. Then track from the right side of the gap with the same processing scheme. Fine tuning is ceased if either of the following two conditions is satisfied: (a) there is no gap over the fine-tuned notes; (b) no candidate can be found to complement the fine-tuned contours. If fine tuning is ceased due to the second condition, and the first condition is not satisfied, the pitch contour is linearly interpolated over the gap. This fine tuning using heuristics improves the performance on recordings with deep vibratos and pitch glides. To show the fine tuning procedure more clearly, it is summarized in Algorithm 2.",
            "score": 0.20365042897263758,
            "section_title": "5:",
            "char_start_offset": 14338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 11,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1685
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "20413675"
                },
                {
                    "start": 609,
                    "end": 613,
                    "matchedPaperCorpusId": "124589334"
                },
                {
                    "start": 1005,
                    "end": 1009,
                    "matchedPaperCorpusId": "6815742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0115966796875
        },
        {
            "corpus_id": "267751376",
            "title": "Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction",
            "text": "Discussion: RAG or fine-tuning. There are three main disadvantages in RAG compared to finetuning: (i) the RAG shows high latency given a long context, (ii) needs a good retrieval model, and (iii) suffers from the lost-in-the-middle though many works attempt to address it [33]. By contrast, the fine-tuning framework has three main disadvantages: (i) it must fine-tune LLMs to adapt to the new data, (ii) suffers from the positional bias as shown in our paper, and (iii) the amount of information storable in the model's parameters is limited. The effectiveness of each framework should depend on the application and the unification of two approaches can be desirable, e.g., fine-tuning a general knowledge learner on a new domain and complementing up-to-date information using RAG.",
            "score": 0.19199576030652604,
            "section_title": "Benchmark on unmodulated Wiki2023+",
            "char_start_offset": 27559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 782
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.201171875
        },
        {
            "corpus_id": "237797431",
            "title": "User-Driven Fine-Tuning for Beat Tracking",
            "text": "In this paper, we explored the use of excerpt-specific fine-tuning of a state-of-the-art beat tracking system based on exposure to a very small annotated region. Across existing datasets, we demonstrated that this approach can lead to improved performance over the state of the art, and furthermore, we illustrated its potential to adapt to challenging conditions in terms of timbre and musical expression. We believe that the principal contribution of this work was to demonstrate the potential of fine-tuning within a userdriven annotation workflow and thus to provide a path towards very accurate analysis on highly challenging musical pieces. Within the wider context of beat tracking, we foresee that this type of approach could be used as a means for rapid, semi-automatic annotation of musical pieces to expand the amount of challenging annotated data for training new approaches. To this end, we will pursue the integration of our fine-tuning approach within a dedicated user interface for annotation, e.g., Sonic Visualiser [58]. \n\nIn spite of the promising results obtained, it is important to recognise several limitations of our work and how they may be addressed in the future. First, our comparison against the state of the art was arguably tilted in favour of the fine-tuned approach, since per excerpt, we essentially created a new model and compared it to a single general model trained over a large amount of data. That said, our evaluation was carefully designed to exclude the interaction of the trained part of the input signal at inference, and furthermore, we did not claim that our fine-tuned approach represents a new state of the art. We simply sought to demonstrate that fine-tuning can be successfully applied across a large amount and variety of musical material. Second, our evaluation was dependent on a rather arbitrary selection of two 5 s regions for training and validation; of course, we can expect that as we increase the duration of these regions, then we will likely obtain better performance for the piece in question, but doing so would require increased annotation effort on the part of the user, which we sought to minimize as much as possible. Indeed, in the limit, this would resolve to the user annotating the entire piece without any need for an automated solution at all.",
            "score": 0.1914798318186695,
            "section_title": "Discussion and Conclusions",
            "char_start_offset": 54293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2319
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057373046875
        },
        {
            "corpus_id": "276575077",
            "title": "Mitigating Bias in RAG: Controlling the Embedder",
            "text": "We validate the utility of the fine-tuned embedder on a small RAG benchmark called RAG Mini-Wikipedia (Smith et al., 2008). We do this by connecting the embedder to Llama 8B as it is not possible to measure RAG utility on this benchmark without the LLM.",
            "score": 0.19120238009716506,
            "section_title": "A.6.3 RAG Mini-Wikipedia",
            "char_start_offset": 34755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 253
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 122,
                    "matchedPaperCorpusId": "9022977"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15576171875
        },
        {
            "corpus_id": "270062725",
            "title": "Large Language Models (LLMs): Deployment, Tokenomics and Sustainability",
            "text": "The choice between RAG and fine-tuning depends on the specific needs of LLM applications.RAG offers a faster and more adaptable solution for retrieval-based tasks, while finetuning provides superior performance for tasks demanding deeper domain expertise.",
            "score": 0.18982882522343458,
            "section_title": "C. Takeaway Message",
            "char_start_offset": 10820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1207275390625
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "Interestingly, larger LMs generally do not benefit from finetuning, while smaller ones do. Therefore, a small fine-tuned LM with RAG can perform on par or better than a large LM; e.g., StableLM2 (1.6B) vs. Llama3 (8B) (Table 3). \u2022 Retrieval model: Comparing retrievers with varying performance in the RAG system, we observe that as the popularity of factual knowledge increases, the performance of the retriever decreases (Figure 7). Moreover, the performance of the RAG system increases by using higher performance retriever (Figures 1 and 8). \u2022 Fine-tuning vs. RAG: Comparing these two knowledge injection methods, RAG substantially outperforms fine-tuning. Fine-tuned LMs combined with RAG either outperform or perform on par with vanilla LMs with RAG in all but one case (Figure 1). \n\nWhile fine-tuning improves accuracy in answering factual questions, both with and without RAG, it demands a considerable amount of effort and resources. This leads us to our second research question: (RQ2): Can we avoid the cost of fine-tuning by developing an advanced RAG approach that surpass the performance of a fine-tuned LM with RAG? To answer this question, we develop Stimulus RAG (SRAG), a new RAG approach that stimulates an LM to generate the correct response based on the provided hint in the prompt. The hint is extracted from the top retrieved documents by the retrieval model. Our results demonstrate that Stimulus RAG outperforms all other combinations of fine-tuning, both with and without retrievethen-generate RAG. \n\nTo summarize, this paper makes the following contributions: \n\n\u2022 We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods.",
            "score": 0.18710393618436313,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244384765625
        },
        {
            "corpus_id": "274131682",
            "title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
            "text": "(3) Fine-tuned models exhibit improved performance over base models, indicating more significant agreement between the generated abstract in the RAG approach and the real meta-analysis abstract. It highlights how well the fine-tuning approach works to help models find the patterns required to generate high-quality meta-analysis abstracts.",
            "score": 0.18634463374635787,
            "section_title": "B. Results and Analysis",
            "char_start_offset": 28834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 340
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3037109375
        },
        {
            "corpus_id": "271916052",
            "title": "RAGLAB: A Modular and Research-Oriented Unified Framework for Retrieval-Augmented Generation",
            "text": "RAGLAB integrates Accelerate (Gugger et al., 2022) and DeepSpeed libraries to provide comprehensive and efficient fine-tuning capabilities. Additionally, the Trainer module supports Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) (Dettmers et al., 2023) techniques, enabling users to fine-tune models with 70 billion parameters or more with limited computational resources. \n\nWe find that recent studies explore a novel from RAGLAB . rag . infer_alg import SelfRag_Reproduction from utils import get_config () args = get_config () query = \" What is Henry Feilden 's occupation ?\" # query for interaction mode Rag = SelfRag_Reproduction ( args ) \n\n# interact mode inference_result , generation_track = rag . inference ( query , mode = ' interact ') print ( inference_result ) # evaluation mode evaluation_result = rag . inference ( mode = ' evaluation ') print ( evaluation_result ) \n\nFigure 2: A script that uses RAGLAB for reproducing Self-RAG algorithm. \n\nmethod: adding special tokens during the generator training process to enhance performance. To facilitate the reproduction of these published works, the Trainer module supports adding special tokens during the fine-tuning phase.",
            "score": 0.1851810001799628,
            "section_title": "Trainer",
            "char_start_offset": 7564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1192
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 259,
                    "matchedPaperCorpusId": "258841328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396240234375
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.",
            "score": 0.18491696089551102,
            "section_title": "E. Tuning Pattern",
            "char_start_offset": 55399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2144
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "273229050",
            "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
            "text": "RAG and the Direct FT model across all evaluated datasets. (2) Robustness to hard negatives: the curve of RAG FT is generally flatter than that of the chat model, which demonstrates that our finetuned LLM is more robust to the hard negatives as the number of retrieved passages increases. (3) Superiority over direct fine-tuning: In most cases, RAG FT demonstrates superior performance compared to Direct FT. This indicates that RAG FT not only enables the LLM to \"memorize\" knowledge during training but also equips it with the ability to effectively \"extract\" relevant information from retrieved context during inference. These findings highlight the effectiveness of RAG-specific tuning in enhancing the generalization capabilities of LLMs for knowledge-intensive tasks. Separate results on those three LLMs are shown in Appendix J, K and L. Qualitative studies can be found in Appendix I.",
            "score": 0.18095190936405842,
            "section_title": "Implicitly improving LLM robustness through fine-tuning",
            "char_start_offset": 22853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 892
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.177490234375
        },
        {
            "corpus_id": "277740883",
            "title": "DRAFT-ing Architectural Design Decisions using LLMs",
            "text": "During inference, the finetuned model processes a new architectural context C and generates a decision D using the adapted parameters. \n\nLimitation: While fine-tuning enhances performance, it has inherent constraints. Unlike RAG, which dynamically fetches relevant examples from external sources, fine-tuning is limited to the knowledge encapsulated during the training process and is constrained by the scope of the training data, making it less adaptable to evolving information. \n\nPlease note Fine-tuning is often referred as training in this paper as fine-tuning is a type of training.",
            "score": 0.18016362628717913,
            "section_title": "Online Phase:",
            "char_start_offset": 15870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 589
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1553955078125
        },
        {
            "corpus_id": "266162497",
            "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
            "text": "The evaluation on the current events task is shown in Table 2. RAG proves particularly effective due to the one-to-one correspondence between the questions and the auxiliary dataset (see Section 4.3). Fine-tuning is not competitive with RAG. However, finetuning with multiple paraphrases still provides a significant improvement over the baseline. We note that combining RAG with fine-tuning shows inferior performance compared to RAG alone. \n\nIt is worth noting that although the questions are based on information the models were not exposed to during training, the results of the base models surpass 1 L = 0.25. This can partially be explained by the models using reasoning and/or pre-existing knowledge when answering questions that are not independent of the past information. Some examples of this can be found in Appendix C. \n\nFine-Tuning vs. RAG: In the results of both the MMLU and current events tasks, a significant advantage for RAG over fine-tuning is evident. While fine-tuning improved results compared to the base model in most cases, it was not competitive with the RAG approach. \n\nSeveral factors might contribute to this behavior. Firstly, RAG not only adds knowledge to a model but also incorporates context relevant to the question, a feature lacking in fine-tuning. Additionally, fine-tuning may impact other capabilities of the model due to a degree of catastrophic forgetting. Finally, it's plausible that unsupervised fine-tuned models might benefit from further alignment through supervised or RL-based fine-tuning, as evidenced by the vastly improved performance of Orca2 over the base Llama2.",
            "score": 0.17698156454284336,
            "section_title": "Current Events Results",
            "char_start_offset": 21657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1620
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385498046875
        },
        {
            "corpus_id": "265128626",
            "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",
            "text": "While we found that RAG significantly out-performed fine-tuning as an LLM-improvement strategy, until comparatively recently they were too technically complex for the majority of users to implement. While creating a fine-tuned model was within the reach of even comparatively un-technical users thanks to the OpenAI fine-tuning API and related documentation, RAG remained the preserve of experts. The growing availability of RAG tools -first in the form of code libraries such as Langchain and LlamaIndex, and then via consumer-facing GUIs such as OpenAI's Chat with PDF tool, or the KIPLEY.AI platform used in this paper -has changed the cost-benefit ratio for non-expert users. \n\nThe fine-tuning exercise conducted for the purpose of this paper takes between 15 and 30 minutes to complete (depending on GPT API traffic), though it should be noted that both the cost and time required would increase were a user to iterate over multiple fine-tuning attempts, as is generally recommend. For comparison, generating the vector database used by the RAG process cost took around five minutes. \n\nFor a non-expert user, the baseline outputs of the RAG approach -whether in combination with a basic system prompt or alone -are significantly more accurate than those provided by the default fine-tuning settings, for a similar investment in time and effort. We thus recommend this approach for commercial users dipping a toe into the field.",
            "score": 0.17649556279009848,
            "section_title": "Conclusion",
            "char_start_offset": 25302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1432
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "273026041",
            "title": "From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems",
            "text": "While fine-tuning models for specific domains improves performance, RAG offers a way to dynamically incorporate context with less extensive model retraining. The balance between RAG and fine-tuning is an area to be explored, with potential future systems leveraging the strengths of both approaches to minimize training time while maintaining contextsensitive query generation.",
            "score": 0.17624241439064411,
            "section_title": "D. Balancing Retrieval-Augmented Generation (RAG) and Fine-Tuning",
            "char_start_offset": 38777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 377
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1026611328125
        },
        {
            "corpus_id": "237940558",
            "title": "MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents",
            "text": "The implementation is in PyTorch. For finetuning RAG (Lewis et al., 2020), we follow the example 4 from HuggingFace and fine-tune it on our dataset for 16 epochs. For fine-tuning DPR (Karpukhin et al., 2020), we train DPR-nq on our dataset using facebookresearch/DPR 5 . Then, we integrate fine-tuned bi-encoder in RAG model facebook/rag-token-nq 6 . For pre-trained DPR, we use the bi-encoder model trained on NQ dataset only from Facebook DPR checkpoint 7 . We train the models for 10 epochs and evaluate using the last checkpoint.",
            "score": 0.1756140371707678,
            "section_title": "B Experiments",
            "char_start_offset": 31216,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 73,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 183,
                    "end": 207,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "269214364",
            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
            "text": "We experimented with and evaluated the outputs of the following technical configurations:\n\n\u2022 Generic RAG: bge-large-en with gpt-3.5-turbo-0125.\n\n\u2022 RAG with Fine-Tuned Generator: bge-large-en with fine-tuned gpt-3.5-turbo-0125.\n\n\u2022 RAG with Fine-Tuned Retriever: fine-tuned bge-large-en with gpt-3.5-turbo-0125.\n\n\u2022 Fully Fine-Tuned RAG: fine-tuned bge-large-en with fine-tuned gpt-3.5-turbo-0125.\n\n\u2022 Generic RAG with OODA Reasoning: bge-large-en with gpt-3.5-turbo-0125and OODA reasoning.\n\nEach system uses the same VectorStoreIndex for the documents, a top-k of 10 documents, and default RAG prompts, all from base LlamaIndex.For the full reproducible experimental setup, see OpenSSA's FinanceBench example 3 .Other combinations such as Fine-Tuned RAG with OODA Reasoning shall be addressed in a future publication (see Section 6).\n\nOur test set consists of the remaining 41 questions in the publicly-available FinanceBench test of valid questions that were not the 100 selected for training.",
            "score": 0.17553977976863797,
            "section_title": "Experiments & Results",
            "char_start_offset": 20500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 91,
                    "end": 143
                },
                {
                    "start": 145,
                    "end": 226
                },
                {
                    "start": 228,
                    "end": 309
                },
                {
                    "start": 311,
                    "end": 394
                },
                {
                    "start": 396,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 486
                },
                {
                    "start": 488,
                    "end": 625
                },
                {
                    "start": 625,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 830
                },
                {
                    "start": 832,
                    "end": 991
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3603515625
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-toend joint training-and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels.",
            "score": 0.1746718969290781,
            "section_title": "C. Hybrid Approaches",
            "char_start_offset": 56875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 632
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5751953125
        },
        {
            "corpus_id": "273695416",
            "title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations",
            "text": "Since the annotations produced through C-RAG are not always of good quality and correct (see Table 7 ), we define these demonstrations as misleading (obtained through prompting C-RAG but with an incorrect final target). To observe their impact on tuning, we produced the experimental setting of \u00a74 by varying the demonstrations as shown in Table 9. Table 9: The instructions used for our CL-RAG-7B and -13B are selected and filtred as detailed in \u00a73.4.2. We conducted tuning on incorrect (i.e., misleading) and mixed (50% misleading and 50% filtred correct demonstrations) and reported the baseline (Llama-2-7b).",
            "score": 0.17429245086740913,
            "section_title": "F Misleading C-RAG",
            "char_start_offset": 32290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 612
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0618896484375
        },
        {
            "corpus_id": "273532677",
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "text": "\u2022 Self-RAG (Asai et al., 2024) utilizes instruction fine-tuning to adaptively retrieve passages based on the question and determine if the passage contains useful information for answering the question. \n\n\u2022 ChatQA (Liu et al., 2024) is a fine-tuning pipeline tailored for RAG and conversational QA tasks via aggregating multiple QA and dialogue datasets. \n\n\u2022 RAFT (Zhang et al., 2024c) is a domain-specific fine-tuning approach that incorporates top-k passages as context during fine-tuning, helping to address discrepancies between training and testing data. \n\n\u2022 EvidenceRAG (Schimanski et al., 2024) leverage off-the-shelf LLMs (GPT-4) to generate contextaware question answering datasets, which is then used to fine-tune the student model.",
            "score": 0.17362283600688413,
            "section_title": "C Baseline Descriptions",
            "char_start_offset": 32680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 742
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 30,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 364,
                    "end": 384,
                    "matchedPaperCorpusId": "268510197"
                },
                {
                    "start": 576,
                    "end": 601,
                    "matchedPaperCorpusId": "267636562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4423828125
        },
        {
            "corpus_id": "271334428",
            "title": "Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models",
            "text": "RAG systems are useful in the mitigation of hallucinations, as they give extra-knowledge to the LM.However, we want to test to what extent finetuning helps the LM generate a more accurate medical paraphrases in the pRAGe pipeline (added knowledge through RefoMed-KB) and inference alone (parametric knowledge only) (RQ2).We therefore test the two SLMs in two settings: non-fine-tuned (NonFT) and fine-tuned (FT) on the RefoMed paraphrase dataset.",
            "score": 0.17248703814971472,
            "section_title": "Vanilla Inference VS Finetuning",
            "char_start_offset": 18059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 446
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10394287109375
        },
        {
            "corpus_id": "271218757",
            "title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing Assistant",
            "text": "We find that all fine-tuning regimes outperform the Pretrained + RAG baselines by a large margin, adapting to the user's writing style. The results are illustrated in Figure 2; we present a qualitative comparison of the generated emails in Appendix F; using leading closed LLMs yielded similarly poor results. RoSA performs on par with FFT, consistently surpassing LoRA, especially in terms of MAUVE score. The same trend is observed for all the backbones we trained, and for all users (see Appendix M for full results). \n\nIn Figure 3 we show the effect of RAG on models fine-tuned with RoSA. We observe that, although RAG clearly improves over the pretrained baseline, it reduces the average BLEU score for the RoSA fine-tuned model (second group), but may slightly increase the average MAUVE score. This is not desirable, as BLEU score (relative to the ground-truth email) is a closer measure of content accuracy than MAUVE. Based on analyzing individual samples, we hypothesize that this may be caused by the model re-using the RAG context too aggressively at deployment time, leading to emails that are very similar to past emails-preserving style-but less accurate in terms of content, leading to lower BLEU. We overcome this issue by introducing RAG during fine-tuning, i.e. using RAFT: the model can then \"learn\" to ignore irrelevant emails present in the RAG context.",
            "score": 0.1712977194378593,
            "section_title": "Results Across Methods and Models",
            "char_start_offset": 22384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1375
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0999755859375
        },
        {
            "corpus_id": "277468036",
            "title": "Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization",
            "text": "While all methods perform well under the first criterion, SR-RAG achieves the highest overall accuracy. Un-   der the second, stricter definition, SR-RAG demonstrates substantially better discrimination, outperforming Self-RAG by 14.7% in average AUROC. Removing kNN-based policy (w/o. kNN) leads to noticeable drops in both accuracy and AUROC, highlighting the benefit of adapting to fine-tuning-induced shifts in LLM ability.",
            "score": 0.17101126159604058,
            "section_title": "SR-RAG Improves Accuracy and Reduces Retrieval",
            "char_start_offset": 22917,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 427
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1707763671875
        },
        {
            "corpus_id": "271902790",
            "title": "Challenges and Responses in the Practice of Large Language Models",
            "text": "When you need to strengthen the model's existing knowledge or adapt to complex instructions, fine-tuning [2] is a good choice. Fine-tuning updates the parameters of the entire model by performing supervised learning on a labeled dataset of the new task, thereby improving the model's performance on the new task. Advantages: It can improve the model's interaction efficiency and make the model better adaptable to new tasks. Disadvantages: It consumes computing resources and training time, and is prone to overfitting problems when resources are limited or data is insufficient. \n\nRAG is suitable for scenarios that require a lot of external knowledge, such as knowledgeintensive tasks. RAG can provide more accurate and relevant answers and enhance the inter-pretability of the model by combining retrievers and generators. Advantages: It can provide richer and more accurate external knowledge and enhance the model's answering ability. Disadvantages: Compared with fine-tuning, RAG has a more complex architecture, and it may be more challenging to optimize the module.",
            "score": 0.17029543257614588,
            "section_title": "Question: When to utilize fine-tuning versus when to employ RAG",
            "char_start_offset": 3463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1073
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        },
        {
            "corpus_id": "263829356",
            "title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models",
            "text": "In this section, we evaluate the effectiveness of instruction fine-tuning and RAG. To validate our method's Algorithm 1 Two-Step Financial Knowledge Retrieval Require: Query Q Ensure: Context C 1: Search for Q on a search engine to obtain a posting list, D, of documents, each of which contains some phrases q \u2208 Q 2: for each document d in D do \n\nScan document D and split it into syntactic units separated by n separative tokens (i.e., paragraphs, bullet points) u i for indexed set {i|0 \u2264 i \u2264 n} 5: \n\nfor each syntactic unit u i do \n\nend for 10: \n\nend if 11: end for performance, we compare it against state-of-the-art sentiment analysis models and the general-purpose LLMs. Our experimental results validate the effectiveness of our approach. With only a small amount of fine-tuning data, our model consistently achieves superior performance in sentiment analysis compared to other baselines and its accuracy can be further enhanced with the RAG module.",
            "score": 0.1699598177888068,
            "section_title": "IV. PERFORMANCE EVALUATION",
            "char_start_offset": 20279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 956
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092041015625
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
            "score": 0.166830777478359,
            "section_title": "Methodology",
            "char_start_offset": 8999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 632
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "Our results show that Finetune-RAG significantly improves a model's ability to resist hallucinations in a RAG setting, even when the prompt includes both correct and misleading context. Fine-tuning with dual-context examples leads to consistent improvements in factual accuracy, while preserving helpfulness, relevance, and depth.",
            "score": 0.16666905827244902,
            "section_title": "Discussion",
            "char_start_offset": 19342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 330
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "273532096",
            "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
            "text": "Fine-tuning has gradually become a popular strategy (Ke et al., 2024) for enhancing the capabilities of components of RAG. Existing works include fine-tuning retrieval-related components to achieve better retrieval outcomes (Yan et al., 2024), fine-tuning generators for more personalized outputs (Zhang et al., 2024b), and employing collaborative fine-tuning (Lin et al., 2023). Additionally, Zhou et al. (2023) discovered that fine-tuning LLMs with a limited quantity of high-quality data significantly enhances the performance of LLMs. This finding provides a robust theoretical basis for collaboratively fine-tuning multiple components within advanced RAG methodologies at a minimal data expense.",
            "score": 0.1665354471763399,
            "section_title": "Domain-Specific Fine-Tuning for RAG",
            "char_start_offset": 8022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 700
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 412,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "266055916",
            "title": "Unipa-GPT: Large Language Models for university-oriented QA in Italian",
            "text": "In this section we outline the differences between the versions of the unipa-corpus used for developing the RAG-only and the fine-tuned system.",
            "score": 0.16607668071165244,
            "section_title": "Corpora",
            "char_start_offset": 6821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01233673095703125
        },
        {
            "corpus_id": "270561141",
            "title": "Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis",
            "text": "To RAG or Fine-Tune, a bards dilemma. Choosing the suitable model is essential to ensure hitting the right target. According to our findings, FTM proved to be the most accurate model, while RAG is unmatched in discovering hidden risks and hallucinating the less. In contrast, the base model excels in actionability and comprehensiveness.",
            "score": 0.16578775626284414,
            "section_title": "5.",
            "char_start_offset": 37718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 337
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0535888671875
        },
        {
            "corpus_id": "274982275",
            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
            "text": "The fine-tuning of GPT-4o-mini was a critical component of the FL RAG system. We trained the model on the dataset of NL and Lean code pairs, optimizing it to accurately translate NL mathematical statements into Lean code. The prompt used during fine-tuning was \"Translate statement into LEAN:\". The training data used to fine tune the model are over 10 million tokens worth of statement pairs in NL and FL taken from the Lean Workbook dataset [Ying et al., 2024], each statement in natural language was paired by a translated statement in Lean, and signed with \"by sorry\" indicating the end of the proof. The following is an example of a statement that we translated using the fine-tuned model:",
            "score": 0.16531521295020074,
            "section_title": "C. Model Fine-Tuning",
            "char_start_offset": 12808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 694
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1260986328125
        },
        {
            "corpus_id": "270923697",
            "title": "A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation",
            "text": "Concluding from the ablations study shared in Section 5, we see that the role of dynamically selected few-shot samples is very important in making RAG useful for syntactically correct generation of DSL as well as improving code similarity ((Table 4)).\n\nFine-Tuning still outperforms the RAG based model in terms of lower hallucinations (see Table 4 where fine-tuned model is the baseline).However, the parsing errors are more common in the fine-tuned model.This could be due to the fact that few shot examples have been successfully teaching the correct syntax to the LLM model.It is, however, surprising that the syntax correctness for RAG is better than that of the fine-tuned model which was trained on a much larger sample set.\n\nIt is also interesting to note that this benefit does not transfer to hallucinated API names and their parameters keys where the fine-tuned model holds the advantage.The increase of 6.76 pts in hallucination rate for parameters due to adding Semantic Function definitions indicates that adding too many API descriptions can confuse rather than help the LLM (Table 4).It also signifies the higher impact of the few shot samples for the scenario of DSL Generation or API selection compared to simply providing the API description.This learning can be used to inform the Tool Selection or orchestration scenario.Providing high quality examples of sample orchestration will reduce the failure rate more.\n\nOverall, we were able to significantly improve the performance of RAG for DSL generation, with hallucination rate for API names dropping by 6.29 pts.and that of parameter keys dropped by approx.20 pts (see Table 2).The performance of RAG is now comparable to that of fine-tuned model (see Avg. Similarity in Table 4), with the potential to bootstrap quickly.Optimized RAG can also allow extending the benefits of metaprompt tuning to include unseen APIs, reducing the need to fine-tune the model frequently.This will be the focus of our future work.",
            "score": 0.16505959068741866,
            "section_title": "CONCLUSION",
            "char_start_offset": 21541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 253,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 578
                },
                {
                    "start": 578,
                    "end": 731
                },
                {
                    "start": 733,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1261
                },
                {
                    "start": 1261,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1432
                },
                {
                    "start": 1434,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1628
                },
                {
                    "start": 1628,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1941
                },
                {
                    "start": 1941,
                    "end": 1983
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323486328125
        },
        {
            "corpus_id": "270766084",
            "title": "Infrrd.ai at SemEval-2024 Task 7: RAG-based end-to-end training to generate headlines and numbers",
            "text": "The fine-tuning of the LLAMA2-7b model was performed using the RAG method from end to end.The other models were also fine-tuned using different sample sets in the knowledge base (KB).The score was not so much improved compared to LLAMA2-7b.In addition, the BRIO model was trained with different headlines generated by different models from Table 3.The fine-tuned BRIO model also provided performance closer to the best performance method (Liu et al., 2022).However, it was used in the post-processing stage to add the headline if the main model did not perform the mathematical operations correctly.The results of the fine-tuned models are presented in Table 4.",
            "score": 0.16478098231708116,
            "section_title": "Fine-tuning",
            "char_start_offset": 13865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 90,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 599
                },
                {
                    "start": 599,
                    "end": 661
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 456,
                    "matchedPaperCorpusId": "247839752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17041015625
        },
        {
            "corpus_id": "7407719",
            "title": "User-Defined Metadata: Using Cues and Changing Perspectives",
            "text": "When you have replaced and stretched the last string make sure all six strings are still in tune.If you have a locking tremolo system, replace the clamps for the locking nut, tighten, and use the bridge fine tuners to get the proper pitch.",
            "score": 0.164375663434288,
            "section_title": "Changing guitar strings (unstructured)",
            "char_start_offset": 39527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00974273681640625
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model. \u2022 RL from LLM/human feedback. Reinforcement learning based on feedback from the final generated answers. In addition to using human evaluations, powerful LLMs can also serve as an evaluative judge. 3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the Fig. 16. Generator fine-tuning pattern, The main methods include SFT, distillation and RL from LLM/human feedback. Fig. 17. Dual fine-tuning pattern. In this mode, both the retriever and generator participate in fine-tuning, and their preferences will be aligned. likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
            "score": 0.1643630500093324,
            "section_title": "E. Tuning Pattern",
            "char_start_offset": 57340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1290
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5732421875
        },
        {
            "corpus_id": "274117194",
            "title": "Boosting GPT models for genomics analysis: generating trusted genetic variant annotations and interpretations through RAG and Fine-tuning",
            "text": "Our results indicated that RAG outperforms fine-tuning when injecting knowledge into LLMs in terms of data volume, accuracy, cost, time efficiency, and flexibility. Using (a) Flowchart illustrating the steps of RAG building process for GPT-4o. To build the RAG system, we first converting the variant annotation data from VCF format to CSV format, and, then stored them at Azure Blob Storage and loaded and indexed them in Azure AI Search to enable efficient retrieval. Finally, we integrate the search index with GPT-4o using the Azure OpenAI platform, allowing the model to retrieve relevant information based on user queries. (b) Performance of RAG-enhanced GPT-4o model in predicting all variant annotation fields (from five datasets) based on input variants. With RAG, GPT-4o achieved 100% accuracy for all annotation fields in the test sets. (c) Example dialogue with GPT-4o on variant annotations before using RAG. GPT-4o provided only general information. (d) Example dialogue with GPT-4o on variant annotations after using RAG. GPT-4o provided accurate annotations and enhanced interpretations. \n\nBoosting GPT models for genomics analysis RAG, we successfully integrated 190 million variant annotations. In contrast, fine-tuning struggled to add 13 annotation fields of 3000 variants to the LLM. As for accuracy, RAG ensured 100% accuracy relying on retrieving correct documents. Fine-tuning, however, found it challenging to achieve such high accuracy levels. The primary cost for RAG lies in creating and storing the search index, which depends on the indexing platform. In our case, we used the Azure AI Search Standard S1 price tier, costing approximately $250 per month (August 2024). Fine-tuning, on the other hand, costs $25 per million input tokens. Fine-tuning 3000 variant annotations for three epochs billed approximately 1 million training tokens. Scaling this to 190 million annotations would be significantly more expensive than using RAG. As for time efficiency, the time required to build a RAG index depends on the search method employed.",
            "score": 0.16404076871382314,
            "section_title": "Comparison between RAG and fine-tuning",
            "char_start_offset": 20273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2242431640625
        },
        {
            "corpus_id": "276903112",
            "title": "TPU-Gen: LLM-Driven Custom Tensor Processing Unit Generator",
            "text": "Takeaway 2. Prompting techniques such as prompt template steer LLM to generate desired results after fine-tuning, as observed 86% success in generation. RAG, a cost-efficient method to generate the hardware modules reliably, completing the entire Verilog design for an application with minimal computational overhead. \n\n3) Experiment 3: Significance of RAG: To assess the effectiveness of RAG in the TPU-Gen framework, we evaluated 1,000 Verilog header codes generated by fine-tuned LLMs under two conditions: with and without RAG integration. Table V presents results over 30 designs tested by our framework to generate complete project files. Without RAG, failures occurred due to output token limitations and hallucinated variables. RAG is essential as the design is not a standalone file to compile. Validated header codes were provided in the RAG-enabled pipeline, and required modules were dynamically retrieved from the RAG database, ensuring fully functional and accurate designs. Conversely, models without RAG relied solely on internal knowledge, leading to hallucinations, token constraints, and incomplete designs. Models using RAG consistently achieved pass rates exceeding 95%, with Mistral-7B and CodeLlama-7B-hf attaining 100% success. In contrast, all models failed entirely without RAG, underscoring its pivotal role in ensuring design accuracy and addressing LLM limitations. RAG provides a robust solution to key challenges in fine-tuned LLMs for TPU hardware design by retrieving external information from the RAG database, ensuring contextual accuracy, and significantly reducing hallucinations. Additionally, RAG dynamically fetches dependencies in a modular manner, enabling the generation of complete and accurate designs without exceeding token limits. RAG is a promising solution in this context since our models were finetuned with only Verilog header data detailing design features. However, fine-tuning models with the entire design data would expose LLMs to severe hallucinations and token limitations, making generating detailed and functional designs challenging. Takeaway 3. The experiment highlights the significance of the RAG usage with a fine-tuned model to avoid hallucinations and let LLM be creative consistently.",
            "score": 0.1632495118269344,
            "section_title": "B. Experiments and Results",
            "char_start_offset": 23527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2254
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0809326171875
        },
        {
            "corpus_id": "271270061",
            "title": "Lightweight Large Language Model for Medication Enquiry: Med-Pal",
            "text": "In the development of Med-Pal we chose to fine-tune instead of using a retrieval augmented generation (RAG) approach.RAG allows LLMs to tailor responses to specified tasks through provision of contextual knowledge e.g.clinical guidelines, medical textbooks, institution specific protocols. 25RAG operates in a manner analogous to a search engine for LLMs, retrieving customized textual data in response to queries.7][28] While there has not been direct comparisons between supervised fine-tuned and RAGbased methods in medical application, preliminary studies suggest enhanced brevity 29 and memory.\n\nFine-tuned models are static in knowledge acquisition, requiring retraining for updates but enables deep customization of the model's behaviour and style of response. 30For patient-serving medical chatbots, fine-tuned models likely provides greater advantages over RAG-models in providing tailored, succinct and comprehensible responses.",
            "score": 0.16235698557081585,
            "section_title": "Harmful output",
            "char_start_offset": 27968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 599
                },
                {
                    "start": 601,
                    "end": 770
                },
                {
                    "start": 770,
                    "end": 938
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 292,
                    "matchedPaperCorpusId": "258740478"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "264303422"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1163330078125
        },
        {
            "corpus_id": "274117194",
            "title": "Boosting GPT models for genomics analysis: generating trusted genetic variant annotations and interpretations through RAG and Fine-tuning",
            "text": "The effectiveness and tradeoffs of these two methods are still under debate (Ovadia et al. 2023, Balaguer et al. 2024). Fine-tuning is a continuous training process that exposes the pre-trained model to a smaller, domain-specific dataset so that the model can adjust its weights and adapt to the specific task. In this article, we employed supervised fine-tuning, using question and answer pairs to guide the model's weight changing toward generating our desired responses. Unlike fine-tuning, RAG does not modify the pre-trained model itself (Lewis et al. 2020). Instead, RAG improves accuracy and relevance in model answer generation by including relevant external information. When using RAG, the user query is not directly inputted into the model for generation. After receiving a user query, we search the external data store to retrieve relevant information, and then construct input prompts using both the user query and the retrieved information and send this prompt to the LLM to generate an enhanced response. \n\nTo harness the power of LLMs in genomics, we integrated genomics domain knowledge, specifically 190 million variant annotations, into GPT-4o and GPT-4 models through RAG and fine-tuning, which significantly improved the model's ability to provide accurate variant annotations and enhanced interpretations. Additionally, we compared the effectiveness of RAG and fine-tuning in injecting variant annotation data to LLMs and evaluated two methods in terms of accuracy, data volume, and cost-effectiveness. As a pioneering study in application of LLMs in genomics, our project paves the way for developing more comprehensive and powerful genomics AI tools to assist in clinical and research uses.",
            "score": 0.16230921123864067,
            "section_title": "Introduction",
            "char_start_offset": 2169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1714
                }
            ],
            "ref_mentions": [
                {
                    "start": 543,
                    "end": 562,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33935546875
        },
        {
            "corpus_id": "271744975",
            "title": "A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case",
            "text": "The final input from the training dataset is formatted as a question-and-answer (Q&A) pair structure. \n\nThe Figure 18 shows the architecture and data flow for the fine-tuning method, QLoRA. [38] Singh (2023) found that if there is a huge corpus of task-specified datasets that have been labeled, then fine-tuning is preferable over the retrieval method, RAG, especially for domain-specific tasks. For example, specialized topics in the travel domain are   [38] Curated datasets are ingested by a baselined LLM which contains quantized 8-bit weights from LoRA (or 4-bit weights in QLoRA). When a user makes a query then prompt engineering, or grounding, is done in a cycle between the user and LLM to fine-tune for new data that comes in. [8] Retrieval Augment Fine-tuning (RAFT) is a novel ap-proach proposed by researchers at UC Berkeley in March 2024. RAFT is a way to combine fine tuning, which memorizes knowledge before running inference like a closed book exam, and RAG, which is like an open book exam that has no prior knowledge or context and just retrieves information. RAFT is a way to prepare for open book exams, helping distinguish and prepare the LLM for both open and closed book exams. Thus, it will be able to answer questions outside the training document domain since it is baked in knowledge Figure 18: Data Flow for QLoRA from model weights and reads information from retrieved results. RAFT is a specific domain of RAG that can help train and prepare LLMs to perform particularly well in a specified domain. The authors state the following, \"RAFT is a training procedure for RAG. If you are doing RAG you should be doing RAFT\". [9] Currently there are two main approaches to inject new knowledge to LLMs, which are RAG and fine tuning. However, current optimal methodologies for the model to gain new knowledge is still an open question to be solved. The proposed RAFT to domain specific RAG addresses the naive nature of LLMs that are not able to distinguish between context and noise. The goal is to adapt pretrained LLMs for RAG in specific domains, such as ours in travel.",
            "score": 0.16204674626473542,
            "section_title": "IV. FINE-TUNING METHODS",
            "char_start_offset": 46983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2099
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406982421875
        },
        {
            "corpus_id": "265308606",
            "title": "Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",
            "text": "This characteristic ensures that the responses generated are current and relevant, while also providing a structured way to handle sensitive or private information within the retrieval data. This setup aims at striking a careful balance between dynamism and privacy. \n\nA question that often arises is whether RAG can be replaced by fine-tuning. The distinction between RAG and fine-tuning becomes apparent when considering the dynamic nature of the information involved, the computational resources available for retraining, and the specific requirements of the task at hand. While both methodologies aim at enhancing the performance and applicability of Large Language Models (LLMs), they cater to different aspects and scenarios. Finetuning is a well-suited approach for tailoring a model to a specific domain or set of long-term tasks, especially when the underlying challenges are relatively static. It involves adjusting the model parameters on a new dataset to make the model's behavior more aligned with the desired task or domain. On the other hand, RAG is designed to tackle scenarios where the model needs to stay updated with rapidly evolving or expansive information without requiring continuous retraining. By leveraging an external retrieval database, RAG enables the model to interact dynamically with the latest data, ensuring its responses are current and relevant. While fine-tuning demands substantial computational resources for retraining with new data, RAG offers a cost-effective alternative by minimizing the need for exhaustive retraining. In summary, the choice between RAG and fine-tuning is not a matter of simple replacement, but rather a strategic decision based on the particular demands of the task, the nature of the data, and the available resources. Each approach has its own set of advantages and is suited to different use cases, necessitating a careful consideration of the project's requirements to determine the most appropriate strategy.",
            "score": 0.16190452828718715,
            "section_title": "Enhancing the Information Retrieval Process",
            "char_start_offset": 8563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1978
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2176513671875
        },
        {
            "corpus_id": "266162497",
            "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
            "text": "For each task and model, we compared four approaches: using just the base model, RAG, FT, and finally combining FT and RAG by using the fine-tuned model as the generator. Furthermore, we tested the MMLU tasks using both 0-shot and 5-shot scenarios. The full results are shown in Table 1. An aggregation of the relative accuracy gain, i.e., \n\nwhere M is the base model and M \u2032 is the knowledgeinjected model, is shown in Figure 2. \n\nIn all cases, RAG performed significantly better compared to the base models. Furthermore, using RAG with the base model as the generator was consistently better than only finetuning. In some cases, using the fine-tuned model instead of the base model as the generator in the RAG pipeline improved results even further. However, this is not consistent and thus demonstrates the inherent instability of fine-tuning. Additionally, we found that the 5-shot approach boosts the results by a small margin in most cases, with a similar trend being observed in all of the different approaches.",
            "score": 0.16034627565531392,
            "section_title": "MMLU Results",
            "char_start_offset": 20612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278076171875
        },
        {
            "corpus_id": "268510325",
            "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
            "text": "In comparing FN and RAG, RAG achieves better results if we want to create a G-LLM-based knowledge system.In the case of RAG, searching in an indexed database is critical, but by indexing with embedded vectors, it is possible to create a dataset that can be searched efficiently and with which RAG can outperform fine-tuned models.The hallucinations of RAG-based systems are smaller and their expansion is simpler since to expand with new information it is only necessary to add the new data, which requires much less calculation than FN.The combination of FN and RAG is not trivial, as the application of a fine-tuned model with RAG\n\ndid not result in an extra performance increase.",
            "score": 0.1599703737601757,
            "section_title": "Conclusions",
            "char_start_offset": 18242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 632
                },
                {
                    "start": 634,
                    "end": 682
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1358642578125
        },
        {
            "corpus_id": "273695766",
            "title": "Are Large-Language Models Graph Algorithmic Reasoners?",
            "text": "All fine-tuning experiments were performed in halfprecision using the aforementioned Huggingface codebase. The models for each algorithm were tuned for the optimal r and alpha values for parameter-efficient fine-tuning.",
            "score": 0.15957397353619912,
            "section_title": "Hyperparameter Tuning and Model Settings",
            "char_start_offset": 24644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05291748046875
        },
        {
            "corpus_id": "274437524",
            "title": "Efficient Learning Content Retrieval with Knowledge Injection",
            "text": "RAGs enable a trained model to generate a response by retrieving data from different contents to establish its relationship with the model-content. After training a model, including new data requires time and human resources. Using RAG, new data can be pulled from a vector database without retraining the model. Also, trying to keep a large amount of data in the model's memory while the model is being trained will increase both time and resource usage. RAG reduces cost while increasing model efficiency by using it when necessary. Besides these, there are some disadvantages such as dependence on external sources in the process of retrieving information from a database, delay in information retrieval and increased complexity. To overcome these disadvantages, a new approach is needed to reduce the response time and complexity of the model. \n\nThe model used in RAGs can be fine-tuned to a domainspecific area in a short time and the model that will emerge as a result of fine-tuning requires as few resources as possible. As it is known, the fine-tuning process allows the model to perform better on a domain-specific task and does not depend on external information sources when generating a response. However, it has disadvantages such as timeliness and time efficiency. So, a new method can be proposed by combining the separate advantages of RAG and fine-tuning. This method can be a RAG system in which SLMs can be fine-tuned in a short time by using QLora and vector databases can ensure that the model is constantly updated are actively used. The advantages we foresee when using the fine-tuned model with RAG are detailed in Table III.",
            "score": 0.15888870520276246,
            "section_title": "D. RAG vs Fine-tuning",
            "char_start_offset": 18453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1650
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.208984375
        },
        {
            "corpus_id": "237797431",
            "title": "User-Driven Fine-Tuning for Beat Tracking",
            "text": "As detailed in Section 4, our fine-tuning process relied on a short annotated region for training and an additional region of equal duration for validation. We reiterate that in this work where we sought to broadly investigate the validity of fine-tuning over a large amount of musical material, we simulated the role of the end-user, and to this end, we obtained these annotated regions from existing beat tracking datasets rather than direct user input. While the duration and location of these regions within the musical excerpt were somewhat arbitrary compared to a practical use case with an end-user, for this evaluation, we chose them to be 5 s in duration each and adjacent to one another starting from the first annotated beat position per excerpt. By choosing the first beat annotation as opposed to the beginning of the excerpt, we could avoid any degenerate training that might otherwise arise if no musical content occurred within the first 10 s of an excerpt (e.g., a long nonmusical intro). For the purposes of evaluation, the impact of this configuration of fine-tuning across the early part of the excerpt had the advantage that it was straightforward to trim these regions to which the network had been exposed prior to inference with the HMM and then offset the annotations accordingly. In this way, we could contrast the performance of the fine-tuned version with the baseline model [22] without any impact of the sharp peaks in the beat activation functions across the training region. Note that due to the removal of the training and validation regions when evaluating, the results we obtained were not directly comparable to those in [22], which used the full-length excerpts. To summarise, our goal in formulating the evaluation was to see the extent to which the adaptation of the network over a short region near the start of each excerpt was reflected through the rest of the piece.",
            "score": 0.15874994433240056,
            "section_title": "Experimental Setup",
            "char_start_offset": 35015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 1403,
                    "end": 1407,
                    "matchedPaperCorpusId": "236095634"
                },
                {
                    "start": 1657,
                    "end": 1661,
                    "matchedPaperCorpusId": "236095634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0318603515625
        },
        {
            "corpus_id": "252734762",
            "title": "AnimeTAB: A new guitar tablature dataset of anime and game music",
            "text": "This is called special tuning. We record the number of semitones by which string is raised or lowered in 3 Beats per minute 4 Lower the sixth string a whole tone 5 Adjust the six strings to D, A, D, F#, A, D the tuning list to avoid mismatches between the pitch and fingering. \n\nEach of the above text tags can be automatically converted by our toolkit-TABprocessor, and we will introduce this the next section.",
            "score": 0.1586852042567799,
            "section_title": "Part 3:Preprocessed Scores",
            "char_start_offset": 8141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0009813308715820312
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
            "score": 0.15864571328242938,
            "section_title": "Retriever and Generator Fine-tuning",
            "char_start_offset": 7670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 551
                },
                {
                    "start": 553,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1037
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 185,
                    "matchedPaperCorpusId": "258865283"
                },
                {
                    "start": 658,
                    "end": 660,
                    "matchedPaperCorpusId": "250340214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80517578125
        },
        {
            "corpus_id": "273502262",
            "title": "Customized FinGPT Search Agents Using Foundation Models",
            "text": "RAG integrates retrieval mechanisms into generative models, ranging from simple keyword matching to advanced neural retrieval models [32]. It represents a cost-efficient way to incorporate the latest information into LLMs since it does not require updating model weights. Liu et al. [21] explored the integration of RAG with finetuned models, demonstrating improved performance in generating contextually rich and precise outputs for domain-specific applications. In Zhang et al.'s work [31] on financial sentiment analysis, they combined RAG with instruction tuning. They identify two challenges: the mismatch between LLM pre-training objectives and sentiment analysis, and the lack of sufficient context in short financial texts like tweets. To overcome this, they use a retrieval module that gathers relevant external financial data, enriching the context for sentiment predictions. Their approach improves performance by 15-48% in accuracy and F1 score over traditional models. \n\nAlthough RAG combined with fine-tuning has been explored in previous work, it has not been used for customizing LLMs. Our work leverages RAG to provide a customized experience for individual users and institutions, supported by a GUI.",
            "score": 0.1584429703645721,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 6319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1218
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "214460999"
                },
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "263829356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369140625
        },
        {
            "corpus_id": "273229050",
            "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
            "text": "While the retrieval reordering strategy presented in Section 4 mitigates the detrimental impact of hard negatives, it does not inherently enhance the LLM's ability to handle such irrelevant information within the context. To address this, we conduct a systematic investigation into RAG-specific tuning as a means of improving long-context LLMs for RAG applications. \n\nOur tuning paradigm involves training LLM to generate the correct answer () given a comprehensive input comprising an instruction (), a query (), and a set of retrieved passages ( 1 ,  2 , ...,   ): \n\n(2) \n\nThis approach aims to implicitly enhance the LLM's robustness to hard negatives by exposing it to a diverse range of retrieved contexts during fine-tuning, thus enabling it to learn to effectively identify and utilize relevant information even in the presence of noise. \n\nTo assess the generalization capabilities of RAG-tuned LLMs, we fine-tune Gemma-2-9B-Base, Mistral-Nemo-12B-Base and Gemini-1.0-Pro using a diverse dataset comprising NQ, WoW, Fever, and MMLU. We then evaluate on a range of unseen datasets, including TriviaQA, PopQA, HotpotQA, 2wikimultihopqa, Webquestions, Bamboogle, ASQA, T-REx, and zsRE. We compare the performance of the RAG-tuned model (RAG FT) with two types of baselines: (1) Chat model with retrieval augmentation: the Gemma-2-9B-Chat/Mistral-Nemo-12B-Instruct/Gemini-1.0-Pro w. RAG; (2) Direct SFT: the ones fine-tuned with standard supervised fine-tuning (SFT) on question-answer pairs without retrieved context (Direct FT w/o RAG). Further details regarding the datasets and experimental setup can be found in Appendix F and G. \n\nFigure 5 shows the three key observations: (1) Consistent improvement over baselines: RAG FT consistently outperforms the chat model w. RAG and the Direct FT model across all evaluated datasets.",
            "score": 0.15795705278451871,
            "section_title": "Implicitly improving LLM robustness through fine-tuning",
            "char_start_offset": 21077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1834
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34814453125
        },
        {
            "corpus_id": "258556916",
            "title": "Accented Text-to-Speech Synthesis With Limited Data",
            "text": "To alter the pitch pattern, the pitch predictor and decoder are fine-tuned.The post-net after the decoder can improve the generalization ability of reconstruction across a large number of Mel-spectrogram during the pre-training stage so that a pre-trained post-net is fixed and shared for all accents during the fine-tuning stage.All the loss functions in Equation 1 are optimized during the fine-tuning stage.",
            "score": 0.15781752397446808,
            "section_title": "B. Accented TTS Training with Limited Data",
            "char_start_offset": 17277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 75,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 410
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00621795654296875
        },
        {
            "corpus_id": "272911196",
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "text": "We release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
            "score": 0.15761382098972154,
            "section_title": "Introduction",
            "char_start_offset": 4339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "266899736",
            "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning",
            "text": "In this section we describe our method for fine tuning. It is divided into two parts, in the first we present necessary background for our exposition, and the second gives details of COLA.",
            "score": 0.1575827138474356,
            "section_title": "Our Method",
            "char_start_offset": 6709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0574951171875
        },
        {
            "corpus_id": "277103763",
            "title": "RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving",
            "text": "In our ablation studies, we mainly investigate the impacts of fine-tuning VLMs and RAG pipeline for spatial perception enhancement based on Qwen-VL-2-2B [9], Qwen-VL-2-7B [9], Qwen-VL-2.5-3B [2] and Qwen-VL-2.5-7B [2] models. The performance of VLMs is evaluated using four distinct methods: vanilla (no fine-tuning), vanilla combined with RAG, only fine-tuning, and fine-tuning combined with RAG (our proposed RAD method). \n\nThe results presented in Table 2 indicate that the combination of fine-tuning and RAG consistently achieves the highest scores across all evaluation metrics, including ExactMatchAccuracy, Macro -F1, Weighted -F1, PartialMatchScore, and OverallScore, for all model variants. \n\nSpecifically, for Qwen-VL-2.5-7B, our RAD method achieves the highest OverallScore of 0.3956, marking a significant improvement over methods that deploy either fine-tuning or RAG separately. Furthermore, the incorporation of RAG consistently enhances performance for both vanilla and fine-tuned settings across most model scales, validating the effectiveness of retrieval-augmented strategies in improving model performance. \n\nNotably, for smaller models such as Qwen-VL-2-2B and Qwen-VL-2.5-3B, employing only fine-tuning leads to performance degradation, suggesting that their limited parameter sizes hinder effective learning of domain-specific knowledge through fine-tuning alone. Additionally, for Qwen-VL-2.5-3B model, using RAG without fine-tuning results in a performance drop, likely due to the unique pre-training characteristics of this model. \n\nOverall, while fine-tuning or RAG independently can enhance performance in largerscale models, the best results are consistently achieved by combining these two strategies, underscoring the importance of an integrated approach to maximize VLM effectiveness. From a practical perspective, the combination of fine-tuning and RAG proves particularly suitable for enhancing decision-making capabilities in VLMs.",
            "score": 0.15747268881988924,
            "section_title": "Ablation Studies",
            "char_start_offset": 15418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1590576171875
        },
        {
            "corpus_id": "232051157",
            "title": "Automatic Transcription of Organ Tablature Music Notation with Deep Neural Networks",
            "text": "An organ tablature page consists of several rows separated by horizontal lines. Some editors additionally divide their tablatures into bars by vertical lines. Each row consists of staves arranged one below the other, separated only by a small distance, with one staff for each voice of the composition. \n\nThe tablature characters of a voice are arranged in two lines, as shown in Figure 4. The upper line contains note duration symbols, while the lower line contains note pitch symbols and pause signs. The position of special characters may vary from editor to editor, but they are usually located in the upper line. We therefore refer to the upper line as duration/special line and to the lower line as pitch/rest line.",
            "score": 0.15682918833008452,
            "section_title": "Page Layout",
            "char_start_offset": 10018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 302
                },
                {
                    "start": 305,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 721
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0010728836059570312
        },
        {
            "corpus_id": "274131682",
            "title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
            "text": "This study represents insights into generating meta-analysis leveraging LLMs using a large-context scientific dataset, MAD. The result section provides evidence of our fine-tuned models' performance, showing the successive relevancy rate for generating meta-analysis. It was observed that the finetuned models for Llama-2 (7B) and Mistral-v0.1 (7B) outperformed their non-fine-tuned versions by generating significantly relevant meta-analyses. As expected, integrating RAG with fine-tuned models allows them to generate highly aligned meta-analyses. Limitations: One key limitation of this study is the maximum context length of the LLMs, which required chunking the input data. To mitigate potential information loss, overlapping context techniques and RAG were employed. However, due to hardware constraints, the model's evaluation was performed on only 50% of the test sets, which proved resource-intensive. Additionally, training the models in a highly quantized configuration limited the fine-tuning potential, impacting the ability to fully optimize the model's parameters for better performance.",
            "score": 0.15682413199300133,
            "section_title": "D. Discussion",
            "char_start_offset": 30880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1102
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1759033203125
        },
        {
            "corpus_id": "277150553",
            "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
            "text": "To incrementally expand the knowledge of a pretrained LLM or to align it with human preferences, fine-tuning stands as one of the most prevalent approaches, encompassing methods such as supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), and direct preference optimization (DPO) (Rafailov et al., 2024). Despite their effectiveness, these techniques are notably annotationheavy and computationally intensive, rendering the fine-tuning of an LLM using these methods prohibitively costly. To circumvent the extensive computational demands of full fine-tuning, which can reach into tens of billions, numerous parameterefficient fine-tuning (PEFT) methods have been explored, including BitFit (Zaken et al., 2021), adapter (Houlsby et al., 2019), andLora (Hu et al., 2021). In this paper, we primarily employ a LoRA to fine-tune a RAG-Tuned-LLM. Methodologically, RAFT (Zhang et al., 2024b) is the closest to our approach, as it explores the potential integration of RAG and fine-tuning. However, there are two fundamental differences between our work and RAFT: First, the model we train is not intended for use in the generation stage of RAG, making our objectives fundamentally different; Second, our training data does not include deliberately introduced noise, which distinguishes our approach significantly in terms of methodology.",
            "score": 0.15640968066394403,
            "section_title": "Fine-Tuning LLMs",
            "char_start_offset": 21885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1383
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 287,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 330,
                    "end": 353,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 770,
                    "end": 797,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68505859375
        },
        {
            "corpus_id": "271218596",
            "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
            "text": "We aim to address this limitation by introducing RAGBench.\n\nFinetuned RAG evaluation models Fine-tuned LLM judges are another a common way to approach the LLM evaluation task [17,44,41].A number of studies also leverage small, fine-tuned Natural Language Inference (NLI) models for RAG hallucination detection [2,22,33].NLI models measure the degree of entailment between a premise and a hypothesis, which has been successfully repurposed for evaluating LLM response attribution in RAG setting.In this work, we train and evaluate an NLI model for RAG evaluation using RAGBench.The fine-tuned model not only outperforms LLM judges in hallucination/attribution detection but also excels on the new RAG evaluation metrics we propose.",
            "score": 0.15640299273565827,
            "section_title": "Related Work",
            "char_start_offset": 5738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 60,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 730
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4091796875
        },
        {
            "corpus_id": "237797431",
            "title": "User-Driven Fine-Tuning for Beat Tracking",
            "text": "For example, those pieces for which the F = 0 for the baseline and F = 1 for the fine-tuning were almost certainly phase corrections from offbeat (i.e., out-of-phase) to onbeat (i.e., in-phase) at the annotated metrical level. Likewise, any improvement of F = 0.67 to F = 1 was very likely a correction in the choice of metrical level by doubling or halving, i.e., a change to the metrical level corresponding to twice or half the tempo, respectively. Alternatively, we can see that for those pieces that straddle the main diagonal, the impact of the fine-tuning is negligible. Finally, at the other end of the spectrum, we can observe that for SMC and GTZAN, there are at least some cases for which the fine-tuning negatively impacted performance. However, we should note that there are very few extreme outliers where it was catastrophically worse to fine-tune. Ultimately, the cases of most interest to us were those which sit on or close to the line F = 1 after fine-tuning, as these represent those for which there was the clearest benefit. \n\nTo obtain a more nuanced perspective, we reported the counts of all the operations necessary to calculate the annotation efficiency, namely the insertions, deletions and shifts required to transform a set of detections so as to maximize the F-measure. This information is displayed in Table 3. By contrasting the baseline and fine-tuned approaches, we see that across all datasets, fewer total editing operations were required. Indeed, per class of operation, the use of fine-tuning also resulted in fewer insertions, deletions and shifts. In this sense, we interpreted that the impact of fine-tuning was more pronounced than merely correcting the metrical level or phase of the detected beats. Thus, even accounting for the fact that, from a user perspective, each of these operations might not be equally easy to perform, and a reduction across all operation classes highlighted the potential for the improved efficiency of an annotation-correction workflow.",
            "score": 0.15624927969450386,
            "section_title": "Performance Across Common Datasets",
            "char_start_offset": 40822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 2008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04083251953125
        },
        {
            "corpus_id": "252089484",
            "title": "Equivariant self-supervision for musical tempo estimation",
            "text": "At the fine-tuning stage, tempo estimation is formulated as a multiclass classification problem, where each class corresponds to a tempo value. Because it is likely that the datasets used for fine-tuning may not contain training samples for each tempo in the full BPM range considered, we also apply a time-stretching augmentation to increase the range of tempi seen during fine tuning. \n\nTable 1 summarises the results using a model pretrained with r p = 0.2 and no invariant augmentations, for a range of fine-tuning time-streching augmentation strength r f . As expected, it appears that applying some timestretching (r f > 0) generally improves performance over not applying any (r f = 0). We also note that the optimal value varies from one dataset to the next. For example results seem to be optimal for relatively large augmentation strength on the GTZAN dataset while they would be optimal for smaller values on ACM Mirum.",
            "score": 0.15587506682184615,
            "section_title": "Influence of fine-tuning augmentation parameters",
            "char_start_offset": 20104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 930
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00661468505859375
        },
        {
            "corpus_id": "272689590",
            "title": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications",
            "text": "8.1 Parameter-Adaptive RAG (PA-RAG) \n\nThe Parameter-Adaptive RAG system integrates our fine-tuned DistilBERT model to classify query complexity and dynamically adjusts retrieval parameters accordingly, as illustrated in Figure 1, but excluding the knowledge graph component. The PA-RAG system adaptively selects the number of query rewrites (Q) and the top-k value based on the complexity classification, with specific parameter mappings provided in Table 6 in Appendix A.6.1. In the 2-class model, simpler queries (label 0) use a top-k of 5 and 3 query rewrites, while more complex queries (label 1) use a top-k of 10 and 5 rewrites. The 3-class model uses a top-k of 7 and 7 rewrites for the most complex queries (label 2).",
            "score": 0.15528534315348475,
            "section_title": "RAG System Architecture",
            "char_start_offset": 16424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 38,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 725
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060638427734375
        },
        {
            "corpus_id": "277103763",
            "title": "RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving",
            "text": "From a practical perspective, the combination of fine-tuning and RAG proves particularly suitable for enhancing decision-making capabilities in VLMs. Deploying this optimal configuration can substantially improve VLM performance, with potential applications extending to semantic comprehension, trajectory planning, and other complex autonomous driving tasks.",
            "score": 0.15504069148055194,
            "section_title": "Ablation Studies",
            "char_start_offset": 17235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10992431640625
        },
        {
            "corpus_id": "271962900",
            "title": "Agentic Retrieval-Augmented Generation for Time Series Analysis",
            "text": "-We evaluated the Agentic-RAG framework using a single, universal sub-agent for all tasks versus specialized subagents for each task. \u2022 (c) Instruction-tuning(IT) vs. no fine-tuning(NIT): \n\n-We compared the performance of SLMs with instructiontuning against their performance without any fine-tuning.",
            "score": 0.15500176764943996,
            "section_title": "\u2022 (b) Role of sub-agent specialization(SAS):",
            "char_start_offset": 41423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10821533203125
        },
        {
            "corpus_id": "268510325",
            "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
            "text": "\u2022 In the case of RAG, we calculated the scores using Llama-2-7b base and fine-tuned models as well.\n\n\u2022 The threshold value of the search engine used for the RAG presented in section 3.5 was tested through all possible variations between 0 and 1 with a step interval of 0.1 using the indexed datasets ID s , and ID q .\n\nWe summarize our measurement results in the radar plot in (Figure 1) which illustrates the relative performance of the models.Furthermore, the average performance of each model approach is presented in Table 4.As shown in Figure 1 and Table 4, the results suggest that both FN and RAG outperformed the baseline.RAG performed best and was also the best approach.Moreover, the FN did not help RAG.This is supported by the fact that the best threshold parameter for the LlaMA-2-7b base model during the application of RAG was the value of 0.5.\n\nIn the case of the LlaMA-2-7b finely tuned model, the best threshold was 1.0, which practically means 100% rejection.So the fine-tuned model could no longer be helped by context injection.\n\nThe METEOR and BLEU scores of the fine-tuned models were better than those of the RAG models, but in terms of the ROUGE score, they were already inferior compared to the RAG.\n\nFurthermore, the RAG produced a significantly better CS score than the fine-tuned models.\n\nThis shows that RAG significantly improves hallucination and although the association skills of fine-tuned models may be better, the degree of hallucination of fine-tuned models is significantly larger.\n\nOverall, the best result on the test dataset was obtained by using the RAG Llama-2-7b base model with the ID s dataset.The results of the best approaches are the following: ROUGE 0.3, METEOR 0.22, BLEU 0.063 and, CS 0.57.The best construction is presented in detail in",
            "score": 0.1544191982551134,
            "section_title": "Evaulation strategy",
            "char_start_offset": 16437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 101,
                    "end": 317
                },
                {
                    "start": 319,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 859
                },
                {
                    "start": 861,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1049
                },
                {
                    "start": 1051,
                    "end": 1225
                },
                {
                    "start": 1227,
                    "end": 1316
                },
                {
                    "start": 1318,
                    "end": 1520
                },
                {
                    "start": 1522,
                    "end": 1641
                },
                {
                    "start": 1641,
                    "end": 1743
                },
                {
                    "start": 1743,
                    "end": 1790
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1925048828125
        },
        {
            "corpus_id": "273135916",
            "title": "Conversing with business process-aware large language models: the BPLLM framework",
            "text": "During fine-tuning, the pre-trained LLM's parameters are adjusted accordingly to the task-specific dataset through a process known as backpropagation [34]. This process involves iteratively updating the model's parameters to minimize a defined loss function, thereby optimizing the model's performance on the target task [35]. While fine-tuning offers numerous benefits, it also comes with drawbacks, primarily due to its resource-intensive nature, necessitating substantial computational resources and data to be effective. However, these limitations can be mitigated through specific techniques such as Parameter-Efficient Fine-Tuning (PEFT) [36]. This method employs various deep learning techniques to minimize the number of trainable parameters while retaining comparable performance to full fine-tuning, updating only a limited number of additional parameters or a subset of pre-trained parameters. \n\nAn encouraging solution to improve LLM accuracy and credibility (especially in knowledge-intensive tasks) and avoid the effort required by complex fine-tuning is represented by the RAG [8] which is becoming a popular paradigm in LLM's systems. The underlying idea of the RAG approach is the merging of LLMs' knowledge with specialized, vast, and dynamic data coming from external repositories [8]. The initial query prompts the external retrieval of pertinent information via search algorithms. The obtained information is then sent to the LLM's prompts which provides further context information [32]. According to this, the RAG approach combines information retrieval mechanisms with In-Context Learning (ICL) [37] to improve the LLM's performance. The RAG approach includes a retriever and a generator [8] and consists of three steps (retrieve, augment, and generate). In the retrieval step, the user query x is used to retrieve relevant context (text documents z) from an external knowledge source by the retriever p \u03b7 (z|x) with parameters \u03b7 returning distributions over text documents given x. Using an embedding model, the query is embedded into a vector space and included as the additional context in the vector database. According to the similarities between vectors and query, the k closest documents from the vector database are selected. In the augment step, the initial query and the obtained additional context are combined into a prompt template.",
            "score": 0.15425600282387525,
            "section_title": "Large Language Models and RAG",
            "char_start_offset": 11826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2369
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "256826766"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "258439667"
                },
                {
                    "start": 1092,
                    "end": 1095,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1712,
                    "end": 1715,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.236572265625
        },
        {
            "corpus_id": "274116775",
            "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
            "text": "Instruction fine-tuning has become a pivotal approach for improving large pre-trained language models on instruction-based tasks. To enable our model to perform well on e-commerce Retrieval-Augmented Generation (RAG) tasks, we trained an instruction-tuned model using our foundational architecture. Instead of restricting loss computation and backpropagation to just the answer outputs, we utilized the full instruction dataset, resulting in more effective training [Shi et al., 2024]. \n\nOur fine-tuning setup included a learning rate of 6e-5, weight decay of 0.1, a warmup ratio of 0.03, a context length of 4096 tokens, and a batch size of 120, with a cosine annealing scheduler. We constructed the dataset progressively, starting from simple tasks and advancing to more complex ones, integrating the RAG and RAFT datasets [Zhang et al., 2024c] toward the end. The dataset comprised Belle (56.04%) [Ji et al., 2023], infinity-instruct-subject (34.25%), infinity-commonsense (5.97%), RAG_mixed (2.97%), and RAFT_mixed (0.77%). \n\nThe model's RAG performance in the e-commerce domain achieved a satisfaction rate of 92.47%, as evaluated by GPT-4o mini on our e-commerce evaluation dataset.",
            "score": 0.15419426893193988,
            "section_title": "Post-training",
            "char_start_offset": 7788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1573486328125
        },
        {
            "corpus_id": "271710111",
            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
            "text": "We introduced RAG FOUNDRY, an open-source library dedicated to the task of RAG-augmentation of LLMs, namely fine-tuning LLMs to become better at RAG settings. The library is designed to serve as an end-to-end experimentation environment, enabling users to quickly prototype and experiment with different RAG techniques. We demonstrated the usefulness of the library by augmenting two models with RAG configurations, evaluating on three Q&A datasets and showing the benefit of RAG techniques, as well as of using multi-aspect metrics relevant for RAG systems evaluation.",
            "score": 0.1538227580982834,
            "section_title": "Conclusion",
            "char_start_offset": 15878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "274117194",
            "title": "Boosting GPT models for genomics analysis: generating trusted genetic variant annotations and interpretations through RAG and Fine-tuning",
            "text": "As for time efficiency, the time required to build a RAG index depends on the search method employed. We used keyword search, which took approximately 30 h to create the search index for all 190 million annotations. In comparison, fine-tuning 3000 annotations for three epochs took about 4 h. Extending this process to 190 million variants, even assuming a linear increase rate, would require a substantial amount of time. Another advantage of RAG is its flexibility to plug into any LLM, allowing users to choose their preferred base model. It can also be easily added to new models when model's updates occur. Fine-tuning, however, relies on a specific model and cannot be easily migrated. \n\nWe concluded that RAG is more scalable and can deliver more trusted results. RAG is particularly advantageous when handling large-scale knowledge injection and providing accurate answers to specific user queries, such as answering a variant rsID based on its genomic position. On the other hand, fine-tuning excels in learning underlying patterns from training data and can generalize learned knowledge to unseen data. Fine-tuning is beneficial for improving model performance in underrepresented domains by keeping training model with learnable information, such as inferring gene names from variant positions. Rather than being alternative approaches, RAG and fine-tuning can be complementary. Combining them can further enhance model performance.",
            "score": 0.1536654867686328,
            "section_title": "Comparison between RAG and fine-tuning",
            "char_start_offset": 22236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1443
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424072265625
        },
        {
            "corpus_id": "273532096",
            "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
            "text": "This fine-tuning strategy significantly enhances the \"instruction-following\" capabilities of the system's core components. It is also convenient to transfer LongRAG to other domains by leveraging the pipeline and fine-tuning strategy. \n\nExtensive performance comparisons and quantitative ablation studies conducted on three multihop datasets from LongBench (Bai et al., 2023b) demonstrate the superiority and effectiveness of LongRAG. The results suggest that LongRAG significantly outperformed both long-context LLMs and advanced RAG methods. We also discuss LongRAG's performance with different fine-tuned LLMs and confirm its strong robustness and transferability. To sum up, our contributions are summarized as follows: 1) We construct LongRAG, a general, dual-perspective, and robust RAG system paradigm. It significantly surpasses long-context LLM (up by 6.94%), mainstream advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). \n\n2) We identify and address RAG's limitations in LCQA. We develop two plug-and-play components (i.e., Information Extractor and CoT-guided Filter) to explore global information and factual details, enhancing understanding of complex long contexts. \n\n3) We implement a novel automated fine-tuning data construction pipeline and a multi-task training strategy with multi-length long-context data. They facilitate the application of our paradigm to diverse specific-domain data in real-world scenarios.",
            "score": 0.15339490265823247,
            "section_title": "Introduction",
            "char_start_offset": 4225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1188
                },
                {
                    "start": 1191,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1440
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 376,
                    "matchedPaperCorpusId": "261245264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318115234375
        },
        {
            "corpus_id": "273350575",
            "title": "Self-adaptive Multimodal Retrieval-Augmented Generation",
            "text": "Table 2 (top) demonstrates that fine-tuning the retrieval model R or the VLM M improves multimodal RAG performance. However, finetuning M provides more substantial gains. This suggests that the baseline retrieval model R is already sufficiently optimized, leading to smaller performance improvements from finetuning. In contrast, optimizing M directly enhances the model's ability to process multimodal data. Notably, the fine-tuned RAG(R * + M * ) shows slightly lower performance than MuRAG, despite differences in the volume of training data. \n\n3. SAM-RAG vs. Conventional RAG: As shown in Table 2 (middle), SAM-RAG consistently outperforms conventional RAG methods. By dynamically retrieving documents until relevant information is found, SAM-RAG avoids the limitations of fixed retrieval strategies. Fine-tuning the retrieval model R speeds up document retrieval but does not significantly impact overall performance, as the strength of SAM-RAG lies in leveraging the fine-tuned M. The results indicate that fine-tuning M leads to the most notable performance improvements in SAM-RAG, especially for multimodal tasks. \n\n4. Effect of GPT Integration: Table 2 (bottom) highlights the substantial performance gains from integration of G into the SAM-RAG and conventional RAG frameworks. Models incorporating G outperform all other configurations, underscoring G's advanced reasoning and understanding abilities. SAM-RAG combined with G shows the most significant improvements, particularly in visual tasks, where it exceeds textual performance. This suggests that the SAM-RAG framework, when paired with G, achieves a deeper understanding of visual content than other approaches. To validate the effectiveness of each component of SAM-RAG, a series of ablation studies are performed that check the performance of the framework after removing each component, as shown in Table 3. The results indicate that, compared to conventional RAG, the introduction of distinct verification, particularly isRel and isSup , leads to notable improvements in both F1 and EM scores. Specifically, although introducing isRel only brings minimal improvement, the subsequent introduction of isUse and isSup makes a greater impact. When all verifications are combined (labeled \"with all\"), the EM score rises to the maximum value, reflecting an improvement of more than 20% compared to the performance of MuRAG.",
            "score": 0.15331593629777032,
            "section_title": "Performance Comparison with Baselines:",
            "char_start_offset": 17677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "265466391",
            "title": "Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model",
            "text": "Using the RAG method, documents can be provided with a certain level of ability to answer domain-specific questions without training a model. In contrast, fine-tuning enables LLMs to \"internalize\" data as their knowledge when given sufficient information, resulting in outstanding performance in specific domains. \n\nThe RAG method does not require abundant computational resources; it only requires domain-specific text-related data and can be directly applied to LLMs, refining prompts without adjusting the LLM. Fine-tuning, on the other hand, requires adequate data and computational power to be performed on open-source LLMs. Currently, closed-source LLMs still outperform their open-source counterparts by a significant margin. However, it is highly likely that using the RAG method and fine-tuning open-source LLMs (e.g., Llama2, CodeGen [14]) would yield similar performance results, albeit with vastly different costs.",
            "score": 0.15298333575277584,
            "section_title": "RAG vs Fine-tuning",
            "char_start_offset": 12027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 313
                },
                {
                    "start": 316,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 926
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34326171875
        },
        {
            "corpus_id": "267027552",
            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
            "text": "This study aimed to establish a baseline for assessing the capabilities language models, such as LLama 2, GPT-3.5 and GPT-4, in addressing complex problems in agriculture. By evaluating their performance when using RAG or/and fine-tuning, the study provides valuable insights into the strengths and limitations of LLMs within the agricultural domain. \n\nThe primary contributions of the paper include the establishment of performance baselines for LLMs when using RAG or fine-tuning as it presents different benefits and costs. RAG, known for improving accuracy in large models, is highly effective in instances where data is contextually relevant, such as in the interpretation of farm data. The low initial cost of creating embeddings, which are vector representations of data, makes RAG an attractive option. However, it is important to consider that the input token size can increase the prompt size, and the output token size tends to be more verbose and harder to steer. \n\nOn the other hand, fine-tuning offers a precise, succinct output that is attuned to brevity. It is highly effective and presents opportunities to learn new skills in a specific domain, like improving crop yield predictions or optimizing irrigation schedules based on weather patterns. However, the initial cost is high due to the extensive work required to fine-tune the model on new data. In addition, fine-tuning necessitates minimal input token size, making it a more efficient option for handling large data sets. A comparison between two approaches is presented on Table 23. \n\nThis research also serves as a pioneering step in establishing a pipeline for the application of RAG and fine-tuning techniques in various LLMs, enabling innovation and collaboration across multiple industries. With our initial focus on agriculture, we have demonstrated how these strategies can lead to more efficient models starting from the Q&A generation process. \n\nIn this study, we also demonstrated how to generate relevant question and answers for datasets for specific industries, by leveraging structured document understanding, together with GPT-4 for question generation, and RAG for answer generation. The questions generated were highly specific to the respective sections they were derived from, and the model was able to utilize the entire text for generating insightful and comprehensive answers.",
            "score": 0.15223218253979787,
            "section_title": "Conclusion",
            "char_start_offset": 86922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2373
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2276611328125
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "We con-structed a diverse dataset covering legal documents, scientific literature, books, and web data, each paired with a plausible but fictitious counterpart. We then fine-tune instruction-tuned LLMs, specifically Meta's Llama 3.1-8B-Instruct (Grattafiori et al., 2024), on this dataset using two prompt variants: a Baseline format and a Structured XML variant. This setup allows us to assess generalization and prompt sensitivity. To our knowledge, Finetune-RAG provides the first RAG dataset of its kind, as existing RAG finetuning datasets implicitly assume perfect information retrieval, and mostly focus only the LLM's ability to extract coherent answers from relevant chunks. \n\nOur key insight is that LLMs struggle to identify contextual clues that are obvious to the human eye, such as financial reports from a similarly named company or outdated information based on dates indicated by document metadata. Through fine-tuning models with a controlled mixture of true and false context placed alongside, we teach them to ground their answers exclusively in the reliable information provided. \n\nWe evaluated the effectiveness of Finetune-RAG using Bench-RAG, a custom benchmarking suite we have created that leverages GPT-4o (OpenAI, 2024) as an automated judge to assess the accuracy, relevance, helpfulness and depth of the LLM response. Our results show that Finetune-RAG substantially improves factual correctness while maintaining output quality across other dimensions, demonstrating that generation-time defenses are a viable complement to improved retrieval. \n\nOur contributions are as follows: \n\n\u2022 Fine-tuning Approach. We propose a novel fine-tuning strategy for RAG systems that teaches models to ignore misleading context and generate answers based solely on factual input. \n\n\u2022 Training Dataset. We release a curated, multi-domain dataset designed for hallucination resistance training, with both factual and fictitious content. \u2022 Evaluation Setup. We benchmark the effectiveness of our approach using GPT-4o-based evaluations and show significant gains in factual accuracy without compromising helpfulness or relevance. \n\n\u2022 Open-source release. We make our code, models, dataset, and evaluation framework publicly available to facilitate further research. They can be accessed in our open-source repository4 and dataset5 .",
            "score": 0.1515673494698344,
            "section_title": "Introduction",
            "char_start_offset": 1761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1793
                },
                {
                    "start": 1796,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2140
                },
                {
                    "start": 2143,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2343
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "271244714",
            "title": "The Kolmogorov Complexity of Irish traditional dance music",
            "text": "We select a (musically) representative sample of 60 reels from the online database [1].The most common key for reels is D (about 45%), the second most common G (about 30%), with the remaining 25% in a variety of keys.We examined tunes from across the key spectrum.To enable fair comparison, we only examined \"standard\" length reels, i.e. ones with exactly two parts: In our representation this corresponds to a sequence of 128 letters.In summary,\n\n\u2022 Each reel has two parts\n\n\u2022 Each part has exactly 64 (quaver) notes\n\n\u2022 Each bar has exactly 8 (quaver) notes\n\n\u2022 Any longer notes are written as sequences of quavers adding up to the same note length.\n\nSo, any crotchet in the tune is written as two identical quavers, any dotted crotchet as three quavers, etc.\n\n\u2022 Each part of the tune has 8 bars, and overall the tune has 16 bars\n\nOur analysis of the representative sample of 60 reels gives an average compression ratio of 2.79 (to two decimal places) with standard deviation 0.46.",
            "score": 0.15133741517712282,
            "section_title": "Compression ratios for reels",
            "char_start_offset": 2250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 87,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 446
                },
                {
                    "start": 448,
                    "end": 473
                },
                {
                    "start": 475,
                    "end": 516
                },
                {
                    "start": 518,
                    "end": 557
                },
                {
                    "start": 559,
                    "end": 648
                },
                {
                    "start": 650,
                    "end": 758
                },
                {
                    "start": 760,
                    "end": 828
                },
                {
                    "start": 830,
                    "end": 980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0004711151123046875
        },
        {
            "corpus_id": "262187179",
            "title": "Acoustic Feature Identification to Recognize Rag Present in Borgit",
            "text": "In the world of Indian classical music, raga recognition is a crucial undertaking. Due to its particular sound qualities, the traditional wind instrument known as the borgit presents special difficulties for automatic raga recognition. In this research, we investigate the use of auditory feature identification methods to create a reliable raga recognition system for Borgit performances. Each of the Borgits, the devotional song of Assam is enriched with rag and each rag has unique melodious tune. This paper has carried out few experiments on the audio samples of rags and a few Borgits sung with those rugs. In this manuscript three mostly used rags and a few Borgits\u00a0 with these rags are considered for the experiment. Acoustic features considred here are FFT (Fast Fourier Transform), ZCR (Zero Crossing Rates), Mean and Standard deviation of pitch contour and RMS(Root Mean Square). After evaluation and analysis it is seen that FFT\u00a0 and ZCR are two noteworthy acoustic features that helps to identify the rag present in Borgits. At last K-means clustering was applied on the FFT and ZCR values of the Borgits and were able to find correct grouping according to rags present there. This research validates FFT and ZCR as most precise acoustic parameters for rag identification in Borgit. Here researchers had observed roles of Standard deviation of pitch contour and RMS values of the audio samples in rag identification.",
            "score": 0.15117510164373038,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00757598876953125
        },
        {
            "corpus_id": "272689170",
            "title": "SFR-RAG: Towards Contextually Faithful LLMs",
            "text": "One of the most important goals of SFR-RAG is to make full use and complete comprehension of any provided contextual information in the real-world RAG scenarios. This trait includes many capabilities, among which are (i) extracting relevant information from arbitrary long contexts, (ii) recognizing the lack of relevant information and abstaining from hallucinated generation, (iii) recognizing potential conflicting information in contextual passages, and (iv) being resilient to distracting, counter-intuitive information or contents that are out-of-distribution from the pre-training process. We fine-tuned SFR-RAG via standard supervised fine-tuning and preference learning [40,9,31], using extensive instruction-following data that mimic real-world retrieval question answering applications. \n\n3 Evaluation",
            "score": 0.1509336460257821,
            "section_title": "SFR-RAG Fine-tuning Process",
            "char_start_offset": 9487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 812
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357421875
        },
        {
            "corpus_id": "273323657",
            "title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation",
            "text": "Fine-tuning is widely used to improve the capabilities of LLM-augmented components in RAG systems (de Luis Balaguer et al., 2024). Early methods of fine-tuning to enhance LLM-based components in RAG primarily focused on training the retriever and the generator (Ke et al., 2024;Lin et al., 2024). Recent RAG methods have shifted toward modular designs (Gao et al., 2023b). Particularly in complex QA tasks, adaptive RAG often requires base models to follow intricate instructions (Yin et al., 2023a;Xu et al., 2024) to enable the functionality of diverse components (Asai et al., 2024). Classic alignment training methods include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). However, SFT lacks negative feedback and is prone to overfitting. Recently, Rafailov et al. proposed a more efficient reinforcement learning algorithm, direct preference optimization (DPO), which aligns response preferences and enhances the model's instruction-following ability by learning the differences between positive and negative sample pairs. In our work, we focus on using DPO to enhance the model's capability in multiple processes.",
            "score": 0.15007998214881818,
            "section_title": "Fine-Tuning for RAG",
            "char_start_offset": 5440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1156
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 278,
                    "matchedPaperCorpusId": "266998785"
                },
                {
                    "start": 278,
                    "end": 295,
                    "matchedPaperCorpusId": "263605962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67529296875
        },
        {
            "corpus_id": "237797431",
            "title": "User-Driven Fine-Tuning for Beat Tracking",
            "text": "In the final part of our evaluation, we considered the impact of fine-tuning from a different perspective. Having established that fine-tuning is beneficial at the level of individual pieces, we now re-assess the performance of a fine-tuned network adapted to a given piece on other data. To this end, we investigated the presence and extent of \"catastrophic forgetting.\" Known also as catastrophic interference, catastrophic forgetting is a well-known problem for backpropagation-based models [60] and is characterized by the tendency of an artificial neural network to abruptly forget previously learned information upon learning new information. Despite the sequential learning nature of our fine-tuning adaptation, this is merely episodic, as opposed to the continual acquisition of incrementally available information, which is more commonly addressed in catastrophic interference [61]. Nevertheless, it is of interest in the context of this work to examine what a fine-tuned network loses in terms of general knowledge about the beat when adapted to the properties of a specific piece of music. \n\nTo explore this behaviour, we return to the Blue Moon excerpt from the Hainsworth dataset. Across the training epochs of this excerpt, we evaluated the performance of each of the corresponding 24 models over the GTZAN and TapCorrect datasets. More specifically, for every epoch of the fine-tuning of Blue Moon, we saved the intermediate network and used it to estimate the beat in every excerpt of the GTZAN and TapCorrect datasets. In this way, we repeated the evaluation over these datasets 24 separate times. \n\nThus far, we have shown that, for this piece, there is a dramatic improvement in the F-measure once the fine-tuning has completed. However, we have not observed the manner in which the F-measure improves over the intermediate training epochs, nor how the fine-tuning process (i.e., specific to this musical excerpt) impacts performance on other musical content. In the presence of catastrophic forgetting, we should expect some kind of inverse relationship in performance, with the improvement on Blue Moon coming at the expense of that on GTZAN and TapCorrect.",
            "score": 0.14988244504730477,
            "section_title": "Choros \u21161",
            "char_start_offset": 50900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 886,
                    "end": 890,
                    "matchedPaperCorpusId": "73497737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026702880859375
        },
        {
            "corpus_id": "269696779",
            "title": "A Reliable and Accessible Caregiving Language Model (CaLM) to Support Tools for Caregivers: Development and Evaluation Study",
            "text": "The larger FM has more parameters and is able to accommodate having more knowledge encoded into it. \n\nThe RAG framework that was implemented with fine-tuning performed better than RAG-only and vanilla for the 2 small FMs. Because we could not retrain the GPT 3.5 model, we were not able to evaluate a retrained (fine-tuned) version of it. The most interesting result is that RAG + fine-tuned LLaMA 2 7B performed significantly better than vanilla GPT 3.5 across all metrics. Fine-tuned Falcon 7B also performed better than vanilla GPT 3.5 across all metrics. LLaMA 2 7B and Falcon 7B have only 7 billion parameters, while OpenAI GPT 3.5 has 175 billion parameters. This shows that a small FM with the injection of domain-specific knowledge can perform better than a much larger FM. In addition to the general performance of the language models in providing the right answers, we also evaluated their reliability by measuring their capacity to provide accurate references with the answers. Table 3 provides results on the capability of the models to return references in their answers and mentions the number of correct references. The references provided by the models were evaluated for correctness and relevance. In addition to checking whether the generated answer returned a list of references, the annotator verified that the links were correct and active. The annotator also checked each inline reference to determine if the content in the answer was part of the original document. Using the results from these checks, the data annotator decided whether each reference was correct and relevant to the answer. \n\nNone of the vanilla FMs were trained to return references, and therefore, no references were provided for any questions (the results were all zeroes). LLaMA 2 7B in the RAG + fine-tuned setting provided references to all 66 answers. It performed better than GPT 3.5 with the RAG framework, which returned references in only 46 of the 66 answers (70%). Fine-tuning by retraining the FMs using the 415 Q-A pairs of the training set significantly improved the capabilities of the FMs to return references.",
            "score": 0.1498328569717431,
            "section_title": "Experiment Results",
            "char_start_offset": 24147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140869140625
        },
        {
            "corpus_id": "261557451",
            "title": "The Batik-plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations",
            "text": "Our data permits much more detailed investigations into relationships between structural aspects of a piece, and how these are translated into performance decisions by a pianist. As a simple example, we investigate variations in local tempo before various types of cadences. Specifically, we compare the local tempo prior to a cadence annotation across different tempo classes for authentic (perfect and imperfect, i.e., PAC and IAC) and half cadences (HC), and differentiate between the cases when a cadence falls on either a downbeat or a weak beat. The hypothesis to be tested with our distribution implies it might be more severe than expected. here is that a performer will tend to shape cadences differently, in terms of tempo, depending on their type and degree of 'finality'. \n\nTo compute the local tempo curves, we consider a uniform window spanning one quarter note each preceding and following a cadence label. 11 For each score-notealigned performed note in that window, we define the local tempo via the beat period (BP), which we calculate as the ratio of the inter-onset-interval (IOI) between the current performed onset and the subsequent one, and the IOI between the current notated onset and subsequent one. We exclude grace notes and their corresponding performed notes from this calculation in order to remove outliers. \n\nNext, we perform time-wise interpolation on these tempo curves to obtain beat period values at eighth note intervals within the window. Given that we are most interested in the local timing strategy immediately before a label (that is, an eighth note before the label position), we discard those curves where that particular time point is interpolated. Following this procedure, we obtain a total of 3,540 local tempo values (corresponding to 708 curves), of which 251 (7.09%) values are interpolated. \n\nFigure 4 shows the mean of local tempo curves across different tempo classes, for cadence labels annotated on a downbeat (left) and on a weak beat (right), respectively. For both authentic and half cadence types, the differences in local tempo diminish with increasing global tempo for both downbeat and weak beat cadences. Likewise, the tempo profiles tend to flatten out with increasing global tempo, suggesting that the pianist takes more liberty, in terms of expressive timing, in slow pieces.",
            "score": 0.14953181513024463,
            "section_title": "Performance of different cadence types",
            "char_start_offset": 17962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2344
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0014667510986328125
        },
        {
            "corpus_id": "271244714",
            "title": "The Kolmogorov Complexity of Irish traditional dance music",
            "text": "From the same database [1] we select a (musically) representative sample of 60 jigs.We examined tunes from across the key spectrum.To enable fair comparison, we only examined \"standard\" length jigs, i.e. ones with exactly two parts: In our representation this corresponds to a sequence of 96 letters.In summary,\n\n\u2022 Each jig has two parts\n\n\u2022 Each part has exactly 48 (quaver) notes\n\n\u2022 Each bar has exactly 6 (quaver) notes\n\n\u2022 Any longer notes are written as sequences of quavers adding up to the same note length.So, any crotchet in the tune is written as two identical quavers, any dotted crotchet as three quavers, etc.\n\n\u2022 Each part of the tune has 8 bars, and overall the tune has 16 bars Our analysis of the representative sample of 60 jigs gives an average compression ratio of 2.61 (to two decimal places) with standard deviation 0.40.",
            "score": 0.14953181513024463,
            "section_title": "Compression ratios for jigs",
            "char_start_offset": 3808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 311
                },
                {
                    "start": 313,
                    "end": 337
                },
                {
                    "start": 339,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 421
                },
                {
                    "start": 423,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 620
                },
                {
                    "start": 622,
                    "end": 840
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0005297660827636719
        },
        {
            "corpus_id": "277593570",
            "title": "Retrieval augmented generation for 10 large language models and its generalizability in assessing medical fitness",
            "text": "In the rapidly evolving field of LLMs, the challenge of optimizing performance to meet specific needs is a key focus. While out-of-the-box LLMs offer impressive capabilities, techniques like fine-tuning and RAG present promising avenues for further enhancing their accuracy and relevance. \n\nThe primary challenges in fine-tuning LLMs stem from various factors including the need for extensive retraining datasets, particularly for complex fields like healthcare; and technical hurdles such as limitations in context tokens and the computational demands typically quantified in petaflops for Graphics Processing Unit (GPU) memory 10 . \n\nRetrieval Augmented Generation (RAG) is an innovative approach for tailoring LLMs to specific tasks, and a scalable solution agnostic to various LLM-based healthcare applications. It offers an easier solution without the need for extensive training examples or time as required by fine-tuning, and accessibility to updated customized knowledge without significant time in creating up-to-date ground truth and retraining required by fine-tuning. Unlike traditional LLMs, RAG functions similarly to a search engine, retrieving relevant, customized text data in response to queries. This capability effectively turns RAG into a tool that integrates specialized knowledge into LLMs, enhancing their baseline capabilities. In healthcare, for instance, LLMs equipped with RAG and embedded with extensive clinical guidelines (LLM-RAG) can yield more accurate outputs 11 . Currently, two primary open-source frameworks for RAG exist -LangChain 12 and Llamaindex 13 . Although the retrieval process of RAG can be technically challenging, RAG's utility in contexts with smaller, more focused knowledge corpora remains significant. \n\nThis study aims to develop and evaluate an LLM-RAG pipeline for preoperative medicine using various LLMs and perioperative guidelines. The primary objective is to assess the pipeline's accuracy in determining patients' fitness for surgery. The secondary objective is to evaluate the LLM-RAG's ability to provide accurate, consistent, and safe preoperative instructions, including fasting guidelines, pre-operative medication management, and whether the patient should be seen by a nurse or a doctor in the pre-operative clinic.",
            "score": 0.14952077804172736,
            "section_title": "body",
            "char_start_offset": 2223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 629,
                    "end": 631,
                    "matchedPaperCorpusId": "219087919"
                },
                {
                    "start": 1496,
                    "end": 1498,
                    "matchedPaperCorpusId": "258740478"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31396484375
        },
        {
            "corpus_id": "271902989",
            "title": "A Primer on Generative AI for Telecom: From Theory to Practice",
            "text": "There are foundation LLMs meticulously pre-trained on large datasets by harnessing the power of accelerated computing. However, to develop effective LLMs for telecom, it is critical that the LLMs incorporate telecom domain knowledge and/or enterprise data sources. This can be done with RAG which is able to connect LLM prompts with the information retrieved from external sources, leading to improved accuracy and reliability of the LLMs. \n\nFigure 3 shows a typical RAG pipeline, consisting of offline document ingestion and online query and response generation with the following steps: and the retrieved contextual information, the LLMs generate appropriate responses. In addition to RAG, other popular LLM customization techniques include prompt engineering and learning, parameter-efficient fine-tuning (PEFT) that selectively adds or updates a few parameters or layers of the original LLM architecture, and general fine-tuning methods, e.g., supervised fine-tuning and reinforcement learning with human feedback (RLHF). Fine-tuning techniques like PEFT or RLHF can be powerful tools to customize a pretrained LLM for telecom by updating the parameters of the LLM based on telecom domain knowledge and/or enterprise data sources. Compared to the RAG based customization, fine-tuning is more resource intensive but yields higher accuracy for certain use cases. However, RAG and fine-tuning are not mutually exclusive technologies, but rather can be used in tandem. When starting to build a telecom application with an LLM, RAG can be firstly applied to quickly improve accuracy. If the application demands even higher accuracy, fine-tuning can be further used to customize the LLM. In a nutshell, the best approach of customizing an LLM for telecom depends on the application's specific requirements by considering resource availability and computational constraints. \n\nRAG-enhanced LLMs can be very effective and useful for enabling telecom specific use cases like network planning or telecom standards chatbot. Take telecom standards chatbot for example. Generic LLM models can be enhanced with telecomspecific standards documents through RAG mechanism, so that the LLM models become more accurate and efficient in answering domain-specific questions related to telecom standards. A case study illustrating the efficacy of this example is detailed in the following section.",
            "score": 0.14935120098454288,
            "section_title": "B. Information Retrieval and Customization",
            "char_start_offset": 19402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2286
                },
                {
                    "start": 2287,
                    "end": 2379
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373046875
        },
        {
            "corpus_id": "273229050",
            "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
            "text": "Figure 5 | Generalization ability of LLMs fine-tuned with RAG-specific data (RAG FT). RAG FT consistently outperforms the chat LLM w. RAG and the model fine-tuned directly on question-answer pairs (Direct FT). This demonstrates the effectiveness of RAG FT in enabling the LLM to effectively extract knowledge from retrieved context on unseen tasks. Note that Direct FT is evaluated without retrieval to align with its training paradigm and all others are evaluated with retrieval augmentation. (LLMs: Gemma-2-9B-Base, Mistral-Nemo-12B-Base, Gemini-1.0-Pro)",
            "score": 0.14926925803138963,
            "section_title": "RAG Accuracy",
            "char_start_offset": 23762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 556
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "276929371",
            "title": "RAG-Adapter: A Plug-and-Play RAG-enhanced Framework for Long Video Understanding",
            "text": "In Figure 5, we reduce the embedding space of the question and all video frames to two dimensions using UMAP [27] (Uniform Manifold Approximation and Projection) to preserve the global structure of the data. This visualization illustrates the spatial relationship between the question embedding and the embeddings of frames sampled by uniform sampling, the non-fine-tuned RAG-Adapter, and the GCL fine-tuned RAG-Adapter. It can be observed that the embeddings of uniformly sampled frames are highly scattered, while the embeddings of frames sampled by the non-finetuned RAG-Adapter cluster around a few similar frames. In contrast, the embeddings from the GCL fine-tuned RAG-Adapter exhibit greater diversity and are closer to the question embedding.",
            "score": 0.1490112889152924,
            "section_title": "Differences of Embedding spaces",
            "char_start_offset": 24302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 750
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1351318359375
        },
        {
            "corpus_id": "3975888",
            "title": "Bridge the Gap between Neural Networks and Neuromorphic Hardware with a Neural Network Compiler",
            "text": "Fine-Tuning Granularity. We conduct experiments to explore the relationship between accuracy and fine-tuning granularity. In another word, in the step of Graph tuning, we can fine-tune one or more successive parts in G \u2032 simultaneously, even fine-tune the entire model as a whole. It looks like that increasing the fine-tuning scale per time will increase the search space, which may lead to a lower error rate but consume more computations and more time to converge. However, results show that coarse grained fine-tuning does not result in improved accuracy. For example, for CIFAR10-CNN, the inference accuracy (the I/O precision is 7 bits and the weight precision is 4 bits) is 83.07% as the fine-tuning-granularity is two subgraphs (the accuracy is 83.14% as the granularity is one). If we fine-tune the whole NN together, the accuracy is just 83.24%. Thus, one by one fine-tuning is an optimal strategy, which also means that the problem of error accumulation has been well solved. Unless specifically noted, the fine-tuning granularity is just one.",
            "score": 0.14866626507014175,
            "section_title": "Accuracy vs.",
            "char_start_offset": 36443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1054
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0127716064453125
        },
        {
            "corpus_id": "271600624",
            "title": "Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks",
            "text": "Fine-tuning large pre-trained models is a common practice in machine learning applications, yet its mathematical analysis remains largely unexplored. In this paper, we study fine-tuning through the lens of memorization capacity. Our new measure, the Fine-Tuning Capacity (FTC), is defined as the maximum number of samples a neural network can fine-tune, or equivalently, as the minimum number of neurons ($m$) needed to arbitrarily change $N$ labels among $K$ samples considered in the fine-tuning process. In essence, FTC extends the memorization capacity concept to the fine-tuning scenario. We analyze FTC for the additive fine-tuning scenario where the fine-tuned network is defined as the summation of the frozen pre-trained network $f$ and a neural network $g$ (with $m$ neurons) designed for fine-tuning. When $g$ is a ReLU network with either 2 or 3 layers, we obtain tight upper and lower bounds on FTC; we show that $N$ samples can be fine-tuned with $m=\\Theta(N)$ neurons for 2-layer networks, and with $m=\\Theta(\\sqrt{N})$ neurons for 3-layer networks, no matter how large $K$ is. Our results recover the known memorization capacity results when $N = K$ as a special case.",
            "score": 0.14864227248562842,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0258026123046875
        },
        {
            "corpus_id": "271892344",
            "title": "Plan with Code: Comparing approaches for robust NL to DSL generation",
            "text": "To compare the impact of RAG on unseen APIs, not available for fine-tuning, we created an out of domain test set. We selected 10 APIs, and discarded the flows containing these APIs from the train set. The test set contains 136 (NL, flow) pairs having these APIs. \n\nWe share the results in Table 5. The baseline is a fine-tuned Codex model with the updated training data. The RAG-based approach notably enhances average similarity (by 7 pts) and reduces API hallucinations (by 1.5 pts) for out of domain APIs. This indicates that when samples are not present in the train set, grounding with RAG context can provide the LLM support for improving code quality. \n\nHowever, fine-tuned model outperforms RAG model in terms of syntactic errors and parameter key hallucinations. The role of few-shots in informing the syntax of the output code cannot be substituted with just adding function definitions. Since, it is hard to obtain the examples for unseen APIs, we need to find alternate ways to improve syntactic errors. We will look into improving this as future work.",
            "score": 0.148464540288012,
            "section_title": "Out of Domain APIs",
            "char_start_offset": 15316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046112060546875
        },
        {
            "corpus_id": "273452788",
            "title": "Towards an In-Context LLM-Based Approach for Automating the Definition of Model Views",
            "text": "Fine-tuning and RAG. The performance of off-theshelf LLMs on a given task is strongly dependent on how much the task is covered by their training dataset [18]. To extend the application of LLMs to tasks that require additional task-specific knowledge, the two most common techniques are fine tuning and retrieval-augmented generation (RAG). \n\nFine-tuning enhances an LLM, already pre-trained on a vast and diverse corpus of text, by additional training on new task-specific content. It refines the LLM model with specialized datasets relevant to the targeted task [41]. Retrieval Augmented Generation (RAG) enhances the standard LLM response for specific contextual data. It allows the injection of such data for the targeted task by indexing it in a vector database, and making it directly accessible by the LLM [31]. \n\nBoth techniques show promising results. Still, fine-tuning demands a large dataset of examples and high computational resources [31]. While more accessible, RAG applications still need a fairly large dataset and an infrastructure for the retrieval process [27]. The availability of public dataset is a well-known problem in MDE and, especially for view definition, not many examples are publicly available. Thus, in this paper we do not use any of these techniques, and we study a solution that works directly on off-the-shelf LLMS.",
            "score": 0.14842625407352802,
            "section_title": "2.2.2",
            "char_start_offset": 8057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1353
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "269762693"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "269762693"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "265308606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.287109375
        },
        {
            "corpus_id": "269757986",
            "title": "Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing",
            "text": "Fine-tuning Gemma is applied for PRP models (EU and DPO). Different fine-tuning procedures for Gemma share the same set of hyperparameters. 128-rank LoRA is used to fine-tune the model with AdamW (Loshchilov & Hutter, 2019) as the optimizer, learning rate initialized as 2 \u00d7 10 \u22124 . \n\nBased on the number of persona statements, EU for original characters fine-tunes for 20 epochs, while for famous figures fine-tunes for 5 epochs. DPO fine-tunes for 10 epochs for all characters. \n\nFine-tuning DeBERTa is applied for discriminators and RAG retrievers. Different fine-tuning procedures for DeBERTa also share the same set of hyperparameters. The DeBERTa discriminators are fully fine-tuned with AdamW as the optimizer, learning rate initialized as 1 \u00d7 10 \u22125 . The statement-query relevance discriminator is fine-tuned for 5 epochs and the statement-to-response NLI discriminator is fine-tuned for 10 epochs. \n\nPreference Assignment We sample two responses from a PRP agent with temperature 1.0, the sample with a higher APC score is assigned as the preferred one when the difference is larger than a threshold for filtering, which is set to 0.2 in our implementation. We build 100 preference pairs before the filtering for APC-based DPO.",
            "score": 0.1483045920732232,
            "section_title": "D More PRP Method Implementation Details",
            "char_start_offset": 34358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1236
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 223,
                    "matchedPaperCorpusId": "53592270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1658935546875
        },
        {
            "corpus_id": "276557710",
            "title": "Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation",
            "text": "We selected RadFM (Wu et al. 2023), an existing multiimage-trained Med-MLLM, as our base model to evaluate the effectiveness of V-RAG and our proposed fine-tuning tasks on multi-image-trained models. To assess the capability of making single-image-trained MLLMs V-RAG capable, we utilized LLaVA (Liu et al. 2023b) as the backbone model. We employed LoRA (Hu et al. 2021) to fine-tune both LLaVA and RadFM on our designed tasks, applying a learning rate of 5e-5 for all fine-tuning tasks.",
            "score": 0.1481417799485148,
            "section_title": "Experimental Setups",
            "char_start_offset": 13372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 487
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1851806640625
        },
        {
            "corpus_id": "271843111",
            "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
            "text": "There is an assumed bias of such similarity metrics for shorter answers (see below our analysis and comparison on relation between the different metrics). One curious observation after filtering was that RAG and prompt engineering didn't show an advantage for the fine-tuned version. This might be due to the training dataset to which the model fitted its interactions which didn't include RAG. When considering fine-tuning, we compare the different versions of the LLaMA-2 model (all with 13 billion parameters), i.e., shown in orange and the fine-tuned version in green. Fine-tuning improved in all cases the results considerably, in particular when filtering the results afterwards. Last, when comparing the results of the fine-tuned model with GPT-3.5 (which is much larger with 175 billion parameters), we observed that a fine-tuned LLaMA-2 (with filtering) without additional prompting techniques performed considerably better as GPT-3.5. Adding RAG and prompting techniques (see above) improved GPT-3.5 by a huge margin and this model setup was the best we found in evaluation. But the margin towards fine-tuned LLaMA was slim and our fine-tuning training data appeared to have a detrimental effect to RAG which should be further analyzed (see note above). \n\nFor the METEOR score most models perform almost equally good (not shown in detail). Except for the base model with raw data and question only, which performs slightly worse. In BERTScore, we, first, observe as a similar pattern that applying RAG and using a specific prompt dramatically improves the measured similarity (Fig. 4, comparing inside the specific models-LLaMA-2 respectively GPT-3.5-the versions with RAG and without RAG). Secondly, again similar reflecting the results from above, fine-tuning (showing in green) introduces a strong advantage compared to the original model (shown in orange). Again, integrating RAG after fine-tuning appears problematic. What is noteworthy from the BERTScore evaluation is that numbers appear overall more pronounced.",
            "score": 0.14807129619051554,
            "section_title": "Comparison of Different LLM Extension",
            "char_start_offset": 30971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09136962890625
        },
        {
            "corpus_id": "253948124",
            "title": "Toward Robust Facial Authentication for Low-Power Edge-AI Consumer Devices",
            "text": "In this section the selected FR model is fine-tuned, using a similar approach to [80] with samples augmented with directional lighting.",
            "score": 0.14774563551708014,
            "section_title": "VIII. FINE-TUNING THE ArcFace",
            "char_start_offset": 49763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 85,
                    "matchedPaperCorpusId": "67855588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006145477294921875
        },
        {
            "corpus_id": "264514963",
            "title": "Exploring early vocal music and its lute arrangements: Using F-TEMPO as a musicological tool",
            "text": "So, as with OMR-derived queries, our codestrings are potentially highly subject to error, and we cannot expect perfect results. \n\nThe second problem, which is a general issue with arrangements of this period, is that when setting a vocal item for performance on the lute or a keyboard instrument, it was normal to add extra notes in the form of elaborations of the melody or other voice parts. This 'diminution' technique was an expected aspect of the virtuoso performer's skill, and only rarely (usually in the context of a didactic work) are 'plain', unadorned versions encountered. In the same way as described above, a single extra note in a melodic line introduces two errors in an interval representation; for this reason it is desirable to attempt to reduce the ornamented surface as near as possible to a 'vocal' melody which might correspond to the music printed in the vocal cantus or superius. \n\nSince the encoded lute intabulations we work with are reliably accurate in terms of rhythm, we can calculate the time at which each chord is played. In order to mitigate the effects of both the uncertain voicing in lute-tablature derived queries, and the interference from ornamental notes, we extract our diatonic pitch-interval strings from the top note of successive chords which occur at a regular time-interval corresponding to one of the predominant note values in the repertory of vocal models. Here we use time-intervals equivalent to fusa (eighth note), semiminima (quarter) and minima (half) durations, and use the highest notes sounding at these times to extract three different codestrings for each piece as queries. \n\nFor the trials we report here, we used a freely-accessible online resource maintained by Sarge Gerbode containing around 20,000 tablature encodings from all periods, including several arrangements of modern popular music and basso continuo accompaniments. 13 For the most part, these are manually created from original sources, often dealing with the entire contents of a source (printed or manuscript); in this respect, this is an extremely rich resource of performing editions of music from the renaissance period which has been under-exploited for its musicological potential.",
            "score": 0.1477384980768826,
            "section_title": "Queries from Lute Tablatures",
            "char_start_offset": 14496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2217
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0010480880737304688
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "Step@15 These findings suggest that when fine-tuning a retriever for RAG, one needs to be careful on the dataset make-up. In real-world settings, there typically is a large data imbalance that needs to be handled according to the domain.",
            "score": 0.14770199879409363,
            "section_title": "Setup",
            "char_start_offset": 16664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304443359375
        },
        {
            "corpus_id": "273877635",
            "title": "LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation Combining Hierarchical Agents and RAG",
            "text": "Table 3 and Table 4 show all the key parameter Settings used in this experiment. These RAG parameters: RAG technology combines two aspects, Retrieval and Generation, to enhance the ability of models to deal with complex problems. In this study, the parameters chunk_size=300 and chunk_overlap=50 was used. In the specific field of maintenance, the use of LLMs requires precise fine-tuning to ensure they can meet the specific needs and knowledge of users in the field. The key to effective fine-tuning is utilizing special domain datasets, which make the model more accurate and relevant when dealing with maintenance-related expertise.",
            "score": 0.14766877443173948,
            "section_title": "Experimental Settings",
            "char_start_offset": 41818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 636
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1287841796875
        },
        {
            "corpus_id": "267027552",
            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
            "text": "The examples in Table 21 show that these models do not consistently provide a full answer to the question. For example, some responses note soil loss but not air quality as a concern (or vice versa). We therefore also prompted GPT-4 to evaluate the answers on whether they were fully correct. Similar to the valuation with a guideline, we see that GPT-4 and LLama-2-chat 13B outperform Vicuna. With the addition of RAG, we see a large increase in the percent of answers that are fully correct for all models. As with succinctness, we observe that RAG focuses responses on the most relevant subset of information. We see the biggest improvement for Vicuna with the addition of RAG, going from 28% to 56% fully correct. Fine-tuned GPT-4 outperforms the rest of the models with and without RAG, with 45% and 61% of responses fully correct, respectively. \n\nIn conclusion, in this example, the best performing models in terms of accurately and succinctly addressing the reference answer are Vicuna + RAG, GPT-4 + RAG, GPT-4 fine-tuned, and GPT-4 fine-tuned + RAG. These models provide a balanced mix of precision, conciseness, and depth of information.",
            "score": 0.14744319773462902,
            "section_title": "Correctness",
            "char_start_offset": 83844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "271902833",
            "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning",
            "text": "To the end, we summarize our contribution as follows: \n\n\u2022 We evaluate the existing solutions for harmful fine-tuning. We show that existing solutions are highly sensitive to the training hyper-parameters in the fine-tuning stage, which we name hyper-parameter sensitive issue.",
            "score": 0.1473146583946136,
            "section_title": "Introduction",
            "char_start_offset": 3814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 276
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0264129638671875
        },
        {
            "corpus_id": "275758365",
            "title": "Development of Application-Specific Large Language Models to Facilitate Research Ethics Review",
            "text": "LLMs achieve their general-purpose capabilities due to the vast and diverse datasets used in their initial training. However, when high performance is desired within a specific domain rather than for general tasks, the effectiveness of LLMs can be significantly enhanced through a process known as fine-tuning. Finetuning involves further training an already pre-trained LLM on a targeted, specialized dataset relevant to the specific area of interest. This process refines the model's understanding by adjusting its internal weights to align more closely with the patterns, terminology, reasoning, knowledge base, and nuances of the specialized data. As a result, fine-tuned AI models become more adept at generating accurate and contextually relevant outputs within their application-specific domains, typically surpassing the performance of general-purpose models in tasks such as medical diagnosis, legal analysis, and other fieldspecific applications. 26,27 plication-specific fine-tuning helps reduce hallucinations in LLMs by aligning the model's outputs more closely with domain-specific knowledge. This can be further enhanced through RAG: a strategy that grounds the model's outputs in external, reliable data sources (i.e., domain-specific reference information). 28 RAG works by using embeddings, which are numerical representations that encode text into structured vectorized forms, allowing the model to perform efficient computational searches across vast datasets. This structured approach enables the model to locate and retrieve specific, contextually relevant information during text generation, acting as an external memory or knowledge base. By grounding the model's responses in actual data, RAG helps ensure that outputs are more accurate and reliable. Practical examples of RAG include company AIs that access internal documents to provide precise answers to user queries, thereby reducing the risk of generating incorrect or fabricated information. 29  use the term 'application-specific LLMs' to refer to LLMs which have been 1) pre-trained, 2) fine-tuned for a specific application domain (i.e., IRB reviews), 3) enhanced with domain-specific information using RAG, and 4) customized for a specific application through prompt-engineering. The use of applicationspecific LLMs has the potential to address several of the specific concerns raised above with respect to the use of general LLMs in IRB review.",
            "score": 0.14714901892229715,
            "section_title": "IV. Application-Specific IRB LLMs",
            "char_start_offset": 15477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2265
                },
                {
                    "start": 2266,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 960,
                    "end": 962,
                    "matchedPaperCorpusId": "268458631"
                },
                {
                    "start": 1275,
                    "end": 1277,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4169921875
        },
        {
            "corpus_id": "270688725",
            "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
            "text": "Table 5: The tables show the QA results on the NQ test dataset (left) and Hotpot-QA dev set (right). We compare the results with three groups of baselines: closed-book, which involves directly prompting state-ofthe-art LLMs with 16-shot in-context examples; fully-supervised RAG, where the RAG framework is used and the model is fully supervised and trained on the training data; and No Fine-tuning RAG, which employs the RAG framework without any tuning. \n\nthe standard exact match rate definition to more fairly evaluate LongRAG's performance. More details can be found in Section A.2. \n\nFor NQ and HotpotQA, we compare our model with several groups of strong previous models as baselines. \n\nThe first group is \"Closed-Book\": These baselines mean that no retrieval component is used; instead, stateof-the-art LLMs are employed to directly obtain the final result. We evaluate our results on Gemini-1.5-pro (Reid et al., 2024), Claude-3-Opus (Anthropic, 2024) and GPT-4-Turbo (Achiam et al., 2023). All models are evaluated on 16-shot in-context learning with direct prompting; The second group is \"Fully-supervised RAG\", and these baselines involve full-supervised fine-tuning on the training dataset. The third group is \"No Fine-tuning RAG\", and these baselines doesn't involve any supervised fine-tuning. The QA results on NQ and HotpotQA are presented in Table 5. On the NQ dataset, LongRAG achieves a 62.7 exact match rate, which is on par of the strongest fine-tuned RAG model like Atlas. On the HotpotQA dataset, LongRAG achieves a 64.3 exact match rate, which is also close to the SoTA fully-supervised RAG frameworks.",
            "score": 0.14713176020020544,
            "section_title": "64.3",
            "char_start_offset": 23486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1627
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312255859375
        },
        {
            "corpus_id": "271097781",
            "title": "Let Network Decide What to Learn: Symbolic Music Understanding Model Based on Large-scale Adversarial Pre-training",
            "text": "[MASK] tokens during fine-tuning to align pre-training and finetuning phases. This enhances model robustness and performance on downstream tasks by maintaining contextual inference capabilities, especially with limited music data. \u2022 Experimental results demonstrate that our method achieves excellent performance across four music understanding tasks, including composer classification, emotion classification, velocity prediction, and melody extraction. Ablation studies confirm the effectiveness of our adversarial and fine-tuning mechanisms, highlighting the model's versatility for music analysis.",
            "score": 0.14674474133416845,
            "section_title": "Introduction",
            "char_start_offset": 4442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 601
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005535125732421875
        },
        {
            "corpus_id": "271039137",
            "title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
            "text": "Fine-tuning methods. We compared our proposed Redundancy-Aware Tuning (RAT) with two other fine-tuning methods: full-parameter fine-tuning (FT) and Low-Rank Adaptation (LoRA), using the C 3 bench. The results are summarized in Table 5. The results show that the vanilla FT method performs well in learning new tasks but suffers from catastrophic forgetting. The LoRA method mitigates catastrophic forgetting to some extent but struggles to adapt effectively to new tasks. In contrast, our proposed RAT method outperforms both FT and LoRA in terms of mitigating catastrophic forgetting and learning new tasks effectively. Abla-tion results on a wider range of tasks can be found in Appendix E, Table 12. \n\nRAT specifications. We investigate the impact of different values of N on the model performance used in the RAT method, as shown in Table 6. Setting N to 8 provides the best performance, hence we adopt it as the default strategy. \n\nThe effectiveness of CCU-RAG. We evaluated the impact of our proposed CCU-RAG method by comparing TongGu with and without CCU-RAG on the C 3 bench dataset, with results summarized in Table 4. The results reveal that the CCU-RAG method significantly improves TongGu's performance on the knowledge-intensive task of source retrieval, without diminishing the performance of Non-knowledge-intensive tasks such as punctuation and named entity recognition. Results on a wider range of tasks are given in Appendix E, Table 13.",
            "score": 0.14670186592887607,
            "section_title": "Ablation Study",
            "char_start_offset": 19754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1456
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "276557710",
            "title": "Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation",
            "text": "We first evaluate V-RAG's performance for existing Med-MLLM that originally trained on multi-image datasets. Table 1 shows entity probing results comparing our method to baselines. Across both datasets, V-RAG outperforms textonly RAG baselines in F1 scores. This improves the model's ability to extract relevant information for decision-making. Furthermore, with our proposed fine-tuning tasks, V-RAG (fine-tuned) achieves superior F1 scores over both baselines and the un-fine-tuned version. This shows that we have significantly enhanced Med-MLLM's capabilities by equipping it with robust image-text association skills.",
            "score": 0.1465081047446092,
            "section_title": "Overall performance for existing multi-image-trained Med-MLLMs",
            "char_start_offset": 18193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 622
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298095703125
        },
        {
            "corpus_id": "264951369",
            "title": "Convolutional neural network in rice disease recognition: accuracy, speed and lightweight",
            "text": "Schematic diagram of the fine-tuning algorithm. (Reprinted from ref. (Feng L. et al., 2021) under the terms of the Creative Commons CC-BY license).",
            "score": 0.1463679396088611,
            "section_title": "FIGURE 12",
            "char_start_offset": 35810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 147
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 91,
                    "matchedPaperCorpusId": "238206989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0027790069580078125
        },
        {
            "corpus_id": "208334247",
            "title": "MIDI-Sheet Music Alignment Using Bootleg Score Synthesis",
            "text": "For fine-tuning, we manually annotated the location and type of approximately 2200 noteheads in 30 different pages of piano music downloaded from IMSLP.2 These 30 pages of music were selected to maximize diversity across composers and music publishers, and they are a completely separate set from the data used to evaluate alignment. Because we only care about detecting noteheads, we disregard all other musical objects The number below each layer indicates the number of feature maps. In the downsampling and upsampling stages, the length and width of the feature maps change by a factor of two in each successive layer. We train on the DeepScores dataset [30] and fine-tune on a small set of manually labeled noteheads in real scanned music. \n\nin the fine-tuning process. Because the real scanned music contains a variety of font sizes, we scale each input image to match the staff line spacing in the DeepScores data. After fine-tuning, the notehead detector achieves a training mean average precision (mAP) of 0.4201 for all notehead types (black notehead, half notehead, and whole notehead). For reference, in normal object detection tasks (not tiny objects), the state-of-the-art mAP is around 0.4 to 0.6. 3  Figure 5 (top half) shows an example of the notehead detector predictions on a section of Brahms Intermezzo Op. 117 No. 2.",
            "score": 0.14604073592711142,
            "section_title": "Notehead Detection",
            "char_start_offset": 8075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1338
                }
            ],
            "ref_mentions": [
                {
                    "start": 658,
                    "end": 662,
                    "matchedPaperCorpusId": "4553586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004329681396484375
        },
        {
            "corpus_id": "12860220",
            "title": "Maximum a posteriori estimation of piecewise arcs in tempo time-series",
            "text": "1. Mean deviance from the barlines of the arc endpoints inferred for each performance, averaged over the short-timescale arcs in each case. \n\nNevertheless, the longer time scale analysis (centred on four bars) highlights differences between the performances: Pollini's performance appears to contain relatively little variation on this level, as the fit yields long and shallow arcs, with breakpoints near positions 48, 96 and 168 (structurally important positions; 96 is where the key-change occurs). On the other hand, both Barenboim and Schnabel's tempo curves exhibit fairly deep and varied arcs. Schnabel's performance exhibits the most dramatic variation in the first four bars until around measure 48: this first four-bar section corresponds to the opening statement of the basic progression, before the melody enters in the fifth bar (and the underlying progression repeats). Bamberger described Schnabel as performing them \"as if in one long breath\" (quoted in Chew (2012)), not quite reflected in our automatic analysis. \n\nOn the shorter time scale, the analysis tends to group phrases into one-bar or two-bar arcs. Aspects of the musical structure are reflected in the arcs observed. Sections of the melody which lend themselves to two-bar phrasing (e.g. 72-96) are generally reflected in longer arcs crossing bar lines. Conversely, in the region 96-132 the change to the new key unfolds as each new chord enters at the start of a bar, and the tempo curves for all three performers reflect an expressive focus on this feature, with one-bar arcs which are more closely locked to the bar-lines than elsewhere. Note that in this section Schnabel matches Pollini in exhibiting a long and shallow arc on the slow timescale, with all the expressive variation concentrated on the one-bar arcs. \n\nOver the excerpt generally, the breakpoints for Schnabel are further away from the barline than the others, as was observed in Chew's manual analysis. We can quantify this by measuring the mean deviances of arc endpoints from the barlines in each performance. The resulting mean deviances confirm our observations (Table 1).",
            "score": 0.1459573391052383,
            "section_title": "Analysis of Expressive Piano Performance",
            "char_start_offset": 18227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2124
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001949310302734375
        },
        {
            "corpus_id": "229278341",
            "title": "A novel dataset for the identification of computer generated melodies in the CSMT challenge",
            "text": "The paper uses pretty midi 5 to convert the generated piano roll into a MIDI file which requires the MIDI number and the duration of each note. The MIDI number can be directly indexed by the note. The duration of each beat requires a simple calculation. As each column in the binarised piano roll represents a quarter beat, given a tempo value, such as 120 beats per minute (bpm), the duration of each column in the binarised piano roll can be easily calculated. \n\nThe instrument selected in the MIDI file is \"Bright Piano\" with the velocity setting to 127 in MIDI files. The tempi of the MIDI files are randomly selected in the range of 68 bpm, 78 bpm, 88 bpm, 98 bpm, 108 bpm and 118 bpm to avoid the situation where the columns occupied by an individual note would always be the same integer.",
            "score": 0.1459573391052383,
            "section_title": "Data Representation",
            "char_start_offset": 14502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 795
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0006070137023925781
        },
        {
            "corpus_id": "277113527",
            "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
            "text": "2.1 EMBEDDER FINE-TUNING Fine-tuning, the process of adapting a pre-trained model to domain-specific tasks using typically smaller datasets, has been widely explored across various applications. While embedding models exhibit strong zero-shot performance on general benchmarks such as MTEB (Muennighoff et al., 2023;Zhang et al., 2017), recent studies have demonstrated that even modestly sized models can benefit substantially from fine-tuning when applied to domain-specific tasks. For instance, finetuning embedders on specialized datasets has led to notable improvements in areas such as medical question answering Sengupta et al. (2024) and financial question answering Anderson et al. (2024). In the finance domain, prior research on embedders has underscored several inherent challenges: domain-specific vocabulary and semantic patterns, the complexity of multi-hop queries, and multimodal data (e.g. text, tables, and time-series) Tang & Yang (2024); Kim et al. (2024); Xie et al. (2024). These challenges necessitate tailored fine-tuning strategies that can effectively capture the nuanced information contained in financial documents. Within the framework of Retrieval-Augmented Generation (RAG), embedding models are primarily tasked with Information Retrieval (IR), where the semantic similarity between a query and a corpus is assessed and ranked. A prevalent strategy for enhancing this process is contrastive learning or contrastive fine-tuning-which relies on constructing triplets (query, relevant corpus, irrelevant corpus) to form positive and negative training pairs Karpukhin et al. (2020). Despite the effectiveness of contrastive learning in embedders Lu et al. (2024), there remains a notable gap in the literature regarding the impact of embedder fine-tuning on RAG systems, particularly within the finance domain Setty et al. (2024). By addressing this gap, our work aims to explore and quantify the benefits of embedder fine-tuning in RAG applications, thereby contributing to the broader understanding of domain-adaptive IR.",
            "score": 0.14586653288160292,
            "section_title": "RELATED WORK",
            "char_start_offset": 2115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 316,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 316,
                    "end": 335,
                    "matchedPaperCorpusId": "206595438"
                },
                {
                    "start": 619,
                    "end": 641,
                    "matchedPaperCorpusId": "266999115"
                },
                {
                    "start": 675,
                    "end": 697,
                    "matchedPaperCorpusId": "273901204"
                },
                {
                    "start": 1587,
                    "end": 1610,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6181640625
        },
        {
            "corpus_id": "277940163",
            "title": "Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG Come in Handy?",
            "text": "This study explored the viability of LLMs in detecting malicious software packages, using multiple experimental setups, including zero-shot classification, RAG, CRAG, and fine-tuning. The results indicate that fine-tuning is the most effective approach for detecting malware using LLMs than using RAG, leading to more robust classification results. The experiments highlighted the need for further research in curating credible and structured malicious code knowledge bases that can enhance RAG-based methods. While fine-tuning significantly improves model performance, integrating RAG with optimized knowledge bases such as YARA Rules and GitHub advisories can further refine classification accuracy. Overall, our study demonstrates the effectiveness of fine-tuned and retrieval-augmented LLMs for malicious code detection in the PyPI ecosystem.",
            "score": 0.14586369528798082,
            "section_title": "CONCLUSION",
            "char_start_offset": 25984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 846
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2099609375
        },
        {
            "corpus_id": "268819923",
            "title": "ARAGOG: Advanced RAG Output Grading",
            "text": "\u2022 Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs.\n\n\u2022 Experiment replication across diverse datasets: To ensure the robustness and generalizability of our findings, it is imperative for future research to replicate our experiments using a variety of datasets.Conducting these experiments across multiple datasets is important to verify the applicability of our results and to identify any context-specific adjustments needed.\n\n\u2022 Auto-RAG: The idea of automatically optimizing RAG systems, akin to Auto-ML's approach in traditional machine learning, presents a significant opportunity for future exploration.Currently, selecting the optimal configuration of RAG components -e.g., chunking strategies, window sizes, and parameters within rerankers -relies on manual experimentation and intuition.An automated system could systematically explore a vast space of RAG configurations and select the very best model (Markr.AI, 2024).",
            "score": 0.14557398015919568,
            "section_title": "Conclusion",
            "char_start_offset": 24731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 505
                },
                {
                    "start": 507,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 880
                },
                {
                    "start": 882,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58154296875
        },
        {
            "corpus_id": "276580741",
            "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
            "text": "Modular RAG systems (Gao et al., 2024;Xu et al., 2024c) focus on refining external knowledge through different modules implemented by LLMs, which have become a key trend in the RAG area. For instance, Self-RAG (Asai et al., 2024a) uses different tags for adaptive retrieval (Jiang et al., 2023) and self-reflection to refine knowledge. Some approaches also focus on reformulating queries to identify more useful documents for answering ques-tions (Yan et al., 2024;Trivedi et al., 2023). Yan et al. (2024) introduce a retrieval evaluator that acts as a judge to trigger query reformulation, search, and knowledge refinement actions to supply more accurate evidence for generation. \n\nTo further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023;Shi et al., 2024;Izacard and Grave, 2021). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2024) to jointly optimize the modules in a RAG system, aligning their data preferences.",
            "score": 0.14550788887975255,
            "section_title": "Related Work",
            "char_start_offset": 5892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1405
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 230,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 274,
                    "end": 294,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 465,
                    "end": 486,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 979,
                    "end": 996,
                    "matchedPaperCorpusId": "258960666"
                },
                {
                    "start": 996,
                    "end": 1013,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1013,
                    "end": 1037,
                    "matchedPaperCorpusId": "227746078"
                },
                {
                    "start": 1300,
                    "end": 1323,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "251594672",
            "title": "CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks",
            "text": "In real-world practice, it is often time-consuming and difficult to collect a large number of relevance labels to train or fine-tune a retrieval model for various KILT tasks. To answer RQ4, we simulate the low-resource retrieval setting for five datasets, i.e., FEV, AY2, zsRE, TQA and WoW, spanning five varied classes of KILT tasks. Following [14], we randomly select different numbers of instances from the original training set for fine-tuning CorpusBrain. Specifically, we randomly pick 20, 40, 60, 80 and 100 instances for the FEV, zsRE, TQA and WoW dataset, and pick 200, 400, 600, 800 and 1000 instances for AY2. We fine-tune CorpusBrain and BART on each downstream dataset, with batches of 4096 tokens, learning rate as 3e-5, and pick the last checkpoint to evaluate the performance on the original dev set. \n\nAs shown in Figure 3, we can observe that: (1) CorpusBrain outperforms BART on all the five datasets by fine-tuning on the same limited supervised data, demonstrating that CorpusBrain is able to encode the relevance information about a given corpus through the pre-training. Furthermore, CorpusBrain achieves much better zero-shot performance than BART. (2) For the five datasets, Corpus-Brain fine-tuned on limited supervised data can achieve competitive results with RAG fine-tuned on the full supervised datasets. For example, CorpusBrain fine-tuned with only 20 examples has outperformed RAG on TQA and WoW datasets. The results demonstrate that by fine-tuning with small numbers of supervised pairs, Cor-pusBrain is able to adapt to the target task quickly. (3) Under the zero resource setting, for example, CorpusBrain can outperform RAG significantly for the FEV (71.69% vs. 63.50%) and zsRS dataset (90.95% vs. 65.36%). (4) CorpusBrain also beats previous state-ofthe-art baseline, i.e., GENRE, with limited fine-tuning examples.",
            "score": 0.14543925874831048,
            "section_title": "Zero-and Low-Resource Settings",
            "char_start_offset": 34359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "233296292"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06463623046875
        },
        {
            "corpus_id": "273323545",
            "title": "oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness",
            "text": "In the rapidly evolving field of LLMs, the challenge of optimizing performance to meet specific needs is a key focus. While out-of-the-box LLMs offer impressive capabilities, techniques like fine-tuning and RAG present promising avenues for further enhancing their accuracy and relevance. \n\nThe primary challenges in fine-tuning LLMs stem from various factors including the need for extensive retraining datasets, particularly for complex fields like healthcare; and technical hurdles such as limitations in context tokens and the computational demands typically quantified in petaflops for GPU memory 10 . \n\nRetrieval Augmented Generation (RAG) is an innovative approach for tailoring LLMs to specific tasks, and a scalable solution agnostic to various LLM-based healthcare applications. It offers an easier solution without the need for extensive training examples or time as required by fine-tuning, and accessibility to updated customized knowledge without significant time in creating up-to-date ground truth and retraining required by fine-tuning. Unlike traditional LLMs, RAG functions similarly to a search engine, retrieving relevant, customized text data in response to queries. This capability effectively turns RAG into a tool that integrates specialized knowledge into LLMs, enhancing their baseline capabilities. In healthcare, for instance, LLMs equipped with RAG and embedded with extensive clinical guidelines (LLM-RAG) can yield more accurate outputs 11 . Currently, two primary open-source frameworks for RAG exist -LangChain 12 and Llamaindex 13 . Although the retrieval process of RAG can be technically challenging, RAG's utility in contexts with smaller, more focused knowledge corpora remains significant. \n\nThis study aims to develop and evaluate an LLM-RAG pipeline for preoperative medicine using various LLMs and guidelines. The primary objective is to assess the pipeline's accuracy in determining patients' fitness for surgery. The secondary objective is to evaluate the LLM-RAG's ability to provide accurate, consistent and safe preoperative instructions, including if the patient should be seen by a nurse or doctor, fasting guidelines, medication management, and optimization strategies.",
            "score": 0.14402764912423527,
            "section_title": "Optimization of LLMs with Retrieval Augmented Generation (RAG)",
            "char_start_offset": 1207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 602,
                    "end": 604,
                    "matchedPaperCorpusId": "219087919"
                },
                {
                    "start": 1469,
                    "end": 1471,
                    "matchedPaperCorpusId": "258740478"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321044921875
        },
        {
            "corpus_id": "273229491",
            "title": "Studying Large Language Model Behaviors Under Context-Memory Conflicts With Real Documents",
            "text": "We compare the fine-tuned model with the original prompted version used in the main experiments. \n\nFirst, we report our observations on knowledge-updating behaviors and the influence of an incorrect parametric answer naturally appearing in the context in Table 15. As expected, fine-tuning improves open-book accuracy across all studied datasets. We find that fine-tuning reduces the chance of the model retaining its original incorrect answer. The fine-tuned version retains the original answer in only 1.2% of cases on average across datasets, compared to 1.6% for the prompted model. Regarding the parametric bias, we find that when an incorrect parametric answer naturally appears in context -its influence on the failure likelihood is smaller for the fine-tuned model across all datasets. \n\nAdditionally, we run an intervention study, to compare how the fine-tuned model responds to artificially adding the incorrect parametric answers to context. We follow the setup in \u00a7 4.4.2. \n\nThe results are reported in Table 16. The fine-tuned model is still susceptible to adding the incorrect parametric answer in context, however, to a lesser extent than the prompted version. For the original model, the likelihood of retaining the incorrect answer increases by 7.9% on average across all datasets. For the fine-tuned model the average increase only reaches 4.3%. \n\nDiscussion Fine-tuning reduces the parametric bias in our experiments. However, in the examined setup fine-tuning was not enough to completely eradicate it. Overall, our results suggest that fine-tuning is the most promising approach to combat the parametric bias in RAG systems. However, it comes at the cost of potentially losing the generality of an instruction-tuned LLM. 9.1 (-2.9) 68.0 (+0.9) SearchQA 3.3 (-2.1) 72.0 (-0.2) HotpotQA 0.7 (-0.6) 87.5 (-0.5)",
            "score": 0.14400781539508306,
            "section_title": "Results",
            "char_start_offset": 40878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1828
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309814453125
        },
        {
            "corpus_id": "278394304",
            "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes",
            "text": "Although LLMs have been very popular lately, domain-specific tasks are still an issue for them [27]. The major problem is that models generate wrong answers when they are queried outside of their training data on occasions, a condition called hallucination [28]. RAG can withstand such drastic issues caused by hallucination and is widely utilized to enhance the quality and efficiency of generated content nowadays [29]. RAG makes use of the retrieval methods and generation capabilities of LLMs to produce contextually full and relevant responses to the questions asked from external knowledge databases [30]. In a RAG framework, retrievers play a critical role by extracting the relevant context from a large document in response to a query. This retrieved context is then passed to the LLM, which generates the final answer. The effectiveness of a RAG system relies heavily on the quality of the retrieval process. \n\nFine-tuning is another technique for adapting pre-trained LLMs to new tasks and reducing hallucinations by continuing training on new data [31]. Fine-tuning involves training pre-trained models on task-specific datasets, which helps in making the model capable enough to perform better on the desired task. It has been found that fine-tuning the LLM in an RAG system can help improve its performance, as the LLM is responsible for text generation utilizing the retrieved information [32]. However, full fine-tuning, which initializes the model with pre-trained weights and updates all of them, becomes impractical when dealing with models having billions of parameters [33]. Great efforts are put into avoiding training all parameters of the model and promoting an efficient fine-tuning process [34]. There are several techniques that are utilized to fine-tune models efficiently and can resolve the issue of heavy computational dependency that arises due to full fine-tuning. One of the most popular techniques for fine-tuning language models is parameter-efficient fine-tuning (PEFT), which efficiently adapts language models to specific downstream tasks while minimizing the constraints of dependency on computational resources and storage requirements.",
            "score": 0.14397895780695333,
            "section_title": "Introduction",
            "char_start_offset": 4752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "268033066"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "257316425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "235417313",
            "title": "FedBABU: Towards Enhanced Representation for Federated Image Classification",
            "text": "perlist.append(Accper) 10: end for 11: return initlist, perlist 12: function FINE-TUNE(alg, \u03b8i, \u03c4 f ) 13: \n\n14: \n\nfor each fine-tune epoch 1,",
            "score": 0.14389982223237047,
            "section_title": "9:",
            "char_start_offset": 25391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 108,
                    "end": 111
                },
                {
                    "start": 114,
                    "end": 141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0028553009033203125
        },
        {
            "corpus_id": "270870780",
            "title": "Large Language Models Struggle in Token-Level Clinical Named Entity Recognition",
            "text": "On the other hand, while RAG shows some promise, its overall impact on enhancing model performance in this task is limited. The data shows that RAG particularly benefits the identification of Rare Disease and Skin Rare Disease across all three models. This improvement could be attributed to RAG's ability to leverage external knowledge bases, i.e., NORD articles about rare diseases, which might be rich in rare disease terminology. However, for more general and common entities like Disease, Symptom and Sign, RAG's advantages are less impressive. This highlights the need for further refinement of RAG methods in clinical NLP applications. \n\nBesides zero-shot prompting and in-context learning, we also investigate the impact of instruction-fine-tuning on these LLMs in this task. As shown in Table 2 and Table 3, despite the performance gain via few-shot learning, the overall performance of prompting LLMs is far from satisfactory. In this experiment, we aim to investigate LLMs' performance capabilities and limitations compared to fine-tuned BERT-like models under data-rich conditions, which is the common solution to NER in current practice. The results are shown in Figure 3. Essentially, Figure 3 visualizes the F1 scores of various models, including BioClinicalBERT fine-tune, Llama2-MedTuned under different learning strategies (zero-shot, few-shot, fine-tune), and ChatGPT-4 in zero-shot and few-shot scenarios. Specifically, the performance of Llama2-MedTuned models demonstrates significant effectiveness, not only outperforming ChatGPT-4 in most scenarios but also closely rivaling BioClinicalBERT when fine-tuned. This highlights the potential of open-source LLMs in this task. \n\nError Analysis We present an error analysis to further understand the mistakes made by the LLMs in this task. Essentially, we randomly select 50 sentences from the test dataset and manually categorize the errors for each incorrect prediction. We investigate the two best-performing models, i.e., ChatGPT-4 under few-shot learning and Llama2-MedTuned under fine-tuning.",
            "score": 0.14383569854327044,
            "section_title": "Disease",
            "char_start_offset": 17103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.171630859375
        },
        {
            "corpus_id": "269696779",
            "title": "A Reliable and Accessible Caregiving Language Model (CaLM) to Support Tools for Caregivers: Development and Evaluation Study",
            "text": "Multimedia Appendix 1 provides an example of the fine-tuning data set. The goal of the fine-tuning process is to adapt the pretrained FM to the caregiving field so that it can respond more accurately and with more contextual relevance to questions related to caregiving. Once all the components of the CaLM were developed and trained, the model was evaluated using the 66 pairs of questions that were not included in the training. \n\nWe evaluated the performances of the models using the test set consisting of 66 Q-A pairs. The evaluation was conducted after the components of the RAG framework were developed and the small FMs were fine-tuned by retraining the models on the training set of 415 Q-A pairs. The training set (comprising 415 entries) and the test set (comprising 66 entries) were distinct with no overlapping questions or content. After exposing the FMs to the questions from the test set, we checked the FMs' answers against the test set answers to determine whether the FMs were outputting accurate answers to the test questions. The performance of the FMs was measured by the similarities between the reference answers to the test set questions and the output generated by the models. In addition to similarities, we evaluated the capabilities of the models in providing references to the answers. \n\nWe tested the small FMs trained in 3 different settings: vanilla, RAG, and RAG + fine-tuned. Vanilla is the original FM without adding the caregiving knowledge base. In the RAG setting, for every question in the data, additional context from the knowledge base was added before the Q-A pair was sent to the FM. In the RAG + fine-tuned setting, we followed the same procedure as with the RAG setting, except that the FM had already been retrained using the 415 Q-A training set pairs. We compared the small FMs with OpenAI GPT 3.5 as a baseline benchmark. Because GPT 3.5 is a proprietary commercial system, we were not able to control its fine-tuning variables other than providing examples. Therefore, we cannot compare it with other FMs that were retrained using the caregiving Q-A training set.",
            "score": 0.1437572641411989,
            "section_title": "Data Sets for Fine-Tuning the FM and for Model Evaluation",
            "char_start_offset": 18797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2115
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.237060546875
        },
        {
            "corpus_id": "272524643",
            "title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation",
            "text": "The RAG module has achieved a P@3 result of 0.82, and all records have at least one correct retrieval out of the top three results, demonstrating the retrieval effectiveness of the popular RAG method. \n\nTo further investigate the impact of the RAG module on DB Q&A, we conduct an ablation study on the RAG module. Specifically, based on the experiments described in Section 6.2, we conduct experiments for the \"product-specific\" sub-dataset without the external knowledge provided by the RAG module. The WinRate results are shown in Figure 6. \n\nFrom the experimental results, we can observe the following: (1) The RAG module significantly enhances the performance of generalpurpose LLMs of any model size, consistent with the analysis in Section 6.2. (2) The RAG module can boost the performance of smallsized models several times (e.g., 2.4x for Llama2), demonstrating its substantial value for edge deployment models. (3) The RAG module offers smaller performance improvements for models like Baichuan2-sft fine-tuned with domain knowledge in databases. This indicates a high degree of overlap between the improvements from model fine-tuning and those from the RAG module, suggesting that deploying either one is sufficient within a limited budget.",
            "score": 0.14345862421191163,
            "section_title": "Retrieval Augment Generation Ablation",
            "char_start_offset": 49433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1250
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0309600830078125
        },
        {
            "corpus_id": "276575077",
            "title": "Mitigating Bias in RAG: Controlling the Embedder",
            "text": "Given the complexity of bias conflict in a RAG system, is it feasible to mitigate bias in the entire RAG system? In this section, we try to control the embedder to mitigate bias. In \u00a74.1 we first fine-tune several embedders to span a wide bias range. Then in \u00a74.2, we construct a RAG system with these embedders while keeping the LLM and corpus fixed to understand the relationship between the embedder bias and RAG bias (Equation 2).",
            "score": 0.14334442273646486,
            "section_title": "Results: Debiasing RAG",
            "char_start_offset": 14241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.212890625
        },
        {
            "corpus_id": "277150553",
            "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
            "text": "In this paper, we validate RAG's fine-grained retrieval abilities and the global abstraction strengths of LLM-native solutions. However, RAG lacks holistic understanding, and long-context models tend to lose key information over extended contexts. We integrate these strengths of both RAG and LLM-native solutions by fine-tuning an LLM within an RAG framework for data generation. This work is the first to explore LLM and RAG integration within a unified framework, bridging open-domain and domain-specific query-answering tasks. Our RAG-Tuned LLM, equipped with LLMnative memory, outperforms both standard RAG methods and long-context LLMs across diverse datasets, demonstrating superior performance in handling hierarchical queries. \n\nFuture Work. Building on this study, several future directions are worth exploring to further validate and enhance our proposed method. First, we plan to extend RAG-Tuned-LLM to more diverse datasets and domains, enabling us to evaluate its generalizability across different tasks, including complex challenges like multi-hop reasoning and multi-modal query-answering. This will provide a clearer understanding of RAG-Tuned-LLM's effectiveness in both open-domain and domain-specific contexts. Additionally, we will experiment with various foundational models (e.g., the Llama series (Touvron et al., 2023a,b)), evaluating RAG-Tuned-LLM's adaptability to different architectures and model scales. This will highlight the tradeoffs between model size, computational efficiency, and performance when combining RAG and LLMnative methods.",
            "score": 0.143342741966782,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 23299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1572
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48291015625
        },
        {
            "corpus_id": "265351794",
            "title": "LM-Cocktail: Resilient Tuning of Language Models via Model Merging",
            "text": "In this section, we show the experimental results and represent the key findings. Firstly, we compare the performance of fine-tuned models and resilienttuned models. Next, we evaluate the performance of LM-Cocktail when fine-tuning on target task is unavailable. Finally, we investigate the impact of Fine-tune on Performance on BGE Fine-tuned LM- weight \u03b1 and the number of examples.",
            "score": 0.14311387059373076,
            "section_title": "Experimental Results",
            "char_start_offset": 11748,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0120086669921875
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "2) New Patterns: Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple \"Retrieve\" and \"Read\" mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. \n\nInnovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM's capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while Recite-Read [22] emphasizes retrieval from model weights, enhancing the model's ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. \n\nAdjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN [14], showcase the dynamic use of module outputs to bolster another module's functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE [24] and Self-RAG [25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning) [26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27].",
            "score": 0.14305851748280288,
            "section_title": "C. Modular RAG",
            "char_start_offset": 13909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36328125
        },
        {
            "corpus_id": "271745002",
            "title": "Generative Language Models with Retrieval Augmented Generation for Automated Short Answer Scoring",
            "text": "We extracted a certain fraction of the test set to build a vector database, enabling the IR pipeline to provide examples for RAG GLM autoscoring. It's important to note that the IR model was not further fine-tuned on this extracted fraction; the data was solely used for RAG purposes. \n\nOur findings demonstrate that RAG GLM significantly enhances the macro-F1 score, with improvements of up to 13.20% and 19.54% in the unseen question and unseen domain scenarios, respectively. These results underscore the effectiveness of RAG in mitigating the challenges posed by domain shifts in autoscoring tasks.",
            "score": 0.1429411930556791,
            "section_title": "Effectiveness of input fields",
            "char_start_offset": 28419,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 602
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09088134765625
        },
        {
            "corpus_id": "267027552",
            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
            "text": "In this section, we evaluate the performance of fine-tuned models versus base instruction-tuned models. The fine-tuned models are trained on a dataset of questions extracted from the Washington state dataset, a comprehensive collection of 573 documents entailing approximately 2 million tokens. The answers for the questions were generated from the Llama2-13B-chat model with RAG. The evaluation dataset is composed of 273 human-curated for the state of Washington. Each sample in the evaluation dataset consists of a question and an evaluation guideline that describes the contents of a desirable answer. We generated answers for both base and fine-tuned models with and without RAG, and used the metrics discussed in 4.3 to evaluate the answers. \n\nFor base models, we evaluated the open source models Llama2-13B-chat and Vicuna-13B-v1.5-16k. Both models are relatively small, and represent an interesting trade-off of compute vs performance. Both models are fine-tuned version of Llama2-13B using different methods. Llama2-13B-chat was instruction-tuned via supervised fine-tuning and  2023). Additionally, we also evaluated base GPT-4, as a larger, more expensive and more capable alternative. For fine-tuned models, we fine-tune Llama2-13B directly on agriculture data, in order to compare its performance against similar models tuned for more generic tasks. We also fine-tune GPT-4, in order to evaluate if fine-tuning can still be helpful on very large models.",
            "score": 0.14290945210853595,
            "section_title": "Fine-tuning",
            "char_start_offset": 78735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279541015625
        },
        {
            "corpus_id": "276939617",
            "title": "Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey",
            "text": "The most powerful feature of RAG is its ability to adapt recent or specific external knowledge rapidly and dynamically retrieve relevant information from external sources during the generation process [17]. Numerous legal applications have demonstrated that a combination of RAG and fine-tuning methods perform well [18], [19], [20], [21]. \n\nIn 2024, more than 20 legal RAG pipelines were implemented using various embedding, retrieval, enhancement, and generation methods. These pipelines have been frequently integrated with other approaches, e.g., prompt engineering, which is essential for all RAG pipelines, and with knowledge graphs (KG) [22], and fine-tuning (FT) [18], [19], [20], [21], [23], [24] or embedded within multiagent frameworks [25]. In addition, legal RAG pipelines span various applications across the legal domain, ranging from specialized systems focused on specific legal fields to more comprehensive legal platforms.",
            "score": 0.1425468771879479,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 941
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "272574067"
                },
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "272431287"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "263310713"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "269762638"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "272574067"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "272431287"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "263310713"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "264288754"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "273190359"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2548828125
        },
        {
            "corpus_id": "18876242",
            "title": "Music transcription modelling and composition using deep learning",
            "text": "varied. The occasional iamb adds variety and keeps the melody from becoming too monotonous, without breaking the strong metric character, but that idea is abandoned after the first 3 measures. While it serves well in m. 2&3, the iamb variety in the upbeat to the turn is less effective. The tune and turn sound related, with the turn opening with a variation of the stepwise motion of the tune. Measures 9&10 in the turn vary bars 3 and 4 of the tune; and m. 13 in the turn recalls the beginning of the tune and its basic idea. Overall, the turn sounds rather aimless in its last half, and the giant leaps in the final bar are unexpected given the gradual motion in most of the piece. \n\nHere is a second point at which we can improve the composition: make bar 5 of the turn more closely related to its first bar, and change the rhythm of its second bar to that of the tune. The giant leaps in the last bar should be better prepared by the new first ending of the first suggestion above. Finally, in m. 6, change trochee rhythm to iamb and drop the second F-sharp to the D. 17The transcription may be monophonic, but harmony is implicit in the melody. (Chordal accompaniment became prevalent in session music since the early part of the 20th century [18].) In this piece, I (Dmajor) is the most common, (e.g., m. 1-3) with V (Amajor) appearing as well (e.g., m. 3&4), and IV (Gmajor) appearing in m. 10. There are some awkward harmonic moments: the V seems to arrive half a bar too early in m. 3; the first half of m. 10 is IV, but does one switch to V for the last beat, or keep IV and ignore the melodic A? The harmony in m. 12 could be ii (Eminor) -the only minor chord in the piecewhich leaves m. 13 with a V-I cadence but to a weak beat. The second half of the turn is quite static harmonically, which contributes to its aimless quality. That is a third point where we can improve the composition. 18",
            "score": 0.14243100417147786,
            "section_title": "Musical analysis of outputs",
            "char_start_offset": 19761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0011472702026367188
        },
        {
            "corpus_id": "262054313",
            "title": "Harmony and Duality: An introduction to Music Theory",
            "text": "the concept in the simplest setting of one additional tone, the resulting \"blues\" scale is very common in jazz/blues and is often the main component of building professional sounding improvisational lines. See Oscar Peterson's rendition of the song \"Georgia On My Mind\" for a great example. Figure 13: Amazing Grace with a \"blue\" note.\n\nIn our second example, we try to incorporate all extraneous white notes into our pentatonic scale. See figure 14. Once again, these are used as a sort of embellishment to add complexity to the melody without deviating too much from the original idea. Figure 14: Amazing Grace with many \"outside\" tones.\n\nRemark: Notice that some of the white keys have a single black note that is a semitone away (like B) while others have two such notes (like A). When we discuss harmony, we will show that the notes with a single resolution can be included in a harmony such as a major scale while the others cannot be included into a harmonic structure. Using the language from section 2, notes with two resolutions would form a semitone cell if added to the scale. See section 4.3.\n\nFor simplicity, we have mostly focused on improvisation in the right hand. Naturally, there is nothing preventing us from applying these ideas to the left hand or, more precisely, the lower part of the piano range. One small caveat is that forming complex chords below the middle range creates a muddled sound and is generally avoided. Note that harmonically, we only care about tones up to an octave so such restrictions are not strictly part of the main discussion. We saw a similar phenomenon with the 5-chord where it was more desirable to spread out the tones across a larger range to get a clearer sound. Thus, when using the lower range of the piano for improvisation, we tend to restrict to fewer voices moving simultaneously.",
            "score": 0.14243100417147786,
            "section_title": "Playing Outside",
            "char_start_offset": 23093,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016040802001953125
        },
        {
            "corpus_id": "272690355",
            "title": "OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography",
            "text": "In this section, we will introduce the two-phase specialization process of OrthoDoc: multimodal fine-tuning ,the RAG module and the CoT module. The overall training pipeline and capabilities of OrthoDoc are illustrated in the figure below.",
            "score": 0.1422755982363289,
            "section_title": "III. ORTHODOC",
            "char_start_offset": 6342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03948974609375
        },
        {
            "corpus_id": "271218609",
            "title": "Evaluating the Efficacy of Foundational Models: Advancing Benchmarking Practices to Enhance Fine-Tuning Decision-Making",
            "text": "There exist various categories of fine-tuning techniques that are worth mentioning. First, fine-tuning the pre-trained parameters can be performed in either a full [40] or partial [41] manner, aiming to update the pre-trained parameters to suit a new task. Although this approach has demonstrated remarkable performance, particularly in domain-specific tasks, it is computationally expensive. Second, parameter-efficient finetuning (PEFT) involves adding a small trainable parameter for fine-tuning. PEFT utilizes only a small percentage of existing fine-tuned parameters, referred to as low-rank, to adapt to a downstream task and incorporates them into the pre-trained model [42]- [44]. While this strategy balances performance and resource efficiency better than full finetuning, it increases model size. Finally, prompt-based finetuning [45], [46] is a method to construct prompts in a more insightful manner to optimize the model's performance without altering its parameters. In addition, advanced prompt tuning techniques, such as retrieval augmented generation (RAG), have been introduced and demonstrated to effectively mitigate LLM hallucinations [47]. However, a drawback of prompt tuning is that it requires users to have more experience in creating prompts or crafting RAGs that align with their objectives. \n\nIn general, the perspective on LLM fine-tuning may vary depending on the researcher's objectives. A large organization with abundant computing resources may prioritize highaccuracy downstream tasks or specific tasks. Conversely, for a small organization, institution, or individual researcher with limited resources, the objectives may include reducing fine-tuning computational overhead while enhancing overall performance.",
            "score": 0.14224206950957505,
            "section_title": "A. Large Language Foundation Model (LLFM)",
            "char_start_offset": 9185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1747
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "258947572"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "262084134"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "248780177"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "269983737",
            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
            "text": "We consider the following baselines for our experiments.All baselines were based on the LLAMA-7B model.In future versions, we will test more base models to validate the transferability of DuetRAG.\n\n\u2022 LLAMA 2-7B model with 0-shot prompting: this is the commonly used model for QA tasks.We provide clearly written instructions without any reference documents.\n\n\u2022 LLAMA 2-7B model with RAG(Llama2+RAG): much similar to the previous setting, expect we include reference documents.This is a popular technique when dealing with domain specific QA tasks.\n\n\u2022 Domain specific fine-tuning with 0-shot prompting(DSF) : Performing standard supervised fine-tuning without documents in context.Much similar to internal knowledge based model in DuetRAG except DSF is fine-tuned with full trainning data.\n\n\u2022 Domain specific fine-tuning with RAG(DSF+RAG): Performing standard supervised fine-tuning without documents in context.Much similar to external knowledge based model in DuetRAG.",
            "score": 0.14220241006166576,
            "section_title": "Baselines",
            "char_start_offset": 11695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 56,
                    "end": 103
                },
                {
                    "start": 103,
                    "end": 196
                },
                {
                    "start": 198,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 357
                },
                {
                    "start": 359,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 547
                },
                {
                    "start": 549,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 788
                },
                {
                    "start": 790,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 969
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06927490234375
        },
        {
            "corpus_id": "277150553",
            "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
            "text": "We present the winning rates of the long-context LLM compared to VanillaRAG in Table 2. The data reveals that the long-context solution, though more expensive, consistently achieves markedly superior performance on global queries. Conversely, for local queries, the advantages of long-context solutions diminish significantly. Notably, in the news dataset, VanillaRAG outperforms its counterpart across all four evaluation metrics. This aligns with our intuition that RAG is particularly advantageous for extracting fine-grained information needed for local queries, whereas long-context solutions excel in addressing global queries that demand a comprehensive understanding of memory. The above results indicate that, similar to the findings of AInative memory (Shang et al., 2024), although RAG and long-context LLMs can access the correct answer within the provided context, they do not always produce the correct response. \n\n3 Our RAG-Tuned-LLM Building on our findings, we propose a novel LLMnative approach named RAG-Tuned-LLM, which fine-tunes a relatively small (e.g., 7B) LLM using the data synthesized following RAG principles, thereby harnessing the strengths of both RAG and long-context solutions. In this section, we first provide an overview of our approach, followed by a detailed exposition of the global and local data synthesis processes, as well as the fine-tuning stage of the language model.",
            "score": 0.1421910942505775,
            "section_title": "Results",
            "char_start_offset": 7835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181884765625
        },
        {
            "corpus_id": "266933076",
            "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
            "text": "LLMs are great world models due to the amount of training data, and finetuning tasks applied on the model before it's released. However, these models are general-purpose models (may not know the very specifics of your domain) and also not up to date (there is a cutoff date on their knowledge). Fine-tuning and RAG offer two potential customisation pathways, each with distinct tradeoffs. Finetuning requires curating internal datasets to adapt and train the LLM on. However, all your data are baked into the model and you need to",
            "score": 0.14193061294209575,
            "section_title": "RAG vs Finetuning",
            "char_start_offset": 17607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 530
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11181640625
        },
        {
            "corpus_id": "271710111",
            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
            "text": "We explore several techniques for RAG augmentation, and use RAG FOUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and a CoT scheme which guides the model to use the retrieved context, explain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To complement CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt. All prompt templates are included in appendix A.1.",
            "score": 0.1416248627825137,
            "section_title": "RAG Augmentation Techniques",
            "char_start_offset": 11783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4306640625
        },
        {
            "corpus_id": "263134374",
            "title": "Augmenting LLMs with Knowledge: A survey on hallucination prevention",
            "text": "conducts Maximum Inner Product Search (MIPS) in the indexed knowledge base to find the K most similar documents to the input query According to the authors of RAG, training and fine-tuning the parameters of the BERT d encoder is extremely computationally expensive, and not very effective accuracy-wise. Specifically, if they were to train the parameters of BERT d , then for each training iteration, the embeddings of each document in the BERT BASE knowledge base would have to be updated as well, so that they are in-sync with the new BERT d encoder. \n\nTherefore, they use a completely pre-trained BERT d encoder, and during the fine-tuning stage, they only fine-tune the parameters of the query encoder BERT q . \n\nOne interesting aspect of RAG is how it implements the fusion of knowledge from all retrieved documents to produce a final response. In both proposed versions of RAG, RAGtoken and RAG-sequence, fusion is performed right after the decoder. \n\nSpecifically, RAG-token: \n\n\u2022 for each retrieved document z, calculates the probability for each token y i in the vocabulary to be the next token in the sequence: \n\n\u2022 sums the probabilities over all retrieved documents (marginalization): \n\n\u2022 runs Beam Search to find the K most likely next tokens \u2022 chooses the token, y i with the highest transition probability The RAG-sequence model is quite easier to grasp. It takes into account only one retrieved document per sequence that it generates. Specifically, for each retrieved document, it conducts Beam Search to generate K sequences. Then, it simply returns the sequence with the highest probability.",
            "score": 0.14159644947601746,
            "section_title": "III. KNOWLEDGE BASE AUGMENTED GENERATION",
            "char_start_offset": 18440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1409912109375
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "To examine the impact of different fine-tuning methods on the inherent capabilities of LLMs, we evaluate the performance of models finetuned by LoRA and SPRING on several other (non-RAG) tasks.These tasks are commonly used to evaluate LLMs' reasoning, mathematical abilities, and world knowledge, including BoolQ (Clark et al., 2019), CommonsenseQA (Talmor et al., 2019), GSM8K (Cobbe et al., 2021), andMMLU (Hendrycks et al., 2021).The experimental results are shown in Table 2. 4 From the results, we can observe: (1) Thanks to the plugand-play design of our method, SPRING can revert to to the original LLMs by not using virtual tokens.Therefore, it successfully preserves the original capabilities of the LLMs.In contrast, LoRA, which adjusts the model's parameters for RAG tasks, inevitably compromises the model's performance on other tasks.(2) A noticeable decline is observed in the few-shot evaluation, reflecting a decrease in the in-context learning abilities of LLMs.This decline may stem from the fact that RAG fine-tuning does not incorporate in-context learning capabilities.Besides, fine-tuning for RAG tasks may lead the model to overfit to specific task formats, thereby impairing its general generation abilities (more empirical studies are detailed in Appendix G).",
            "score": 0.14137101079111875,
            "section_title": "Performance on Other Tasks",
            "char_start_offset": 20033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 847
                },
                {
                    "start": 847,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1284
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 333,
                    "matchedPaperCorpusId": "165163607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11358642578125
        },
        {
            "corpus_id": "270560495",
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "text": "This process involves training existing pre-trained LLMs on domain-specific curated data, thereby enhancing their answering capabilities by adjusting the weights of the model's parameters.Fine-tuning involves optimizing the model's performance for the target task by adjusting and updating its weights based on domain-specific data during training.This process allows the model to learn task-specific information, thereby improving its ability to generate accurate and relevant responses.Fine-tuning is essential for adapting pre-trained LLMs to new tasks or domains without requiring complete retraining from scratch, which results in improved cost efficiency and reduced computational overhead.\n\nIn this study, we examine the effects of fine-tuning LLMs within a RAG pipeline on their question-answering performance.Initially conducted on private and proprietary datasets from a telecommunications company consisting of customer inquiries to the support team, we replicated the experiment using publicly available datasets.This research seeks to answer two research questions: Firstly, how do fine-tuned models compare to its baseline counterparts in a RAG pipeline?Secondly, does the size of the training dataset have an impact on the effectiveness of fine-tuning?By addressing these questions, we aim to contribute to a better understanding of how fine-tuning affects the question-answering abilities of RAG-integrated LLMs across various domains.",
            "score": 0.14136569889205197,
            "section_title": "Introduction",
            "char_start_offset": 2321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 188,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 696
                },
                {
                    "start": 698,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1168
                },
                {
                    "start": 1168,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1451
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.689453125
        },
        {
            "corpus_id": "263829356",
            "title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models",
            "text": "Accuracy F1 BloombergGPT [6] -0.51 ChatGPT 4.0 [8] 0.64 0.51 ChatGLM2-6B [23]   The results of this evaluation are presented in Table II. The data suggests that the instruction-tuned LLaMA-7B model outperforms the others, achieving the highest accuracy and F1 score. The process of fine-tuning with sentiment instruction data appears to have enhanced the model's ability to discern sentiment in financial phrases, leading to superior zero-shot performance compared to both ChatGPT and the original LLaMA-7B model. From these findings, it is evident that the instruction tuning method significantly improves the model's performance on financial sentiment analysis. \n\n3) Performance of RAG Module: The evaluation results presented in Table III allow us to draw two main conclusions. Firstly, our instruction-tuned LLaMA-7B model (without context) consistently surpasses both FinBERT and LLaMA-7B in terms of accuracy and F1 score. Secondly, the introduction of RAG context to our model resulted in an increase in accuracy from 0.86 to 0.88, and an improvement in the F1 score from 0.811 to 0.842. This demonstrates that the inclusion of context enhances the ability of LLMs to comprehend prompts, thereby improving performance. \n\na) Example of using RAG to retrieve context: To further demonstrate the effectiveness of RAG module, we take the example of the news \"$ENR -Energizer shakes off JPMorgan's bear call.\" Without context, this statement is quite vague and doesn't provide much information about what's happening. Our instruction-tuned model erroneously Metrics Accuracy F1 FinBert [2] 0.725 0.668 ChatGLM2-6B [23] 0.482 0.381 LLaMA-7B [9] 0  classifies this statement as \"neutral.\" However, with the application of RAG, we can retrieve relevant information from the internet to provide a more comprehensive understanding of the situation. Our multi-source knowledge query step extracts information from the article titled \"Energizer shakes off JP-Morgan's bear call\" on Seeking Alpha.",
            "score": 0.14132978273777838,
            "section_title": "Metrics",
            "char_start_offset": 25631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1991
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0531005859375
        },
        {
            "corpus_id": "267061013",
            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
            "text": "In our concluding set of evaluations, we investigated the domain-specific fine-tuning of the Wizard Vicuna 7B model to determine if a smaller, fine-tuned model could match the accuracy of its larger, non-fine-tuned counterpart. Utilizing a dataset [55,56] comprising 1,250 question-answer pairs curated from the MIMIC dataset, we fine-tuned the Wizard Vicuna 7B model using QLoRA [17]. QLoRA, known for its efficiency, enables memory-efficient fine-tuning, making it possible to fine-tune large models even on a single 48GB GPU without compromising performance. \n\nUpon deploying the fine-tuning approach, the results were notably suboptimal. In Table 7, the fine-tuned model response to specific query is presented, highlighting instances where the model exhibits a phenomenon known as model hallucination. \n\nThis outcome reinforced our initial hypothesis and underscored the efficacy of our chosen strategy. The utilization of the RAG framework has consistently demonstrated superior performance in the context of information retrieval with LLMs for our specific use case. The contrast between the fine-tuning results and the RAG-based approach confirms the latter as the most robust and effective method for tackling the intricate challenges of information extraction using LLMs in our particular application. One possible hypothesis for this observed phenomenon is that fine-tuning a model on a small dataset of 1,250 records may not provide sufficient diversity and complexity for learning complex patterns.",
            "score": 0.14118209402629722,
            "section_title": "Domain specific fine tuning",
            "char_start_offset": 24879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1511
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.159912109375
        },
        {
            "corpus_id": "273532207",
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "text": "The RAG-Original model served as our baseline for evaluation, following the architecture proposed by Lewis et al. We indexed our HotelConvQA knowledge base and utilized this index to retrieve documents in response to queries. This model was evaluated without any kind of domain-specific fine-tuning, serving as a benchmark against which other models could be evaluated. \n\nRAG-Finetuned-QA : The RAG-Finetuned-QA model builds upon the baseline RAG-Original architecture by incorporating domain-specific fine-tuning on our hotel domain dataset. Like the RAG-Original model, it utilizes a Hotel Domain Knowledge Base, with dense vector representations indexed using the FAISS library. The loss function is designed to fine-tune both the generator and question encoder collectively. We have used ray as the distributed retriever with 4 retrieval workers as described in the original RAG paper. The model was trained for 20 epochs while the final checkpoint has been selected based on the highest validation accuracy. The purpose of this experiment was to assess the improvements gained from domain-specific training, comparing its performance against the baseline RAG-Original model.",
            "score": 0.14101835054696354,
            "section_title": "RAG-Original :",
            "char_start_offset": 27197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "237797431",
            "title": "User-Driven Fine-Tuning for Beat Tracking",
            "text": "Likewise, if the model is unable to make reliable predictions of the beat-like structure given the presence of different signal properties (e.g., timbre), then this user provided information may only be useful in very localised regions. \n\nIn light of these limitations, we proposed a user centric approach to beat tracking in which a very limited amount of manual annotation by a hypothetical end-user is used to fine-tune an existing state-of-the-art system [22] in order to adapt it to the specific properties of the musical piece being analysed. In essence, we sought to leverage the general musical knowledge of a beat-tracking system exposed to a large amount of training data and then to recalibrate the weights of the network so that it can rapidly learn how to track the remainder of the given piece of music to a high degree of accuracy. A high-level overview of this concept is illustrated in Figure 1. However, in order for this to be a practical use case, it is important that the fine-tuning process be computationally efficient and not require specialist hardware, i.e., that the fine-tuning can be completed in a matter of seconds on a regular personal computer. To demonstrate the validity of our approach, we showed the improvement over the current state of the art offered by our fine-tuning approach on existing datasets and by the specific examples, demonstrating that our approach can learn what is the beat, and also what is not the beat. In addition, we investigated the trade-off between learning the specific properties of a given piece and forgetting more general information. In summary, the main contributions of this work were: (i) to reformulate the beat-tracking problem to target high accuracy in individual challenging pieces where the current state-of-the-art is not effective; (ii) to introduce the use of in situ fine-tuning over a small annotated region as a straightforward means to adapt a state-of-the-art beat-tracking system so that it is more effective for this type of content; and (iii) to conduct a detailed beat-tracking evaluation from an annotation-correction perspective, which demonstrates and quantifies the set of steps required to transform an initial estimate of the beat into a highly accurate output.",
            "score": 0.1408972324506822,
            "section_title": "Introduction",
            "char_start_offset": 7303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 463,
                    "matchedPaperCorpusId": "236095634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040679931640625
        },
        {
            "corpus_id": "250267055",
            "title": "Effort inference and prediction by acoustic and movement descriptors in interactions with imaginary objects during Dhrupad vocal improvisation",
            "text": "Dhrupad (Widdess & Sanyal, 2004) is one of the two predominant styles of Hindustani music. It is a monophonic (primarily) vocal music tradition, and it relies heavily on improvisation, called \u0101l\u0101p, which is not free but strictly rule based and conforms to the so-called r\u0101ga system (a form of melodic mode lying between scale and tune; Powers & Widdess, 2001). The tonic is defined by the main performer's most comfortable pitch and all other pitches are tuned relative to this. The improvisation starts at a strikingly low pace in the vocalist's lowest pitch range; it has a very slow melodic development, which builds up only gradually in pace, pitch, and melodic tension as the melody ascends over about 2.5 octaves toward the climax (typically the 3 rd degree of the highest octave) and it is sung without apparent rhythm (nonmetered) to a repertoire of non-lexical syllables (e.g., \"ra,\" \"na,\" and \"num\"). Melodic tension is periodically released by longer stops on the tonic and shorter stops on the 5 th and is finally resolved when the climax is reached.",
            "score": 0.1406855867126535,
            "section_title": "Introduction to Dhrupad",
            "char_start_offset": 20413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1062
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 31,
                    "matchedPaperCorpusId": "191452065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0008459091186523438
        },
        {
            "corpus_id": "190670809",
            "title": "The temperament police",
            "text": "In general, the current dataset displays a significant degree of within-piece and within-CD temperament agreement, as statistically indicated above. Certain pieces are of particular interest due to, for example, the requirement placed on the temperament by key changes between movements. Such is the case with J. S. Bach's Goldberg Variations and The Well-Tempered Clavier, both of which are quite well represented in the dataset (Tables 4-5). 39 The former is in G major apart from three variations in G minor, which are of a highly chromatic character. The tuning challenge involved arises from the fact that the variations are (unless interrupted by Count Kaiserling, falling asleep) deemed to be played as one piece and the temperament chosen needs to accommodate both G major and minor. It is therefore particularly revealing to check the within-piece temperament agreement for this particular case, as well as to estimate the different temperaments chosen for recordings of the piece. \n\nThe case of The Well-Tempered Clavier is slightly more complicated, for several reasons. First, it covers all 24 major and minor keys (twice). Second, the title indicates a strong relevance of temperament, a fact that has given rise to different interpretations over the years. Third, the question whether it is a piece rather than a collection of pieces, i.e. whether it is intended to be played in one go, is under debate (at least regarding Book 1). The current datatset includes six recordings of the first book of The Well-Tempered Clavier, dating from between 1973 (Gustav Leonhardt) and 2006 (Peter Watchorn), none of which seems to be in one single temperament throughout. The maximal degree of homogeneity is observed in the latest recording (Watchorn 2006), in which two-thirds of the tracks were classified as being in Lehman, and the remaining one-third in two variants of Neidhardt. The other examples typically include five or six different temperaments in each version, mixing fifth-comma-based temperaments with sixth-comma, and occasionally quarter-comma-based ones (such as Kirnberger 3). 3 Trends in popularity of some selected temperaments as reflected in relative number of occurrences in the dataset across decades.",
            "score": 0.1406855867126535,
            "section_title": "Specific repertory",
            "char_start_offset": 30827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2230
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00044083595275878906
        },
        {
            "corpus_id": "69738051",
            "title": "Notating disfluencies and temporal deviations in music and arrhythmia",
            "text": "When a score is interpreted by a human musician, the performed timings and durations are more often than not different, sometimes significantly so, from the notation in the score. Some of these deviations will be due to human inconsistency, but in skilled performance, the bulk of it can be ascribed to deliberate shaping of time, called rubato, either according to established convention or individual idiosyncracy. Cook (1987) encodes rubato using note and bar durations, and percentage deviation from the norm. Repp (1992) represents rubato in melodies using eighthnote durations (longer-duration notes are subdivided equally into eighth notes) and show the durations to frequently follow the shape of a quadratic curve. This method of representing tempo rubato persists to today and can be found, for example, in the work of Spiro, Rink, and Gold (2016). \n\nThis section seeks to represent several different kinds of timing deviations in music performance. Curve fitting, where present, is done with the Matlab spline function and the precisely quantified durations transcribed to common music notation.",
            "score": 0.1406855867126535,
            "section_title": "Choreographed rhythms",
            "char_start_offset": 27398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1106
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 428,
                    "matchedPaperCorpusId": "191384079"
                },
                {
                    "start": 829,
                    "end": 857,
                    "matchedPaperCorpusId": "151653873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0017824172973632812
        },
        {
            "corpus_id": "277510500",
            "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation",
            "text": "Recent works have demonstrated that it is beneficial to finetune an LLM to encourage it to better use retrieved context [21,22,32,37]. It has been shown to improve RAG performances in-domain, and we investigate whether this holds for out-of-domain RAG generalization. To do so, we run supervised fine-tuning on the MultiQA dataset9 : a dataset consisting of 450k general domain questions and answers, described in Appendix Table 5. Its associated document collection consists of Wikipedia [30] and MSMARCO [2] documents. Each supervised fine-tuning sample then consists of a prompt (taken from [32]) with 5 retrieved documents, the question and its answer. Models are trained with LoRA [12]. After training, we evaluate the RAG-adapted models on the multi-domain benchmark. \n\nTraining hyper-parameters are given in Appendix B.",
            "score": 0.14063117619650303,
            "section_title": "LLM Finetuning for RAG",
            "char_start_offset": 10052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 826
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "272593422",
            "title": "Synthetic continued pretraining",
            "text": "In our experiments, we compare two LMs used in the RAG pipeline above: EntiGraph CPT and its base model, Llama 3 8B Base. As mentioned above, we fix the retrieved number of chunks to K = 128, but vary the number of reranked chunks k which are ultimately placed in the context window. For each language model + RAG pipeline, we independently tune the following hyperparameters with a grid search on accuracy using a QuALITY QA validation split: \n\n\u2022 Order of chunks \u2208 {best first, best last} \n\nWe refer the reader to our codebase for tuned hyperparameters.",
            "score": 0.1406266981727508,
            "section_title": "E.3 HYPERPARAMETER TUNING",
            "char_start_offset": 51954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 554
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.198486328125
        },
        {
            "corpus_id": "229292481",
            "title": "How \"BERTology\" Changed the State-of-the-Art also for Italian NLP",
            "text": "Macro F1 (Zanoli et al., 2009) 82.00 (Basile et al., 2017) 82.34 Fine-Tuning giUC 82.37\u00b10.31 Fine-Tuning bertMC 85.07\u00b10.29 Fine-Tuning umC 87.66\u00b10.44 \n\nTable 6: Macro-averaged F1-score for the various systems when evaluated with the EVALITA 2009 NER benchmark.",
            "score": 0.14062392354208425,
            "section_title": "System",
            "char_start_offset": 7820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 260
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 58,
                    "matchedPaperCorpusId": "29947941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00734710693359375
        },
        {
            "corpus_id": "270870786",
            "title": "Macroeconomic Forecasting with Large Language Models",
            "text": "Fine-tuning is a training process that is used to adapt a pretrained TSLM, containing general foundational knowledge, to a specific task such as forecasting, or classification. Fine-tuning updates existing knowledge (parameters) of TSLM to better handle unique characteristics of the task-specific data. This can potentially improve the performance of a TSLM compared to zero-shot predictions for the given task. Fine-tuning can also be used to continually learn the model parameters and incorporate fresh data, improving TSLM's forecasting accuracy over time. For example, in macroeconomic forecasting, we can use fine-tuning to incorporate the data from the current quarter to update the parameters of the TSLM.",
            "score": 0.1405326453190892,
            "section_title": "Fine-tuning",
            "char_start_offset": 20821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05511474609375
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "We experimented with five different combinations of finetuning and RAG in order to determine what the most effective approaches could be for the SLR-automation process.Our experimental setup investigated the following methodologies:\n\n1. Baseline: Evaluate the Mistral-7B-Instruct on its knowledge and ability to answer SLR-related test dataset questions.\n\n2. Fine-tuning LLMs using LoRA: Leveraging Low-Rank Adaptation for fast and efficient parameter adjustment.\n\n3. Fine-tuning LLMs using NEFTune: Introducing noise into embedding vectors to investigate effects on generalization improvements.\n\n4. Instruct LLM + RAG with Raw Articles: Combining LLMs with Retrieval-Augmented Generation, using unprocessed article text as the retrieval corpus.\n\n5. Instruct LLM + RAG with Auto-Extracted Data: Employing RAG with a knowledge base of automatically extracted data comprising the finetuning dataset for focused information retrieval.\n\n6. Best Finetuned LLMs + Best RAG Solution: Integrating the top-performing fine-tuning and RAG methods to optimize SLR automation.\n\nEach method also summarised in Table 3, was evaluated for factually correct answers with respect to the SLR dataset.",
            "score": 0.14051501797611093,
            "section_title": "Experimental Design",
            "char_start_offset": 44331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 232
                },
                {
                    "start": 234,
                    "end": 354
                },
                {
                    "start": 356,
                    "end": 463
                },
                {
                    "start": 465,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 745
                },
                {
                    "start": 747,
                    "end": 931
                },
                {
                    "start": 933,
                    "end": 1063
                },
                {
                    "start": 1065,
                    "end": 1181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382568359375
        },
        {
            "corpus_id": "277972551",
            "title": "RAGing ahead in rheumatology: new language model architectures to tame artificial intelligence",
            "text": "Several advancements in the field of NLP are being applied to the RAG ecosystem that will enhance its future use. Analyzing the various strategies is beyond the scope of this manuscript, and other studies, such as Gao et al., have dedicated entire lines of research to this topic. One strategy that is worth noting is modular RAG, an approach where different components are used to retrieve relevant information from various sources, which is then processed and combined to generate accurate and context-aware answers or summaries for complex queries. 21 In addition, knowledge graphs are gaining popularity because they improve RAG by organizing and linking complex information in a structured, interconnected way, allowing the system to retrieve more accurate, contextually relevant data. 35  promising trend in enhancing RAG applications in medicine is the integration of hybrid approaches, combining RAG with fine-tuning. 36 Fine-tuning an LLM involves further training on a specific dataset to refine its broad knowledge for improved performance in specialized tasks or domains. For instance, a small language modela scaled-down version designed for efficient, taskspecific performance with reduced computational resources-can be fine-tuned with highly specialized rheumatology knowledge. 37 When combined with the relevant context provided by RAG, this approach enables highly accurate and efficient responses, all while maintaining low computational demands. \n\nThe advent of multimodal models, which integrate and process data from diverse sources such as text, images, and structured information, will surely revolutionize our field. 38 These models are especially impactful in our domain, where we rely on a variety of data types, including clinical notes, lab reports, medical images, and genetic information. When used in an RAG setting, these models may significantly enhance the depth of context and the accuracy of generated responses.",
            "score": 0.1404973966972033,
            "section_title": "Future directions and rheumatology applications",
            "char_start_offset": 23818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1949
                }
            ],
            "ref_mentions": [
                {
                    "start": 791,
                    "end": 793,
                    "matchedPaperCorpusId": "245131215"
                },
                {
                    "start": 1642,
                    "end": 1644,
                    "matchedPaperCorpusId": "252309887"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37646484375
        },
        {
            "corpus_id": "271903789",
            "title": "Meta Knowledge for Retrieval Augmented Large Language Models",
            "text": "Our work integrates concepts from methodologies that generate QA from document collections for downstream fine-tuning of either LLMs or encoder models, with techniques leveraging query augmentation to boost the performance of retrievers in RAG pipelines. Below, we outline related work relevant to these two areas of RAG enhancement.",
            "score": 0.14044956703136746,
            "section_title": "RELATED WORK",
            "char_start_offset": 6525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 333
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
            "score": 0.14037626811673734,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 145958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 528
                },
                {
                    "start": 530,
                    "end": 781
                },
                {
                    "start": 783,
                    "end": 979
                },
                {
                    "start": 981,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1547
                },
                {
                    "start": 1549,
                    "end": 1721
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1301,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.301513671875
        },
        {
            "corpus_id": "260351190",
            "title": "Delving into Commit-Issue Correlation to Enhance Commit Message Generation Models",
            "text": "The fine-tuning stage is relatively simple: Given an input sequence of code change, F grounded is fine-tuned to generate a commit message in this stage. Note that the structure of F grounded and F are the same, but the parameters are different.",
            "score": 0.14029963300199078,
            "section_title": "C. Fine-tuning Stage",
            "char_start_offset": 25822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 244
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030914306640625
        },
        {
            "corpus_id": "263334347",
            "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
            "text": "Using instruction to guide speech synthesis has received great attention (Guo et al., 2023;Yang et al., 2023a). In this part, we fine-tune the UniAudio model on the PromptSpeech (Guo et al., 2023) dataset. Before Fine-Tuning After Fine-Tuning \n\nFigure 3: Performance comparison over 7 audio generation tasks before/after fine-tuning.",
            "score": 0.14003338440796373,
            "section_title": "B.6 INSTRUCTED TTS",
            "char_start_offset": 38871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 333
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 91,
                    "matchedPaperCorpusId": "253761189"
                },
                {
                    "start": 178,
                    "end": 196,
                    "matchedPaperCorpusId": "253761189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0083770751953125
        },
        {
            "corpus_id": "276928032",
            "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
            "text": "In this section, we introduce Open-Rag, an OPtimized ENd-to-end RAG framework designed to fine-tune a retriever to capture in-context, open-ended relevance, optimizing it for the downstream RAG pipeline. \n\nTo summarize, Open-Rag training comprises two stages: offline RAG and online RAG. The primary goal is to onthe-fly identify positive and negative documents for the contrastive learning of the retriever. An illustration of our framework is depicted in Figure 2.",
            "score": 0.14002983408653524,
            "section_title": "Methodology",
            "char_start_offset": 9182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45751953125
        },
        {
            "corpus_id": "236459956",
            "title": "XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation",
            "text": "Given a pre-trained model, vanilla fine-tuning updates the parameters of the pre-trained model solely on the dataset of the downstream task. For example, for German AMR parsing, we fine-tune the pre-trained model on the fine-tuning dataset of the German AMR parsing task. In other words, vanilla fine-tuning involves only a single-task learning.",
            "score": 0.13996987075539444,
            "section_title": "Vanilla Fine-Tuning",
            "char_start_offset": 13916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 345
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019012451171875
        },
        {
            "corpus_id": "270068240",
            "title": "CataLM: Empowering Catalyst Design Through Large Language Models",
            "text": "From Table 4, it can be seen that our method (i.e.Fine-tuned LLM + Few shot) performs the best.We can also see that both the fine-tuned module and the RAG module contribute to the improvement of model extraction accuracy.",
            "score": 0.13991052099496742,
            "section_title": "Named Entity Recognition Task",
            "char_start_offset": 17167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 50,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 221
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0345458984375
        },
        {
            "corpus_id": "277994112",
            "title": "Synergizing RAG and Reasoning: A Systematic Review",
            "text": "Tuning-Based. The tuning-based approach improves the integration of RAG and reasoning by optimizing model parameters to internalize the retrieval-augmented chain-ofthought mechanism within LLMs. Current research mainly targets three goals: retrieval pathway optimization, structured generation enhancement, and collaborative training with external modules. \n\nFor retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy. \n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning. \n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically.",
            "score": 0.1398961831249341,
            "section_title": "5.2.2",
            "char_start_offset": 63935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 2065
                },
                {
                    "start": 2068,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 1183,
                    "end": 1186,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1947,
                    "end": 1951,
                    "matchedPaperCorpusId": "258841328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "277780516",
            "title": "RTLRepoCoder: Repository-Level RTL Code Completion through the Combination of Fine-Tuning and Retrieval Augmentation",
            "text": "In this section, detailed experimental results and in-depth analyses are provided, including main results, discussion of fine-tuning and RAG, and the ablation study of the initial decision-making process.",
            "score": 0.13987466423329487,
            "section_title": "Results and Analysis",
            "char_start_offset": 14112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2130126953125
        },
        {
            "corpus_id": "115677979",
            "title": "Melody Extraction Using Chroma-Level Note Tracking and Pitch Mapping",
            "text": "The possible pitch frequencies corresponding to each pitch class are \n\nwhere N oct takes integers that force f p to be located within the possible melody pitch range, and p j is the HPCP pitch bin. After all possible frequencies are obtained by Equation ( 11), coarse note mapping determines which frequency is the accurate pitch according to the following criterion \n\nwhere A h,t is the magnitude of the h-th harmonic for candidate f p at frame t, and \n\nin Equation ( 12) accounts for the salience of candidate frequencies, while \n\nThe pitches mapped by Equation ( 11) are standard frequencies in the equal tempered scale. However, most pitches have marginal shifts from these standard frequencies in real-world recordings. Hence, a fine tuning is necessary to obtain more accurate pitch frequencies. \n\nAccurate pitch frequency can be obtained by the modified Euclidean algorithm based on the magnitudes and frequencies of the harmonics [12]. The detailed fine tuning procedure is described as follows. \n\nSuppose that f i,t and f j,t are frequencies of the i-th and j-th peaks at frame t, respectively. Let y = max( f i,t , f j,t ), and x = min( \n\nwhere [\u2022] represents rounding towards the nearest integer. If r(x, y) \u2265 \u03b6, z = modf(y, x), where \u03b6 is the threshold determining whether a pitch candidate is deduced, modf(\u2022, \u2022) means modulus after division, and both input variables are float numbers. Then, let y = max(z, x), x = min(z, x), iterate the above procedure again. \n\nIf r(x, y) < \u03b6, the estimated pitch is \n\nA salience value is assigned to the estimated pitch, i.e., \n\nwhere A h,t is the magnitude of the h-th harmonic for candidate f 0,a,t , and \n\nFor detailed information of the above procedure, please refer to Algorithm 1 [12]. \n\nAlgorithm 1. Computation for the parameters of pitch candidates: \u03b8 t = {F t , S F t }.",
            "score": 0.13982805307730733,
            "section_title": "Melodic Pitch Mapping",
            "char_start_offset": 11725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 71,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1003
                },
                {
                    "start": 1006,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1741
                },
                {
                    "start": 1744,
                    "end": 1830
                }
            ],
            "ref_mentions": [
                {
                    "start": 938,
                    "end": 942,
                    "matchedPaperCorpusId": "124589334"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "124589334"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00263214111328125
        },
        {
            "corpus_id": "270199477",
            "title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
            "text": "Table VII shows the result of the benchmark. MAET, CWET, and KCV do not have source URLs provided as a context, and hence can accurately evaluate customized models performance against general LLMs. We can observe that both of the customized models perform better on MAET. Fine-tuned model outperform RAG on CWET. While fine-tuned model performs comparable to base model on KCV, RAG model has a significant drop. One key weakness of \"Retrieve-Read\" framework for RAG is that during the retrieval phase, the model can select irrelevant chunks and miss crucial information for accurate response. Hence, if the content is not supported by the retrieved context, the model faces hallucinations [66]. This could be one of the reasons behind lower performance in CWET and KCV. We will explore advanced RAGs framework in our future works.",
            "score": 0.13966926387476256,
            "section_title": "C. Results:",
            "char_start_offset": 49247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06878662109375
        },
        {
            "corpus_id": "274859726",
            "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
            "text": "In this work, we propose PA-RAG, a method for optimizing the generator of RAG systems to align with specific RAG requirements comprehensively. The training process includes instruction fine-tuning and multi-perspective preference optimization. We conducted extensive experiments on four QA benchmarks and three LLMs, demonstrating that PA-RAG can significantly enhance the generator's response informativeness, response robustness, and citation quality.",
            "score": 0.1395736506681763,
            "section_title": "Conclusion",
            "char_start_offset": 25916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 453
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35205078125
        },
        {
            "corpus_id": "273695416",
            "title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations",
            "text": "Figure 4 shows the behaviour of C-RAG when scaling-up the number of training examples. While we found that the quantity of the demonstrations used in C-RAG is important in determining the final performance, we found that C-RAG can outperform the baselines RAG models with only 50% of training demonstrations, also achieving superior training performance when compared to the fine-tuned SFT model (i.e., the model fine-tuned without contrastive reasoning demonstrations as explained in \u00a74). This further highlights the quality of the training signal provided by the contrastive explanations.",
            "score": 0.13957037475881204,
            "section_title": "Quantity of Instructions",
            "char_start_offset": 23265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 590
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1435546875
        },
        {
            "corpus_id": "262460765",
            "title": "EMelodyGen: Emotion-Conditioned Melody Generation in ABC Notation with the Musical Feature Template",
            "text": "Four datasets were created for various experiments: processed EMOPIA and processed VGMIDI were used to compare the music21 parsing rates of the backbone after being fine-tuned on these datasets versus well-structured sheet music; the analysis dataset was utilized to examine the correlations between various musical features and emotional labels; and Rough4Q was employed for embedding features to the final model via fine-tuning. The details of data structures and statistical summaries of these four datasets are described in subsequent subsections.",
            "score": 0.13953647300297867,
            "section_title": "II. DATASETS",
            "char_start_offset": 4655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 551
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00323486328125
        },
        {
            "corpus_id": "276725311",
            "title": "Retrieval Augmented Generation for Topic Modeling in Organizational Research: An Introduction with Empirical Demonstration",
            "text": "The introduced scores for validity and reliability not only facilitate comparisons with other topic modeling methods but also serve as diagnostic tools for assessing and refining the results of the topic modeling. Furthermore, the flexibility of the Agentic RAG framework allows it to be implemented with various LLMs (e.g., LLaMA), ensuring its continued applicability as more advanced models are developed. annotations. This method involves pretraining the model on a subset of the dataset for a specific task before applying the fine-tuned model to the entire dataset to extract the most relevant topics. According to the authors, their approach successfully reduced the number of hallucinated topics while producing coherent and relevant topics. \n\nWe argue that Agentic RAG is a superior method for topic modeling than fine-tuning, for several reasons. First, fine-tuning an LLM requires substantial computational resources, whereas Agentic RAG operates efficiently on standard computer hardware. Second, finetuning is inherently a more static approach, as it depends on pretraining and may struggle with generalizing to unseen or unfamiliar data (Gao et al., 2024). Third, fine-tuning typically requires a predefined data structure for both input and output, making it less adaptable to new datasets. In contrast, RAG-particularly Agentic RAG-is more flexible, easier to evaluate, and provides greater interpretability (Gao et al., 2024). We suggest that Agentic RAG is more applicable for the task of topic modeling due to its adaptability and efficiency. Fourth, in contrast to LLM prompting or fine-tuning, Agentic RAG shows the reasoning process that the model uses to generate an output, such providing the necessary transparency for a scientific context.",
            "score": 0.13947258853014521,
            "section_title": "Discussion",
            "char_start_offset": 33607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1765
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2069091796875
        },
        {
            "corpus_id": "268185412",
            "title": "ANALYSIS OF WHISPER AUTOMATIC SPEECH RECOGNITION PERFORMANCE ON LOW RESOURCE LANGUAGE",
            "text": "Fine-tune done in Google colaboratory environment with system specification 14.7GB T4 GPU and 12.7GB RAM. The hyperparameter that used for fine-tuning can be seen in Table 2.",
            "score": 0.13943235291011857,
            "section_title": "Model Fine-Tuning",
            "char_start_offset": 11179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002590179443359375
        },
        {
            "corpus_id": "273502262",
            "title": "Customized FinGPT Search Agents Using Foundation Models",
            "text": "Fine-tuning adapts generic LLMs to specific tasks or domains. Recent techniques like Low-Rank Adaptation (LoRA) [14] and (QLoRA) [9] improves efficiency and reduces memory usage of tuning. As shown in these works [14] [9] [20], these methods indeed specializes LLMs in financial applications. However, fine-tuning alone is insufficient for addressing the time-sensitiveness [19] of financial data as this data constant updates. To overcome this, we combine fine-tuning with Retrieval Augmented Generation (RAG).",
            "score": 0.13938474298318457,
            "section_title": "LLM Finetuning Methods",
            "char_start_offset": 5767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 511
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "258331934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303466796875
        },
        {
            "corpus_id": "232382606",
            "title": "Fine-Tuning Approach for Segmentation of Gliomas in Brain Magnetic Resonance Images with a Machine Learning Method to Normalize Image Differences among Facilities",
            "text": "As shown in Figure 2, the Dice coefficient score of the fine-tuning model improved significantly compared with that of the BraTS model, and it was comparable to that of the JC model. In facilities B and J, the fine-tuning model's Dice coefficient score was higher than that of the JC model (Supplementary Table S2). Welch's ANOVA showed significant differences among the three groups (p = 0.0002). The Games-Howell Post-Hoc Test showed that there was a significant difference between the BraTS and JC models (p < 0.0001) and between the BraTS and fine-tuning models (p = 0.002) in terms of the Dice coefficient score. In contrast, there was no significant difference between the JC and fine-tuning models (p = 0.673). \n\nnections of 3D U-Net on fine-tuning (Supplementary Figure S4). In this experiment, we re-examined three different methods (see method). As shown in Supplementary Figure S4, 'down2_up2 model' and 'fine_all model' showed comparable performances. In general, the fewer the parameters to be tuned, the lower the computational cost and the lesser the overfitting for the target domains with small data [37]; hence, 'down2_up2 model' was considered to be the most efficient fine-tuning model.",
            "score": 0.1391916497678,
            "section_title": "Comparison of the Three Models",
            "char_start_offset": 17724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1206
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02288818359375
        },
        {
            "corpus_id": "271693752",
            "title": "Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base",
            "text": "This paper presents Golden-Retriever, a novel agentic RAG system designed to efficiently navigate vast industrial knowledge bases and overcome the challenges of domain-specific jargon and context interpretation. Experiment on a dedicated questionanswer dataset shows that Golden-Retriever significantly improves answer accuracy, demonstrating its superior performance compared with traditional RAG method. \n\nA Fine-tuning or Retrieval Augmented Generation? \n\nKnowledge injection via fine-tuning has several significant drawbacks. For instance, when fine-tuned on a knowledge statement like \"A is B,\" the fine-tuned LLM can correctly answer \"What is A?\" but fails to answer \"What is B?\" with \"A\" for arbitrary A and B. This phenomenon is famously known as The Reversal Curse (Berglund et al., 2023). Although remedies such as generating reversed training data (Golovneva et al., 2024) have been proposed, they require higher training costs and do not guarantee that the tuned LLM will answer all possible forms of a query. Additionally, incorporating knowledge through fine-tuning necessitates a new fine-tuning job for each new piece of knowledge, which incurs computational costs and hinders efficient integration of new information. The amount of knowledge a model can effectively incorporate depends on the capacity of the fine-tuned model part (Roberts et al., 2020), while excessive fine-tuning may lead to catastrophic forgetting, where the model forgets previously learned knowledge (Zhai et al., 2024). \n\nIn contrast, RAG does not suffer from these drawbacks. The Reversal Curse, observed in fine-tuning methods, does not occur when knowledge statements are presented in-context, as part of the prompt. In RAG, the LLM learns knowledge statements in-context, significantly improving its reasoning capacity and enabling efficient instruction prompt tuning (Singhal et al., 2023). Furthermore, RAG does not require model retraining and can efficiently incorporate new knowledge corpora. These properties make RAG a superior choice for industrial knowledge bases.",
            "score": 0.13919145319413173,
            "section_title": "Conclusion",
            "char_start_offset": 18546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 1863,
                    "end": 1885,
                    "matchedPaperCorpusId": "255124952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2335205078125
        },
        {
            "corpus_id": "271843213",
            "title": "Hybrid Student-Teacher Large Language Model Refinement for Cancer Toxicity Symptom Extraction",
            "text": "The performance evaluation of the Zephyr-7b-beta and Phi3-mini-128 models in extracting symptoms from clinical notes is presented across three refinement techniques: RAG, fine-tuning, and the hybrid method. These results, shown in Figure 2, highlight the improvement in symptom extraction accuracy over five epochs during the iterative refinement process of prompts, RAG examples, and model weights. \n\nFor the Zephyr-7b-beta model using the RAG method, the average score increased from 0.32 \u00b1 0.20 to 0.73 \u00b1 0.15, demonstrating a significant lift in performance with a mean increase of 0.41 and a standard deviation decrease of 0.05. Notable improvements were observed in symptoms like Urothelial Carcinoma, which increased from 0 to 1 in two epochs, and Proctitis, which rose from 0.1 to 0.9 in three epochs. In the fine-tuning approach, the average score improved to 0.46 \u00b1 0.23, Using a student-teacher architecture with Zephyr-7b-beta and Phi3-mini-128 as student models and GPT-4o as the teacher, we evaluated prompt refinement with RAG, fine-tuning, and a hybrid method. \n\nOur findings revealed that the RAG method provided significant performance improvements and costefficiency. This approach substantially enhanced accuracy and F1-macro scores while maintaining low operational costs, making it a practical solution for clinical applications. Fine-tuning, on the other hand, was less effective when used alone, suggesting its role is best as a complementary technique rather than a standalone solution. The hybrid method, which combines RAG and fine-tuning, demonstrated the best overall performance in Phi3-mini-128 and outperformed other approaches in various scenarios. This method effectively harnessed the strengths of both RAG and fine-tuning, particularly for specific symptoms where combined strategies were essential. \n\nInterestingly, our results showed that the smaller Phi3-mini-128, with approximately half the parameters of Zephyr-7b-beta, generally outperformed Zephyr in the task of clinical symptom extraction. This outcome suggests that model size alone does not determine effectiveness; rather, the refinement technique plays a crucial role in performance optimization.",
            "score": 0.13901858545299597,
            "section_title": "Results",
            "char_start_offset": 22086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1835
                },
                {
                    "start": 1838,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137939453125
        },
        {
            "corpus_id": "195971662",
            "title": "A swingogram representation for tracking micro-rhythmic variation in jazz performances",
            "text": "In Figure 9, we analyze an excerpt from 'The Red Door', recorded by the Gerry Mulligan group in 1960, in order to study the micro-rhythmic interplay of solo baritone saxophonist Gerry Mulligan and drummer Mel Lewis. In Figure 9(a), it can be observed that Lewis keeps the idealized tied triplet swing ratio s \u2248 2.0 for more than 60 s at an average tempo of 236 BPM. Mulligan plays in the same precise fashion, almost always synchronizing his onbeat onsets to those of Lewis. However, he shows quite some variability in his offbeat onsets, indeed showing higher swing ratio at phrase endings, as hypothesized by Benadon (2006).",
            "score": 0.1389518442869985,
            "section_title": "The red door",
            "char_start_offset": 51487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 626
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 625,
                    "matchedPaperCorpusId": "192642862"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0008726119995117188
        },
        {
            "corpus_id": "267027552",
            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
            "text": "Our study aimed to explore the potential of fine-tuning to help GPT-4 learn new knowledge, which is crucial for applied research. To test this, we selected questions across the 50 states in the US that were similar across at least three states. \n\nWe then computed the cosine similarity from the embeddings and identified a list of 1000 such questions. These questions were removed from the training set, and we used both fine-tuning and fine-tuning with RAG to assess whether GPT-4 could learn new knowledge based on similarities across different states. \n\nThe results from our study, as shown in Table 22, were quite promising. The data revealed that GPT-4 was only able to learn 47% of the new knowledge presented to it. However, with the help of fine-tuning, we were able to significantly increase this percentage. Specifically, the fine-tuned model was able to learn up to 72% and 74% of the new knowledge, depending on whether we used RAG or not. This upper bound represents the maximum amount of new knowledge that the model could have learned, given the data it was presented with. \n\nIt is worth noting that the potential applications of this research are numerous. For instance, consider a scenario where we need to evaluate the impact of a particular pest or disease on corn. In some cases, we may not have access to advisory information specific to our region. However, we may have access to advisory information from a similar region, which could still be useful. By leveraging fine-tuning and RAG, we can help the model learn this new knowledge and apply it to our specific situation. \n\nThe results from Table 22 demonstrate that GPT-4 learned just 47% of this new knowledge and with fine-tuning we were able to get this number way higher to 72% and 74% with RAG and the fine-tuned model. Interestingly, this can be defined as the upper bound of what the model could have learned.",
            "score": 0.13891460942266087,
            "section_title": "Knowledge Discovery",
            "char_start_offset": 85015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1892
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.269775390625
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "For next steps, one thing we can do is more testing to tune different model parameters to see if it would enhance performance. Only chunk size and chunk overlap were adjusted in RAG, but most of the training parameters were kept at the default for the training process. Testing out different embedding models, or even fine tuning the embedding model for the finance specific domain would be a method to try in the near future. Additionally, while supervised fine tuning (SFT) was the primary technique used in this paper, it is also possible to in corporate other methods such as unsupervised fine tuning and reinforcement learning with human feedback into these models to further improve performance. We are also looking to test out more methods to improve retreival algorithms in addition to FLARE and HyDE to help find the best and optimal chunk of text. These include implementing a re-ranking algorithm, such as Cohere's that will rearrange the document chunks based on relevant instead of just similarity. 13 Since we are working with a variety of financial documents with various different and complex sections, metadata annotations could provide some value to further filter document chunks for RAG. In summary, we will continue to strive for more ways to make these models and the fine tuning process faster and more efficient while maintaining and improving model performance.",
            "score": 0.1387902285918625,
            "section_title": "Next Steps",
            "char_start_offset": 23054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "261049520",
            "title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models",
            "text": "We compare our results with those of the BARTlarge model (Lewis et al., 2020a) for the closedbook setting and the RAG model (Lewis et al., 2020b) for the open-book setting, which presented in Petroni et al. (2021). Notably, these baseline models were specifically fine-tuned on the KILT benchmark, whereas our chosen LLMs and constructed R-LLMs were not. See also Appendix A.5 for additional information of the baselines.",
            "score": 0.13866052840793122,
            "section_title": "Baseline",
            "char_start_offset": 11439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 421
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 78,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 124,
                    "end": 145,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0345458984375
        },
        {
            "corpus_id": "273186680",
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "text": "Fine-tuning \n\nFigure 1: Overview of our Reward-RAG. Given a query and its knowledge database, a retrieval model is used to retrieve the top-k relevant documents, which then are rated for the relevance by a CriticGPT. These \u27e8query, document\u27e9 pairs and their CriticGPTs' feedback are used to train a reward model, which is used to fine-tune the RAG retrieval to better align with human preferences. \n\nIn this section, we present our Reward-RAG. We first describe the dense retrieval problem in RAG in section 3.1, then present how we apply reinforcement learning to this problem in section 3.2. Fig. 2 illustrates the high-level design for Reward-RAG.",
            "score": 0.13858380907663292,
            "section_title": "Reward Model",
            "char_start_offset": 8093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 14,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 649
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5732421875
        },
        {
            "corpus_id": "271218757",
            "title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing Assistant",
            "text": "We validate the usefulness of several popular open-source foundation models the base model for Panza, as well as our statement that fine-tuning, rather than simply RAG, is necessary to obtain a performant Panza model. In Figure 7 we show that similar performance levels can be obtained by fine-tuning various LLM backbones, when performing FFT or RoSA-RAFT across Mistral-7B-Instruct-v0.2, Llama3-8B-Instruct, and Phi-3-mini-4k-instruct. While the achieved BLEU scores are very similar across models, the only significant difference is the higher MAUVE score achieved by the Llama3 model. \n\nM i s t r a l P r e t r a i n e d ( R A G )",
            "score": 0.13851807890020762,
            "section_title": "C Base model comparison",
            "char_start_offset": 38342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 634
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0306243896484375
        },
        {
            "corpus_id": "267750557",
            "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
            "text": "In this work, we have introduced Mafin, a novel methodology for fine-tuning black-box embedding models, thereby addressing a significant gap in the field of Retrieval Augmented Generation (RAG). \n\nRecognizing the need for enhanced performance in black-box embedding models, especially when applied to new documents or within specific domains, Mafin effectively meets this challenge by augmenting a black-box model with a small tunable embedding model thus significantly boosting its performance while only requiring a minimal fine-tuning cost. This method leverages both the powerful language representation provided by large pre-trained models and the benefits of fine-tuning with a small embedding model. The low fine-tuning cost of the small models enables its use for large-scale, customized online fine-tuning tailored to each company and individual, promising to be a performance-effective and cost-efficient framework for the RAG infrastructure. \n\nWe have demonstrated Mafin's excellence in fine-tuning embedding models for text retrieval tasks within the RAG framework. Future work will explore Mafin's potential across a broader range of fields. We aim to test and validate our fine-tuning methodology's effectiveness in tasks such as classification and clustering, thereby further expanding the applicability and impact of our approach. \n\nhttp://arxiv.org/ps/2402.12177v4",
            "score": 0.1384442519572369,
            "section_title": "Conclusion",
            "char_start_offset": 23095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "207847259",
            "title": "Transfer Learning in Spatial-Temporal Forecasting of the Solar Magnetic Field",
            "text": "Fine-tuning (2 layers copied), no regularization Fine-tuning (1 layer copied w/L2-SP) \n\nFine-tuning (2 layers copied w/L2-SP) \n\nFine-tuning (3 layers copied w/L2-SP) \n\nFine-tuning (4 layers copied w/L2-SP) \n\nFine-tuning (5 layers copied w/L2-SP) \n\nFine-tuning (6 layers copied w/L2-SP)",
            "score": 0.13824685800057293,
            "section_title": "FIGURE 4",
            "char_start_offset": 27640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 88,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001255035400390625
        },
        {
            "corpus_id": "267548092",
            "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
            "text": "The larger the range and the finer the granularity, the longer the search process will take. We represented the additional fine-tuning time using the default setting (\u03bb = 0.1, n = 100) in Table 7, where the original fine-tuning time is the time used for baseline finetuning, and the additional fine-tuning time is the additional time used by IR-QLoRA fine-tuning. Compared with the original fine-tuning time, the additional fine-tuning time is only up to 0.84%, which is extremely low additional overhead.",
            "score": 0.13787948509284562,
            "section_title": "Ablation Study",
            "char_start_offset": 21178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 505
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0279541015625
        },
        {
            "corpus_id": "272398082",
            "title": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for PostNL",
            "text": "There is a irresistible necessity from enterprises for fine-tuning LLMs to get them trained on proprietary domain knowledge. Fine-tuning is the process of continuing the training of an already pre-trained model on a new dataset that is typically smaller and task-specific [10]. This allows the model to adjust its weights and parameters to better fit the nuances of the new data and the specific requirements of the target task. Though there is an option to use OpenAI (opensource) models to solve most of the use-cases, there is a high demand for domain specific LLMs due to data privacy and pricing concerns as mentioned earlier. \n\nThe data of an enterprise can stay on premise as the LLMs are also present on premise. In-house development ensures this. Fine-tuned LLMs provide quality and custom feel to the stakeholder and also has low latency in displaying the results. [10] 2.5 Making LLMS suitable for specific tasks through Retrieval Augmented Generation \n\nWhen a LLM model that is not fine-tuned is used for domain specific tasks and asked to handle queries beyond its training data or current information, hallucinations can happen [11]. As fine-tuning can be done to make LLMs suitable for specific use-cases, another approach next to or instead of fine-tuning can be to use Retrieval-Augmented Generation (RAG) architecture. RAG enhances LLMs by retrieving relevant document chunks (in real time) from external knowledge bases through semantic similarity calculation [12]. So RAG retrieves additional data, and augments it to the existing knowledge of the LLM based on semantic similarity. In fine-tuning, the weights of the existing parameters of the LLM get adjusted to the learned knowledge, but vectors are not added. Therefore to keep the knowledge base updated, fine-tuning would be computationally expensive.",
            "score": 0.137875972799169,
            "section_title": "Making LLMs suitable for specific tasks through fine-tuning",
            "char_start_offset": 11013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24365234375
        },
        {
            "corpus_id": "248780213",
            "title": "Improving Multiple Documents Grounded Goal-Oriented Dialog Systems via Diverse Knowledge Enhanced Pretrained Language Model",
            "text": "This section mainly discusses results for modulespecific training methods. We fine-tune RAG's retriever and pretrain generator, DPR and BART, with contrastive learning and conversational QA datasets. We set the baseline model as the same configuration with section 4.2.1. \n\nPretraining We pretrain BART-large on CoQA and Doc2Dial before integrating it into RAG. We train 10 epochs for each dataset using hyperparameters suggested by the DialDoc2021 baseline code on subtask2.10 Table 3 shows the result for pretraining. We report two results; pretrained on CoQA only and pretrained on both CoQA and Doc2Dial. Both datasets enhanced the model performance in terms of F1 and EM scores. There is extra room for improvement since we pretrain BART only for a few epochs due to long training time and limited resources. \n\nContrastive Learning We fine-tune DPR using the settings implemented by the shared task. We fine-tune the recently released version of DPR, checkpoint.retriever.single-advhn.nq.bert-base-encoder, for 50 epochs on our new DPR dataset with one extra hard negative sample generated by antonym augmentation. Table 3 reports the results for contrastive learning. Despite using the same hyperparameters for DPR, there is degradation in the score for fine-tuning on our setting. However, after adding another hard negative sample, the model shows better performance on the shared task.",
            "score": 0.13776911942216472,
            "section_title": "Module Specific Methods",
            "char_start_offset": 10362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1394
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47314453125
        },
        {
            "corpus_id": "269757831",
            "title": "Automated Conversion of Static to Dynamic Scheduler via Natural Language",
            "text": "With minimizing technical expertise and computational resource requirements for end-users as our goal, Retrieval-Augmented Generation (RAG) techniques can be applied.RAG is commonly used to introduce context-specific knowledge which the pre-trained model may not have been trained on, as shown in [6], without the need for further fine-tuning.In the case of dynamic scheduling, worker scheduling problems can vary significantly and have context specific constraints which pre-trained LLMs are unlikely to be trained on.While there are various RAG techniques, as surveyed in [4], we opted to use Naive RAG for our method which has minimal fine-tuning steps.",
            "score": 0.1377511706624886,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 8918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 519
                },
                {
                    "start": 519,
                    "end": 656
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0574951171875
        },
        {
            "corpus_id": "271218596",
            "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
            "text": "RAG benchmarks Numerous general LLM evaluation benchmarks, such as ChatbotArena [46] have been proposed in past work.However, human preference datasets, constructed through pairwise comparisons, have limitations.While these data are appropriate for fine-tuning general purpose LLM judges, they are insufficient for building RAG evaluation systems because preference judgements under-represent important RAG dimensions like factuality and completeness of the response [13].\n\nCHATRAGBENCH [24] is a recent initiative that is similar in intent to our work in that it contributes a large-scale unified RAG benchmark.However, CHATRAGBENCH only contains ground truth responses and lacks the granular component-specific labels that we release with RAGBench.As future work, we can consider annotating CHATRAGBENCH with the schema proposed in this paper, to further scale RAGBench.\n\nRAGTruth [41] is another recent effort at a RAG Benchmark.RAGTruth combines QA, Data-toText, and Summarization RAG data with human annotated hallucinated spans in the response.While it is an excellent benchmark for hallucination detection, it does not offer the level of granularity we present with RAGBench that is necessary to understand the RAG system as a whole.\n\nRAG evaluation Recently, several parallel efforts have proposed approaches to automated RAG evaluation.In RAGAS [9], the authors query an LLM-judge (GPT-3.5)with a curated prompt to evaluate context relevance, answer relevance and faithfulness of a RAG response.Next, Saad-Falcon et al. [33] propose ARES, a framework for fine-tuning smaller NLI models to predict the same metrics.This approach benefits from fine-tuning, though domain-specific annotated validation sets are required for each domain adaptation.In parallel, Chen et al. [4] develop a heuristic system to probe LLM's robustness to noisy and irrelevant context documents, and Adlakha et al. [1] explore heuristic algorithms to estimate RAG correctness and faithfulness.The lack of established RAG benchmarks makes it difficult to compare these approaches against each other.We aim to address this limitation by introducing RAGBench.",
            "score": 0.13765904383196403,
            "section_title": "Related Work",
            "char_start_offset": 3658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 472
                },
                {
                    "start": 474,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 872
                },
                {
                    "start": 874,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1240
                },
                {
                    "start": 1242,
                    "end": 1345
                },
                {
                    "start": 1345,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1504
                },
                {
                    "start": 1504,
                    "end": 1623
                },
                {
                    "start": 1623,
                    "end": 1753
                },
                {
                    "start": 1753,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2080
                },
                {
                    "start": 2080,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "263134280"
                },
                {
                    "start": 1354,
                    "end": 1357,
                    "matchedPaperCorpusId": "263152733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32373046875
        },
        {
            "corpus_id": "267027552",
            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
            "text": "Fine-tuning, on the other hand, was found to be useful in teaching the model new skills specific to the agricultural domain, and providing more precise and succinct responses. However, the high initial cost due to the extensive work required to fine-tune the model on new data is a significant consideration. \u2022 Implications for potential uses of LLMs in different industries: this research serves as a pioneering step in establishing a pipeline for the application of RAG and fine-tuning techniques in LLMs, fostering innovation and collaboration across multiple industries. With our initial focus on agriculture, we demonstrated how these strategies can lead to more efficient models starting from the Q&A generation process. The insights gained",
            "score": 0.13756781487980752,
            "section_title": "Introduction",
            "char_start_offset": 6558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 746
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390380859375
        },
        {
            "corpus_id": "265128626",
            "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",
            "text": "Because the principal aim in preparing this paper was to compare LLM-improvement schemas available to non-expert users, we made a deliberate choice to use the most accessible fine-tuning and RAG frameworks, and doing so using the default settings where possible in order to establish a solid shared baseline for future research.",
            "score": 0.13752072041121477,
            "section_title": "Technical Background",
            "char_start_offset": 7651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306396484375
        },
        {
            "corpus_id": "269983737",
            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
            "score": 0.13747280483600285,
            "section_title": "Related Work",
            "char_start_offset": 2461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 890
                },
                {
                    "start": 890,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1656
                },
                {
                    "start": 1656,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 260,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 260,
                    "end": 282,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 723,
                    "end": 746,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "273812370",
            "title": "Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors",
            "text": "In this paper, we evaluate the feasibility of backdooring an LLM by poisoning its fine-tuning dataset, leading to a compromised LLM that, when used as a component in a RAG system, leaks the contents of another component-the knowledge database. We assume that the RAG's retriever and knowledge database are benign, while the LLM is the target of the backdoor attack. \n\nWe explore two key settings for this threat model: (ii) Using Off-the-shelf fine-tuned LLM: \n\nThe RAG system employs a third-party LLM that has already been fine-tuned for specific tasks, with no further fine-tuning required. (ii) Joint Fine-tuning: Recent studies have shown that joint fine-tuning LLMs on both public datasets and local documents enhances the model's ability to effectively leverage retrieved information. In this setting, the RAG owner selects a publicly available LLM (pre-trained on general tasks) and jointly fine-tunes it on a task-specific dataset alongside their own local documents. \n\nTo illustrate these settings, consider a healthcare provider aiming to build a medical questionanswering system (e.g., an AI doctor). In Setting (i), the provider may use a pre-finetuned medical QA LLM and integrate it with local medical content to deliver domain-specific answers without additional fine-tuning during inference. In Setting (ii), the provider could fine-tune an LLM on a public medical QA dataset combined with proprietary medical documents to improve the system's accuracy in responding to medical queries. In both cases, the RAG system is deployed to provide specialized responses to user queries. In our experiments, we primarily used Setting (I) as it is more commonly adopted, and we discuss Setting (II) in detail in Appendix A.3.",
            "score": 0.13742840940798676,
            "section_title": "Threat model",
            "char_start_offset": 11169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1732
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2239990234375
        },
        {
            "corpus_id": "270688478",
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "text": "In this section, we will explore the steps involved in training a large language model, including pretraining, instruction fine-tuning, and alignment.We will then discuss the fundamentals of RAG.",
            "score": 0.1374032500813722,
            "section_title": "Background",
            "char_start_offset": 6176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1026611328125
        },
        {
            "corpus_id": "254366552",
            "title": "iQuery: Instruments as Queries for Audio-Visual Sound Separation",
            "text": "Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. ber of new instrument classes for fine-tuning on MUSIC dataset. We hold out 1, 2, 4, and 6 instrument classes in the pre-training stage and fine-tune our method on these new classes with only the query embedding layer unfrozen. MUSIC dataset contains in total of 11 instruments. Notably, our method still yields good results when the network is only pre-trained on 5 instrument types, even fewer than the unseen classes. Fig. 7",
            "score": 0.1373547945181974,
            "section_title": "14681",
            "char_start_offset": 25500,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0032100677490234375
        },
        {
            "corpus_id": "266162355",
            "title": "Accurate Differential Operators for Hybrid Neural Fields",
            "text": "Surface Normals Mean Curvature Table 6: Fine-tuning using polynomial-fitting on Dense Grid. Full results for fine-tuning using polynomial-fitting over the FamousShape dataset [12]. \u03c3 denotes the hyperparameter value that obtained the best results.",
            "score": 0.13735060369719954,
            "section_title": "Shape",
            "char_start_offset": 36426,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 247
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "226298441"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01229095458984375
        },
        {
            "corpus_id": "268531923",
            "title": "HDLdebugger: Streamlining HDL debugging with Large Language Models",
            "text": "Our contributions are summarized as follows:\n\n\u2022 We introduce an advanced LLM-based HDL debugging framework supporting chip designs in the industry, namely HDLdebugger, which consists of buggy data generation, search engine, and retrieval-augmented LLM fine-tuning.\u2022 To address the scarcity of high-quality HDL debugging training data, we propose a data generation approach based on reverse engineeering to comprehensively generate diverse and realistic HDL buggy codes with the correct version.\u2022 We propose a search engine to create code RAG (resp.doc RAG)\n\nfor HDL buggy code (resp.relevant information) effectively and efficiently, enhancing the fine-tuning and inference of LLMs.\u2022 We present a novel retrieval-augmented fine-tuning approach for HDL debugging, which integrates self-guided thought generation with RAG-based fine-tuning strategies.\n\n\u2022 Extensive experiments on the HDL code dataset from Huawei demonstrate superior performance against 13 state-of-the-art baselines, including GPT4 and various HDL debugging LLMs.",
            "score": 0.1372942037720284,
            "section_title": "Table 1: Pilot Debugging Experiments",
            "char_start_offset": 4350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 46,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 548
                },
                {
                    "start": 548,
                    "end": 556
                },
                {
                    "start": 558,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 849
                },
                {
                    "start": 851,
                    "end": 1029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051361083984375
        },
        {
            "corpus_id": "266188262",
            "title": "VetLLM: Large Language Model for Predicting Diagnosis from Veterinary Notes.",
            "text": "A subset of 5,000 notes were randomly sampled from the CSU training split, and this subset was used for fine-tuning Alpaca-7B using low-rank adaption (LoRA) 20 and four A4000 GPUs. We chose LoRA as it generally provides superior performances and induces no extra inference overhead. The fine-tuning samples were generated using the same prompt template described To study the data efficiency of fine-tuning, the 5,000 notes subset was further sampled into 2000, 1000, 500 and 200 notes sequentially. Consequently, each subset of a smaller size is strictly a subset of the one of a larger size. And these subsets were each used as the fine-tuning set. In short, five fine-tuned Alpaca-7B models were trained.",
            "score": 0.13724918116332885,
            "section_title": "Finetuning",
            "char_start_offset": 9606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 707
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00763702392578125
        },
        {
            "corpus_id": "67922270",
            "title": "\u201cEquiheptatonic\u201d Tuning in Thai Classical Music: Strict Propriety and Step Sizes",
            "text": "Similarly, in the ideal 24-quartertone equal temperament of North African, Middle Eastern, and Turkish music (Ayari & McAdams, 2003;Marcus, 1993;Marcus, 1998Marcus, /2001)), the tuning of ra \u00afst is strictly proper, having as it does 1-step intervals whose successive sizes, in twenty-fourths of an ideal octave whose fundamental-frequency ratio is 2:1, are 4, 3, 3, 4, 4, 3, and 3: respectively, 200, 150, 150, 200, 200, 150, and 150 cents. A tuning that satisfied the constraints of the grid or ruler models discussed above would be strictly proper, for every 0-step interval would be smaller than every 1-step interval, which would be smaller than every 2-step interval, and so forth. Moreover, as ideal ra \u00afst and meantone diatonic tunings illustrate, a tuning might be strictly proper without the constraints on absolute step sizes that the grid and ruler models specify. Indeed, even the 8-degree, octave interval of size 2:1 in ideal ra \u00afst and meantone diatonic tunings is not requisite for a tuning to be an instance of strict propriety. In order to identify a particular model of Thai tunings as strictly proper without implying the grid and ruler models' constraints on absolute interval sizes, such a model will be termed simply a \"strict-propriety\" model or a \"strictlyproper\" model. \n\nAccording to a strictly-proper model, each pair of tones that spanned a particular number of steps would be compared with each pair of tones that spanned a different number of steps. Since the calibration error of both the upper tone and the lower tone in each interval would be +e, the largest measured fundamental-frequency ratio of one interval would have to be more than 4e smaller than the smallest measured fundamental-frequency ratio of any interval that spanned more steps in order to verify the model in a way that would avoid false positives for the distinction between an interval of n steps and an interval of n \u00fe 1 steps. That is, tone-pairs that would span n \u00bc 0, 1, 2, . . . steps would have as their fundamental-frequency ratios the magnitudes x 0 , x 1 , x 2 , . . .",
            "score": 0.13722966014418125,
            "section_title": "A Strict-Propriety Model",
            "char_start_offset": 16067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2081
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 145,
                    "matchedPaperCorpusId": "191374692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0014047622680664062
        },
        {
            "corpus_id": "236635259",
            "title": "DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models",
            "text": "As clarified before, each note/rest event is followed by a wait token which specifies the number of ticks between it and the succeeding event. In DadaGP, tick resolution uniformly corresponds to 960 ticks per quarter note. For a tempo of 100 bpm, a tick corresponds to 60/(100 * 960) = 0.000625 seconds. Referring to the excerpt in Figure 2, eighth note events are separated by wait:480 tokens, and sixteenth note ones by wait:240. A histogram with the most common durations in DadaGP is presented in Figure 4(c), in both token and standard staff notation formats, to ease visualization. \n\nUsually, in a GuitarPro file a default tempo is specified for the entire song, although it supports the inclusion of tempo changes throughout the piece. This is addressed by our encoder/decoder with the tokens tempo and bfx:tempo_change respectively, which affects note/rest duration. In Figure 4(b) and Figure 4(f) are presented plots corresponding to the most frequent tempos and tempo changes. \n\nThe encoder/decoder also supports the representation of measure repetitions with the measure:repeat token. Although time signatures are not tokenized, they are inferred by summing the wait tokens between the occurrences of new_measure. However, this method is insufficient to distinguish between 3/4 and 6/8 measures, for example. To circumvent this, for the plot presented in Figure 4(d) we leveraged PyGuitarPro functions to extract accurate information about the most prevalent time signatures for each measure in our dataset.",
            "score": 0.13722966014418125,
            "section_title": "Meter",
            "char_start_offset": 12865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0011653900146484375
        },
        {
            "corpus_id": "216438831",
            "title": "Automatic Tuning of High Piano Tones",
            "text": "With the results of the listening test, this analysis confirms that listeners prefer octaves where the beating is minimum. Thus, the first outcome of this paper is a simple rule to tune high piano tones, based on a conducted listening test. The second outcome is a method of measuring beating rates, which is proposed as an alternative method to test the success of a tuning algorithm. \n\nThis paper is organized as follows. Section 2 explains the preparations needed for the listening test, including recording of piano tones, selection of the tuning rules, and the generation of test signals. Section 3 describes the design of the listening test. Section 4 presents the listening test results. Section 5 proposes a signal analysis method to explain the audibility of mistuning in octaves, compares the various tuning rules in the light of signal analysis, and discusses why comparison using Railsback curves may not be a reliable method to compare different tunings. Section 6 concludes.",
            "score": 0.13722966014418125,
            "section_title": "Introduction",
            "char_start_offset": 4417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 988
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0029582977294921875
        },
        {
            "corpus_id": "276939617",
            "title": "Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey",
            "text": "Fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from the For example, HyPA-RAG fine-tunes its distilBERT model on legal corpora and CASEGPT adopts a fine-tuned version of Legal-BERT to achieve enhanced retrieval performance [39], [49]. In addition, CamemBERT is fully fine-tuned on the long-form LQA (LLeQA) dataset to improve its ability to handle complex legal queries [21]. Table 4 shows the best Transformers-based retrievers, of which four models are enhanced by fine-tuning processes.",
            "score": 0.1371084870438863,
            "section_title": "3) FINE-TUNING RETRIEVAL MODELS",
            "char_start_offset": 27999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 593
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "263310713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "269148560",
            "title": "nnU-Net Revisited: A Call for Rigorous Validation in 3D Medical Image Segmentation",
            "text": "\"SD\": Standard Deviation   6.Since STU-Net was presented as a model for transfer learning, we fine-tuned a STU-Net L network, that was pre-trained on the totalsegmentator dataset [35] for 4000 epochs, on the other datasets, analogous to the corresponding publication [20].Fine-tuning on BraTS did not converge using the default fine-tuning learning rate of 0.001.",
            "score": 0.13710109814412386,
            "section_title": "Conclusion",
            "char_start_offset": 20004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 29,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 363
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "260790662"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00763702392578125
        },
        {
            "corpus_id": "269457298",
            "title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning",
            "text": "Increased Gap\n\nFull Parameter Fine-Tuning Parameter-Efficient Fine-Tuning Heterogeneity signifies the diversity within the dataset, often leading to interference due to its varied content and style [2].Parameter-efficient approaches are particularly sensitive, suffering greater performance losses in heterogeneous cases.",
            "score": 0.1370061902421636,
            "section_title": "Performance",
            "char_start_offset": 5475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 15,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019683837890625
        },
        {
            "corpus_id": "271843111",
            "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
            "text": "The evaluation of a Large Language Model-based tutoring system for a University robotics course highlighted several insights into the application of advanced LLM techniques and the resulting performance in an educational setting. First, our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations [Shuster et al., 2021]. Furthermore, even though our human evaluation is currently restricted to two test subjects, their answers already point out that added references increases trustworthiness. Therefore, RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first. As further advantages, in our experience RAG is quite straight forward to realize, in particular in a course setting in which well-curated background material is readily available. Furthermore, from a teaching point of view a tutor should stick to PREPRINT the lecture material, e.g., when going over a concept the tutoring system should carefully choose examples and ideally stick-or at least start-with the ones provided in the lecture. This should positively affect the learning of students. \n\nFine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG. As an explanation, fine-tuning aims to specialize a model to a specific task and a specific type of interaction. A fine-tuned model might loose some of its general flexibility. As a consequence, when interacting very differently with the model, the model might produce worse results or even behave erratically.",
            "score": 0.13670514272343717,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 42602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 617,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.131591796875
        },
        {
            "corpus_id": "268681001",
            "title": "Towards a RAG-based summarization for the Electron Ion Collider",
            "text": "Fine-tuning an LLM involves refining its abilities and performance in specific tasks or domains by training it further in domain-specific datasets after pretraining, improving effectiveness without retraining the entire model [10,12].However, conventional fine-tuning requires substantial computational power and time, posing challenges, especially for extensive models such as GPT-3 (175-B parameters) [3] and Meta LLaMA2 (13-B parameters) [13].Despite the advent of Low-Rank Adaptation (LoRA) which enables efficient fine-tuning, even on consumer-grade GPUs, fine-tuning remains computationally intensive.Integrating strategies like in-context learning and chain-of-thought techniques [4] with a live knowledge repository forms the concept of Retrieval Augmented Generation (RAG), reducing hallucinations and anchoring the LLMs to reality",
            "score": 0.13664737674801797,
            "section_title": "Fine tuning of Large Language Models (LLMs):",
            "char_start_offset": 1071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 840
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "245537907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376220703125
        },
        {
            "corpus_id": "271050312",
            "title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
            "text": "The students need to fine-tune an LLM to enable it to generate Tang poems using supervised finetuning. The students learn how to adjust the hyperparameters when fine-tuning the LLM. After fine-tuning, the students generate some Tang poems based on the prefixes we give them. The task of the LLM TA in this assignment is to evaluate whether the format and content of the Tang poems are appropriate.",
            "score": 0.13651222672626662,
            "section_title": "B.3 HW4: Supervised Fine-tuning an LLM",
            "char_start_offset": 46137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0289306640625
        },
        {
            "corpus_id": "270210369",
            "title": "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation",
            "text": "Practical RAG Performance. One experiment is in RAG setting for short-form Q&A given retrieved texts with different qualities, it evaluates the robustness and performance of RAG. The other is RAG for many long-form text generation tasks including dialogue, code generation, slot filling, language modeling and long-form Q&A. Baselines include the methods that use additional modules to filter irrelevant texts (NLI+RAG (Yoran et al., 2024)) or as action triggers (CRAG (Yan et al., 2024)), fine-tune LLMs for robust RAG (RetRobust (Yoran et al., 2024) and INFO-RAG (Xu et al., 2024b)) and fine-tune LLMs to dynamically retrieve and critique retrieved texts (Self-RAG (Asai et al., 2023)). \n\nSetup for Benefit-Detriment Comparison Experiment. Given prefix x 1:i\u22121 and retrieved texts R, our motivation aims to build a theory to predict whether the positive impact of the retrieved texts R on x i (benefit) outweighs the potential risk of misleading LLM (detriment). This is a binary classification task at token-level. To evaluate this, we construct test data and ground-truth as: \n\n(1) For a sentence x, we truncate it at the i-th token to obtain the prefix x 1:i\u22121 and the next token x i . \n\n(2) Input prefix x 1:i\u22121 to LLM w/o RAG and LLM w/ RAG to get the predicted token a and b. \n\n(3) If b is x i but a not, it means LLM w/ RAG performs better than LLM w/o RAG, so the benefit is greater than detriment, the ground-truth label is 1. (4) If a is x i but b not, it means LLM w/o RAG performs better than LLM w/ RAG, so the benefit is lower than detriment, the ground-truth label is 0. We use the above method to traverse all sentences in the datasets to obtain prefix and its ground-truth as samples.",
            "score": 0.1364607471739231,
            "section_title": "EXPERIMENTAL DETAILS",
            "char_start_offset": 26628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1703
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383544921875
        },
        {
            "paperId": "659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
            "corpusId": 218869575,
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "venue": "Neural Information Processing Systems",
            "year": 2020,
            "referenceCount": 67,
            "citationCount": 6476,
            "influentialCitationCount": 662,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.11401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145222654",
                    "name": "Patrick Lewis"
                },
                {
                    "authorId": "3439053",
                    "name": "Ethan Perez"
                },
                {
                    "authorId": "1716179427",
                    "name": "Aleksandara Piktus"
                },
                {
                    "authorId": "40052301",
                    "name": "F. Petroni"
                },
                {
                    "authorId": "2067091563",
                    "name": "Vladimir Karpukhin"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "103131985",
                    "name": "Heinrich Kuttler"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "144105277",
                    "name": "Wen-tau Yih"
                },
                {
                    "authorId": "2620211",
                    "name": "Tim Rockt\u00e4schel"
                },
                {
                    "authorId": "48662861",
                    "name": "Sebastian Riedel"
                },
                {
                    "authorId": "1743722",
                    "name": "Douwe Kiela"
                }
            ],
            "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
            "corpus_id": "218869575",
            "text": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2047119140625
        },
        {
            "paperId": "3bfd1ea55479eed2cda35a56c6e6bd09b7487463",
            "corpusId": 277822352,
            "title": "Fine-Tuning Large Language Model (LLM) to Answer Basic Questions for Prospective New Students at Syiah Kuala University Using the Retrieval-Augmented Generation (RAG) Method",
            "venue": "International Conference on Intelligent Computing",
            "year": 2024,
            "referenceCount": 22,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIC64337.2024.10956296?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIC64337.2024.10956296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2355623025",
                    "name": "Hary Rachmat"
                },
                {
                    "authorId": "2242951973",
                    "name": "Hammam Riza"
                },
                {
                    "authorId": "2179491140",
                    "name": "T. F. Abidin"
                }
            ],
            "abstract": "USK Mistral 7B is a large language model designed to answer basic admission questions at Universitas Syiah Kuala (USK). The model was fine-tuned using the open-source model of Mistral 7B using collected data from admissions and lectures at the university. The QLoRA and RAG techniques were used to train the model and retrieve relevant information from external data sources. The results were evaluated using the ROUGE score. Responses were generated with a score of >0.5 on ten out of 46 questions with the RAG method, and testing with the fine-tuning method was carried out on 20 questions and resulted in responses with a score of 1.0 from all questions asked. The performance of USK Mistral 7B shows its potential as an effective tool in helping students querying information about admission and lectures at USK.",
            "corpus_id": "277822352",
            "text": "USK Mistral 7B is a large language model designed to answer basic admission questions at Universitas Syiah Kuala (USK). The model was fine-tuned using the open-source model of Mistral 7B using collected data from admissions and lectures at the university. The QLoRA and RAG techniques were used to train the model and retrieve relevant information from external data sources. The results were evaluated using the ROUGE score. Responses were generated with a score of >0.5 on ten out of 46 questions with the RAG method, and testing with the fine-tuning method was carried out on 20 questions and resulted in responses with a score of 1.0 from all questions asked. The performance of USK Mistral 7B shows its potential as an effective tool in helping students querying information about admission and lectures at USK.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.205810546875
        },
        {
            "paperId": "80478de9c7a81561e2f3dac9b8b1ef3df389ff2d",
            "corpusId": 270878612,
            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 98,
            "citationCount": 74,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2259265562",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "2253664013",
                    "name": "Wei Ping"
                },
                {
                    "authorId": "2256582287",
                    "name": "Zihan Liu"
                },
                {
                    "authorId": "2256656241",
                    "name": "Boxin Wang"
                },
                {
                    "authorId": "2287859963",
                    "name": "Jiaxuan You"
                },
                {
                    "authorId": "2256776233",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "1911755",
                    "name": "M. Shoeybi"
                },
                {
                    "authorId": "2264406909",
                    "name": "Bryan Catanzaro"
                }
            ],
            "abstract": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
            "corpus_id": "270878612",
            "text": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5283203125
        },
        {
            "paperId": "b442c46df7878a07190b22a482c65c038ee943d1",
            "corpusId": 276929344,
            "title": "TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 48,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349538058",
                    "name": "Kanghui Ning"
                },
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "2350433940",
                    "name": "Yu Liu"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2349739182",
                    "name": "James Y. Zhang"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2253908414",
                    "name": "Lintao Ma"
                },
                {
                    "authorId": "2774914",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "2276324326",
                    "name": "Dongjin Song"
                }
            ],
            "abstract": "Large Language Models (LLMs) and Foundation Models (FMs) have recently become prevalent for time series forecasting tasks. While fine-tuning LLMs enables domain adaptation, they often struggle to generalize across diverse and unseen datasets. Moreover, existing Time Series Foundation Models (TSFMs) still face challenges in handling non-stationary dynamics and distribution shifts, largely due to the lack of effective mechanisms for adaptation. To this end, we present TS-RAG, a retrieval-augmented generation framework for time series forecasting that enhances the generalization and interpretability of TSFMs. Specifically, TS-RAG leverages pre-trained time series encoders to retrieve semantically relevant segments from a dedicated knowledge base, enriching the contextual representation of the input query. Furthermore, we propose an Adaptive Retrieval Mixer (ARM) module that dynamically fuses the retrieved patterns with the TSFM's internal representation, improving forecasting accuracy without requiring task-specific fine-tuning. Thorough empirical studies on seven public benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot forecasting performance, outperforming the existing TSFMs by up to 6.84% across diverse domains while also providing desirable interpretability.",
            "corpus_id": "276929344",
            "text": "Large Language Models (LLMs) and Foundation Models (FMs) have recently become prevalent for time series forecasting tasks. While fine-tuning LLMs enables domain adaptation, they often struggle to generalize across diverse and unseen datasets. Moreover, existing Time Series Foundation Models (TSFMs) still face challenges in handling non-stationary dynamics and distribution shifts, largely due to the lack of effective mechanisms for adaptation. To this end, we present TS-RAG, a retrieval-augmented generation framework for time series forecasting that enhances the generalization and interpretability of TSFMs. Specifically, TS-RAG leverages pre-trained time series encoders to retrieve semantically relevant segments from a dedicated knowledge base, enriching the contextual representation of the input query. Furthermore, we propose an Adaptive Retrieval Mixer (ARM) module that dynamically fuses the retrieved patterns with the TSFM's internal representation, improving forecasting accuracy without requiring task-specific fine-tuning. Thorough empirical studies on seven public benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot forecasting performance, outperforming the existing TSFMs by up to 6.84% across diverse domains while also providing desirable interpretability.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0263519287109375
        },
        {
            "paperId": "e8a9b5a32836af499dcb9ccdee1008638309a6cc",
            "corpusId": 271909559,
            "title": "InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "referenceCount": 123,
            "citationCount": 29,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "122835755",
                    "name": "Zhepei Wei"
                },
                {
                    "authorId": "2316519239",
                    "name": "Wei-Lin Chen"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) has shown promising potential to enhance the accuracy and factuality of language models (LMs). However, imperfect retrievers or noisy corpora can introduce misleading or even erroneous information to the retrieved contents, posing a significant challenge to the generation quality. Existing RAG methods typically address this challenge by directly predicting final answers despite potentially noisy inputs, resulting in an implicit denoising process that is difficult to interpret and verify. On the other hand, the acquisition of explicit denoising supervision is often costly, involving significant human efforts. In this work, we propose InstructRAG, where LMs explicitly learn the denoising process through self-synthesized rationales -- First, we instruct the LM to explain how the ground-truth answer is derived from retrieved documents. Then, these rationales can be used either as demonstrations for in-context learning of explicit denoising or as supervised fine-tuning data to train the model. Compared to standard RAG approaches, InstructRAG requires no additional supervision, allows for easier verification of the predicted answers, and effectively improves generation accuracy. Experiments show InstructRAG consistently outperforms existing RAG methods in both training-free and trainable scenarios, achieving a relative improvement of 8.3% over the best baseline method on average across five knowledge-intensive benchmarks. Extensive analysis indicates that InstructRAG scales well with increased numbers of retrieved documents and consistently exhibits robust denoising ability even in out-of-domain datasets, demonstrating strong generalizability.",
            "corpus_id": "271909559",
            "text": "Retrieval-augmented generation (RAG) has shown promising potential to enhance the accuracy and factuality of language models (LMs). However, imperfect retrievers or noisy corpora can introduce misleading or even erroneous information to the retrieved contents, posing a significant challenge to the generation quality. Existing RAG methods typically address this challenge by directly predicting final answers despite potentially noisy inputs, resulting in an implicit denoising process that is difficult to interpret and verify. On the other hand, the acquisition of explicit denoising supervision is often costly, involving significant human efforts. In this work, we propose InstructRAG, where LMs explicitly learn the denoising process through self-synthesized rationales -- First, we instruct the LM to explain how the ground-truth answer is derived from retrieved documents. Then, these rationales can be used either as demonstrations for in-context learning of explicit denoising or as supervised fine-tuning data to train the model. Compared to standard RAG approaches, InstructRAG requires no additional supervision, allows for easier verification of the predicted answers, and effectively improves generation accuracy. Experiments show InstructRAG consistently outperforms existing RAG methods in both training-free and trainable scenarios, achieving a relative improvement of 8.3% over the best baseline method on average across five knowledge-intensive benchmarks. Extensive analysis indicates that InstructRAG scales well with increased numbers of retrieved documents and consistently exhibits robust denoising ability even in out-of-domain datasets, demonstrating strong generalizability.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.4248046875
        },
        {
            "paperId": "3d51b9cf8d23fd038dac0ae7ade4fe27f1c03e9d",
            "corpusId": 270620428,
            "title": "InstructRAG: Instructing Retrieval-Augmented Generation with Explicit Denoising",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 99,
            "citationCount": 22,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2406.13629?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2406.13629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "122835755",
                    "name": "Zhepei Wei"
                },
                {
                    "authorId": "2128184431",
                    "name": "Wei-Lin Chen"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) has shown promising potential to enhance the accuracy and factuality of language models (LMs). However, imperfect retrievers or noisy corpora can introduce misleading or even erroneous information to the retrieved contents, posing a significant challenge to the generation quality. Existing RAG methods typically address this challenge by directly predicting final answers despite potentially noisy inputs, resulting in an implicit denoising process that is difficult to interpret and verify. On the other hand, the acquisition of explicit denoising supervision is often costly, involving significant human efforts. In this work, we propose I NSTRUCT RAG, where LMs explicitly learn the denoising process through self-synthesized rationales \u2014 First, we instruct the LM to explain how the ground-truth answer is derived from retrieved documents. Then, these rationales can be used either as demonstrations for in-context learning of explicit denoising or as supervised fine-tuning data to train the model. Compared to standard RAG approaches, I NSTRUCT RAG requires no additional supervision, allows for easier verification of the predicted answers, and effectively improves generation accuracy. Experiments show I NSTRUCT RAG consistently outperforms existing RAG meth-ods in both training-free and trainable scenarios, achieving a relative improvement of 8.3% over the best baseline method on average across five knowledge-intensive benchmarks. Extensive analysis indicates that I NSTRUCT RAG scales well with increased numbers of retrieved documents and consistently exhibits robust denoising ability even in out-of-domain datasets, demonstrating strong generalizability. 1",
            "corpus_id": "270620428",
            "text": "Retrieval-augmented generation (RAG) has shown promising potential to enhance the accuracy and factuality of language models (LMs). However, imperfect retrievers or noisy corpora can introduce misleading or even erroneous information to the retrieved contents, posing a significant challenge to the generation quality. Existing RAG methods typically address this challenge by directly predicting final answers despite potentially noisy inputs, resulting in an implicit denoising process that is difficult to interpret and verify. On the other hand, the acquisition of explicit denoising supervision is often costly, involving significant human efforts. In this work, we propose I NSTRUCT RAG, where LMs explicitly learn the denoising process through self-synthesized rationales \u2014 First, we instruct the LM to explain how the ground-truth answer is derived from retrieved documents. Then, these rationales can be used either as demonstrations for in-context learning of explicit denoising or as supervised fine-tuning data to train the model. Compared to standard RAG approaches, I NSTRUCT RAG requires no additional supervision, allows for easier verification of the predicted answers, and effectively improves generation accuracy. Experiments show I NSTRUCT RAG consistently outperforms existing RAG meth-ods in both training-free and trainable scenarios, achieving a relative improvement of 8.3% over the best baseline method on average across five knowledge-intensive benchmarks. Extensive analysis indicates that I NSTRUCT RAG scales well with increased numbers of retrieved documents and consistently exhibits robust denoising ability even in out-of-domain datasets, demonstrating strong generalizability. 1",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.3408203125
        },
        {
            "paperId": "7047d94171efc72f868339302d966b51122fe6a1",
            "corpusId": 269282528,
            "title": "Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 14,
            "citationCount": 8,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.12560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297667617",
                    "name": "Dayton G. Thorpe"
                },
                {
                    "authorId": "2297667553",
                    "name": "Andrew Duberstein"
                },
                {
                    "authorId": "2297667296",
                    "name": "Ian A. Kinsey"
                }
            ],
            "abstract": "The current state-of-the-art (SOTA) for automated text-to-SQL still falls well short of expert human performance as measured by execution accuracy (EX) on the BIRD-SQL benchmark. The most accurate methods are also slow and expensive. To advance the SOTA for text-to-SQL while reducing cost and improving speed, we explore the combination of low-cost fine tuning, novel methods for diverse retrieval-augmented generation (RAG) and new input and output formats that help large language models (LLMs) achieve higher EX. We introduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record for EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher performance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI, but uses the low-cost GPT-3.5 Turbo while exceeding the performance of the next-best model using OpenAI, which instead uses the more expensive GPT-4. Dubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by over 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push EX higher.",
            "corpus_id": "269282528",
            "text": "The current state-of-the-art (SOTA) for automated text-to-SQL still falls well short of expert human performance as measured by execution accuracy (EX) on the BIRD-SQL benchmark. The most accurate methods are also slow and expensive. To advance the SOTA for text-to-SQL while reducing cost and improving speed, we explore the combination of low-cost fine tuning, novel methods for diverse retrieval-augmented generation (RAG) and new input and output formats that help large language models (LLMs) achieve higher EX. We introduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record for EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher performance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI, but uses the low-cost GPT-3.5 Turbo while exceeding the performance of the next-best model using OpenAI, which instead uses the more expensive GPT-4. Dubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by over 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push EX higher.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06842041015625
        },
        {
            "paperId": "446c8f9343bc835e677667fbaccb0cfc2badf58b",
            "corpusId": 273901598,
            "title": "RAG-HAT: A Hallucination-Aware Tuning Pipeline for LLM in Retrieval-Augmented Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 7,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-industry.113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277244244",
                    "name": "Juntong Song"
                },
                {
                    "authorId": "2303049750",
                    "name": "Xingguang Wang"
                },
                {
                    "authorId": "2277249776",
                    "name": "Juno Zhu"
                },
                {
                    "authorId": "2277251434",
                    "name": "Yuanhao Wu"
                },
                {
                    "authorId": "2302812421",
                    "name": "Xuxin Cheng"
                },
                {
                    "authorId": "2277247396",
                    "name": "Randy Zhong"
                },
                {
                    "authorId": "2277247851",
                    "name": "Cheng Niu"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) has emerged as a significant advancement in the field of large language models (LLMs). By integrating up-to-date information not available during their initial training, RAG greatly enhances the practical utility of LLMs in real-world applications. However, even with RAG, LLMs can still produce inaccurate outputs, such as distorting or misinterpreting source content, posing risks in high-trust scenarios. To address these issues, we introduce a novel approach called Hallucination Aware Tuning (HAT). This method involves training hallucination detection models that generate detection labels and provide detailed descriptions of the detected hallucinations. Utilizing these detection results\u2014particularly the hallucination descriptions\u2014GPT-4 Turbo is employed to correct any detected hallucinations. The corrected outputs, free of hallucinations, along with the original versions, are used to create a preference dataset for Direct Preference Optimization (DPO) training. The fine-tuning through DPO leads to LLMs that exhibit a reduced rate of hallucinations and deliver improved answer quality.",
            "corpus_id": "273901598",
            "text": "Retrieval-augmented generation (RAG) has emerged as a significant advancement in the field of large language models (LLMs). By integrating up-to-date information not available during their initial training, RAG greatly enhances the practical utility of LLMs in real-world applications. However, even with RAG, LLMs can still produce inaccurate outputs, such as distorting or misinterpreting source content, posing risks in high-trust scenarios. To address these issues, we introduce a novel approach called Hallucination Aware Tuning (HAT). This method involves training hallucination detection models that generate detection labels and provide detailed descriptions of the detected hallucinations. Utilizing these detection results\u2014particularly the hallucination descriptions\u2014GPT-4 Turbo is employed to correct any detected hallucinations. The corrected outputs, free of hallucinations, along with the original versions, are used to create a preference dataset for Direct Preference Optimization (DPO) training. The fine-tuning through DPO leads to LLMs that exhibit a reduced rate of hallucinations and deliver improved answer quality.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11859130859375
        },
        {
            "paperId": "339d2a56f0e5176b691c358a86891e2923045c8c",
            "corpusId": 272827955,
            "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 192,
            "citationCount": 42,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268432582",
                    "name": "Siyun Zhao"
                },
                {
                    "authorId": "2125051198",
                    "name": "Yuqing Yang"
                },
                {
                    "authorId": "2294387070",
                    "name": "Zilong Wang"
                },
                {
                    "authorId": "2260609693",
                    "name": "Zhiyuan He"
                },
                {
                    "authorId": "2180993402",
                    "name": "Luna K. Qiu"
                },
                {
                    "authorId": "2259937079",
                    "name": "Lili Qiu"
                }
            ],
            "abstract": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
            "corpus_id": "272827955",
            "text": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.383544921875
        },
        {
            "paperId": "5c538b02e876fd39f8a60907a792556f73fb74b0",
            "corpusId": 271329372,
            "title": "Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA",
            "venue": "International Conference on Computer Aided Design",
            "year": 2024,
            "referenceCount": 44,
            "citationCount": 12,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220363044",
                    "name": "Yuan Pu"
                },
                {
                    "authorId": "8822971",
                    "name": "Zhuolun He"
                },
                {
                    "authorId": "2312323764",
                    "name": "Tairu Qiu"
                },
                {
                    "authorId": "2275801674",
                    "name": "Haoyuan Wu"
                },
                {
                    "authorId": "2257320153",
                    "name": "Bei Yu"
                }
            ],
            "abstract": "Retrieval augmented generation (RAG) enhances the accuracy and reliability of generative AI models by sourcing factual information from external databases, which is extensively employed in document-grounded question-answering (QA) tasks. Off-the-shelf RAG flows are well pretrained on general-purpose documents, yet they encounter significant challenges when being applied to knowledge-intensive vertical domains, such as electronic design automation (EDA). This paper addresses such issue by proposing a customized RAG framework along with three domain-specific techniques for EDA tool documentation QA, including a contrastive learning scheme for text embedding model fine-tuning, a reranker distilled from proprietary LLM, and a generative LLM fine-tuned with high-quality domain corpus. Furthermore, we have developed and released a documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced RTL-to-GDSII design platform. Experimental results demonstrate that our proposed RAG flow and techniques have achieved superior performance on ORD-QA as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA benchmark and the training dataset for our customized RAG flow are open-source at https://github.com/lesliepy99/RAG-EDA.",
            "corpus_id": "271329372",
            "text": "Retrieval augmented generation (RAG) enhances the accuracy and reliability of generative AI models by sourcing factual information from external databases, which is extensively employed in document-grounded question-answering (QA) tasks. Off-the-shelf RAG flows are well pretrained on general-purpose documents, yet they encounter significant challenges when being applied to knowledge-intensive vertical domains, such as electronic design automation (EDA). This paper addresses such issue by proposing a customized RAG framework along with three domain-specific techniques for EDA tool documentation QA, including a contrastive learning scheme for text embedding model fine-tuning, a reranker distilled from proprietary LLM, and a generative LLM fine-tuned with high-quality domain corpus. Furthermore, we have developed and released a documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced RTL-to-GDSII design platform. Experimental results demonstrate that our proposed RAG flow and techniques have achieved superior performance on ORD-QA as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA benchmark and the training dataset for our customized RAG flow are open-source at https://github.com/lesliepy99/RAG-EDA.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.155517578125
        },
        {
            "paperId": "0d42d42b31c47b17ec5fff15c9ac61151d54eb99",
            "corpusId": 276674022,
            "title": "Building Custom Large Language Models for Industries: A Comparative Analysis of Fine-Tuning and Retrieval-Augmented Generation Techniques",
            "venue": "2024 International Conference of Adisutjipto on Aerospace Electrical Engineering and Informatics (ICAAEEI)",
            "year": 2024,
            "referenceCount": 8,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAAEEI63658.2024.10899129?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAAEEI63658.2024.10899129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347736698",
                    "name": "Varsha Shukla"
                },
                {
                    "authorId": "2347736198",
                    "name": "Geoffrey G Parker"
                }
            ],
            "abstract": "In this study, we delve into the development and optimization of custom Large Language Models (LLMs) tailored for specific industrial applications, concentrating on two predominant adaptation techniques: fine-tuning and Retrieval-Augmented Generation (RAG). We employ a rigorous, mathematically-driven approach to compare the efficacy of these methodologies, using a dual dataset derived from TACIT Company, which specializes in the tacit knowledge of retiring workforce documentation, and Sight Machine, which focuses on manufacturing process data. Our evaluation framework uses three metrics: Accuracy, calculated as the ratio of correctly predicted instances to total predictions; Adaptability, which measures the model's performance stability when exposed to novel, evolving datasets; and Industry Relevance, assessed through expert-based qualitative scoring converted to quantitative analysis. This comprehensive comparative analysis not only quantifies the performance differences between finetuning and RAG but also explores the computational dynamics and potential synergies when implementing these models. Additionally, we address the ethical considerations and practical implications of deploying LLMs in sensitive industries, framing these technologies within the context of operational integrity and data governance. The outcome of this research aims to guide stakeholders in selecting and optimizing LLM strategies that align with specific industrial needs, balancing technical efficiency, and ethical standards.",
            "corpus_id": "276674022",
            "text": "In this study, we delve into the development and optimization of custom Large Language Models (LLMs) tailored for specific industrial applications, concentrating on two predominant adaptation techniques: fine-tuning and Retrieval-Augmented Generation (RAG). We employ a rigorous, mathematically-driven approach to compare the efficacy of these methodologies, using a dual dataset derived from TACIT Company, which specializes in the tacit knowledge of retiring workforce documentation, and Sight Machine, which focuses on manufacturing process data. Our evaluation framework uses three metrics: Accuracy, calculated as the ratio of correctly predicted instances to total predictions; Adaptability, which measures the model's performance stability when exposed to novel, evolving datasets; and Industry Relevance, assessed through expert-based qualitative scoring converted to quantitative analysis. This comprehensive comparative analysis not only quantifies the performance differences between finetuning and RAG but also explores the computational dynamics and potential synergies when implementing these models. Additionally, we address the ethical considerations and practical implications of deploying LLMs in sensitive industries, framing these technologies within the context of operational integrity and data governance. The outcome of this research aims to guide stakeholders in selecting and optimizing LLM strategies that align with specific industrial needs, balancing technical efficiency, and ethical standards.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1356201171875
        },
        {
            "paperId": "5606901a2fd18257c6a000f5268b94e8d188ab57",
            "corpusId": 274152702,
            "title": "A Framework for automated selective Fine-Tuning of Domain-Specific Large Language Models Using Graph-Based Retrieval Augmented Generation",
            "venue": "Ubiquitous Computing, Electronics & Mobile Communication Conference",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/UEMCON62879.2024.10754778?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/UEMCON62879.2024.10754778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331525442",
                    "name": "Hariharan Govindharajan"
                },
                {
                    "authorId": "2331526623",
                    "name": "Senthilkumar Vijayakumar"
                }
            ],
            "abstract": "Graph based retrieval augmented generation technique in Large Language Model (LLM) brings in major advantages by providing deep context to LLMs through relational knowledge graph for text generation, classification, question and answering use cases. However, maintaining vast data volume of domain specific data in a knowledge graph with complex relationships and querying from it every time a prompt is being posted to LLM, is a time consuming and expensive process. This paper presents a novel framework for selectively fine-tuning domain-specific large language models (LLMs) using a multi-stage Knowledge Graph (KG) based Retrieval Augmented Generation (RAG) pipeline and an Automated Incremental Fine-tuning System (AIFS). The proposed system aims to enhance the accuracy and relevance of LLM responses for text generation and Question Answering use cases by finetuning the LLM incrementally based on highly sought and highly relevant information in knowledge graph identified by leveraging page rank algorithm in KG. The framework comprises three major subsystems: Knowledge Graph Generation, Automated Incremental fine-tuning system (AIFS), and Domain Based Information Retrieval (DBIR). The effectiveness of the system is demonstrated through its ability to incrementally fine-tune LLMs based on selected highly relevant nodes within the KG, thereby improving the model\u2019s domain-specific knowledge, response accuracy by 90% and reduce cost by 71.8%.",
            "corpus_id": "274152702",
            "text": "Graph based retrieval augmented generation technique in Large Language Model (LLM) brings in major advantages by providing deep context to LLMs through relational knowledge graph for text generation, classification, question and answering use cases. However, maintaining vast data volume of domain specific data in a knowledge graph with complex relationships and querying from it every time a prompt is being posted to LLM, is a time consuming and expensive process. This paper presents a novel framework for selectively fine-tuning domain-specific large language models (LLMs) using a multi-stage Knowledge Graph (KG) based Retrieval Augmented Generation (RAG) pipeline and an Automated Incremental Fine-tuning System (AIFS). The proposed system aims to enhance the accuracy and relevance of LLM responses for text generation and Question Answering use cases by finetuning the LLM incrementally based on highly sought and highly relevant information in knowledge graph identified by leveraging page rank algorithm in KG. The framework comprises three major subsystems: Knowledge Graph Generation, Automated Incremental fine-tuning system (AIFS), and Domain Based Information Retrieval (DBIR). The effectiveness of the system is demonstrated through its ability to incrementally fine-tune LLMs based on selected highly relevant nodes within the KG, thereby improving the model\u2019s domain-specific knowledge, response accuracy by 90% and reduce cost by 71.8%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0894775390625
        },
        {
            "paperId": "71b45d00f0239eb098462f2a8d94e41b9d2040bc",
            "corpusId": 275993994,
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275628230",
                    "name": "Yiteng Tu"
                },
                {
                    "authorId": "2147219374",
                    "name": "Weihang Su"
                },
                {
                    "authorId": "2290870875",
                    "name": "Yujia Zhou"
                },
                {
                    "authorId": "2260835922",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2256982003",
                    "name": "Qingyao Ai"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "corpus_id": "275993994",
            "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5341796875
        },
        {
            "paperId": "64ee29d6ddb2c2167a201783ddd4d0a9b744f352",
            "corpusId": 270764659,
            "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
            "venue": "The Web Conference",
            "year": 2024,
            "referenceCount": 121,
            "citationCount": 13,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.18676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51490462",
                    "name": "Guanting Dong"
                },
                {
                    "authorId": "1900406",
                    "name": "Yutao Zhu"
                },
                {
                    "authorId": "2279440154",
                    "name": "Chenghao Zhang"
                },
                {
                    "authorId": "26423877",
                    "name": "Zechen Wang"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2260701602",
                    "name": "Ji-Rong Wen"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) has effectively mitigated the hallucination problem of large language models (LLMs). However, the difficulty of aligning the retriever with the LLMs' diverse knowledge preferences inevitably poses a challenge in developing a reliable RAG system. To address this issue, we propose DPA-RAG, a universal framework designed to align diverse knowledge preferences within RAG systems. Specifically, we initially introduce a preference knowledge construction pipeline and incorporate five novel query augmentation strategies to alleviate preference data scarcity. Based on preference data, DPA-RAG accomplishes both external and internal preference alignment: 1) It jointly integrates pairwise, pointwise, and contrastive preference alignment abilities into the reranker, achieving external preference alignment among RAG components. 2) It further introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT), enabling LLMs to implicitly capture knowledge aligned with their reasoning preferences, achieving LLMs' internal alignment. Experimental results across four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all baselines and seamlessly integrates both black-box and open-sourced LLM readers. Further qualitative analysis and discussions provide empirical guidance for achieving reliable RAG systems. Our code and example dataset are available at https://github.com/dongguanting/DPA-RAG.",
            "corpus_id": "270764659",
            "text": "Retrieval-augmented generation (RAG) has effectively mitigated the hallucination problem of large language models (LLMs). However, the difficulty of aligning the retriever with the LLMs' diverse knowledge preferences inevitably poses a challenge in developing a reliable RAG system. To address this issue, we propose DPA-RAG, a universal framework designed to align diverse knowledge preferences within RAG systems. Specifically, we initially introduce a preference knowledge construction pipeline and incorporate five novel query augmentation strategies to alleviate preference data scarcity. Based on preference data, DPA-RAG accomplishes both external and internal preference alignment: 1) It jointly integrates pairwise, pointwise, and contrastive preference alignment abilities into the reranker, achieving external preference alignment among RAG components. 2) It further introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT), enabling LLMs to implicitly capture knowledge aligned with their reasoning preferences, achieving LLMs' internal alignment. Experimental results across four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all baselines and seamlessly integrates both black-box and open-sourced LLM readers. Further qualitative analysis and discussions provide empirical guidance for achieving reliable RAG systems. Our code and example dataset are available at https://github.com/dongguanting/DPA-RAG.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.271728515625
        },
        {
            "paperId": "ff173ea5f3c63127a2c8fb4cbb98dc6ab4f3696c",
            "corpusId": 271533837,
            "title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation",
            "venue": "IEEE Conference on High Performance Extreme Computing",
            "year": 2024,
            "referenceCount": 21,
            "citationCount": 12,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.19619, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "38065704",
                    "name": "Manish Bhattarai"
                },
                {
                    "authorId": "2249081311",
                    "name": "Javier E. Santos"
                },
                {
                    "authorId": "2313694479",
                    "name": "Shawn Jones"
                },
                {
                    "authorId": "2313634281",
                    "name": "Ayan Biswas"
                },
                {
                    "authorId": "2025666",
                    "name": "Boian Alexandrov"
                },
                {
                    "authorId": "2228335537",
                    "name": "Dan O\u2019Malley"
                }
            ],
            "abstract": "The advent of large language models (LLMs) has revolutionized the field of code translation, enabling automated translation between programming languages. Despite these advancements, the accuracy and reliability of these models often falter in complex translation tasks due to a lack of contextual understanding. This paper introduces a novel approach to enhance code translation through Few-Shot Learning augmented with retrieval-based techniques. By leveraging a repository of existing code translations, we dynamically retrieve the most relevant examples to guide the model in translating new code segments. Our method, based on Retrieval-Augmented Generation (RAG), significantly improves translation quality by providing contextual examples that the model can learn from in real-time. We chose RAG over traditional fine-tuning methods due to its ability to leverage existing codebases or a locally stored corpus of code, allowing it to dynamically adapt to diverse translation tasks without the need for extensive retraining. Extensive experiments on diverse datasets, using open LLM models such as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code Instruct, and Mixtral-8\u00d722B, and commercial LLM models such as GPT-3.5 turbo, and GPT-4o demonstrate the superiority of our approach over traditional zero-shot, particularly in translating between Fortran and C++. We also explored different numbers of shots (examples provided to the model during inference) \u2013 specifically 1, 2, and 3 shots \u2013 and various embedding models for RAG, including Nomic-Embed, Starencoder, and CodeBERT, to evaluate the robustness and effectiveness of our approach.",
            "corpus_id": "271533837",
            "text": "The advent of large language models (LLMs) has revolutionized the field of code translation, enabling automated translation between programming languages. Despite these advancements, the accuracy and reliability of these models often falter in complex translation tasks due to a lack of contextual understanding. This paper introduces a novel approach to enhance code translation through Few-Shot Learning augmented with retrieval-based techniques. By leveraging a repository of existing code translations, we dynamically retrieve the most relevant examples to guide the model in translating new code segments. Our method, based on Retrieval-Augmented Generation (RAG), significantly improves translation quality by providing contextual examples that the model can learn from in real-time. We chose RAG over traditional fine-tuning methods due to its ability to leverage existing codebases or a locally stored corpus of code, allowing it to dynamically adapt to diverse translation tasks without the need for extensive retraining. Extensive experiments on diverse datasets, using open LLM models such as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code Instruct, and Mixtral-8\u00d722B, and commercial LLM models such as GPT-3.5 turbo, and GPT-4o demonstrate the superiority of our approach over traditional zero-shot, particularly in translating between Fortran and C++. We also explored different numbers of shots (examples provided to the model during inference) \u2013 specifically 1, 2, and 3 shots \u2013 and various embedding models for RAG, including Nomic-Embed, Starencoder, and CodeBERT, to evaluate the robustness and effectiveness of our approach.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.09075927734375
        },
        {
            "paperId": "09022e0e75fa472e9a1f6b743223c016a5ec35e2",
            "corpusId": 273566432,
            "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications",
            "venue": "Machine Learning and Knowledge Extraction",
            "year": 2024,
            "referenceCount": 11,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/make6040116?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/make6040116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327432431",
                    "name": "Arunabh Bora"
                },
                {
                    "authorId": "1806041",
                    "name": "H. Cuay\u00e1huitl"
                }
            ],
            "abstract": "Artificial Intelligence (AI) has the potential to revolutionise the medical and healthcare sectors. AI and related technologies could significantly address some supply-and-demand challenges in the healthcare system, such as medical AI assistants, chatbots and robots. This paper focuses on tailoring LLMs to medical data utilising a Retrieval-Augmented Generation (RAG) database to evaluate their performance in a computationally resource-constrained environment. Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning. Open-source LLMs (Flan-T5-Large, LLaMA-2-7B, and Mistral-7B) are fine-tuned using the medical datasets Meadow-MedQA and MedMCQA. Experiments are reported for response generation and multiple-choice question answering. The latter uses two distinct methodologies: Type A, as standard question answering via direct choice selection; and Type B, as language generation and probability confidence score generation of choices available. Results in the medical domain revealed that Fine-tuning and RAG are crucial for improved performance, and that methodology Type A outperforms Type B.",
            "corpus_id": "273566432",
            "text": "Artificial Intelligence (AI) has the potential to revolutionise the medical and healthcare sectors. AI and related technologies could significantly address some supply-and-demand challenges in the healthcare system, such as medical AI assistants, chatbots and robots. This paper focuses on tailoring LLMs to medical data utilising a Retrieval-Augmented Generation (RAG) database to evaluate their performance in a computationally resource-constrained environment. Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning. Open-source LLMs (Flan-T5-Large, LLaMA-2-7B, and Mistral-7B) are fine-tuned using the medical datasets Meadow-MedQA and MedMCQA. Experiments are reported for response generation and multiple-choice question answering. The latter uses two distinct methodologies: Type A, as standard question answering via direct choice selection; and Type B, as language generation and probability confidence score generation of choices available. Results in the medical domain revealed that Fine-tuning and RAG are crucial for improved performance, and that methodology Type A outperforms Type B.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1981201171875
        }
    ],
    "quotes": {
        "cost": 0.150717,
        "quotes": [
            {
                "idx": 0,
                "key": "[265128626 | Dodgson et al. | 2023 | Citations: 8]",
                "snippets": "Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[266359151 | Gao et al. | 2023 | Citations: 1819]",
                "snippets": "Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-toend joint training-and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Hybrid Approaches",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 632,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 65
                            },
                            {
                                "start": 66,
                                "end": 301
                            },
                            {
                                "start": 302,
                                "end": 420
                            },
                            {
                                "start": 421,
                                "end": 632
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-toend joint training-and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[267412954 | Zhang et al. | 2024 | Citations: 20]",
                "snippets": "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[267750557 | Zhang et al. | 2024 | Citations: 3]",
                "snippets": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning...We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "quote": "We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[268819923 | Eibich et al. | 2024 | Citations: 4]",
                "snippets": "Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 2,
                        "end": 505,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 250
                            },
                            {
                                "start": 250,
                                "end": 384
                            },
                            {
                                "start": 384,
                                "end": 505
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[269214364 | Nguyen et al. | 2024 | Citations: 4]",
                "snippets": "Using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\nFine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Key Findings",
                        "pdf_hash": "",
                        "start": 203,
                        "end": 570,
                        "sentence_offsets": [
                            {
                                "start": 66,
                                "end": 231
                            },
                            {
                                "start": 231,
                                "end": 359
                            },
                            {
                                "start": 361,
                                "end": 463
                            },
                            {
                                "start": 463,
                                "end": 615
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\nFine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[269983737 | Jiao et al. | 2024 | Citations: 1]",
                "snippets": "Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1190,
                        "end": 2094,
                        "sentence_offsets": [
                            {
                                "start": 1190,
                                "end": 1340
                            },
                            {
                                "start": 1340,
                                "end": 1487
                            },
                            {
                                "start": 1487,
                                "end": 1656
                            },
                            {
                                "start": 1656,
                                "end": 1848
                            },
                            {
                                "start": 1848,
                                "end": 2094
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270560495 | Barnett et al. | 2024 | Citations: 2]",
                "snippets": "This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270870251 | Wang et al. | 2024 | Citations: 61]",
                "snippets": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258865283 | Luo et al. | 2023 | Citations: 27]": "Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking."
                },
                "metadata": [
                    {
                        "section_title": "Retriever and Generator Fine-tuning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 551,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 94
                            },
                            {
                                "start": 94,
                                "end": 242
                            },
                            {
                                "start": 242,
                                "end": 345
                            },
                            {
                                "start": 345,
                                "end": 551
                            }
                        ],
                        "ref_mentions": [
                            "258865283"
                        ],
                        "quote": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[270878612 | Yu et al. | 2024 | Citations: 74]",
                "snippets": "In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[271571401 | Gao et al. | 2024 | Citations: 20]",
                "snippets": "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning...3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "E. Tuning Pattern",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 411,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 67
                            },
                            {
                                "start": 68,
                                "end": 142
                            },
                            {
                                "start": 143,
                                "end": 267
                            },
                            {
                                "start": 268,
                                "end": 529
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning"
                    },
                    {
                        "quote": "3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[271710111 | Fleischer et al. | 2024 | Citations: 6]",
                "snippets": "We explore several techniques for RAG augmentation, and use RAG FOUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and a CoT scheme which guides the model to use the retrieved context, explain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To complement CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RAG Augmentation Techniques",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 921,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 119
                            },
                            {
                                "start": 120,
                                "end": 286
                            },
                            {
                                "start": 287,
                                "end": 553
                            },
                            {
                                "start": 554,
                                "end": 605
                            },
                            {
                                "start": 606,
                                "end": 671
                            },
                            {
                                "start": 672,
                                "end": 921
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We explore several techniques for RAG augmentation, and use RAG FOUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and a CoT scheme which guides the model to use the retrieved context, explain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To complement CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[271903789 | Mombaerts et al. | 2024 | Citations: 2]",
                "snippets": "Our methodology is cost-effective, costing less than $20 per 2000 research papers using Claude 3 Haiku, and can be adapted with any fine-tuning of either the language or embedding models to further enhance the performance of end-to-end RAG pipelines.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Our methodology is cost-effective, costing less than $20 per 2000 research papers using Claude 3 Haiku, and can be adapted with any fine-tuning of either the language or embedding models to further enhance the performance of end-to-end RAG pipelines.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[272911196 | Chung et al. | 2024 | Citations: 0]",
                "snippets": "Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[273186680 | Nguyen et al. | 2024 | Citations: 5]",
                "snippets": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[273323657 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "Fine-tuning is widely used to improve the capabilities of LLM-augmented components in RAG systems (de Luis Balaguer et al., 2024). Early methods of fine-tuning to enhance LLM-based components in RAG primarily focused on training the retriever and the generator (Ke et al., 2024)(Lin et al., 2023). Recent RAG methods have shifted toward modular designs (Gao et al., 2023b). Particularly in complex QA tasks, adaptive RAG often requires base models to follow intricate instructions (Yin et al., 2023a;Xu et al., 2024) to enable the functionality of diverse components (Asai et al., 2024). Classic alignment training methods include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). However, SFT lacks negative feedback and is prone to overfitting. Recently, Rafailov et al. proposed a more efficient reinforcement learning algorithm, direct preference optimization (DPO), which aligns response preferences and enhances the model's instruction-following ability by learning the differences between positive and negative sample pairs. In our work, we focus on using DPO to enhance the model's capability in multiple processes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263605962 | Lin et al. | 2023 | Citations: 153]": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.",
                    "[266998785 | Ke et al. | 2024 | Citations: 35]": "Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-\"friendly\"information and assembling a LLM-\"friendly\"context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks."
                },
                "metadata": [
                    {
                        "section_title": "Fine-Tuning for RAG",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1156,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 130
                            },
                            {
                                "start": 131,
                                "end": 296
                            },
                            {
                                "start": 297,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 586
                            },
                            {
                                "start": 587,
                                "end": 713
                            },
                            {
                                "start": 714,
                                "end": 779
                            },
                            {
                                "start": 780,
                                "end": 1064
                            },
                            {
                                "start": 1065,
                                "end": 1156
                            }
                        ],
                        "ref_mentions": [
                            "266998785",
                            "263605962"
                        ],
                        "quote": "Fine-tuning is widely used to improve the capabilities of LLM-augmented components in RAG systems (de Luis Balaguer et al., 2024). Early methods of fine-tuning to enhance LLM-based components in RAG primarily focused on training the retriever and the generator (Ke et al., 2024)(Lin et al., 2023). Recent RAG methods have shifted toward modular designs (Gao et al., 2023b). Particularly in complex QA tasks, adaptive RAG often requires base models to follow intricate instructions (Yin et al., 2023a;Xu et al., 2024) to enable the functionality of diverse components (Asai et al., 2024). Classic alignment training methods include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). However, SFT lacks negative feedback and is prone to overfitting. Recently, Rafailov et al. proposed a more efficient reinforcement learning algorithm, direct preference optimization (DPO), which aligns response preferences and enhances the model's instruction-following ability by learning the differences between positive and negative sample pairs. In our work, we focus on using DPO to enhance the model's capability in multiple processes."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[273350575 | Zhai | 2024 | Citations: 0]",
                "snippets": "Table 2 (top) demonstrates that fine-tuning the retrieval model R or the VLM M improves multimodal RAG performance. However, finetuning M provides more substantial gains. This suggests that the baseline retrieval model R is already sufficiently optimized, leading to smaller performance improvements from finetuning. In contrast, optimizing M directly enhances the model's ability to process multimodal data. Notably, the fine-tuned RAG(R * + M * ) shows slightly lower performance than MuRAG, despite differences in the volume of training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Performance Comparison with Baselines:",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 545,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 115
                            },
                            {
                                "start": 116,
                                "end": 170
                            },
                            {
                                "start": 171,
                                "end": 316
                            },
                            {
                                "start": 317,
                                "end": 408
                            },
                            {
                                "start": 409,
                                "end": 545
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Table 2 (top) demonstrates that fine-tuning the retrieval model R or the VLM M improves multimodal RAG performance. However, finetuning M provides more substantial gains. This suggests that the baseline retrieval model R is already sufficiently optimized, leading to smaller performance improvements from finetuning. In contrast, optimizing M directly enhances the model's ability to process multimodal data. Notably, the fine-tuned RAG(R * + M * ) shows slightly lower performance than MuRAG, despite differences in the volume of training data."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[273532096 | Zhao et al. | 2024 | Citations: 14]",
                "snippets": "Fine-tuning has gradually become a popular strategy (Ke et al., 2024) for enhancing the capabilities of components of RAG. Existing works include fine-tuning retrieval-related components to achieve better retrieval outcomes (Yan et al., 2024), fine-tuning generators for more personalized outputs (Zhang et al., 2024b), and employing collaborative fine-tuning (Lin et al., 2023). Additionally, (Zhou et al., 2023) discovered that fine-tuning LLMs with a limited quantity of high-quality data significantly enhances the performance of LLMs. This finding provides a robust theoretical basis for collaboratively fine-tuning multiple components within advanced RAG methodologies at a minimal data expense.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258822910 | Zhou et al. | 2023 | Citations: 850]": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                },
                "metadata": [
                    {
                        "section_title": "Domain-Specific Fine-Tuning for RAG",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 700,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 122
                            },
                            {
                                "start": 123,
                                "end": 379
                            },
                            {
                                "start": 380,
                                "end": 538
                            },
                            {
                                "start": 539,
                                "end": 700
                            }
                        ],
                        "ref_mentions": [
                            "258822910"
                        ],
                        "quote": "Fine-tuning has gradually become a popular strategy (Ke et al., 2024) for enhancing the capabilities of components of RAG. Existing works include fine-tuning retrieval-related components to achieve better retrieval outcomes (Yan et al., 2024), fine-tuning generators for more personalized outputs (Zhang et al., 2024b), and employing collaborative fine-tuning (Lin et al., 2023). Additionally, (Zhou et al., 2023) discovered that fine-tuning LLMs with a limited quantity of high-quality data significantly enhances the performance of LLMs. This finding provides a robust theoretical basis for collaboratively fine-tuning multiple components within advanced RAG methodologies at a minimal data expense."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273532207 | Rakin et al. | 2024 | Citations: 3]",
                "snippets": "RAG-Finetuned-QA : The RAG-Finetuned-QA model builds upon the baseline RAG-Original architecture by incorporating domain-specific fine-tuning on our hotel domain dataset. Like the RAG-Original model, it utilizes a Hotel Domain Knowledge Base, with dense vector representations indexed using the FAISS library. The loss function is designed to fine-tune both the generator and question encoder collectively. We have used ray as the distributed retriever with 4 retrieval workers as described in the original RAG paper. The model was trained for 20 epochs while the final checkpoint has been selected based on the highest validation accuracy. The purpose of this experiment was to assess the improvements gained from domain-specific training, comparing its performance against the baseline RAG-Original model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RAG-Original :",
                        "pdf_hash": "",
                        "start": 372,
                        "end": 1179,
                        "sentence_offsets": [
                            {
                                "start": 372,
                                "end": 542
                            },
                            {
                                "start": 543,
                                "end": 681
                            },
                            {
                                "start": 682,
                                "end": 778
                            },
                            {
                                "start": 779,
                                "end": 889
                            },
                            {
                                "start": 890,
                                "end": 1012
                            },
                            {
                                "start": 1013,
                                "end": 1179
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RAG-Finetuned-QA : The RAG-Finetuned-QA model builds upon the baseline RAG-Original architecture by incorporating domain-specific fine-tuning on our hotel domain dataset. Like the RAG-Original model, it utilizes a Hotel Domain Knowledge Base, with dense vector representations indexed using the FAISS library. The loss function is designed to fine-tune both the generator and question encoder collectively. We have used ray as the distributed retriever with 4 retrieval workers as described in the original RAG paper. The model was trained for 20 epochs while the final checkpoint has been selected based on the highest validation accuracy. The purpose of this experiment was to assess the improvements gained from domain-specific training, comparing its performance against the baseline RAG-Original model."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[275993994 | Tu et al. | 2025 | Citations: 6]",
                "snippets": "To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[276580741 | Wu et al. | 2025 | Citations: 2]",
                "snippets": "To further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023)(Shi et al., 2023)(Izacard et al., 2020). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2023) to jointly optimize the modules in a RAG system, aligning their data preferences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[227746078 | Izacard et al. | 2020 | Citations: 267]": "The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results.",
                    "[256389797 | Shi et al. | 2023 | Citations: 641]": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.",
                    "[258959321 | Rafailov et al. | 2023 | Citations: 4159]": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
                    "[258960666 | Yu et al. | 2023 | Citations: 69]": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM\u2019s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 683,
                        "end": 1405,
                        "sentence_offsets": [
                            {
                                "start": 683,
                                "end": 816
                            },
                            {
                                "start": 817,
                                "end": 1038
                            },
                            {
                                "start": 1039,
                                "end": 1227
                            },
                            {
                                "start": 1228,
                                "end": 1405
                            }
                        ],
                        "ref_mentions": [
                            "258960666",
                            "256389797",
                            "227746078",
                            "258959321"
                        ],
                        "quote": "To further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023)(Shi et al., 2023)(Izacard et al., 2020). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2023) to jointly optimize the modules in a RAG system, aligning their data preferences."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[276939617 | Hindi et al. | 2025 | Citations: 2]",
                "snippets": "Fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from the For example, HyPA-RAG fine-tunes its distilBERT model on legal corpora and CASEGPT adopts a fine-tuned version of Legal-BERT to achieve enhanced retrieval performance [39], [49]. In addition, CamemBERT is fully fine-tuned on the long-form LQA (LLeQA) dataset to improve its ability to handle complex legal queries (Louis et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263310713 | Louis et al. | 2023 | Citations: 40]": "Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a \"retrieve-then-read\" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising performance on automatic evaluation metrics, but a qualitative analysis uncovers areas for refinement. As one of the only comprehensive, expert-annotated long-form LQA dataset, LLeQA has the potential to not only accelerate research towards resolving a significant real-world issue, but also act as a rigorous benchmark for evaluating NLP models in specialized domains. We publicly release our code, data, and models."
                },
                "metadata": [
                    {
                        "section_title": "3) FINE-TUNING RETRIEVAL MODELS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 479,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 338
                            },
                            {
                                "start": 339,
                                "end": 479
                            }
                        ],
                        "ref_mentions": [
                            "263310713"
                        ],
                        "quote": "Fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from the For example, HyPA-RAG fine-tunes its distilBERT model on legal corpora and CASEGPT adopts a fine-tuned version of Legal-BERT to achieve enhanced retrieval performance [39], [49]. In addition, CamemBERT is fully fine-tuned on the long-form LQA (LLeQA) dataset to improve its ability to handle complex legal queries (Louis et al., 2023)."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[277113527 | Kim et al. | 2025 | Citations: 3]",
                "snippets": "Despite the effectiveness of contrastive learning in embedders Lu et al. (2024), there remains a notable gap in the literature regarding the impact of embedder fine-tuning on RAG systems, particularly within the finance domain Setty et al. (2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 1612,
                        "end": 1859,
                        "sentence_offsets": [
                            {
                                "start": 1612,
                                "end": 1859
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite the effectiveness of contrastive learning in embedders Lu et al. (2024), there remains a notable gap in the literature regarding the impact of embedder fine-tuning on RAG systems, particularly within the finance domain Setty et al. (2024)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[277150553 | Wei et al. | 2025 | Citations: 0]",
                "snippets": "In this paper, we validate RAG's fine-grained retrieval abilities and the global abstraction strengths of LLM-native solutions. However, RAG lacks holistic understanding, and long-context models tend to lose key information over extended contexts. We integrate these strengths of both RAG and LLM-native solutions by fine-tuning an LLM within an RAG framework for data generation. This work is the first to explore LLM and RAG integration within a unified framework, bridging open-domain and domain-specific query-answering tasks. Our RAG-Tuned LLM, equipped with LLMnative memory, outperforms both standard RAG methods and long-context LLMs across diverse datasets, demonstrating superior performance in handling hierarchical queries.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion and Future Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 735,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 127
                            },
                            {
                                "start": 128,
                                "end": 247
                            },
                            {
                                "start": 248,
                                "end": 380
                            },
                            {
                                "start": 381,
                                "end": 530
                            },
                            {
                                "start": 531,
                                "end": 735
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this paper, we validate RAG's fine-grained retrieval abilities and the global abstraction strengths of LLM-native solutions. However, RAG lacks holistic understanding, and long-context models tend to lose key information over extended contexts. We integrate these strengths of both RAG and LLM-native solutions by fine-tuning an LLM within an RAG framework for data generation. This work is the first to explore LLM and RAG integration within a unified framework, bridging open-domain and domain-specific query-answering tasks. Our RAG-Tuned LLM, equipped with LLMnative memory, outperforms both standard RAG methods and long-context LLMs across diverse datasets, demonstrating superior performance in handling hierarchical queries."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[277510500 | Misrahi et al. | 2025 | Citations: 0]",
                "snippets": "Recent works have demonstrated that it is beneficial to finetune an LLM to encourage it to better use retrieved context [21,22,32,37]. It has been shown to improve RAG performances in-domain, and we investigate whether this holds for out-of-domain RAG generalization. To do so, we run supervised fine-tuning on the MultiQA dataset9 : a dataset consisting of 450k general domain questions and answers, described in Appendix Table 5. Its associated document collection consists of Wikipedia [30] and MSMARCO [2] documents. Each supervised fine-tuning sample then consists of a prompt (taken from [32]) with 5 retrieved documents, the question and its answer. Models are trained with LoRA [12].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM Finetuning for RAG",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 691,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 134
                            },
                            {
                                "start": 135,
                                "end": 267
                            },
                            {
                                "start": 268,
                                "end": 431
                            },
                            {
                                "start": 432,
                                "end": 520
                            },
                            {
                                "start": 521,
                                "end": 656
                            },
                            {
                                "start": 657,
                                "end": 691
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent works have demonstrated that it is beneficial to finetune an LLM to encourage it to better use retrieved context [21,22,32,37]. It has been shown to improve RAG performances in-domain, and we investigate whether this holds for out-of-domain RAG generalization. To do so, we run supervised fine-tuning on the MultiQA dataset9 : a dataset consisting of 450k general domain questions and answers, described in Appendix Table 5. Its associated document collection consists of Wikipedia [30] and MSMARCO [2] documents. Each supervised fine-tuning sample then consists of a prompt (taken from [32]) with 5 retrieved documents, the question and its answer. Models are trained with LoRA [12]."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[277994112 | Gao et al. | 2025 | Citations: 7]",
                "snippets": "For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy...Self-RAG (Asai et al., 2023) fine-tune models for precise special token generation...O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA (Dettmers et al., 2023) quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258841328 | Dettmers et al. | 2023 | Citations: 2606]": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
                    "[264288947 | Asai et al. | 2023 | Citations: 780]": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models."
                },
                "metadata": [
                    {
                        "section_title": "5.2.2",
                        "pdf_hash": "",
                        "start": 359,
                        "end": 1118,
                        "sentence_offsets": [
                            {
                                "start": 359,
                                "end": 542
                            },
                            {
                                "start": 543,
                                "end": 722
                            },
                            {
                                "start": 723,
                                "end": 913
                            },
                            {
                                "start": 914,
                                "end": 1119
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy"
                    },
                    {
                        "section_title": "5.2.2",
                        "pdf_hash": "",
                        "start": 1174,
                        "end": 1241,
                        "sentence_offsets": [
                            {
                                "start": 1122,
                                "end": 1241
                            }
                        ],
                        "ref_mentions": [
                            "264288947"
                        ],
                        "quote": "Self-RAG (Asai et al., 2023) fine-tune models for precise special token generation"
                    },
                    {
                        "section_title": "5.2.2",
                        "pdf_hash": "",
                        "start": 1681,
                        "end": 2066,
                        "sentence_offsets": [
                            {
                                "start": 1667,
                                "end": 2065
                            }
                        ],
                        "ref_mentions": [
                            "258841328"
                        ],
                        "quote": "O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA (Dettmers et al., 2023) quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[278394304 | Aqib et al. | 2025 | Citations: 0]",
                "snippets": "Fine-tuning is another technique for adapting pre-trained LLMs to new tasks and reducing hallucinations by continuing training on new data [31]. Fine-tuning involves training pre-trained models on task-specific datasets, which helps in making the model capable enough to perform better on the desired task. It has been found that fine-tuning the LLM in an RAG system can help improve its performance, as the LLM is responsible for text generation utilizing the retrieved information (Rangan et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268033066 | Rangan et al. | 2024 | Citations: 12]": "This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases. This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs. Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model\u2019s continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative \u201cAI Judge\u201d mechanism to enhance the precision of result selection, further refining the system\u2019s accuracy. Accompanied by an executive diagram and a detailed algorithm for fine-tuning QLoRA, our work provides a comprehensive framework for implementing these advancements within chatbot technologies. This research contributes significant insights into LLM optimization for specific uses and heralds new directions for further development in retrieval-augmented models. Through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems. We make the dataset, the data processing package huggify-data, the model, and the app publicly available for the community."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 921,
                        "end": 1409,
                        "sentence_offsets": [
                            {
                                "start": 921,
                                "end": 1065
                            },
                            {
                                "start": 1066,
                                "end": 1227
                            },
                            {
                                "start": 1228,
                                "end": 1409
                            }
                        ],
                        "ref_mentions": [
                            "268033066"
                        ],
                        "quote": "Fine-tuning is another technique for adapting pre-trained LLMs to new tasks and reducing hallucinations by continuing training on new data [31]. Fine-tuning involves training pre-trained models on task-specific datasets, which helps in making the model capable enough to perform better on the desired task. It has been found that fine-tuning the LLM in an RAG system can help improve its performance, as the LLM is responsible for text generation utilizing the retrieved information (Rangan et al., 2024)."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[278714952 | Lee et al. | 2025 | Citations: 0]",
                "snippets": "We propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model.\n\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model.\n\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.027552,
        "cot": "Looking at the user query \"anyone fine tuning RAG or work this,\" I interpret this as asking about current work and approaches in fine-tuning Retrieval Augmented Generation (RAG) systems. The quotes contain extensive information about different aspects of fine-tuning RAG systems, including methods, benefits, and specific implementations.\n\nFor the dimensions, I'll organize the content into logical sections that provide a comprehensive overview of RAG fine-tuning:\n\n1. \"Introduction to RAG and Fine-tuning\" - This will be a synthesis paragraph providing background on what RAG is and how fine-tuning is broadly applied to it. This sets the foundation for understanding the rest of the content.\n\n2. \"Types of RAG Fine-tuning Approaches\" - This will be a synthesis paragraph that categorizes the main approaches to fine-tuning in RAG systems, such as retriever fine-tuning, generator fine-tuning, and dual/joint fine-tuning.\n\n3. \"Specific RAG Fine-tuning Techniques and Implementations\" - This will be a list format showcasing specific techniques, models, and implementations mentioned in the quotes.\n\n4. \"Benefits and Performance Improvements from Fine-tuning RAG\" - This will be a synthesis paragraph summarizing the reported benefits and performance improvements gained through fine-tuning RAG systems.\n\n5. \"Domain-Specific RAG Fine-tuning\" - This will be a synthesis paragraph covering how fine-tuning is applied to RAG systems in specific domains like finance, legal, etc.\n\n6. \"Challenges and Future Directions in RAG Fine-tuning\" - This will be a synthesis paragraph highlighting current challenges and future research directions in the field.\n\nThis organization allows for a comprehensive overview of the topic while ensuring all quotes are covered. It follows a logical flow from introduction to specific implementations to benefits to specialized applications to future work.",
        "plan": {
            "Introduction to RAG and Fine-tuning (synthesis)": [
                0,
                2,
                3,
                7
            ],
            "Types of RAG Fine-tuning Approaches (synthesis)": [
                1,
                8,
                10,
                15,
                17,
                20
            ],
            "Specific RAG Fine-tuning Techniques and Implementations (list)": [
                6,
                9,
                11,
                13,
                14,
                18,
                19,
                23,
                24,
                25,
                27
            ],
            "Benefits and Performance Improvements from Fine-tuning RAG (synthesis)": [
                2,
                3,
                5,
                12,
                16
            ],
            "Domain-Specific RAG Fine-tuning (synthesis)": [
                4,
                21,
                22
            ],
            "Challenges and Future Directions in RAG Fine-tuning (synthesis)": [
                4,
                26
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to RAG and Fine-tuning",
                "tldr": "Retrieval Augmented Generation (RAG) enhances LLM performance by integrating external knowledge retrieval with text generation. Fine-tuning RAG components has emerged as a critical approach to improve accuracy and reduce hallucinations, especially in domain-specific applications. (4 sources)",
                "text": "\nRetrieval Augmented Generation (RAG) has emerged as a powerful approach for enhancing the capabilities of Large Language Models (LLMs) by combining information retrieval with text generation. This technique allows LLMs to access and leverage external knowledge sources when generating responses, which helps to improve accuracy and reduce hallucinations. The combination of fine-tuning LLMs with RAG processes has been demonstrated to generate responses with significantly improved accuracy <Paper corpusId=\"267412954\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nOne of the primary motivations for fine-tuning RAG systems is to address the limitations of standard pre-trained embedding models, which may exhibit sub-optimal performance when applied to specific domain knowledge. Fine-tuning the retrieval components enables these systems to better capture domain-specific semantics and improve overall performance <Paper corpusId=\"267750557\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. This is particularly important as RAG's promise of delivering optimal responses often falls short in complex query scenarios, necessitating specialized approaches to enhance performance across multiple domains <Paper corpusId=\"270560495\" paperTitle=\"(Barnett et al., 2024)\" isShortName></Paper>.\n\nWhile research into methods for improving LLM performance through fine-tuning, RAG, and soft-prompting has expanded rapidly, many of these approaches have tended to focus on highly technical or high-cost techniques. This has made many of the newly discovered approaches comparatively inaccessible to non-technical users <Paper corpusId=\"265128626\" paperTitle=\"(Dodgson et al., 2023)\" isShortName></Paper>. Consequently, there is growing interest in developing more accessible fine-tuning methods for RAG systems that can be implemented by a wider range of users while still delivering substantial performance improvements.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."
                        ],
                        "paper": {
                            "corpus_id": 267412954,
                            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
                            "authors": [
                                {
                                    "authorId": "2279813822",
                                    "name": "Liang Zhang"
                                },
                                {
                                    "authorId": "2279831793",
                                    "name": "Katherine Jijo"
                                },
                                {
                                    "authorId": "2282528163",
                                    "name": "Spurthi Setty"
                                },
                                {
                                    "authorId": "2279830841",
                                    "name": "Eden Chung"
                                },
                                {
                                    "authorId": "2282539958",
                                    "name": "Fatima Javid"
                                },
                                {
                                    "authorId": "2279830757",
                                    "name": "Natan Vidra"
                                },
                                {
                                    "authorId": "2279838243",
                                    "name": "Thomas Clifford"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.7958984375
                    },
                    {
                        "id": "(Zhang et al._1, 2024)",
                        "snippets": [
                            "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning",
                            "We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model."
                        ],
                        "paper": {
                            "corpus_id": 267750557,
                            "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2108795448",
                                    "name": "Mingtian Zhang"
                                },
                                {
                                    "authorId": "2284682723",
                                    "name": "Shawn Lan"
                                },
                                {
                                    "authorId": "2067492948",
                                    "name": "Peter Hayes"
                                },
                                {
                                    "authorId": "2282542157",
                                    "name": "David Barber"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7607421875
                    },
                    {
                        "id": "(Barnett et al., 2024)",
                        "snippets": [
                            "This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains."
                        ],
                        "paper": {
                            "corpus_id": 270560495,
                            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2279752649",
                                    "name": "Scott Barnett"
                                },
                                {
                                    "authorId": "2279020735",
                                    "name": "Zach Brannelly"
                                },
                                {
                                    "authorId": "2266469333",
                                    "name": "Stefanus Kurniawan"
                                },
                                {
                                    "authorId": "2307101480",
                                    "name": "Sheng Wong"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.689453125
                    },
                    {
                        "id": "(Dodgson et al., 2023)",
                        "snippets": [
                            "Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users."
                        ],
                        "paper": {
                            "corpus_id": 265128626,
                            "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",
                            "authors": [
                                {
                                    "authorId": "2266237514",
                                    "name": "Jennifer Dodgson"
                                },
                                {
                                    "authorId": "2266281858",
                                    "name": "Nanzheng Lin"
                                },
                                {
                                    "authorId": "2266237540",
                                    "name": "Julian Peh"
                                },
                                {
                                    "authorId": "2266241068",
                                    "name": "Akira Rafhael Janson Pattirane"
                                },
                                {
                                    "authorId": "2047388958",
                                    "name": "Alfath Daryl Alhajir"
                                },
                                {
                                    "authorId": "2266238002",
                                    "name": "Eko Ridho Dinarto"
                                },
                                {
                                    "authorId": "2266365094",
                                    "name": "Joseph Lim"
                                },
                                {
                                    "authorId": "2266346433",
                                    "name": "Syed Danyal Ahmad"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.53759765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types of RAG Fine-tuning Approaches",
                "tldr": "RAG fine-tuning can target the retriever component, the generator component, or both simultaneously through dual fine-tuning approaches, each addressing different aspects of the RAG pipeline. These approaches range from optimizing retriever-generator alignment to enhancing domain-specific performance through specialized training techniques. (12 sources)",
                "text": "\nFine-tuning strategies for RAG systems can be broadly categorized into three main approaches: retriever fine-tuning, generator fine-tuning, and dual fine-tuning that optimizes both components simultaneously <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\n**Retriever Fine-tuning** focuses on enhancing the retrieval component to return more relevant and helpful documents for the generator. This approach aims to align the retriever's output with the generator's needs, essentially teaching the retriever to find information that will be most beneficial for the LLM to generate accurate responses <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Methods such as REPLUG demonstrate how retrieval models can be tuned based on feedback from language models, creating a more cohesive information pipeline <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. Similarly, Augmentation-Adapted Retriever (AAR) learns language model preferences to improve zero-shot generalization across various target LLMs <Paper corpusId=\"258960666\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\n**Generator Fine-tuning** involves optimizing the LLM to better utilize and incorporate retrieved context. Research like SAIL (Search-Augmented Instruction Learning) grounds language generation on complex search results, teaching models to select trustworthy information and perform multi-hop reasoning when working with retrieved passages <Paper corpusId=\"258865283\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This approach ensures that generators can produce more faithful and robust content based on the retrieved information <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n**Dual Fine-tuning** represents a more holistic approach where both the retriever and generator are fine-tuned simultaneously to enhance overall system performance <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. A notable implementation is Retrieval-Augmented Dual Instruction Tuning (RA-DIT), which updates the LLM to maximize the likelihood of correct answers given retrieval-augmented instructions while simultaneously optimizing the retriever to align with LLM preferences <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. Despite the increased complexity and integration challenges, this approach has demonstrated significant performance improvements across knowledge-intensive tasks <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nRecent advancements have introduced more sophisticated fine-tuning techniques like Direct Preference Optimization (DPO), which enhances models' instruction-following capabilities by learning from positive and negative sample pairs <Paper corpusId=\"273323657\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"258959321\" paperTitle=\"(Rafailov et al., 2023)\" isShortName></Paper>. Additionally, research by Zhou et al. has revealed that fine-tuning LLMs with even limited high-quality data can significantly enhance performance, providing a foundation for efficiently fine-tuning multiple RAG components with minimal data <Paper corpusId=\"273532096\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\nAs RAG systems evolve toward more modular designs, fine-tuning approaches are expanding to include specialized components and training objectives. Some methods focus on aligning information needs between retrievers and generators through optimization based on generation model feedback <Paper corpusId=\"276580741\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. Others, like CRAG, train lightweight retrieval evaluators to assess document quality and trigger different retrieval actions based on confidence levels <Paper corpusId=\"266359151\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>. These evolving approaches demonstrate how RAG fine-tuning continues to integrate with broader LLM technologies to create more effective knowledge-enhanced systems.",
                "citations": [
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning",
                            "3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference."
                        ],
                        "paper": {
                            "corpus_id": 271571401,
                            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.7880859375
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."
                        ],
                        "paper": {
                            "corpus_id": 270870251,
                            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2273537815",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "2308276345",
                                    "name": "Zhenghua Wang"
                                },
                                {
                                    "authorId": "2292070745",
                                    "name": "Xuan Gao"
                                },
                                {
                                    "authorId": "2308226671",
                                    "name": "Feiran Zhang"
                                },
                                {
                                    "authorId": "2308043953",
                                    "name": "Yixin Wu"
                                },
                                {
                                    "authorId": "2308044030",
                                    "name": "Zhibo Xu"
                                },
                                {
                                    "authorId": "2308036711",
                                    "name": "Tianyuan Shi"
                                },
                                {
                                    "authorId": "2309182278",
                                    "name": "Zhengyuan Wang"
                                },
                                {
                                    "authorId": "2309656885",
                                    "name": "Shizheng Li"
                                },
                                {
                                    "authorId": "2309176521",
                                    "name": "Qi Qian"
                                },
                                {
                                    "authorId": "2292032843",
                                    "name": "Ruicheng Yin"
                                },
                                {
                                    "authorId": "2220896023",
                                    "name": "Changze Lv"
                                },
                                {
                                    "authorId": "2257315404",
                                    "name": "Xiaoqing Zheng"
                                },
                                {
                                    "authorId": "2257129987",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 61
                        },
                        "score": 0.80517578125
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG."
                        ],
                        "paper": {
                            "corpus_id": 256389797,
                            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                            "authors": [
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "19168196",
                                    "name": "Michihiro Yasunaga"
                                },
                                {
                                    "authorId": "4418074",
                                    "name": "Minjoon Seo"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 641
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM\u2019s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever."
                        ],
                        "paper": {
                            "corpus_id": 258960666,
                            "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
                            "authors": [
                                {
                                    "authorId": "2275526493",
                                    "name": "Zichun Yu"
                                },
                                {
                                    "authorId": "2139787803",
                                    "name": "Chenyan Xiong"
                                },
                                {
                                    "authorId": "150311558",
                                    "name": "S. Yu"
                                },
                                {
                                    "authorId": "2109232579",
                                    "name": "Zhiyuan Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 69
                        },
                        "score": 0
                    },
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking."
                        ],
                        "paper": {
                            "corpus_id": 258865283,
                            "title": "SAIL: Search-Augmented Instruction Learning",
                            "authors": [
                                {
                                    "authorId": "1944274",
                                    "name": "Hongyin Luo"
                                },
                                {
                                    "authorId": "2475831",
                                    "name": "Yung-Sung Chuang"
                                },
                                {
                                    "authorId": "145802952",
                                    "name": "Yuan Gong"
                                },
                                {
                                    "authorId": "2146333115",
                                    "name": "Tianhua Zhang"
                                },
                                {
                                    "authorId": "143827730",
                                    "name": "Yoon Kim"
                                },
                                {
                                    "authorId": "1847260",
                                    "name": "Xixin Wu"
                                },
                                {
                                    "authorId": "31997718",
                                    "name": "D. Fox"
                                },
                                {
                                    "authorId": "145199941",
                                    "name": "H. Meng"
                                },
                                {
                                    "authorId": "145898106",
                                    "name": "James R. Glass"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 27
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average."
                        ],
                        "paper": {
                            "corpus_id": 263605962,
                            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2255374957",
                                    "name": "Xi Victoria Lin"
                                },
                                {
                                    "authorId": "1769736",
                                    "name": "Xilun Chen"
                                },
                                {
                                    "authorId": "46221498",
                                    "name": "Mingda Chen"
                                },
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2253400960",
                                    "name": "Maria Lomeli"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "2253404757",
                                    "name": "Pedro Rodriguez"
                                },
                                {
                                    "authorId": "2253401183",
                                    "name": "Jacob Kahn"
                                },
                                {
                                    "authorId": "2253402270",
                                    "name": "Gergely Szilvasy"
                                },
                                {
                                    "authorId": "2253417398",
                                    "name": "Mike Lewis"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2253400757",
                                    "name": "Scott Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 153
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al._1, 2024)",
                        "snippets": [
                            "Fine-tuning is widely used to improve the capabilities of LLM-augmented components in RAG systems (de Luis Balaguer et al., 2024). Early methods of fine-tuning to enhance LLM-based components in RAG primarily focused on training the retriever and the generator (Ke et al., 2024)(Lin et al., 2023). Recent RAG methods have shifted toward modular designs (Gao et al., 2023b). Particularly in complex QA tasks, adaptive RAG often requires base models to follow intricate instructions (Yin et al., 2023a;Xu et al., 2024) to enable the functionality of diverse components (Asai et al., 2024). Classic alignment training methods include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). However, SFT lacks negative feedback and is prone to overfitting. Recently, Rafailov et al. proposed a more efficient reinforcement learning algorithm, direct preference optimization (DPO), which aligns response preferences and enhances the model's instruction-following ability by learning the differences between positive and negative sample pairs. In our work, we focus on using DPO to enhance the model's capability in multiple processes."
                        ],
                        "paper": {
                            "corpus_id": 273323657,
                            "title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2314784069",
                                    "name": "Ruobing Wang"
                                },
                                {
                                    "authorId": "2325728623",
                                    "name": "Daren Zha"
                                },
                                {
                                    "authorId": "2314785970",
                                    "name": "Shi Yu"
                                },
                                {
                                    "authorId": "2220669262",
                                    "name": "Qingfei Zhao"
                                },
                                {
                                    "authorId": "2311726706",
                                    "name": "Yuxuan Chen"
                                },
                                {
                                    "authorId": "2310129232",
                                    "name": "Yixuan Wang"
                                },
                                {
                                    "authorId": "2267033597",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "2277242040",
                                    "name": "Yukun Yan"
                                },
                                {
                                    "authorId": "49047064",
                                    "name": "Zhenghao Liu"
                                },
                                {
                                    "authorId": "2284728053",
                                    "name": "Xu Han"
                                },
                                {
                                    "authorId": "2266886975",
                                    "name": "Zhiyuan Liu"
                                },
                                {
                                    "authorId": "2273551430",
                                    "name": "Maosong Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.67529296875
                    },
                    {
                        "id": "(Rafailov et al., 2023)",
                        "snippets": [
                            "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train."
                        ],
                        "paper": {
                            "corpus_id": 258959321,
                            "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
                            "authors": [
                                {
                                    "authorId": "102801230",
                                    "name": "Rafael Rafailov"
                                },
                                {
                                    "authorId": "50465276",
                                    "name": "Archit Sharma"
                                },
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2490652",
                                    "name": "Stefano Ermon"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4159
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Fine-tuning has gradually become a popular strategy (Ke et al., 2024) for enhancing the capabilities of components of RAG. Existing works include fine-tuning retrieval-related components to achieve better retrieval outcomes (Yan et al., 2024), fine-tuning generators for more personalized outputs (Zhang et al., 2024b), and employing collaborative fine-tuning (Lin et al., 2023). Additionally, (Zhou et al., 2023) discovered that fine-tuning LLMs with a limited quantity of high-quality data significantly enhances the performance of LLMs. This finding provides a robust theoretical basis for collaboratively fine-tuning multiple components within advanced RAG methodologies at a minimal data expense."
                        ],
                        "paper": {
                            "corpus_id": 273532096,
                            "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
                            "authors": [
                                {
                                    "authorId": "2220669262",
                                    "name": "Qingfei Zhao"
                                },
                                {
                                    "authorId": "2314784069",
                                    "name": "Ruobing Wang"
                                },
                                {
                                    "authorId": "83546711",
                                    "name": "Yukuo Cen"
                                },
                                {
                                    "authorId": "2325728623",
                                    "name": "Daren Zha"
                                },
                                {
                                    "authorId": "2327738081",
                                    "name": "Shicheng Tan"
                                },
                                {
                                    "authorId": "2243402027",
                                    "name": "Yuxiao Dong"
                                },
                                {
                                    "authorId": "2327297599",
                                    "name": "Jie Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 14
                        },
                        "score": 0.82763671875
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                        ],
                        "paper": {
                            "corpus_id": 258822910,
                            "title": "LIMA: Less Is More for Alignment",
                            "authors": [
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "144118452",
                                    "name": "Pengfei Liu"
                                },
                                {
                                    "authorId": "2214843767",
                                    "name": "Puxin Xu"
                                },
                                {
                                    "authorId": "1900163",
                                    "name": "Srini Iyer"
                                },
                                {
                                    "authorId": "145478138",
                                    "name": "Jiao Sun"
                                },
                                {
                                    "authorId": "3375249",
                                    "name": "Yuning Mao"
                                },
                                {
                                    "authorId": "2378954",
                                    "name": "Xuezhe Ma"
                                },
                                {
                                    "authorId": "1388010852",
                                    "name": "Avia Efrat"
                                },
                                {
                                    "authorId": "2114104308",
                                    "name": "Ping Yu"
                                },
                                {
                                    "authorId": "49297123",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2108244542",
                                    "name": "Susan Zhang"
                                },
                                {
                                    "authorId": "134007132",
                                    "name": "Gargi Ghosh"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 850
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "To further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023)(Shi et al., 2023)(Izacard et al., 2020). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2023) to jointly optimize the modules in a RAG system, aligning their data preferences."
                        ],
                        "paper": {
                            "corpus_id": 276580741,
                            "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
                            "authors": [
                                {
                                    "authorId": "2347252099",
                                    "name": "Mingyan Wu"
                                },
                                {
                                    "authorId": "2323176343",
                                    "name": "Zhenghao Liu"
                                },
                                {
                                    "authorId": "2277242040",
                                    "name": "Yukun Yan"
                                },
                                {
                                    "authorId": "2261354998",
                                    "name": "Xinze Li"
                                },
                                {
                                    "authorId": "2314785970",
                                    "name": "Shi Yu"
                                },
                                {
                                    "authorId": "1633538428",
                                    "name": "Zheni Zeng"
                                },
                                {
                                    "authorId": "2261295920",
                                    "name": "Yu Gu"
                                },
                                {
                                    "authorId": "2204644192",
                                    "name": "Ge Yu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.5234375
                    },
                    {
                        "id": "(Gao et al., 2023)",
                        "snippets": [
                            "Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-toend joint training-and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels."
                        ],
                        "paper": {
                            "corpus_id": 266359151,
                            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2275341478",
                                    "name": "Xinyu Gao"
                                },
                                {
                                    "authorId": "2275191447",
                                    "name": "Kangxiang Jia"
                                },
                                {
                                    "authorId": "2275530552",
                                    "name": "Jinliu Pan"
                                },
                                {
                                    "authorId": "2275171009",
                                    "name": "Yuxi Bi"
                                },
                                {
                                    "authorId": "2276187454",
                                    "name": "Yi Dai"
                                },
                                {
                                    "authorId": "2275540959",
                                    "name": "Jiawei Sun"
                                },
                                {
                                    "authorId": "2258800561",
                                    "name": "Qianyu Guo"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1819
                        },
                        "score": 0.5751953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Specific RAG Fine-tuning Techniques and Implementations",
                "tldr": "Recent RAG fine-tuning techniques focus on improving retrieval quality, document utilization, and robustness against imperfect information. These implementations range from dual-purpose models like RankRAG to specialized approaches such as RAFT, CRAFT, and Robust Fine-Tuning that address specific RAG limitations. (13 sources)",
                "text": "\nBelow are notable RAG fine-tuning techniques and implementations that researchers have developed to enhance various aspects of retrieval-augmented generation:\n\n- **RankRAG**: A novel instruction fine-tuning framework that trains a single LLM to perform both context ranking and answer generation. Despite using only a small fraction of ranking data in the training mix, RankRAG outperforms existing expert ranking models, including those exclusively fine-tuned on larger amounts of ranking data. <Paper corpusId=\"270878612\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>\n\n- **RAFT (Retrieval Augmented Fine-Tuning)**: This approach uses fine-tuning data containing related documents and reasoning chains to improve the LLM's ability to understand retrieved documents. RAFT has demonstrated superior performance compared to standard RAG setups, even when using smaller 7B parameter models versus larger models like GPT-3.5. <Paper corpusId=\"269983737\" paperTitle=\"(Jiao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper>\n\n- **CRAFT (Compute-efficient RAFT)**: Combines RAFT with Low-Rank Adaptation (LoRA) to reduce fine-tuning and storage requirements while maintaining comparable RAG performance. This approach is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited. <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>\n\n- **Reward-RAG**: Enhances RAG through Reward-Driven Supervision by employing CriticGPT to train a dedicated reward model. This model generates synthesized datasets for fine-tuning the RAG encoder, aligning outputs more closely with human preferences and adapting retrieval information to specific domains. <Paper corpusId=\"273186680\" paperTitle=\"(Nguyen et al., 2024)\" isShortName></Paper>\n\n- **Domain-Specific RAG Fine-tuning**: Adapts RAG models to particular domains by fine-tuning both the generator and question encoder collectively on domain-specific datasets. Experiments in the hotel domain demonstrated significant improvements compared to baseline RAG models. <Paper corpusId=\"273532207\" paperTitle=\"(Rakin et al., 2024)\" isShortName></Paper>\n\n- **Robust Fine-Tuning (RbFT)**: Designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results show that RbFT significantly improves RAG system robustness across diverse retrieval conditions while maintaining high inference efficiency. <Paper corpusId=\"275993994\" paperTitle=\"(Tu et al., 2025)\" isShortName></Paper>\n\n- **RAG-Tuned LLM**: Integrates RAG and native LLM strengths by fine-tuning an LLM within an RAG framework for data generation. This approach bridges open-domain and domain-specific query-answering tasks, outperforming both standard RAG methods and long-context LLMs across diverse datasets, particularly for hierarchical queries. <Paper corpusId=\"277150553\" paperTitle=\"(Wei et al., 2025)\" isShortName></Paper>\n\n- **MultiQA Fine-tuning**: Uses supervised fine-tuning on general domain questions and answers to improve RAG performance. This approach employs LoRA to train models using prompts with retrieved documents, demonstrating benefits for both in-domain and out-of-domain RAG generalization. <Paper corpusId=\"277510500\" paperTitle=\"(Misrahi et al., 2025)\" isShortName></Paper>\n\n- **CoRAG and DeepRAG**: Focus on retrieval pathway optimization through multistep reasoning frameworks. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks like sub-query generation and intermediate answer prediction. DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>\n\n- **Self-RAG**: Fine-tunes models to generate special reflection tokens that make the LLM controllable during inference, enabling it to retrieve passages on-demand and reflect on both retrieved information and its own generations. Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on open-domain QA, reasoning, and fact verification tasks. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264288947\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper>\n\n- **Finetune-RAG**: A simple yet effective approach that trains LLMs to distinguish between correct and fictitious context within RAG. Unlike methods that focus on improving retrieval, Finetune-RAG enhances the model's ability to handle imperfect inputs by fine-tuning it with examples containing both correct and incorrect information, improving factual accuracy by 21.2% over the base model. <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>\n\n- **RAG-sft and CoT-sft**: Implementation recipes that fine-tune models in the RAG setup. CoT-sft incorporates a Chain-of-Thought approach where gold documents and distractor documents are used in the prompt with specified probabilities, guiding the model to use retrieved context, explain steps, quote relevant parts, and produce a final answer. <Paper corpusId=\"271710111\" paperTitle=\"(Fleischer et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Yu et al., 2024)",
                        "snippets": [
                            "In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data."
                        ],
                        "paper": {
                            "corpus_id": 270878612,
                            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
                            "authors": [
                                {
                                    "authorId": "2259265562",
                                    "name": "Yue Yu"
                                },
                                {
                                    "authorId": "2253664013",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "2256582287",
                                    "name": "Zihan Liu"
                                },
                                {
                                    "authorId": "2256656241",
                                    "name": "Boxin Wang"
                                },
                                {
                                    "authorId": "2287859963",
                                    "name": "Jiaxuan You"
                                },
                                {
                                    "authorId": "2256776233",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2264406909",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 74
                        },
                        "score": 0.5283203125
                    },
                    {
                        "id": "(Jiao et al., 2024)",
                        "snippets": [
                            "Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework."
                        ],
                        "paper": {
                            "corpus_id": 269983737,
                            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2302798653",
                                    "name": "Dian Jiao"
                                },
                                {
                                    "authorId": "2303434387",
                                    "name": "Li Cai"
                                },
                                {
                                    "authorId": "2303044665",
                                    "name": "Jingsheng Huang"
                                },
                                {
                                    "authorId": "2108125912",
                                    "name": "Wenqiao Zhang"
                                },
                                {
                                    "authorId": "2118071462",
                                    "name": "Siliang Tang"
                                },
                                {
                                    "authorId": "2253660817",
                                    "name": "Yueting Zhuang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.56884765625
                    },
                    {
                        "id": "(Chung et al., 2024)",
                        "snippets": [
                            "Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited."
                        ],
                        "paper": {
                            "corpus_id": 272911196,
                            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                            "authors": [
                                {
                                    "authorId": "2322992640",
                                    "name": "Isaac Chung"
                                },
                                {
                                    "authorId": "2322982756",
                                    "name": "Phat Vo"
                                },
                                {
                                    "authorId": "2322991957",
                                    "name": "Arman Kizilkale"
                                },
                                {
                                    "authorId": "2322982549",
                                    "name": "Aaron Reite"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6171875
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nguyen et al., 2024)",
                        "snippets": [
                            "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences."
                        ],
                        "paper": {
                            "corpus_id": 273186680,
                            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
                            "authors": [
                                {
                                    "authorId": "2324796381",
                                    "name": "Thang Nguyen"
                                },
                                {
                                    "authorId": "2324790937",
                                    "name": "Peter Chin"
                                },
                                {
                                    "authorId": "2324792268",
                                    "name": "Yu-Wing Tai"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.5732421875
                    },
                    {
                        "id": "(Rakin et al., 2024)",
                        "snippets": [
                            "RAG-Finetuned-QA : The RAG-Finetuned-QA model builds upon the baseline RAG-Original architecture by incorporating domain-specific fine-tuning on our hotel domain dataset. Like the RAG-Original model, it utilizes a Hotel Domain Knowledge Base, with dense vector representations indexed using the FAISS library. The loss function is designed to fine-tune both the generator and question encoder collectively. We have used ray as the distributed retriever with 4 retrieval workers as described in the original RAG paper. The model was trained for 20 epochs while the final checkpoint has been selected based on the highest validation accuracy. The purpose of this experiment was to assess the improvements gained from domain-specific training, comparing its performance against the baseline RAG-Original model."
                        ],
                        "paper": {
                            "corpus_id": 273532207,
                            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
                            "authors": [
                                {
                                    "authorId": "2327211339",
                                    "name": "Salman Rakin"
                                },
                                {
                                    "authorId": "2327211291",
                                    "name": "Md. A.R. Shibly"
                                },
                                {
                                    "authorId": "2327211491",
                                    "name": "Zahin M. Hossain"
                                },
                                {
                                    "authorId": "2327258807",
                                    "name": "Zeeshan Khan"
                                },
                                {
                                    "authorId": "2327212434",
                                    "name": "Md. Mostofa Akbar"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.51220703125
                    },
                    {
                        "id": "(Tu et al., 2025)",
                        "snippets": [
                            "To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques."
                        ],
                        "paper": {
                            "corpus_id": 275993994,
                            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
                            "authors": [
                                {
                                    "authorId": "2275628230",
                                    "name": "Yiteng Tu"
                                },
                                {
                                    "authorId": "2147219374",
                                    "name": "Weihang Su"
                                },
                                {
                                    "authorId": "2290870875",
                                    "name": "Yujia Zhou"
                                },
                                {
                                    "authorId": "2260835922",
                                    "name": "Yiqun Liu"
                                },
                                {
                                    "authorId": "2256982003",
                                    "name": "Qingyao Ai"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.5341796875
                    },
                    {
                        "id": "(Wei et al., 2025)",
                        "snippets": [
                            "In this paper, we validate RAG's fine-grained retrieval abilities and the global abstraction strengths of LLM-native solutions. However, RAG lacks holistic understanding, and long-context models tend to lose key information over extended contexts. We integrate these strengths of both RAG and LLM-native solutions by fine-tuning an LLM within an RAG framework for data generation. This work is the first to explore LLM and RAG integration within a unified framework, bridging open-domain and domain-specific query-answering tasks. Our RAG-Tuned LLM, equipped with LLMnative memory, outperforms both standard RAG methods and long-context LLMs across diverse datasets, demonstrating superior performance in handling hierarchical queries."
                        ],
                        "paper": {
                            "corpus_id": 277150553,
                            "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
                            "authors": [
                                {
                                    "authorId": "2349833035",
                                    "name": "Jiale Wei"
                                },
                                {
                                    "authorId": "2351464722",
                                    "name": "Shuchi Wu"
                                },
                                {
                                    "authorId": "2351243441",
                                    "name": "Ruochen Liu"
                                },
                                {
                                    "authorId": "2308276200",
                                    "name": "Xiang Ying"
                                },
                                {
                                    "authorId": "2308276676",
                                    "name": "Jingbo Shang"
                                },
                                {
                                    "authorId": "2351054847",
                                    "name": "Fangbo Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.68505859375
                    },
                    {
                        "id": "(Misrahi et al., 2025)",
                        "snippets": [
                            "Recent works have demonstrated that it is beneficial to finetune an LLM to encourage it to better use retrieved context [21,22,32,37]. It has been shown to improve RAG performances in-domain, and we investigate whether this holds for out-of-domain RAG generalization. To do so, we run supervised fine-tuning on the MultiQA dataset9 : a dataset consisting of 450k general domain questions and answers, described in Appendix Table 5. Its associated document collection consists of Wikipedia [30] and MSMARCO [2] documents. Each supervised fine-tuning sample then consists of a prompt (taken from [32]) with 5 retrieved documents, the question and its answer. Models are trained with LoRA [12]."
                        ],
                        "paper": {
                            "corpus_id": 277510500,
                            "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation",
                            "authors": [
                                {
                                    "authorId": "2353386625",
                                    "name": "Alexandre Misrahi"
                                },
                                {
                                    "authorId": "2258716783",
                                    "name": "Nadezhda Chirkova"
                                },
                                {
                                    "authorId": "2342407524",
                                    "name": "Maxime Louis"
                                },
                                {
                                    "authorId": "2841761",
                                    "name": "Vassilina Nikoulina"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.70556640625
                    },
                    {
                        "id": "(Gao et al., 2025)",
                        "snippets": [
                            "For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy",
                            "Self-RAG (Asai et al., 2023) fine-tune models for precise special token generation",
                            "O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA (Dettmers et al., 2023) quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning."
                        ],
                        "paper": {
                            "corpus_id": 277994112,
                            "title": "Synergizing RAG and Reasoning: A Systematic Review",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291322497",
                                    "name": "Yijie Zhong"
                                },
                                {
                                    "authorId": "2275171009",
                                    "name": "Yuxi Bi"
                                },
                                {
                                    "authorId": "2356716546",
                                    "name": "Ming Xue"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.57958984375
                    },
                    {
                        "id": "(Asai et al., 2023)",
                        "snippets": [
                            "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models."
                        ],
                        "paper": {
                            "corpus_id": 264288947,
                            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
                            "authors": [
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "7806955",
                                    "name": "Zeqiu Wu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2707234",
                                    "name": "Avirup Sil"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 780
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "We propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model.\n\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response."
                        ],
                        "paper": {
                            "corpus_id": 278714952,
                            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2362089035",
                                    "name": "Zhan Peng Lee"
                                },
                                {
                                    "authorId": "2362188632",
                                    "name": "Andre Lin"
                                },
                                {
                                    "authorId": "2363425126",
                                    "name": "Calvin Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.876953125
                    },
                    {
                        "id": "(Fleischer et al., 2024)",
                        "snippets": [
                            "We explore several techniques for RAG augmentation, and use RAG FOUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and a CoT scheme which guides the model to use the retrieved context, explain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To complement CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt."
                        ],
                        "paper": {
                            "corpus_id": 271710111,
                            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2296788981",
                                    "name": "Daniel Fleischer"
                                },
                                {
                                    "authorId": "2077591838",
                                    "name": "Moshe Berchansky"
                                },
                                {
                                    "authorId": "2134755",
                                    "name": "Moshe Wasserblat"
                                },
                                {
                                    "authorId": "2477428",
                                    "name": "Peter Izsak"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.65576171875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Benefits and Performance Improvements from Fine-tuning RAG",
                "tldr": "Fine-tuning RAG components delivers significant performance improvements, with accuracy gains of up to 20 percentage points in domain-specific applications. The benefits extend beyond accuracy to include cost-effectiveness, reduced hallucinations, and enhanced ability to process specialized information. (5 sources)",
                "text": "\nFine-tuning RAG systems has demonstrated substantial performance improvements across various applications. Most notably, the combination of fine-tuning the LLM with Retrieval Augmented Generation produces responses with significantly improved accuracy compared to baseline models <Paper corpusId=\"267412954\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. These accuracy improvements can be particularly dramatic in domain-specific applications, with studies showing gains of up to 20 percentage points over baseline RAG implementations when applied to specialized datasets like FinanceBench <Paper corpusId=\"269214364\" paperTitle=\"(Nguyen et al._1, 2024)\" isShortName></Paper>.\n\nA key insight from recent research is that fine-tuning different components of the RAG pipeline yields different magnitudes of improvement. Fine-tuning the retriever model generally results in higher accuracy gains compared to fine-tuning generators alone, which is particularly advantageous since fine-tuning embedding models for retrieval is typically less costly and less labor-intensive than fine-tuning LLMs for generation <Paper corpusId=\"269214364\" paperTitle=\"(Nguyen et al._1, 2024)\" isShortName></Paper>. This cost-effectiveness extends to end-to-end RAG implementations, with some methodologies costing less than $20 per 2000 research papers when using efficient models like Claude 3 Haiku <Paper corpusId=\"271903789\" paperTitle=\"(Mombaerts et al., 2024)\" isShortName></Paper>.\n\nFor domain-specific applications where standard pre-trained embedding models may exhibit sub-optimal performance, approaches like Model augmented fine-tuning (Mafin) have emerged. This novel technique enhances performance by augmenting black-box embedding models with trainable components, significantly improving retrieval performance while only requiring the training of a small augmented model <Paper corpusId=\"267750557\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. Such approaches effectively address the challenge of adapting general-purpose models to specialized knowledge domains.\n\nIn multimodal RAG applications, research has shown that while fine-tuning either the retrieval model or the vision-language model (VLM) improves performance, fine-tuning the VLM provides more substantial gains. This suggests that baseline retrieval models may already be sufficiently optimized for many applications, while directly enhancing the model's ability to process multimodal data yields greater performance improvements <Paper corpusId=\"273350575\" paperTitle=\"(Zhai, 2024)\" isShortName></Paper>. These findings highlight the importance of strategically targeting fine-tuning efforts to the components that will yield the greatest benefits.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."
                        ],
                        "paper": {
                            "corpus_id": 267412954,
                            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
                            "authors": [
                                {
                                    "authorId": "2279813822",
                                    "name": "Liang Zhang"
                                },
                                {
                                    "authorId": "2279831793",
                                    "name": "Katherine Jijo"
                                },
                                {
                                    "authorId": "2282528163",
                                    "name": "Spurthi Setty"
                                },
                                {
                                    "authorId": "2279830841",
                                    "name": "Eden Chung"
                                },
                                {
                                    "authorId": "2282539958",
                                    "name": "Fatima Javid"
                                },
                                {
                                    "authorId": "2279830757",
                                    "name": "Natan Vidra"
                                },
                                {
                                    "authorId": "2279838243",
                                    "name": "Thomas Clifford"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.7958984375
                    },
                    {
                        "id": "(Nguyen et al._1, 2024)",
                        "snippets": [
                            "Using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\nFine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation."
                        ],
                        "paper": {
                            "corpus_id": 269214364,
                            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
                            "authors": [
                                {
                                    "authorId": "2297189569",
                                    "name": "Zooey Nguyen"
                                },
                                {
                                    "authorId": "2297188041",
                                    "name": "Anthony Annunziata"
                                },
                                {
                                    "authorId": "69442223",
                                    "name": "Vinh Luong"
                                },
                                {
                                    "authorId": "2297188221",
                                    "name": "Sang Dinh"
                                },
                                {
                                    "authorId": "2297190249",
                                    "name": "Quynh Le"
                                },
                                {
                                    "authorId": "2297189614",
                                    "name": "A. Ha"
                                },
                                {
                                    "authorId": "2297189915",
                                    "name": "Chanh Le"
                                },
                                {
                                    "authorId": "2297189697",
                                    "name": "Hong An Phan"
                                },
                                {
                                    "authorId": "2058395065",
                                    "name": "Shruti Raghavan"
                                },
                                {
                                    "authorId": "2297324474",
                                    "name": "Christopher Nguyen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.52392578125
                    },
                    {
                        "id": "(Mombaerts et al., 2024)",
                        "snippets": [
                            "Our methodology is cost-effective, costing less than $20 per 2000 research papers using Claude 3 Haiku, and can be adapted with any fine-tuning of either the language or embedding models to further enhance the performance of end-to-end RAG pipelines."
                        ],
                        "paper": {
                            "corpus_id": 271903789,
                            "title": "Meta Knowledge for Retrieval Augmented Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2287923849",
                                    "name": "Laurent Mombaerts"
                                },
                                {
                                    "authorId": "2316430921",
                                    "name": "Terry Ding"
                                },
                                {
                                    "authorId": "2316476835",
                                    "name": "Adi Banerjee"
                                },
                                {
                                    "authorId": "2316429297",
                                    "name": "Florian Felice"
                                },
                                {
                                    "authorId": "2287922211",
                                    "name": "Jonathan Taws"
                                },
                                {
                                    "authorId": "3322455",
                                    "name": "Tarik Borogovac"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.50927734375
                    },
                    {
                        "id": "(Zhang et al._1, 2024)",
                        "snippets": [
                            "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning",
                            "We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model."
                        ],
                        "paper": {
                            "corpus_id": 267750557,
                            "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2108795448",
                                    "name": "Mingtian Zhang"
                                },
                                {
                                    "authorId": "2284682723",
                                    "name": "Shawn Lan"
                                },
                                {
                                    "authorId": "2067492948",
                                    "name": "Peter Hayes"
                                },
                                {
                                    "authorId": "2282542157",
                                    "name": "David Barber"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7607421875
                    },
                    {
                        "id": "(Zhai, 2024)",
                        "snippets": [
                            "Table 2 (top) demonstrates that fine-tuning the retrieval model R or the VLM M improves multimodal RAG performance. However, finetuning M provides more substantial gains. This suggests that the baseline retrieval model R is already sufficiently optimized, leading to smaller performance improvements from finetuning. In contrast, optimizing M directly enhances the model's ability to process multimodal data. Notably, the fine-tuned RAG(R * + M * ) shows slightly lower performance than MuRAG, despite differences in the volume of training data."
                        ],
                        "paper": {
                            "corpus_id": 273350575,
                            "title": "Self-adaptive Multimodal Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2325948461",
                                    "name": "Wenjia Zhai"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.50390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Domain-Specific RAG Fine-tuning",
                "tldr": "Domain-specific RAG fine-tuning tailors retrieval and generation components to specialized fields like legal, finance, and healthcare, where standard models often perform poorly. These approaches typically involve fine-tuning embedding models on domain-specific corpora and adapting retrieval mechanisms to handle specialized terminology and knowledge structures. (4 sources)",
                "text": "\nDomain-specific RAG fine-tuning has emerged as a critical approach for adapting general-purpose RAG systems to specialized fields where standard pre-trained models often exhibit suboptimal performance. This \"unfrozen\" approach allows for fine-tuning RAG components directly on nuanced use-case data, potentially enhancing system specificity and output quality for particular domains <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>. Rather than relying on static applications of RAG systems, this approach enables the adaptation of embedding models and rerankers to better align with the specific terminologies, knowledge structures, and reasoning patterns of specialized domains.\n\nThe legal domain provides a compelling example of the necessity for domain-specific RAG fine-tuning. Research has demonstrated that fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from general language patterns <Paper corpusId=\"276939617\" paperTitle=\"(Hindi et al., 2025)\" isShortName></Paper>. For instance, systems like HyPA-RAG fine-tune distilBERT models on legal corpora, while CASEGPT employs fine-tuned versions of Legal-BERT to achieve enhanced retrieval performance <Paper corpusId=\"276939617\" paperTitle=\"(Hindi et al., 2025)\" isShortName></Paper>. Similarly, CamemBERT has been fully fine-tuned on the Long-form Legal Question Answering (LLeQA) dataset to improve its capacity to handle complex legal queries <Paper corpusId=\"276939617\" paperTitle=\"(Hindi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263310713\" paperTitle=\"(Louis et al., 2023)\" isShortName></Paper>.\n\nThe finance domain represents another area where specialized RAG fine-tuning can yield significant benefits. Despite the proven effectiveness of contrastive learning techniques in embedding models, there remains a notable gap in research regarding the specific impact of embedder fine-tuning on RAG systems within the finance sector <Paper corpusId=\"277113527\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>. Financial documents contain domain-specific terminology, complex numerical relationships, and specialized knowledge structures that general-purpose RAG systems may struggle to process effectively. Fine-tuning embedding models on financial corpora can help bridge this gap, enabling more accurate retrieval and interpretation of financial information.\n\nDomain-specific RAG fine-tuning typically involves several key steps: first, identifying the unique characteristics and requirements of the target domain; second, collecting or creating domain-specific datasets for fine-tuning; third, adapting both the retrieval components (embedding models, rerankers) and potentially the generator components to better align with domain-specific patterns; and finally, evaluating performance against domain-specific benchmarks <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This approach enables RAG systems to overcome the limitations of general-purpose models when applied to specialized knowledge domains, resulting in more accurate and contextually appropriate responses.",
                "citations": [
                    {
                        "id": "(Eibich et al., 2024)",
                        "snippets": [
                            "Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs."
                        ],
                        "paper": {
                            "corpus_id": 268819923,
                            "title": "ARAGOG: Advanced RAG Output Grading",
                            "authors": [
                                {
                                    "authorId": "2294361167",
                                    "name": "Matouvs Eibich"
                                },
                                {
                                    "authorId": "2294361283",
                                    "name": "Shivay Nagpal"
                                },
                                {
                                    "authorId": "2294362877",
                                    "name": "Alexander Fred-Ojala"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.58154296875
                    },
                    {
                        "id": "(Hindi et al., 2025)",
                        "snippets": [
                            "Fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from the For example, HyPA-RAG fine-tunes its distilBERT model on legal corpora and CASEGPT adopts a fine-tuned version of Legal-BERT to achieve enhanced retrieval performance [39], [49]. In addition, CamemBERT is fully fine-tuned on the long-form LQA (LLeQA) dataset to improve its ability to handle complex legal queries (Louis et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 276939617,
                            "title": "Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey",
                            "authors": [
                                {
                                    "authorId": "2349663965",
                                    "name": "Mahd Hindi"
                                },
                                {
                                    "authorId": "2349670786",
                                    "name": "Linda Mohammed"
                                },
                                {
                                    "authorId": "2349663960",
                                    "name": "Ommama Maaz"
                                },
                                {
                                    "authorId": "3360346",
                                    "name": "Abdulmalik Alwarafy"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Access",
                            "n_citations": 2
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Louis et al., 2023)",
                        "snippets": [
                            "Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a \"retrieve-then-read\" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising performance on automatic evaluation metrics, but a qualitative analysis uncovers areas for refinement. As one of the only comprehensive, expert-annotated long-form LQA dataset, LLeQA has the potential to not only accelerate research towards resolving a significant real-world issue, but also act as a rigorous benchmark for evaluating NLP models in specialized domains. We publicly release our code, data, and models."
                        ],
                        "paper": {
                            "corpus_id": 263310713,
                            "title": "Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2124444236",
                                    "name": "Antoine Louis"
                                },
                                {
                                    "authorId": "30445438",
                                    "name": "G. van Dijck"
                                },
                                {
                                    "authorId": "3266578",
                                    "name": "Gerasimos Spanakis"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 40
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kim et al., 2025)",
                        "snippets": [
                            "Despite the effectiveness of contrastive learning in embedders Lu et al. (2024), there remains a notable gap in the literature regarding the impact of embedder fine-tuning on RAG systems, particularly within the finance domain Setty et al. (2024)."
                        ],
                        "paper": {
                            "corpus_id": 277113527,
                            "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
                            "authors": [
                                {
                                    "authorId": "2350956983",
                                    "name": "Sejong Kim"
                                },
                                {
                                    "authorId": "2350857179",
                                    "name": "Hyunseo Song"
                                },
                                {
                                    "authorId": "2351317331",
                                    "name": "Hyunwoo Seo"
                                },
                                {
                                    "authorId": "2350867590",
                                    "name": "Hyunjun Kim"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.6181640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges and Future Directions in RAG Fine-tuning",
                "tldr": "Despite impressive advances, RAG fine-tuning faces challenges including balancing domain specialization with general knowledge, addressing computational costs, and establishing standardized evaluation frameworks. Future research will focus on more adaptive RAG systems that continuously learn from user feedback and automatically adjust fine-tuning strategies based on query characteristics. (3 sources)",
                "text": "\nWhile fine-tuning RAG systems has demonstrated significant benefits, several challenges persist that researchers and practitioners must address to fully realize the potential of these systems. One fundamental challenge is striking the optimal balance between domain specialization and general knowledge preservation. As RAG systems become increasingly specialized through fine-tuning for specific domains, they risk losing their broader capabilities that make them valuable for general-purpose applications <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe computational and resource requirements for comprehensive fine-tuning of RAG systems present another significant barrier. Although parameter-efficient methods like LoRA and QLoRA have emerged to address these concerns, fine-tuning both retrieval and generation components simultaneously remains resource-intensive, particularly for smaller organizations with limited computational infrastructure <Paper corpusId=\"278394304\" paperTitle=\"(Aqib et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268033066\" paperTitle=\"(Rangan et al., 2024)\" isShortName></Paper>. These resource constraints often necessitate trade-offs between fine-tuning effectiveness and computational efficiency.\n\nCurrent evaluation metrics for RAG systems also present challenges, as traditional metrics may not fully capture the nuanced improvements that fine-tuning provides, especially in domain-specific applications. Developing more comprehensive and standardized evaluation frameworks that account for both retrieval quality and generation fidelity would help better assess the true impact of fine-tuning approaches <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nLooking toward future directions, research is increasingly moving away from static RAG implementations toward more dynamic, \"unfrozen\" approaches that continuously adapt to evolving data and user needs. These adaptable systems can fine-tune embedding models and rerankers directly on nuanced use-case data, potentially enhancing system specificity and output quality for particular applications <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>. Such adaptive systems represent a promising frontier for RAG fine-tuning research.\n\nThe integration of user feedback directly into the training process presents another promising avenue for advancement. By incorporating feedback loops that continuously refine RAG components based on user interactions, these systems can achieve ongoing improvement in both retrieval relevance and generation quality <Paper corpusId=\"278394304\" paperTitle=\"(Aqib et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268033066\" paperTitle=\"(Rangan et al., 2024)\" isShortName></Paper>. This human-in-the-loop approach may help mitigate some of the challenges associated with dataset creation and evaluation metric selection.\n\nFuture research is also likely to explore more sophisticated hybridization of fine-tuning approaches, where the system automatically determines which components (retriever, generator, or both) should be fine-tuned based on query characteristics and domain requirements. These context-aware fine-tuning strategies could optimize resource allocation while maximizing performance improvements for specific use cases <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nAdditionally, as RAG systems continue to evolve, there will be increasing focus on techniques that maintain model integrity while adapting to new domains. This includes exploring regularization methods that prevent catastrophic forgetting during fine-tuning and developing more efficient approaches for knowledge transfer between domains without requiring complete retraining <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Aqib et al., 2025)",
                        "snippets": [
                            "Fine-tuning is another technique for adapting pre-trained LLMs to new tasks and reducing hallucinations by continuing training on new data [31]. Fine-tuning involves training pre-trained models on task-specific datasets, which helps in making the model capable enough to perform better on the desired task. It has been found that fine-tuning the LLM in an RAG system can help improve its performance, as the LLM is responsible for text generation utilizing the retrieved information (Rangan et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 278394304,
                            "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes",
                            "authors": [
                                {
                                    "authorId": "2359632364",
                                    "name": "Mohammad Aqib"
                                },
                                {
                                    "authorId": "2359630758",
                                    "name": "Mohd Hamza"
                                },
                                {
                                    "authorId": "2335471239",
                                    "name": "Qipei Mei"
                                },
                                {
                                    "authorId": "2273503155",
                                    "name": "Y. Chui"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.64306640625
                    },
                    {
                        "id": "(Rangan et al., 2024)",
                        "snippets": [
                            "This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases. This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs. Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model\u2019s continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to enhance the precision of result selection, further refining the system\u2019s accuracy. Accompanied by an executive diagram and a detailed algorithm for fine-tuning QLoRA, our work provides a comprehensive framework for implementing these advancements within chatbot technologies. This research contributes significant insights into LLM optimization for specific uses and heralds new directions for further development in retrieval-augmented models. Through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems. We make the dataset, the data processing package huggify-data, the model, and the app publicly available for the community."
                        ],
                        "paper": {
                            "corpus_id": 268033066,
                            "title": "A fine-tuning enhanced RAG system with quantized influence measure as AI judge",
                            "authors": [
                                {
                                    "authorId": "1845889141",
                                    "name": "K. Rangan"
                                },
                                {
                                    "authorId": "2287852165",
                                    "name": "Yiqiao Yin"
                                }
                            ],
                            "year": 2024,
                            "venue": "Scientific Reports",
                            "n_citations": 12
                        },
                        "score": 0
                    },
                    {
                        "id": "(Eibich et al., 2024)",
                        "snippets": [
                            "Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).This \"unfrozen\" approach allows for fine-tuning on nuanced use-case data, potentially enhancing system specificity and output quality.Exploring these adaptations could lead to more adaptable and effective RAG systems tailored to diverse application needs."
                        ],
                        "paper": {
                            "corpus_id": 268819923,
                            "title": "ARAGOG: Advanced RAG Output Grading",
                            "authors": [
                                {
                                    "authorId": "2294361167",
                                    "name": "Matouvs Eibich"
                                },
                                {
                                    "authorId": "2294361283",
                                    "name": "Shivay Nagpal"
                                },
                                {
                                    "authorId": "2294362877",
                                    "name": "Alexander Fred-Ojala"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.58154296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.15290700000000002
    }
}