{
    "query": "Are there formal, quantitative scaling laws for inference-time compute cost in language models, and how do these compare to the established training compute scaling laws?",
    "user_id": "lib_user",
    "task_id": "9b9b7545-3d07-45c7-82e6-108b180ada5f",
    "timestamp": "2025-06-23T21:28:14.083905",
    "n_retrieval": 256,
    "n_retrieved": 268,
    "n_candidates": 40,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.294744,
    "decomposed_query": {
        "rewritten_query": "Formal, quantitative scaling laws for inference-time compute cost in language models, and how these compare to the established training compute scaling laws.",
        "keyword_query": "scaling laws inference-time compute cost language models comparison training compute",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009849,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2411.19477?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2411.19477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2272979112",
                    "name": "Yanxi Chen"
                },
                {
                    "authorId": "2211993531",
                    "name": "Xuchen Pan"
                },
                {
                    "authorId": "2237607166",
                    "name": "Yaliang Li"
                },
                {
                    "authorId": "2266389996",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "2237499232",
                    "name": "Jingren Zhou"
                }
            ],
            "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N \u00d7 ( K + 1) highly parallelizable LLM calls are needed for solving an input problem. For an input problem, assuming that a generated candidate solution is correct with probability p gen > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p comp > 0 . 5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K : P ( final output is incorrect ) \u2264 (1 \u2212 p gen ) N + \u2308 log 2 N \u2309 e \u2212 2 K ( p comp \u2212 0 . 5) 2 . Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.",
            "corpus_id": 277110444,
            "sentences": [],
            "relevance_judgement": 0.93408203125,
            "relevance_judgment_input_expanded": "# Title: A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models\n# Venue: arXiv.org\n# Authors: Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou\n## Abstract\nWe propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N \u00d7 ( K + 1) highly parallelizable LLM calls are needed for solving an input problem. For an input problem, assuming that a generated candidate solution is correct with probability p gen > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p comp > 0 . 5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K : P ( final output is incorrect ) \u2264 (1 \u2212 p gen ) N + \u2308 log 2 N \u2309 e \u2212 2 K ( p comp \u2212 0 . 5) 2 . Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.\n",
            "reference_string": "[277110444 | Chen et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 130,
            "influential_citation_count": 10,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00724, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314347409",
                    "name": "Yangzhen Wu"
                },
                {
                    "authorId": "48064856",
                    "name": "Zhiqing Sun"
                },
                {
                    "authorId": "2257057803",
                    "name": "Shanda Li"
                },
                {
                    "authorId": "2129663",
                    "name": "S. Welleck"
                },
                {
                    "authorId": "2257099254",
                    "name": "Yiming Yang"
                }
            ],
            "abstract": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.",
            "corpus_id": 271601023,
            "sentences": [
                {
                    "corpus_id": "271601023",
                    "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
                    "text": "We study the relationship between task performance and the amount of compute expended during inference for various model sizes, model families, and inference strategies, to form empirical inference scaling laws. These relationships let us reason about compute-optimal inference: inference configurations that give the best performance at a given compute budget. \n\nOur results lead to three main takeaways. First, we find that using a smaller model and generating more tokens in an inference strategy often outperforms using a larger model at a fixed compute budget. This has implications for models deployed in the real world, where inference compute is constrained in various ways. Specifically, it is potentially beneficial to deploy smaller models with more sophisticated inference strategies for better cost-performance trade-off. Second, we show that in the limit of infinite compute (allocated by drawing more samples), sampling-based majority voting strategies inevitably saturate to a distribution that depends on the underlying generation policy. Hence, it is of interest to alter the sampling distribution by designing an alternative inference strategy. Third, we design such an inference strategy-the novel REBASE tree search-and find it is Pareto optimal, in that it achieves the best performance across all tested compute budgets. Notably, it outperforms commonly used weighted majority voting and MCTS methods that have attracted much interest and widespread use. This finding not only shows the strength of REBASE, but also indicates that there is large headroom to improve language model performances via inference-time algorithms. \n\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning, acting, and planning in language models. In Proceedings of the 41st International Conference on Machine Learning, pp. 62138-62160, 2024.",
                    "score": 0.6967606130216627,
                    "section_title": "CONCLUSIONS",
                    "char_start_offset": 26183,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 361
                        },
                        {
                            "start": 364,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1647
                        },
                        {
                            "start": 1650,
                            "end": 1729
                        },
                        {
                            "start": 1730,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 1912
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                },
                {
                    "corpus_id": "271601023",
                    "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
                    "text": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.",
                    "score": 0.9696707021503846,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8837890625
                },
                {
                    "corpus_id": "271601023",
                    "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
                    "text": "Scaling laws of neural networks (Hestness et al., 2017;Rosenfeld et al., 2020) have been established across a range of domains, including language modeling (Kaplan et al., 2020;Hoffmann et al., 2022;OpenAI, 2023), image modeling (Henighan et al., 2020;Yu et al., 2022;Peebles & Xie, 2023), video modeling (Brooks et al., 2024), reward modeling (Gao et al., 2023), and board games (Jones, 2021). These studies have demonstrated how model performance is influenced by both the size of the model and the amount of training compute. However, there is limited knowledge on how varying the compute during inference affects model performance after the model has been trained. \n\nTo improve the task performance of large language models (LLMs), inference techniques typically involve additional compute as a performance maximization step at inference time (Nye et al., 2021;Wei et al., 2022;Wang et al., 2023b;Yao et al., 2023;Chen et al., 2024b). The computational cost of these techniques must be taken into account for compute-optimal inference. For example, Monte Carlo Tree Search (MCTS) may improve task performance, but it potentially requires much more compute than simply sampling solutions multiple times (Jones, 2021). Generally speaking, we need a comprehensive understanding of how various inference-time methods (e.g., best-of-n, majority voting (Wang et al., 2023a;Li et al., 2023)) trade off between performance and cost. To improve our understanding, this paper presents a thorough empirical evaluation with careful analysis over various configurations of representative LLMs and inference algorithms. \n\nSpecifically, we explore how to select an optimal size for the language model and an effective inference strategy (e.g., greedy search, majority voting, best-of-n, weighted voting, and their treesearch variants) to maximize performance (i.e., accuracy) with a given compute budget. We control the inference compute (FLOPs) of a fixed model by generating more tokens through the language model1 , sampling further candidate solutions, and ranking them with a reward model.",
                    "score": 0.6949184040428467,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 609,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 668
                        },
                        {
                            "start": 671,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1039
                        },
                        {
                            "start": 1040,
                            "end": 1220
                        },
                        {
                            "start": 1221,
                            "end": 1428
                        },
                        {
                            "start": 1429,
                            "end": 1609
                        },
                        {
                            "start": 1612,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2083
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 55,
                            "end": 78,
                            "matchedPaperCorpusId": "203592013"
                        },
                        {
                            "start": 252,
                            "end": 268,
                            "matchedPaperCorpusId": "249926846"
                        },
                        {
                            "start": 268,
                            "end": 288,
                            "matchedPaperCorpusId": "254854389"
                        },
                        {
                            "start": 344,
                            "end": 362,
                            "matchedPaperCorpusId": "252992904"
                        },
                        {
                            "start": 865,
                            "end": 882,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 882,
                            "end": 901,
                            "matchedPaperCorpusId": "254877310"
                        },
                        {
                            "start": 901,
                            "end": 918,
                            "matchedPaperCorpusId": "258762525"
                        },
                        {
                            "start": 918,
                            "end": 937,
                            "matchedPaperCorpusId": "267740392"
                        },
                        {
                            "start": 1351,
                            "end": 1371,
                            "matchedPaperCorpusId": "247595263"
                        },
                        {
                            "start": 1371,
                            "end": 1387,
                            "matchedPaperCorpusId": "259370847"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86865234375
                },
                {
                    "corpus_id": "271601023",
                    "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
                    "text": "To compare the inference compute budgets of different models, we plot the figures with the number of FLOPs used per question during inference. We compute the inference FLOPs based on the commonly-used formula proposed by Kaplan et al. (2020). \n\nScaling law of compute-optimal inference for model size. Fig. 1 shows the relationship between inference compute and error rate for different model sizes. The error rate first decreases steadily and then starts to saturate. Initially, sampling many times from smaller models is compute-optimal. At larger compute budgets the larger models are preferable, since the performance of small models has saturated. As highlighted in the right panel of Fig. 1, the optimal model size varies based on the inference budget. We performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget. \n\nLlemma-7B achieves competitive accuracy to Llemma-34B with less compute. Fig. 4 and Fig. 5 shows the relationship between error rate and inference FLOPs for Llemma 7B and Llemma 34B using different inference strategies. Llemma-7B requires around 2\u00d7 less total FLOPs than Llemma-34B to achieve comparable accuracy. This held across inference strategies (sampling strategies, MCTS, REBASE) and tasks (MATH, GSM8K). This result suggests that, with the same training dataset and model family, generating more tokens with a suitable inference strategy using a smaller model can have more favorable cost-performance tradeoffs than using a larger model.",
                    "score": 0.5966332554342604,
                    "section_title": "COMPUTE-OPTIMAL MODEL SIZE",
                    "char_start_offset": 21697,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 242
                        },
                        {
                            "start": 245,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1068
                        },
                        {
                            "start": 1071,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1717
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7890625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving\n# Venue: International Conference on Learning Representations\n# Authors: Yangzhen Wu, Zhiqing Sun, Shanda Li, S. Welleck, Yiming Yang\n## Abstract\nWhile the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.\n## INTRODUCTION\nScaling laws of neural networks (Hestness et al., 2017;Rosenfeld et al., 2020) have been established across a range of domains, including language modeling (Kaplan et al., 2020;Hoffmann et al., 2022;OpenAI, 2023), image modeling (Henighan et al., 2020;Yu et al., 2022;Peebles & Xie, 2023), video modeling (Brooks et al., 2024), reward modeling (Gao et al., 2023), and board games (Jones, 2021). These studies have demonstrated how model performance is influenced by both the size of the model and the amount of training compute. However, there is limited knowledge on how varying the compute during inference affects model performance after the model has been trained. \n\nTo improve the task performance of large language models (LLMs), inference techniques typically involve additional compute as a performance maximization step at inference time (Nye et al., 2021;Wei et al., 2022;Wang et al., 2023b;Yao et al., 2023;Chen et al., 2024b). The computational cost of these techniques must be taken into account for compute-optimal inference. For example, Monte Carlo Tree Search (MCTS) may improve task performance, but it potentially requires much more compute than simply sampling solutions multiple times (Jones, 2021). Generally speaking, we need a comprehensive understanding of how various inference-time methods (e.g., best-of-n, majority voting (Wang et al., 2023a;Li et al., 2023)) trade off between performance and cost. To improve our understanding, this paper presents a thorough empirical evaluation with careful analysis over various configurations of representative LLMs and inference algorithms. \n\nSpecifically, we explore how to select an optimal size for the language model and an effective inference strategy (e.g., greedy search, majority voting, best-of-n, weighted voting, and their treesearch variants) to maximize performance (i.e., accuracy) with a given compute budget. We control the inference compute (FLOPs) of a fixed model by generating more tokens through the language model1 , sampling further candidate solutions, and ranking them with a reward model.\n\n## COMPUTE-OPTIMAL MODEL SIZE\nTo compare the inference compute budgets of different models, we plot the figures with the number of FLOPs used per question during inference. We compute the inference FLOPs based on the commonly-used formula proposed by Kaplan et al. (2020). \n\nScaling law of compute-optimal inference for model size. Fig. 1 shows the relationship between inference compute and error rate for different model sizes. The error rate first decreases steadily and then starts to saturate. Initially, sampling many times from smaller models is compute-optimal. At larger compute budgets the larger models are preferable, since the performance of small models has saturated. As highlighted in the right panel of Fig. 1, the optimal model size varies based on the inference budget. We performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget. \n\nLlemma-7B achieves competitive accuracy to Llemma-34B with less compute. Fig. 4 and Fig. 5 shows the relationship between error rate and inference FLOPs for Llemma 7B and Llemma 34B using different inference strategies. Llemma-7B requires around 2\u00d7 less total FLOPs than Llemma-34B to achieve comparable accuracy. This held across inference strategies (sampling strategies, MCTS, REBASE) and tasks (MATH, GSM8K). This result suggests that, with the same training dataset and model family, generating more tokens with a suitable inference strategy using a smaller model can have more favorable cost-performance tradeoffs than using a larger model.\n\n## CONCLUSIONS\nWe study the relationship between task performance and the amount of compute expended during inference for various model sizes, model families, and inference strategies, to form empirical inference scaling laws. These relationships let us reason about compute-optimal inference: inference configurations that give the best performance at a given compute budget. \n\nOur results lead to three main takeaways. First, we find that using a smaller model and generating more tokens in an inference strategy often outperforms using a larger model at a fixed compute budget. This has implications for models deployed in the real world, where inference compute is constrained in various ways. Specifically, it is potentially beneficial to deploy smaller models with more sophisticated inference strategies for better cost-performance trade-off. Second, we show that in the limit of infinite compute (allocated by drawing more samples), sampling-based majority voting strategies inevitably saturate to a distribution that depends on the underlying generation policy. Hence, it is of interest to alter the sampling distribution by designing an alternative inference strategy. Third, we design such an inference strategy-the novel REBASE tree search-and find it is Pareto optimal, in that it achieves the best performance across all tested compute budgets. Notably, it outperforms commonly used weighted majority voting and MCTS methods that have attracted much interest and widespread use. This finding not only shows the strength of REBASE, but also indicates that there is large headroom to improve language model performances via inference-time algorithms. \n\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning, acting, and planning in language models. In Proceedings of the 41st International Conference on Machine Learning, pp. 62138-62160, 2024.",
            "reference_string": "[271601023 | Wu et al. | 2024 | Citations: 130]"
        },
        {
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 45,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01005, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1970199394",
                    "name": "Nishad Singhi"
                },
                {
                    "authorId": "2317010356",
                    "name": "Hritik Bansal"
                },
                {
                    "authorId": "2090537547",
                    "name": "Arian Hosseini"
                },
                {
                    "authorId": "2263888488",
                    "name": "Aditya Grover"
                },
                {
                    "authorId": "2256646491",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2329738651",
                    "name": "Marcus Rohrbach"
                },
                {
                    "authorId": "2329738066",
                    "name": "Anna Rohrbach"
                }
            ],
            "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.",
            "corpus_id": 277467695,
            "sentences": [
                {
                    "corpus_id": "277467695",
                    "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
                    "text": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.",
                    "score": 0.6128118588730337,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9208984375
                }
            ],
            "relevance_judgement": 0.9208984375,
            "relevance_judgment_input_expanded": "# Title: When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning\n# Venue: arXiv.org\n# Authors: Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, Anna Rohrbach\n## Abstract\nScaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.\n",
            "reference_string": "[277467695 | Singhi et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Scaling Laws for Precision",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 99,
            "citation_count": 29,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.04330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256990733",
                    "name": "Tanishq Kumar"
                },
                {
                    "authorId": "2172356226",
                    "name": "Zachary Ankner"
                },
                {
                    "authorId": "2259914006",
                    "name": "B. Spector"
                },
                {
                    "authorId": "77327149",
                    "name": "Blake Bordelon"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "1690452",
                    "name": "Mansheej Paul"
                },
                {
                    "authorId": "2577481",
                    "name": "Cengiz Pehlevan"
                },
                {
                    "authorId": "2313917068",
                    "name": "Christopher R'e"
                },
                {
                    "authorId": "2334471571",
                    "name": "Aditi Raghunathan"
                }
            ],
            "abstract": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise\"precision-aware\"scaling laws for both training and inference. We propose that training in lower precision reduces the model's\"effective parameter count,\"allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
            "corpus_id": 273877632,
            "sentences": [
                {
                    "corpus_id": "273877632",
                    "title": "Scaling Laws for Precision",
                    "text": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise\"precision-aware\"scaling laws for both training and inference. We propose that training in lower precision reduces the model's\"effective parameter count,\"allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
                    "score": 0.6388677463636746,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89892578125
                }
            ],
            "relevance_judgement": 0.89892578125,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws for Precision\n# Venue: International Conference on Learning Representations\n# Authors: Tanishq Kumar, Zachary Ankner, B. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher R'e, Aditi Raghunathan\n## Abstract\nLow precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise\"precision-aware\"scaling laws for both training and inference. We propose that training in lower precision reduces the model's\"effective parameter count,\"allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.\n",
            "reference_string": "[273877632 | Kumar et al. | 2024 | Citations: 29]"
        },
        {
            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338865687",
                    "name": "Chien-Ping Lu"
                }
            ],
            "abstract": "As large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the\"efficiency-doubling rate\"parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.",
            "corpus_id": 275336968,
            "sentences": [
                {
                    "corpus_id": "275336968",
                    "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
                    "text": "The study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
                    "score": 0.7339258260535553,
                    "section_title": "Related Work",
                    "char_start_offset": 4305,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 595
                        },
                        {
                            "start": 596,
                            "end": 744
                        },
                        {
                            "start": 747,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1516
                        },
                        {
                            "start": 1517,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2046
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1221,
                            "end": 1225,
                            "matchedPaperCorpusId": "246473179"
                        },
                        {
                            "start": 1803,
                            "end": 1806,
                            "matchedPaperCorpusId": "6519532"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89794921875
                }
            ],
            "relevance_judgement": 0.89794921875,
            "relevance_judgment_input_expanded": "# Title: The Race to Efficiency: A New Perspective on AI Scaling Laws\n# Venue: arXiv.org\n# Authors: Chien-Ping Lu\n## Abstract\nAs large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the\"efficiency-doubling rate\"parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.\n## Related Work\nThe study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
            "reference_string": "[275336968 | Lu | 2025 | Citations: 1]"
        },
        {
            "title": "Scaling Inference-Efficient Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 75,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18107, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342276462",
                    "name": "Song Bian"
                },
                {
                    "authorId": "2263988785",
                    "name": "Minghao Yan"
                },
                {
                    "authorId": "2257307243",
                    "name": "Shivaram Venkataraman"
                }
            ],
            "abstract": "Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to 3.5x difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by 1.8x while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff. Notably, our experiments reveal that wider and shallower models can yield efficiency gains while preserving accuracy.",
            "corpus_id": 275993733,
            "sentences": [
                {
                    "corpus_id": "275993733",
                    "title": "Scaling Inference-Efficient Language Models",
                    "text": "Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to 3.5x difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training a total of 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by 1.8x while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff.",
                    "score": 0.5758763397795322,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89453125
                }
            ],
            "relevance_judgement": 0.89453125,
            "relevance_judgment_input_expanded": "# Title: Scaling Inference-Efficient Language Models\n# Venue: arXiv.org\n# Authors: Song Bian, Minghao Yan, Shivaram Venkataraman\n## Abstract\nScaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to 3.5x difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by 1.8x while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff. Notably, our experiments reveal that wider and shallower models can yield efficiency gains while preserving accuracy.\n",
            "reference_string": "[275993733 | Bian et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 330,
            "influential_citation_count": 28,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283198901",
                    "name": "Bradley Brown"
                },
                {
                    "authorId": "50875781",
                    "name": "Jordan Juravsky"
                },
                {
                    "authorId": "2283134957",
                    "name": "Ryan Ehrlich"
                },
                {
                    "authorId": "2313919316",
                    "name": "Ronald Clark"
                },
                {
                    "authorId": "2151097303",
                    "name": "Quoc V. Le"
                },
                {
                    "authorId": "2313917068",
                    "name": "Christopher R'e"
                },
                {
                    "authorId": "1861312",
                    "name": "Azalia Mirhoseini"
                }
            ],
            "abstract": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.",
            "corpus_id": 271571035,
            "sentences": [
                {
                    "corpus_id": "271571035",
                    "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
                    "text": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.",
                    "score": 0.6232342499845855,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88818359375
                }
            ],
            "relevance_judgement": 0.88818359375,
            "relevance_judgment_input_expanded": "# Title: Large Language Monkeys: Scaling Inference Compute with Repeated Sampling\n# Venue: arXiv.org\n# Authors: Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher R'e, Azalia Mirhoseini\n## Abstract\nScaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.\n",
            "reference_string": "[271571035 | Brown et al. | 2024 | Citations: 330]"
        },
        {
            "title": "Parallel Scaling Law for Language Models",
            "venue": "",
            "year": 2025,
            "reference_count": 100,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2125101083",
                    "name": "Mouxiang Chen"
                },
                {
                    "authorId": "2321578848",
                    "name": "Binyuan Hui"
                },
                {
                    "authorId": "2248072386",
                    "name": "Zeyu Cui"
                },
                {
                    "authorId": "2328943044",
                    "name": "Jiaxin Yang"
                },
                {
                    "authorId": "2248487202",
                    "name": "Dayiheng Liu"
                },
                {
                    "authorId": "2362357192",
                    "name": "Jianling Sun"
                },
                {
                    "authorId": "2326803484",
                    "name": "Junyang Lin"
                },
                {
                    "authorId": "2361700209",
                    "name": "Zhongxin Liu"
                }
            ],
            "abstract": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.",
            "corpus_id": 278636433,
            "sentences": [
                {
                    "corpus_id": "278636433",
                    "title": "Parallel Scaling Law for Language Models",
                    "text": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.",
                    "score": 0.870959457044782,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88671875
                },
                {
                    "corpus_id": "278636433",
                    "title": "Parallel Scaling Law for Language Models",
                    "text": "Training Inference-Optimal Language Models Chinchilla (Hoffmann et al., 2022) explored the scaling law to determine the training-optimal amounts for parameters and training data under a training FLOP budget. On the other hands, modern LLMs are increasingly interested on inference-optimal models. Some practitioners use much more data than the Chinchilla recommendation to train small models due to their high inference efficiency (Qwen Team, 2024;Allal et al., 2025;Sardana et al., 2024). Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2024) is a promising direction. \n\nFurther Theoretical Analysis for Parallel Scaling Laws One of our contributions is quantitatively computing the impact of parameters and computation on model capability. Although we present some theoretical results (Proposition 1), the challenge of directly modeling DIVERSITY limits us to using extensive experiments to fit parallel scaling laws. Why the diversity is related to log P, is there a growth rate that exceeds O(log P), and whether there is a performance upper bound for P \u226b 8, remain open questions.",
                    "score": 0.7870669596041316,
                    "section_title": "Discussion and Future Work",
                    "char_start_offset": 32802,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 966
                        },
                        {
                            "start": 969,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1482
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 467,
                            "end": 488,
                            "matchedPaperCorpusId": "266693796"
                        },
                        {
                            "start": 605,
                            "end": 622,
                            "matchedPaperCorpusId": "271601023"
                        },
                        {
                            "start": 622,
                            "end": 641,
                            "matchedPaperCorpusId": "278498044"
                        },
                        {
                            "start": 918,
                            "end": 940,
                            "matchedPaperCorpusId": "266693796"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8056640625
                }
            ],
            "relevance_judgement": 0.88671875,
            "relevance_judgment_input_expanded": "# Title: Parallel Scaling Law for Language Models\n# Venue: \n# Authors: Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxin Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu\n## Abstract\nIt is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.\n## Discussion and Future Work\nTraining Inference-Optimal Language Models Chinchilla (Hoffmann et al., 2022) explored the scaling law to determine the training-optimal amounts for parameters and training data under a training FLOP budget. On the other hands, modern LLMs are increasingly interested on inference-optimal models. Some practitioners use much more data than the Chinchilla recommendation to train small models due to their high inference efficiency (Qwen Team, 2024;Allal et al., 2025;Sardana et al., 2024). Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2024) is a promising direction. \n\nFurther Theoretical Analysis for Parallel Scaling Laws One of our contributions is quantitatively computing the impact of parameters and computation on model capability. Although we present some theoretical results (Proposition 1), the challenge of directly modeling DIVERSITY limits us to using extensive experiments to fit parallel scaling laws. Why the diversity is related to log P, is there a growth rate that exceeds O(log P), and whether there is a performance upper bound for P \u226b 8, remain open questions.",
            "reference_string": "[278636433 | Chen et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Long Is More Important Than Difficult for Training Reasoning Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283092321",
                    "name": "Si Shen"
                },
                {
                    "authorId": "2353326337",
                    "name": "Fei Huang"
                },
                {
                    "authorId": "2214963769",
                    "name": "Zhixiao Zhao"
                },
                {
                    "authorId": "2351851812",
                    "name": "Chang Liu"
                },
                {
                    "authorId": "2351808837",
                    "name": "Tiansheng Zheng"
                },
                {
                    "authorId": "2283097884",
                    "name": "Danhao Zhu"
                }
            ],
            "abstract": "Difficult problems, which often result in long reasoning traces, are widely recognized as key factors for enhancing the performance of reasoning models. However, such high-challenge problems are scarce, limiting the size of available datasets. In this paper, we propose a simple method to decouple the reliance on problem difficulty. First, we empirically demonstrate that reasoning length, rather than problem difficulty, primarily influences the performance of trained models. Second, we identify a scaling law on reasoning length, showing that model performance increases in a log-linear fashion as the reasoning data length grows. Finally, we introduce a straightforward technique to generate reasoning data of arbitrary length, and show that synthesized data is effective for training reasoning models. After fine-tuning the Qwen2.5-32B-Instruct language model on our Long1K dataset, we present our model, Long1K-32B, which achieves remarkable performance with only 1,000 training samples, achieving 95.6\\% accuracy on MATH, and 71.1\\% on GPQA outperforming DeepSeek-R1-Distill-Qwen-32B. The model, code, and dataset are all open-sourced, available at https://huggingface.co/ZTss/LONG1.",
            "corpus_id": 277271576,
            "sentences": [
                {
                    "corpus_id": "277271576",
                    "title": "Long Is More Important Than Difficult for Training Reasoning Models",
                    "text": "Scaling laws have emerged as a crucial tool in the research of large language models (LLMs) for guiding model design and predicting performance. During the training phase, researchers [11] have identified that model performance scales predictably with model size, dataset size, and computational resources. In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters , and it is proposed that optimizing inference strategies can significantly improve inference accuracy. Our research introduces data length as a crucial scaling factor, demonstrating that the performance of LLMs in multi-step mathematical reasoning scales significantly with the length of reasoning chains.",
                    "score": 0.6189412311883326,
                    "section_title": "Scaling law on LLM",
                    "char_start_offset": 23880,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 865
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88623046875
                }
            ],
            "relevance_judgement": 0.88623046875,
            "relevance_judgment_input_expanded": "# Title: Long Is More Important Than Difficult for Training Reasoning Models\n# Venue: arXiv.org\n# Authors: Si Shen, Fei Huang, Zhixiao Zhao, Chang Liu, Tiansheng Zheng, Danhao Zhu\n## Abstract\nDifficult problems, which often result in long reasoning traces, are widely recognized as key factors for enhancing the performance of reasoning models. However, such high-challenge problems are scarce, limiting the size of available datasets. In this paper, we propose a simple method to decouple the reliance on problem difficulty. First, we empirically demonstrate that reasoning length, rather than problem difficulty, primarily influences the performance of trained models. Second, we identify a scaling law on reasoning length, showing that model performance increases in a log-linear fashion as the reasoning data length grows. Finally, we introduce a straightforward technique to generate reasoning data of arbitrary length, and show that synthesized data is effective for training reasoning models. After fine-tuning the Qwen2.5-32B-Instruct language model on our Long1K dataset, we present our model, Long1K-32B, which achieves remarkable performance with only 1,000 training samples, achieving 95.6\\% accuracy on MATH, and 71.1\\% on GPQA outperforming DeepSeek-R1-Distill-Qwen-32B. The model, code, and dataset are all open-sourced, available at https://huggingface.co/ZTss/LONG1.\n## Scaling law on LLM\nScaling laws have emerged as a crucial tool in the research of large language models (LLMs) for guiding model design and predicting performance. During the training phase, researchers [11] have identified that model performance scales predictably with model size, dataset size, and computational resources. In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters , and it is proposed that optimizing inference strategies can significantly improve inference accuracy. Our research introduces data length as a crucial scaling factor, demonstrating that the performance of LLMs in multi-step mathematical reasoning scales significantly with the length of reasoning chains.",
            "reference_string": "[277271576 | Shen et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18941, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312897913",
                    "name": "Hongru Cai"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2273661632",
                    "name": "Ruifeng Yuan"
                },
                {
                    "authorId": "2298418808",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2284702952",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "2284641622",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ],
            "abstract": "Generative retrieval reformulates retrieval as an autoregressive generation task, where large language models (LLMs) generate target documents directly from a query. As a novel paradigm, the mechanisms that underpin its performance and scalability remain largely unexplored. We systematically investigate training and inference scaling laws in generative retrieval, exploring how model size, training data scale, and inference-time compute jointly influence performance. We propose a novel evaluation metric inspired by contrastive entropy and generation loss, providing a continuous performance signal that enables robust comparisons across diverse generative retrieval methods. Our experiments show that n-gram-based methods align strongly with training and inference scaling laws. We find that increasing model size, training data scale, and inference-time compute all contribute to improved performance, highlighting the complementary roles of these factors in enhancing generative retrieval. Across these settings, LLaMA models consistently outperform T5 models, suggesting a particular advantage for larger decoder-only models in generative retrieval. Our findings underscore that model sizes, data availability, and inference computation interact to unlock the full potential of generative retrieval, offering new insights for designing and optimizing future systems.",
            "corpus_id": 277940798,
            "sentences": [],
            "relevance_judgement": 0.8740234375,
            "relevance_judgment_input_expanded": "# Title: Exploring Training and Inference Scaling Laws in Generative Retrieval\n# Venue: arXiv.org\n# Authors: Hongru Cai, Yongqi Li, Ruifeng Yuan, Wenjie Wang, Zhen Zhang, Wenjie Li, Tat-Seng Chua\n## Abstract\nGenerative retrieval reformulates retrieval as an autoregressive generation task, where large language models (LLMs) generate target documents directly from a query. As a novel paradigm, the mechanisms that underpin its performance and scalability remain largely unexplored. We systematically investigate training and inference scaling laws in generative retrieval, exploring how model size, training data scale, and inference-time compute jointly influence performance. We propose a novel evaluation metric inspired by contrastive entropy and generation loss, providing a continuous performance signal that enables robust comparisons across diverse generative retrieval methods. Our experiments show that n-gram-based methods align strongly with training and inference scaling laws. We find that increasing model size, training data scale, and inference-time compute all contribute to improved performance, highlighting the complementary roles of these factors in enhancing generative retrieval. Across these settings, LLaMA models consistently outperform T5 models, suggesting a particular advantage for larger decoder-only models in generative retrieval. Our findings underscore that model sizes, data availability, and inference computation interact to unlock the full potential of generative retrieval, offering new insights for designing and optimizing future systems.\n",
            "reference_string": "[277940798 | Cai et al. | 2025 | Citations: 1]"
        },
        {
            "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 190,
            "citation_count": 77,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2129663",
                    "name": "S. Welleck"
                },
                {
                    "authorId": "2138301112",
                    "name": "Amanda Bertsch"
                },
                {
                    "authorId": "1580418311",
                    "name": "Matthew Finlayson"
                },
                {
                    "authorId": "2184031883",
                    "name": "Hailey Schoelkopf"
                },
                {
                    "authorId": "2253395527",
                    "name": "Alex Xie"
                },
                {
                    "authorId": "2265547593",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "2308102420",
                    "name": "Ilia Kulikov"
                },
                {
                    "authorId": "2265540561",
                    "name": "Zaid Harchaoui"
                }
            ],
            "abstract": "One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.",
            "corpus_id": 270703266,
            "sentences": [
                {
                    "corpus_id": "270703266",
                    "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
                    "text": "One of the most striking findings in modern research on large language models (LLMs) is that, given a model and dataset of sufficient scale, scaling up compute at training time leads to better final results (Kaplan et al., 2020;Hoffmann et al., 2022).However, there is also another lesser-mentioned scaling phenomenon, where adopting more sophisticated methods or scaling compute at inference time (Jones, 2021)",
                    "score": 0.6438978997865911,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 251
                        },
                        {
                            "start": 251,
                            "end": 411
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 228,
                            "end": 250,
                            "matchedPaperCorpusId": "258509679"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86376953125
                }
            ],
            "relevance_judgement": 0.86376953125,
            "relevance_judgment_input_expanded": "# Title: From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models\n# Venue: Trans. Mach. Learn. Res.\n# Authors: S. Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui\n## Abstract\nOne of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.\n## Introduction\nOne of the most striking findings in modern research on large language models (LLMs) is that, given a model and dataset of sufficient scale, scaling up compute at training time leads to better final results (Kaplan et al., 2020;Hoffmann et al., 2022).However, there is also another lesser-mentioned scaling phenomenon, where adopting more sophisticated methods or scaling compute at inference time (Jones, 2021)",
            "reference_string": "[270703266 | Welleck et al. | 2024 | Citations: 77]"
        },
        {
            "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 117,
            "influential_citation_count": 18,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2127328758",
                    "name": "Hongyu Wang"
                },
                {
                    "authorId": "2118866998",
                    "name": "Shuming Ma"
                },
                {
                    "authorId": "145307652",
                    "name": "Li Dong"
                },
                {
                    "authorId": "3110003",
                    "name": "Shaohan Huang"
                },
                {
                    "authorId": "2216204941",
                    "name": "Huaijie Wang"
                },
                {
                    "authorId": "2259609416",
                    "name": "Lingxiao Ma"
                },
                {
                    "authorId": "2321493763",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2282983050",
                    "name": "Ruiping Wang"
                },
                {
                    "authorId": "2336830631",
                    "name": "Yi Wu"
                },
                {
                    "authorId": "2253471545",
                    "name": "Furu Wei"
                }
            ],
            "abstract": "The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.",
            "corpus_id": 264172438,
            "sentences": [
                {
                    "corpus_id": "264172438",
                    "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
                    "text": "Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models. \n\nTo study the scaling law of binarized Transformer, we start by plotting the scaling curve of both BitNet and the FP16 Transformer baseline against the parameter count. We fix the number of training tokens and vary the model sizes. Figure 3 shows that the loss scaling of BitNet is similar to the FP16 Transformer, which follows a power-law. We then fit the scaling law with an irreducible loss term: To evaluate whether the scaling law can accurately predict the loss, we choose the models from 125M to 6.7B to fit the parameters in the power-law and use the law to predict the loss of 13B and 30B. It shows that the fitted scaling law predicted BitNet's loss with high accuracy. Besides, the gap between BitNet and FP16 Transformer becomes smaller as the model size grows. \n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once. We estimate the energy consumption as in Section 2.3. Figure 3 shows the scaling curve against the inference energy cost at 7nm process nodes. It proves that BitNet has much higher scaling efficiency. Given a fixed computation budget, BitNet achieves a significantly better loss. Meanwhile, the inference cost is much smaller to get the same performance as the FP16 models.",
                    "score": 0.7984850104899606,
                    "section_title": "Inference-Optimal Scaling Law",
                    "char_start_offset": 13008,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 344
                        },
                        {
                            "start": 347,
                            "end": 514
                        },
                        {
                            "start": 515,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1026
                        },
                        {
                            "start": 1027,
                            "end": 1120
                        },
                        {
                            "start": 1123,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1543
                        },
                        {
                            "start": 1544,
                            "end": 1671
                        },
                        {
                            "start": 1672,
                            "end": 1724
                        },
                        {
                            "start": 1725,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 1985
                        },
                        {
                            "start": 1986,
                            "end": 2043
                        },
                        {
                            "start": 2044,
                            "end": 2122
                        },
                        {
                            "start": 2123,
                            "end": 2216
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86083984375
                }
            ],
            "relevance_judgement": 0.86083984375,
            "relevance_judgment_input_expanded": "# Title: BitNet: Scaling 1-bit Transformers for Large Language Models\n# Venue: arXiv.org\n# Authors: Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei\n## Abstract\nThe increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.\n## Inference-Optimal Scaling Law\nNeural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models. \n\nTo study the scaling law of binarized Transformer, we start by plotting the scaling curve of both BitNet and the FP16 Transformer baseline against the parameter count. We fix the number of training tokens and vary the model sizes. Figure 3 shows that the loss scaling of BitNet is similar to the FP16 Transformer, which follows a power-law. We then fit the scaling law with an irreducible loss term: To evaluate whether the scaling law can accurately predict the loss, we choose the models from 125M to 6.7B to fit the parameters in the power-law and use the law to predict the loss of 13B and 30B. It shows that the fitted scaling law predicted BitNet's loss with high accuracy. Besides, the gap between BitNet and FP16 Transformer becomes smaller as the model size grows. \n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once. We estimate the energy consumption as in Section 2.3. Figure 3 shows the scaling curve against the inference energy cost at 7nm process nodes. It proves that BitNet has much higher scaling efficiency. Given a fixed computation budget, BitNet achieves a significantly better loss. Meanwhile, the inference cost is much smaller to get the same performance as the FP16 models.",
            "reference_string": "[264172438 | Wang et al. | 2023 | Citations: 117]"
        },
        {
            "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268494857",
                    "name": "Harsha Nori"
                },
                {
                    "authorId": "2637252",
                    "name": "N. Usuyama"
                },
                {
                    "authorId": "2214807281",
                    "name": "Nicholas King"
                },
                {
                    "authorId": "2047820455",
                    "name": "S. McKinney"
                },
                {
                    "authorId": "2329372335",
                    "name": "Xavier Fernandes"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2268493614",
                    "name": "Eric Horvitz"
                }
            ],
            "abstract": "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.",
            "corpus_id": 273850391,
            "sentences": [
                {
                    "corpus_id": "273850391",
                    "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
                    "text": "Scaling laws [KMH + 20] for large language models (LLMs) have proven to be valuable for predicting how model capabilities grow with increasing data, compute, and model size. However, inference-time scaling-which focuses on the value of investing in additional computation at model run-time-is a promising new area of study. We see opportunities for innovating on how best to guide such run-time allocations for advancing efficiency, accuracy, and reasoning abilities. We review in this section promising directions in this emerging area of research and development, partly framed by our experimental results and prior research in AI on reasoning, ensembling, and control of the nature and extent of inference.",
                    "score": 0.7652103430898813,
                    "section_title": "Directions with LLM Run-Time Strategies",
                    "char_start_offset": 19823,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 709
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85986328125
                }
            ],
            "relevance_judgement": 0.85986328125,
            "relevance_judgment_input_expanded": "# Title: From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond\n# Venue: arXiv.org\n# Authors: Harsha Nori, N. Usuyama, Nicholas King, S. McKinney, Xavier Fernandes, Sheng Zhang, Eric Horvitz\n## Abstract\nRun-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.\n## Directions with LLM Run-Time Strategies\nScaling laws [KMH + 20] for large language models (LLMs) have proven to be valuable for predicting how model capabilities grow with increasing data, compute, and model size. However, inference-time scaling-which focuses on the value of investing in additional computation at model run-time-is a promising new area of study. We see opportunities for innovating on how best to guide such run-time allocations for advancing efficiency, accuracy, and reasoning abilities. We review in this section promising directions in this emerging area of research and development, partly framed by our experimental results and prior research in AI on reasoning, ensembling, and control of the nature and extent of inference.",
            "reference_string": "[273850391 | Nori et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 128,
            "citation_count": 48,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1387466862",
                    "name": "S. Gadre"
                },
                {
                    "authorId": "1438310376",
                    "name": "G. Smyrnis"
                },
                {
                    "authorId": "34961417",
                    "name": "Vaishaal Shankar"
                },
                {
                    "authorId": "40895369",
                    "name": "Suchin Gururangan"
                },
                {
                    "authorId": "52193502",
                    "name": "Mitchell Wortsman"
                },
                {
                    "authorId": "2287946530",
                    "name": "Rulin Shao"
                },
                {
                    "authorId": "72847120",
                    "name": "Jean-Pierre Mercat"
                },
                {
                    "authorId": "46372713",
                    "name": "Alex Fang"
                },
                {
                    "authorId": "2287821949",
                    "name": "Jeffrey Li"
                },
                {
                    "authorId": "150299584",
                    "name": "Sedrick Scott Keh"
                },
                {
                    "authorId": "2291068532",
                    "name": "Rui Xin"
                },
                {
                    "authorId": "2174178585",
                    "name": "Marianna Nezhurina"
                },
                {
                    "authorId": "2291068185",
                    "name": "Igor Vasiljevic"
                },
                {
                    "authorId": "2191688",
                    "name": "J. Jitsev"
                },
                {
                    "authorId": "2257301126",
                    "name": "Alexandros G. Dimakis"
                },
                {
                    "authorId": "1387994137",
                    "name": "Gabriel Ilharco"
                },
                {
                    "authorId": "2265621012",
                    "name": "Shuran Song"
                },
                {
                    "authorId": "2283843631",
                    "name": "Thomas Kollar"
                },
                {
                    "authorId": "2444742",
                    "name": "Y. Carmon"
                },
                {
                    "authorId": "2298523",
                    "name": "Achal Dave"
                },
                {
                    "authorId": "145639495",
                    "name": "Reinhard Heckel"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "2253541812",
                    "name": "Ludwig Schmidt"
                }
            ],
            "abstract": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e.,\"Chinchilla optimal\"regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.",
            "corpus_id": 268379614,
            "sentences": [
                {
                    "corpus_id": "268379614",
                    "title": "Language models scale reliably with over-training and on downstream tasks",
                    "text": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e.,\"Chinchilla optimal\"regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.",
                    "score": 0.9476544960333888,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8525390625
                },
                {
                    "corpus_id": "268379614",
                    "title": "Language models scale reliably with over-training and on downstream tasks",
                    "text": "Training large language models is expensive.Furthermore, training high-quality models requires a complex recipe of algorithmic techniques and training data.To reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate their efficacy to larger model and data regimes via scaling laws.With reliable extrapolation, it is possible to quickly iterate at small scale and still pick the method that will perform best for the final large training run.Indeed, this workflow has become commonplace for training state-of-the-art language models like Chinchilla 70B [45], PaLM 540B [19], GPT-4 [76], and many others.We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training.We are able to extrapolate, in both N and M , the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law.(right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss.We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction.We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute.For this figure, we train all models on RedPajama [112].\n\nDespite their importance for model development, published scaling laws differ from the goals of training state-of-the-art models in important ways.For instance, scaling studies usually focus on the compute-optimal training regime (\"Chinchilla optimality\" [45]), where model and dataset size are set to yield minimum loss for a given compute budget.However, this setting ignores inference costs.As larger models are more expensive at inference, it is now common practice to over-train smaller models [113].Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets.",
                    "score": 0.8218023613064185,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 44
                        },
                        {
                            "start": 44,
                            "end": 156
                        },
                        {
                            "start": 156,
                            "end": 357
                        },
                        {
                            "start": 357,
                            "end": 517
                        },
                        {
                            "start": 517,
                            "end": 678
                        },
                        {
                            "start": 678,
                            "end": 958
                        },
                        {
                            "start": 958,
                            "end": 1121
                        },
                        {
                            "start": 1121,
                            "end": 1230
                        },
                        {
                            "start": 1230,
                            "end": 1372
                        },
                        {
                            "start": 1372,
                            "end": 1471
                        },
                        {
                            "start": 1471,
                            "end": 1527
                        },
                        {
                            "start": 1529,
                            "end": 1676
                        },
                        {
                            "start": 1676,
                            "end": 1877
                        },
                        {
                            "start": 1877,
                            "end": 1923
                        },
                        {
                            "start": 1923,
                            "end": 2034
                        },
                        {
                            "start": 2034,
                            "end": 2207
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 628,
                            "end": 632,
                            "matchedPaperCorpusId": "247778764"
                        },
                        {
                            "start": 644,
                            "end": 648,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 1784,
                            "end": 1788,
                            "matchedPaperCorpusId": "247778764"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8037109375
                }
            ],
            "relevance_judgement": 0.8525390625,
            "relevance_judgment_input_expanded": "# Title: Language models scale reliably with over-training and on downstream tasks\n# Venue: International Conference on Learning Representations\n# Authors: S. Gadre, G. Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean-Pierre Mercat, Alex Fang, Jeffrey Li, Sedrick Scott Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, J. Jitsev, Alexandros G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Y. Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, Ludwig Schmidt\n## Abstract\nScaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e.,\"Chinchilla optimal\"regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.\n## Introduction\nTraining large language models is expensive.Furthermore, training high-quality models requires a complex recipe of algorithmic techniques and training data.To reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate their efficacy to larger model and data regimes via scaling laws.With reliable extrapolation, it is possible to quickly iterate at small scale and still pick the method that will perform best for the final large training run.Indeed, this workflow has become commonplace for training state-of-the-art language models like Chinchilla 70B [45], PaLM 540B [19], GPT-4 [76], and many others.We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training.We are able to extrapolate, in both N and M , the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law.(right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss.We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction.We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute.For this figure, we train all models on RedPajama [112].\n\nDespite their importance for model development, published scaling laws differ from the goals of training state-of-the-art models in important ways.For instance, scaling studies usually focus on the compute-optimal training regime (\"Chinchilla optimality\" [45]), where model and dataset size are set to yield minimum loss for a given compute budget.However, this setting ignores inference costs.As larger models are more expensive at inference, it is now common practice to over-train smaller models [113].Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets.",
            "reference_string": "[268379614 | Gadre et al. | 2024 | Citations: 48]"
        },
        {
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 62,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.10938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "82939895",
                    "name": "Yangjun Ruan"
                },
                {
                    "authorId": "2772217",
                    "name": "Chris J. Maddison"
                },
                {
                    "authorId": "2302156937",
                    "name": "Tatsunori B. Hashimoto"
                }
            ],
            "abstract": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.",
            "corpus_id": 269899695,
            "sentences": [
                {
                    "corpus_id": "269899695",
                    "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
                    "text": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.",
                    "score": 0.7551384511725324,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85205078125
                },
                {
                    "corpus_id": "269899695",
                    "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
                    "text": "(e.g., agentic or \"emergent\" behaviors) and post-training interventions, can be done using a lower-cost, higher-resolution, and broader-coverage alternative to the standard approach of training (or using) a single family of LMs across compute scales. \n\nThe starting point of our work is the observation that there now exist hundreds of open models spanning a large range of scales and capabilities. While we cannot directly use these models for compute scaling laws (as the training compute efficiency varies widely across model families), we might hope that there exists a more general scaling law that holds across model families. In particular, we hypothesize that the downstream performance of an LM is a function of a low-dimensional space of capabilities (e.g., natural language understanding, reasoning, and code generation), and that model families vary only in the efficiency by which they convert training compute to these capabilities. If such a relationship held, it would imply that there is a log-linear relationship from low-dimensional capabilities to downstream capabilities across model families (which would allow us to build scaling laws that leverage all existing models), as well as a log-linear relationship between training compute and capabilities within each model family (as in standard compute scaling) (Fig. 1). \n\nThrough an analysis of existing standardized LM benchmarks (e.g., Open LLM Leaderboard [8]), we find a few such capability measures that have scaling law relationships with compute within model families (R 2 > 0.9) (Fig. 3), and with downstream metrics across model families. We call such scaling relationships observational scaling laws as they enable the predictions of complex downstream capabilities from simple observable quantities that we expect to scale with compute (like standardized benchmark performance). \n\nThe ability to build scaling laws across a large number of existing LMs from their standard benchmark metrics has significant advantages in cost, resolution, and coverage: Observational scaling incurs no training cost, while leveraging models spanning a much larger compute range than any single model family. It also significantly increases the resolution of scaling laws by virtue of using more models, which is useful for studying nearly discontinuous phenomena like \"emergent\" capabilities.",
                    "score": 0.592653850483096,
                    "section_title": "Introduction",
                    "char_start_offset": 1988,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 250
                        },
                        {
                            "start": 253,
                            "end": 398
                        },
                        {
                            "start": 399,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1340
                        },
                        {
                            "start": 1343,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1860
                        },
                        {
                            "start": 1863,
                            "end": 2172
                        },
                        {
                            "start": 2173,
                            "end": 2357
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8251953125
                },
                {
                    "corpus_id": "269899695",
                    "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
                    "text": "Language model (LM) scaling plays a central role in discussions of model capabilities and affects everything from the tasks they can perform to the effectiveness of post-training techniques such as Chain-of-Thought [99]. Due to this importance, understanding and predicting LM behaviors across scales, benchmarks, and algorithmic interventions is a major question for many researchers and engineers. Machine learning researchers may wish to understand whether their proposed algorithmic interventions remain effective in the face of future model scaling, while engineers and benchmark builders may wish to understand whether complex capabilities such as agentic abilities will scale predictably in the same way as existing LM benchmarks. \n\nScaling laws [6,36,37,44,65] have been powerful tools for understanding the scaling trend of LMs, which have shown that LMs follow a precise power-law relationship between compute measures (such as training FLOPs) and downstream capabilities ranging from perplexity [37,44] to benchmark performance [34,35]. This power-law relationship has been used in a variety of waysincluding hyperparameter and architecture selection [9,37,44] as well as model capability forecasting [25,66,67]. Unfortunately, scaling analyses remain uncommon in many benchmarking and posttraining studies, as most researchers do not have the compute resources to build scaling laws from scratch, and open models are trained at too few scales (3)(4)(5) for reliable scaling predictions. training compute to downstream capabilities (dashed line) by hypothesizing the existence of a lowrank space of LM capabilities that have a log-linear relationship with compute (center), and can be extracted directly from standardized LM benchmarks (left). This enables us to get low-cost, high-resolution scaling predictions of LMs' complex downstream capabilities from their observable standard benchmark metrics using nearly 100 publicly accessible LMs (left to right). \n\n(e.g., agentic or \"emergent\" behaviors) and post-training interventions, can be done using a lower-cost, higher-resolution, and broader-coverage alternative to the standard approach of training (or using) a single family of LMs across compute scales.",
                    "score": 0.769814897823177,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 737
                        },
                        {
                            "start": 740,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1970
                        },
                        {
                            "start": 1973,
                            "end": 2223
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 215,
                            "end": 219,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 765,
                            "end": 768,
                            "matchedPaperCorpusId": "258888192"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8056640625
                }
            ],
            "relevance_judgement": 0.85205078125,
            "relevance_judgment_input_expanded": "# Title: Observational Scaling Laws and the Predictability of Language Model Performance\n# Venue: Neural Information Processing Systems\n# Authors: Yangjun Ruan, Chris J. Maddison, Tatsunori B. Hashimoto\n## Abstract\nUnderstanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.\n## Introduction\nLanguage model (LM) scaling plays a central role in discussions of model capabilities and affects everything from the tasks they can perform to the effectiveness of post-training techniques such as Chain-of-Thought [99]. Due to this importance, understanding and predicting LM behaviors across scales, benchmarks, and algorithmic interventions is a major question for many researchers and engineers. Machine learning researchers may wish to understand whether their proposed algorithmic interventions remain effective in the face of future model scaling, while engineers and benchmark builders may wish to understand whether complex capabilities such as agentic abilities will scale predictably in the same way as existing LM benchmarks. \n\nScaling laws [6,36,37,44,65] have been powerful tools for understanding the scaling trend of LMs, which have shown that LMs follow a precise power-law relationship between compute measures (such as training FLOPs) and downstream capabilities ranging from perplexity [37,44] to benchmark performance [34,35]. This power-law relationship has been used in a variety of waysincluding hyperparameter and architecture selection [9,37,44] as well as model capability forecasting [25,66,67]. Unfortunately, scaling analyses remain uncommon in many benchmarking and posttraining studies, as most researchers do not have the compute resources to build scaling laws from scratch, and open models are trained at too few scales (3)(4)(5) for reliable scaling predictions. training compute to downstream capabilities (dashed line) by hypothesizing the existence of a lowrank space of LM capabilities that have a log-linear relationship with compute (center), and can be extracted directly from standardized LM benchmarks (left). This enables us to get low-cost, high-resolution scaling predictions of LMs' complex downstream capabilities from their observable standard benchmark metrics using nearly 100 publicly accessible LMs (left to right). \n\n(e.g., agentic or \"emergent\" behaviors) and post-training interventions, can be done using a lower-cost, higher-resolution, and broader-coverage alternative to the standard approach of training (or using) a single family of LMs across compute scales.\n...\n(e.g., agentic or \"emergent\" behaviors) and post-training interventions, can be done using a lower-cost, higher-resolution, and broader-coverage alternative to the standard approach of training (or using) a single family of LMs across compute scales. \n\nThe starting point of our work is the observation that there now exist hundreds of open models spanning a large range of scales and capabilities. While we cannot directly use these models for compute scaling laws (as the training compute efficiency varies widely across model families), we might hope that there exists a more general scaling law that holds across model families. In particular, we hypothesize that the downstream performance of an LM is a function of a low-dimensional space of capabilities (e.g., natural language understanding, reasoning, and code generation), and that model families vary only in the efficiency by which they convert training compute to these capabilities. If such a relationship held, it would imply that there is a log-linear relationship from low-dimensional capabilities to downstream capabilities across model families (which would allow us to build scaling laws that leverage all existing models), as well as a log-linear relationship between training compute and capabilities within each model family (as in standard compute scaling) (Fig. 1). \n\nThrough an analysis of existing standardized LM benchmarks (e.g., Open LLM Leaderboard [8]), we find a few such capability measures that have scaling law relationships with compute within model families (R 2 > 0.9) (Fig. 3), and with downstream metrics across model families. We call such scaling relationships observational scaling laws as they enable the predictions of complex downstream capabilities from simple observable quantities that we expect to scale with compute (like standardized benchmark performance). \n\nThe ability to build scaling laws across a large number of existing LMs from their standard benchmark metrics has significant advantages in cost, resolution, and coverage: Observational scaling incurs no training cost, while leveraging models spanning a much larger compute range than any single model family. It also significantly increases the resolution of scaling laws by virtue of using more models, which is useful for studying nearly discontinuous phenomena like \"emergent\" capabilities.",
            "reference_string": "[269899695 | Ruan et al. | 2024 | Citations: 62]"
        },
        {
            "title": "Scaling Laws for Neural Language Models",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 59,
            "citation_count": 4919,
            "influential_citation_count": 381,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.08361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152724169",
                    "name": "J. Kaplan"
                },
                {
                    "authorId": "52238703",
                    "name": "Sam McCandlish"
                },
                {
                    "authorId": "103143311",
                    "name": "T. Henighan"
                },
                {
                    "authorId": "31035595",
                    "name": "Tom B. Brown"
                },
                {
                    "authorId": "1490681878",
                    "name": "Benjamin Chess"
                },
                {
                    "authorId": "48422824",
                    "name": "R. Child"
                },
                {
                    "authorId": "145565184",
                    "name": "Scott Gray"
                },
                {
                    "authorId": "38909097",
                    "name": "Alec Radford"
                },
                {
                    "authorId": "49387725",
                    "name": "Jeff Wu"
                },
                {
                    "authorId": "2698777",
                    "name": "Dario Amodei"
                }
            ],
            "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
            "corpus_id": 210861095,
            "sentences": [
                {
                    "corpus_id": "210861095",
                    "title": "Scaling Laws for Neural Language Models",
                    "text": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
                    "score": 0.7163580906245168,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83837890625
                }
            ],
            "relevance_judgement": 0.83837890625,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws for Neural Language Models\n# Venue: arXiv.org\n# Authors: J. Kaplan, Sam McCandlish, T. Henighan, Tom B. Brown, Benjamin Chess, R. Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei\n## Abstract\nWe study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.\n",
            "reference_string": "[210861095 | Kaplan et al. | 2020 | Citations: 4919]"
        },
        {
            "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 56,
            "citation_count": 28,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320149516",
                    "name": "Enze Xie"
                },
                {
                    "authorId": "2212250873",
                    "name": "Junsong Chen"
                },
                {
                    "authorId": "2109814772",
                    "name": "Yuyang Zhao"
                },
                {
                    "authorId": "2193887687",
                    "name": "Jincheng Yu"
                },
                {
                    "authorId": "20515689",
                    "name": "Ligeng Zhu"
                },
                {
                    "authorId": "49417466",
                    "name": "Yujun Lin"
                },
                {
                    "authorId": "2286139423",
                    "name": "Zhekai Zhang"
                },
                {
                    "authorId": "2288740166",
                    "name": "Muyang Li"
                },
                {
                    "authorId": "2325895662",
                    "name": "Junyu Chen"
                },
                {
                    "authorId": "2114069742",
                    "name": "Han Cai"
                },
                {
                    "authorId": "2342993953",
                    "name": "Bingchen Liu"
                },
                {
                    "authorId": "2344575401",
                    "name": "Daquan Zhou"
                },
                {
                    "authorId": "2283171540",
                    "name": "Song Han"
                }
            ],
            "abstract": "This paper presents SANA-1.5, a linear Diffusion Transformer for efficient scaling in text-to-image generation. Building upon SANA-1.0, we introduce three key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that enables scaling from 1.6B to 4.8B parameters with significantly reduced computational resources, combined with a memory-efficient 8-bit optimizer. (2) Model Depth Pruning: A block importance analysis technique for efficient model compression to arbitrary sizes with minimal quality loss. (3) Inference-time Scaling: A repeated sampling strategy that trades computation for model capacity, enabling smaller models to match larger model quality at inference time. Through these strategies, SANA-1.5 achieves a text-image alignment score of 0.81 on GenEval, which can be further improved to 0.96 through inference scaling with VILA-Judge, establishing a new SoTA on GenEval benchmark. These innovations enable efficient model scaling across different compute budgets while maintaining high quality, making high-quality image generation more accessible. Our code and pre-trained models are released.",
            "corpus_id": 275993956,
            "sentences": [
                {
                    "corpus_id": "275993956",
                    "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
                    "text": "Training Scaling in LLM and DiT Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34]. Furthermore, extensive experiments on scaling both denoising backbones and training sets reveal that increasing transformer blocks is more parameter-efficient than increasing channel numbers for improving text-image alignment. The quality and diversity of the training set prove more crucial than mere dataset size [33]. These findings provide valuable insights for determining optimal model architectures and data requirements in both domains. \n\nInference Scaling Law Recent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship. Building upon this, self-consistency approaches demonstrated that sampling multiple reasoning paths and selecting the most consistent answer can substantially improve model accuracy [53]. This was further enhanced by progressive-hint prompting techniques [54], achieving significant gains on various reasoning benchmarks. Recent theoretical work [55] shows that smaller models paired with advanced inference algorithms can outperform larger models under the same computation budget. However, studies on compound inference systems [56] reveal that increasing LLM calls shows non-monotonic behavior, performing better on \"easy\" queries but worse on \"hard\" ones. These findings collectively demonstrate the importance of optimizing inference strategies rather than simply scaling up model size or increasing the sampling budget. Concurrent works [37,38] have also independently explored and validated the effectiveness of inference scaling in diffusion models.",
                    "score": 0.7004899592904988,
                    "section_title": "A. Full Related Work",
                    "char_start_offset": 25352,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1038
                        },
                        {
                            "start": 1041,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1346
                        },
                        {
                            "start": 1347,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1668
                        },
                        {
                            "start": 1669,
                            "end": 1829
                        },
                        {
                            "start": 1830,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2172
                        },
                        {
                            "start": 2173,
                            "end": 2304
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 105,
                            "end": 108,
                            "matchedPaperCorpusId": "252220884"
                        },
                        {
                            "start": 120,
                            "end": 124,
                            "matchedPaperCorpusId": "268875775"
                        },
                        {
                            "start": 909,
                            "end": 913,
                            "matchedPaperCorpusId": "268875775"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.837890625
                }
            ],
            "relevance_judgement": 0.837890625,
            "relevance_judgment_input_expanded": "# Title: SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer\n# Venue: arXiv.org\n# Authors: Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, Bingchen Liu, Daquan Zhou, Song Han\n## Abstract\nThis paper presents SANA-1.5, a linear Diffusion Transformer for efficient scaling in text-to-image generation. Building upon SANA-1.0, we introduce three key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that enables scaling from 1.6B to 4.8B parameters with significantly reduced computational resources, combined with a memory-efficient 8-bit optimizer. (2) Model Depth Pruning: A block importance analysis technique for efficient model compression to arbitrary sizes with minimal quality loss. (3) Inference-time Scaling: A repeated sampling strategy that trades computation for model capacity, enabling smaller models to match larger model quality at inference time. Through these strategies, SANA-1.5 achieves a text-image alignment score of 0.81 on GenEval, which can be further improved to 0.96 through inference scaling with VILA-Judge, establishing a new SoTA on GenEval benchmark. These innovations enable efficient model scaling across different compute budgets while maintaining high quality, making high-quality image generation more accessible. Our code and pre-trained models are released.\n## A. Full Related Work\nTraining Scaling in LLM and DiT Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34]. Furthermore, extensive experiments on scaling both denoising backbones and training sets reveal that increasing transformer blocks is more parameter-efficient than increasing channel numbers for improving text-image alignment. The quality and diversity of the training set prove more crucial than mere dataset size [33]. These findings provide valuable insights for determining optimal model architectures and data requirements in both domains. \n\nInference Scaling Law Recent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship. Building upon this, self-consistency approaches demonstrated that sampling multiple reasoning paths and selecting the most consistent answer can substantially improve model accuracy [53]. This was further enhanced by progressive-hint prompting techniques [54], achieving significant gains on various reasoning benchmarks. Recent theoretical work [55] shows that smaller models paired with advanced inference algorithms can outperform larger models under the same computation budget. However, studies on compound inference systems [56] reveal that increasing LLM calls shows non-monotonic behavior, performing better on \"easy\" queries but worse on \"hard\" ones. These findings collectively demonstrate the importance of optimizing inference strategies rather than simply scaling up model size or increasing the sampling budget. Concurrent works [37,38] have also independently explored and validated the effectiveness of inference scaling in diffusion models.",
            "reference_string": "[275993956 | Xie et al. | 2025 | Citations: 28]"
        },
        {
            "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.05661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324996886",
                    "name": "Siqi Wang"
                },
                {
                    "authorId": "2325223230",
                    "name": "Zhengyu Chen"
                },
                {
                    "authorId": "2325002673",
                    "name": "Bei Li"
                },
                {
                    "authorId": "2058349088",
                    "name": "Keqing He"
                },
                {
                    "authorId": "2272195992",
                    "name": "Min Zhang"
                },
                {
                    "authorId": "2324990466",
                    "name": "Jingang Wang"
                }
            ],
            "abstract": "The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size/learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.",
            "corpus_id": 273228196,
            "sentences": [
                {
                    "corpus_id": "273228196",
                    "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
                    "text": "The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size/learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.",
                    "score": 0.6225578879406044,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8330078125
                }
            ],
            "relevance_judgement": 0.8330078125,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Siqi Wang, Zhengyu Chen, Bei Li, Keqing He, Min Zhang, Jingang Wang\n## Abstract\nThe scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size/learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.\n",
            "reference_string": "[273228196 | Wang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
            "venue": "International Conference on Learning Representations",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.12486, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2341534973",
                    "name": "Tian Jin"
                },
                {
                    "authorId": "34657588",
                    "name": "Ahmed Imtiaz Humayun"
                },
                {
                    "authorId": "3399348",
                    "name": "Utku Evci"
                },
                {
                    "authorId": "1929462",
                    "name": "Suvinay Subramanian"
                },
                {
                    "authorId": "2296991252",
                    "name": "Amir Yazdanbakhsh"
                },
                {
                    "authorId": "3311387",
                    "name": "Dan Alistarh"
                },
                {
                    "authorId": "2533850",
                    "name": "G. Dziugaite"
                }
            ],
            "abstract": "Pruning eliminates unnecessary parameters in neural networks; it offers a promising solution to the growing computational demands of large language models (LLMs). While many focus on post-training pruning, sparse pre-training--which combines pruning and pre-training into a single phase--provides a simpler alternative. In this work, we present the first systematic exploration of optimal sparse pre-training configurations for LLMs through an examination of 80 unique pruning schedules across different sparsity levels and training durations. We find that initiating pruning at 25% of total training compute and concluding at 75% achieves near-optimal final evaluation loss. These findings provide valuable insights for efficient and effective sparse pre-training of LLMs. Furthermore, we propose a new scaling law that modifies the Chinchilla scaling law to use the average parameter count over pre-training. Through empirical and theoretical validation, we demonstrate that this modified scaling law accurately models evaluation loss for both sparsely and densely pre-trained LLMs, unifying scaling laws across pre-training paradigms. Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference.",
            "corpus_id": 275789021,
            "sentences": [
                {
                    "corpus_id": "275789021",
                    "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
                    "text": "Our work examines sparse pre-training for large language models (LLMs) and presents a unified scaling law that effectively models both sparse and dense scaling. \n\nValue of sparse pre-training. Sparsely pre-trained LLMs match the final evaluation loss of dense models when their average parameter counts are the same (Figure 1). et al., 2023;Grattafiori et al., 2024) often train longer to reduce inference costs. Sardana et al. (2024) formalizes this practice by minimizing lifetime compute-the sum of training and inference compute. Applying the same analysis with our scaling law (see Appendix B), we find that training with less data can achieve the same target loss while reducing lifetime compute, compared with the prescription by Sardana et al. (2024). This surprising advantage stems from sparse pre-training's key feature: it decouples the average parameter count during training, which governs model quality, from the final parameter count after training, which determines inference compute. This decoupling enables a better balance between training efficiency and inference costs, making sparsity a crucial factor in designing compute-optimal training configurations for LLMs. \n\nCompression rate. Our findings suggest that up to a certain compression rate, sparse pre-training compresses the LLM during training without loss in quality. For a given sparse pre-training configuration, we define its compression rate as the ratio between two model sizes: the smallest dense model that matches the sparse model's evaluation loss (while keeping other factors like compute, data constant) and the final sparse model size. From our analysis, we know that a dense model needs as many parameters as the sparse model's average parameter count during pre-training to match its quality. Therefore, for a sparse pre-training configuration, its compression rate is also the ratio between average and final parameter counts. Within the LLMs we explore, we reach the maximum compression rate at 80% final sparsity, where our sparsity schedule results in an average parameter count of about 40% of the initial dense parameter count, yielding a 2x lossless compression rate. \n\nLimitation. We note that, due to the lack of adequate software and hardware support for executing matrix multiplications with unstructured sparsity, we are unable to demonstrate computational savings from sparse pre-training.",
                    "score": 0.5936399507003169,
                    "section_title": "CONCLUDING REMARK",
                    "char_start_offset": 25384,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 163,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1187
                        },
                        {
                            "start": 1190,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1627
                        },
                        {
                            "start": 1628,
                            "end": 1786
                        },
                        {
                            "start": 1787,
                            "end": 1921
                        },
                        {
                            "start": 1922,
                            "end": 2168
                        },
                        {
                            "start": 2171,
                            "end": 2182
                        },
                        {
                            "start": 2183,
                            "end": 2396
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 413,
                            "end": 434,
                            "matchedPaperCorpusId": "266693796"
                        },
                        {
                            "start": 737,
                            "end": 758,
                            "matchedPaperCorpusId": "266693796"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83154296875
                }
            ],
            "relevance_judgement": 0.83154296875,
            "relevance_judgment_input_expanded": "# Title: The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws\n# Venue: International Conference on Learning Representations\n# Authors: Tian Jin, Ahmed Imtiaz Humayun, Utku Evci, Suvinay Subramanian, Amir Yazdanbakhsh, Dan Alistarh, G. Dziugaite\n## Abstract\nPruning eliminates unnecessary parameters in neural networks; it offers a promising solution to the growing computational demands of large language models (LLMs). While many focus on post-training pruning, sparse pre-training--which combines pruning and pre-training into a single phase--provides a simpler alternative. In this work, we present the first systematic exploration of optimal sparse pre-training configurations for LLMs through an examination of 80 unique pruning schedules across different sparsity levels and training durations. We find that initiating pruning at 25% of total training compute and concluding at 75% achieves near-optimal final evaluation loss. These findings provide valuable insights for efficient and effective sparse pre-training of LLMs. Furthermore, we propose a new scaling law that modifies the Chinchilla scaling law to use the average parameter count over pre-training. Through empirical and theoretical validation, we demonstrate that this modified scaling law accurately models evaluation loss for both sparsely and densely pre-trained LLMs, unifying scaling laws across pre-training paradigms. Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference.\n## CONCLUDING REMARK\nOur work examines sparse pre-training for large language models (LLMs) and presents a unified scaling law that effectively models both sparse and dense scaling. \n\nValue of sparse pre-training. Sparsely pre-trained LLMs match the final evaluation loss of dense models when their average parameter counts are the same (Figure 1). et al., 2023;Grattafiori et al., 2024) often train longer to reduce inference costs. Sardana et al. (2024) formalizes this practice by minimizing lifetime compute-the sum of training and inference compute. Applying the same analysis with our scaling law (see Appendix B), we find that training with less data can achieve the same target loss while reducing lifetime compute, compared with the prescription by Sardana et al. (2024). This surprising advantage stems from sparse pre-training's key feature: it decouples the average parameter count during training, which governs model quality, from the final parameter count after training, which determines inference compute. This decoupling enables a better balance between training efficiency and inference costs, making sparsity a crucial factor in designing compute-optimal training configurations for LLMs. \n\nCompression rate. Our findings suggest that up to a certain compression rate, sparse pre-training compresses the LLM during training without loss in quality. For a given sparse pre-training configuration, we define its compression rate as the ratio between two model sizes: the smallest dense model that matches the sparse model's evaluation loss (while keeping other factors like compute, data constant) and the final sparse model size. From our analysis, we know that a dense model needs as many parameters as the sparse model's average parameter count during pre-training to match its quality. Therefore, for a sparse pre-training configuration, its compression rate is also the ratio between average and final parameter counts. Within the LLMs we explore, we reach the maximum compression rate at 80% final sparsity, where our sparsity schedule results in an average parameter count of about 40% of the initial dense parameter count, yielding a 2x lossless compression rate. \n\nLimitation. We note that, due to the lack of adequate software and hardware support for executing matrix multiplications with unstructured sparsity, we are unable to demonstrate computational savings from sparse pre-training.",
            "reference_string": "[275789021 | Jin et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 69,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279750963",
                    "name": "Nanye Ma"
                },
                {
                    "authorId": "2058178039",
                    "name": "Shangyuan Tong"
                },
                {
                    "authorId": "2340654494",
                    "name": "Haolin Jia"
                },
                {
                    "authorId": "2307548497",
                    "name": "Hexiang Hu"
                },
                {
                    "authorId": "2269866136",
                    "name": "Yu-Chuan Su"
                },
                {
                    "authorId": "2326256475",
                    "name": "Mingda Zhang"
                },
                {
                    "authorId": "2340726220",
                    "name": "Xuan Yang"
                },
                {
                    "authorId": "2324838111",
                    "name": "Yandong Li"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                },
                {
                    "authorId": "2325917399",
                    "name": "Xuhui Jia"
                },
                {
                    "authorId": "2324769373",
                    "name": "Saining Xie"
                }
            ],
            "abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "corpus_id": 275570556,
            "sentences": [],
            "relevance_judgement": 0.8232421875,
            "relevance_judgment_input_expanded": "# Title: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps\n# Venue: arXiv.org\n# Authors: Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, T. Jaakkola, Xuhui Jia, Saining Xie\n## Abstract\nGenerative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.\n",
            "reference_string": "[275570556 | Ma et al. | 2025 | Citations: 69]"
        },
        {
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 59,
            "citation_count": 88,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.00448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277217297",
                    "name": "Nikhil Sardana"
                },
                {
                    "authorId": "2040790531",
                    "name": "Sasha Doubov"
                },
                {
                    "authorId": "2277215716",
                    "name": "Jonathan Frankle"
                }
            ],
            "abstract": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
            "corpus_id": 266693796,
            "sentences": [
                {
                    "corpus_id": "266693796",
                    "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
                    "text": "Large language models (LLMs) have substantial training and inference compute and energy costs (Knight, 2023;Pope et al., 2022). Training computation costs are primarily determined by the size of the model and the amount of data seen during training (Hoffmann et al., 2022). For state-ofthe-art models with tens of billions of parameters trained on trillions of tokens, training costs can easily exceed millions of dollars. Similarly, inference costs depend on the size of the model and the volume of user queries over the lifetime Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). of the model. This volume can be significant; demand for popular models can exceed billions of tokens per day (OpenAI & Pilipiszyn, 2021;Shazeer & Freitas, 2022). \n\nAccounting for both training and inference, how does one minimize the cost required to produce and serve a high quality model? \n\nRecent studies have proposed scaling laws, empirical formulas that estimate how changes in model and training data size impact model quality (Kaplan et al., 2020;Hoffmann et al., 2022). Hoffmann et al. (2022) is perhaps the most influential of these works, finding that to scale language models most efficiently, parameters and tokens should grow approximately linearly. The authors applied this scaling law to train a 70B parameter model (dubbed Chinchilla) that outperformed much larger and more expensive models such as GPT-3. As a result, many subsequent LLMs have been trained following the Chinchilla scaling laws (Dey et al., 2023;Muennighoff et al., 2023). \n\nHowever, the Chinchilla scaling laws only account for the computational costs of training. By contrast, the Llama 2 family of models were trained on 2 trillion tokens and the Llama 3 family of models were trained on 15 trillion tokens, which is far more data than the Chinchilla scaling laws would deem \"optimal\" (Touvron et al., 2023a;b;AI@Meta, 2024).",
                    "score": 0.7431607158895432,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 830
                        },
                        {
                            "start": 833,
                            "end": 959
                        },
                        {
                            "start": 962,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1332
                        },
                        {
                            "start": 1333,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1626
                        },
                        {
                            "start": 1629,
                            "end": 1719
                        },
                        {
                            "start": 1720,
                            "end": 1982
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.818359375
                },
                {
                    "corpus_id": "266693796",
                    "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
                    "text": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
                    "score": 0.7368656151730907,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80615234375
                }
            ],
            "relevance_judgement": 0.818359375,
            "relevance_judgment_input_expanded": "# Title: Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws\n# Venue: International Conference on Machine Learning\n# Authors: Nikhil Sardana, Sasha Doubov, Jonathan Frankle\n## Abstract\nLarge language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.\n## Introduction\nLarge language models (LLMs) have substantial training and inference compute and energy costs (Knight, 2023;Pope et al., 2022). Training computation costs are primarily determined by the size of the model and the amount of data seen during training (Hoffmann et al., 2022). For state-ofthe-art models with tens of billions of parameters trained on trillions of tokens, training costs can easily exceed millions of dollars. Similarly, inference costs depend on the size of the model and the volume of user queries over the lifetime Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). of the model. This volume can be significant; demand for popular models can exceed billions of tokens per day (OpenAI & Pilipiszyn, 2021;Shazeer & Freitas, 2022). \n\nAccounting for both training and inference, how does one minimize the cost required to produce and serve a high quality model? \n\nRecent studies have proposed scaling laws, empirical formulas that estimate how changes in model and training data size impact model quality (Kaplan et al., 2020;Hoffmann et al., 2022). Hoffmann et al. (2022) is perhaps the most influential of these works, finding that to scale language models most efficiently, parameters and tokens should grow approximately linearly. The authors applied this scaling law to train a 70B parameter model (dubbed Chinchilla) that outperformed much larger and more expensive models such as GPT-3. As a result, many subsequent LLMs have been trained following the Chinchilla scaling laws (Dey et al., 2023;Muennighoff et al., 2023). \n\nHowever, the Chinchilla scaling laws only account for the computational costs of training. By contrast, the Llama 2 family of models were trained on 2 trillion tokens and the Llama 3 family of models were trained on 15 trillion tokens, which is far more data than the Chinchilla scaling laws would deem \"optimal\" (Touvron et al., 2023a;b;AI@Meta, 2024).",
            "reference_string": "[266693796 | Sardana et al. | 2023 | Citations: 88]"
        },
        {
            "title": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 27,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287946530",
                    "name": "Rulin Shao"
                },
                {
                    "authorId": "2188998958",
                    "name": "Jacqueline He"
                },
                {
                    "authorId": "2290402940",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2288469507",
                    "name": "Tim Dettmers"
                },
                {
                    "authorId": "48872685",
                    "name": "Sewon Min"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2303396379",
                    "name": "Pang Wei Koh"
                }
            ],
            "abstract": "Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.",
            "corpus_id": 271269939,
            "sentences": [],
            "relevance_judgement": 0.8154296875,
            "relevance_judgment_input_expanded": "# Title: Scaling Retrieval-Based Language Models with a Trillion-Token Datastore\n# Venue: Neural Information Processing Systems\n# Authors: Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke S. Zettlemoyer, Pang Wei Koh\n## Abstract\nScaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.\n",
            "reference_string": "[271269939 | Shao et al. | 2024 | Citations: 27]"
        },
        {
            "title": "A Simple Model of Inference Scaling Laws",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16377, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261494026",
                    "name": "Noam Levi"
                }
            ],
            "abstract": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.",
            "corpus_id": 273507997,
            "sentences": [
                {
                    "corpus_id": "273507997",
                    "title": "A Simple Model of Inference Scaling Laws",
                    "text": "Advancements in deep learning have demonstrated that the performance of neural networks scales predictably as a function of model size, data size, and computational resources [Hestness et al., 2017, Kaplan et al., 2020a, Rosenfeld et al., 2020, Henighan et al., 2020a]. These trends, known as neural scaling laws, have motivated research into understanding how scaling influences model performance in a range of domains, in particular, Large Language Models (LLMs) [Brown et al., 2020, Hoffmann et al., 2022]. \n\nHowever, scaling during inference-the process by which a trained model makes predictions on new data-has received less attention. Recent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials. A natural question then arises: \n\nCan we interpret, or predict the inference scaling behavior of a model with repeated attempts? \n\nTo answer this question, we propose a simple toy model that isolates the inference scaling laws which dictate how certain performance metrics improve as a function of the number of inference attempts. Inspired by the work of Hutter [2021], which introduced a model to study scaling behavior for memorization and generalization, we devise a simple setting to capture the effect of repeated inference attempts, focusing on the coverage metric, also known as pass@k. \n\nIn this work, we present analytical predictions for coverage from a probabilistic perspective and demonstrate how inference improves with the number of repeated trials in a predictable way, which matches the observed",
                    "score": 0.6353357268450773,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 509
                        },
                        {
                            "start": 512,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1101
                        },
                        {
                            "start": 1104,
                            "end": 1198
                        },
                        {
                            "start": 1201,
                            "end": 1401
                        },
                        {
                            "start": 1402,
                            "end": 1664
                        },
                        {
                            "start": 1667,
                            "end": 1883
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 219,
                            "end": 243,
                            "matchedPaperCorpusId": "203592013"
                        },
                        {
                            "start": 484,
                            "end": 508,
                            "matchedPaperCorpusId": "258509679"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81494140625
                },
                {
                    "corpus_id": "273507997",
                    "title": "A Simple Model of Inference Scaling Laws",
                    "text": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.",
                    "score": 0.7464773222835295,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7841796875
                }
            ],
            "relevance_judgement": 0.81494140625,
            "relevance_judgment_input_expanded": "# Title: A Simple Model of Inference Scaling Laws\n# Venue: arXiv.org\n# Authors: Noam Levi\n## Abstract\nNeural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.\n## Introduction\nAdvancements in deep learning have demonstrated that the performance of neural networks scales predictably as a function of model size, data size, and computational resources [Hestness et al., 2017, Kaplan et al., 2020a, Rosenfeld et al., 2020, Henighan et al., 2020a]. These trends, known as neural scaling laws, have motivated research into understanding how scaling influences model performance in a range of domains, in particular, Large Language Models (LLMs) [Brown et al., 2020, Hoffmann et al., 2022]. \n\nHowever, scaling during inference-the process by which a trained model makes predictions on new data-has received less attention. Recent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials. A natural question then arises: \n\nCan we interpret, or predict the inference scaling behavior of a model with repeated attempts? \n\nTo answer this question, we propose a simple toy model that isolates the inference scaling laws which dictate how certain performance metrics improve as a function of the number of inference attempts. Inspired by the work of Hutter [2021], which introduced a model to study scaling behavior for memorization and generalization, we devise a simple setting to capture the effect of repeated inference attempts, focusing on the coverage metric, also known as pass@k. \n\nIn this work, we present analytical predictions for coverage from a probabilistic perspective and demonstrate how inference improves with the number of repeated trials in a predictable way, which matches the observed",
            "reference_string": "[273507997 | Levi | 2024 | Citations: 13]"
        },
        {
            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 100,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1749176844",
                    "name": "Rylan Schaeffer"
                },
                {
                    "authorId": "2327048379",
                    "name": "Joshua Kazdan"
                },
                {
                    "authorId": "2294572631",
                    "name": "John Hughes"
                },
                {
                    "authorId": "50875781",
                    "name": "Jordan Juravsky"
                },
                {
                    "authorId": "2333593482",
                    "name": "Sara Price"
                },
                {
                    "authorId": "2287830769",
                    "name": "Aengus Lynch"
                },
                {
                    "authorId": "2334069920",
                    "name": "Erik Jones"
                },
                {
                    "authorId": "2311693657",
                    "name": "Robert Kirk"
                },
                {
                    "authorId": "1861312",
                    "name": "Azalia Mirhoseini"
                },
                {
                    "authorId": "143812875",
                    "name": "Oluwasanmi Koyejo"
                }
            ],
            "abstract": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.",
            "corpus_id": 276580891,
            "sentences": [
                {
                    "corpus_id": "276580891",
                    "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
                    "text": "Our findings have important practical implications for the deployment of large language models, as they can help organizations more accurately forecast compute requirements and make informed trade-offs between model size, inference costs, and performance targets. The mathematical framework we develop could also generalize beyond language models to other domains where similar scaling phenomena emerge. While our work is primarily theoretical, we acknowledge that advances in language model capabilities can have broad societal impacts. We hope that better understanding these fundamental scaling behaviors will help the research community develop more efficient and reliable AI systems. \n\nof",
                    "score": 0.6254129495633101,
                    "section_title": "Impact Statement",
                    "char_start_offset": 22120,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 688
                        },
                        {
                            "start": 691,
                            "end": 693
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80908203125
                },
                {
                    "corpus_id": "276580891",
                    "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
                    "text": "This work advances our mathematical understanding of how and why language model performance improves with additional inference compute through repeat sampling. By establishing rigorous theoretical foundations for these empirically-observed power laws, our work provides practitioners with principled ways to understand and predict model performance when scaling inference compute. The distributional perspective we develop explains previously puzzling deviations from power law scaling and enables more efficient estimation of scaling parameters. \n\nTwo related questions are why such distributional structure exists in the single-attempt success rates and whether one should expect such structure to appear in future benchmarks. We conjecture there are at least two reasons: (1) benchmark design, in that benchmarks are intentionally crafted that problems have a spread of difficulty without being too easy or too hard, and (2) selection bias, in that more interesting patterns such as power law scaling are more likely to garner more interest from the research community. \n\nDespite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws.",
                    "score": 0.810227628116417,
                    "section_title": "Discussion and Future Directions",
                    "char_start_offset": 19791,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 546
                        },
                        {
                            "start": 549,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 1072
                        },
                        {
                            "start": 1075,
                            "end": 1256
                        },
                        {
                            "start": 1257,
                            "end": 1580
                        },
                        {
                            "start": 1583,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1920
                        },
                        {
                            "start": 1923,
                            "end": 2199
                        },
                        {
                            "start": 2200,
                            "end": 2308
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77880859375
                }
            ],
            "relevance_judgement": 0.80908203125,
            "relevance_judgment_input_expanded": "# Title: How Do Large Language Monkeys Get Their Power (Laws)?\n# Venue: arXiv.org\n# Authors: Rylan Schaeffer, Joshua Kazdan, John Hughes, Jordan Juravsky, Sara Price, Aengus Lynch, Erik Jones, Robert Kirk, Azalia Mirhoseini, Oluwasanmi Koyejo\n## Abstract\nRecent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.\n## Discussion and Future Directions\nThis work advances our mathematical understanding of how and why language model performance improves with additional inference compute through repeat sampling. By establishing rigorous theoretical foundations for these empirically-observed power laws, our work provides practitioners with principled ways to understand and predict model performance when scaling inference compute. The distributional perspective we develop explains previously puzzling deviations from power law scaling and enables more efficient estimation of scaling parameters. \n\nTwo related questions are why such distributional structure exists in the single-attempt success rates and whether one should expect such structure to appear in future benchmarks. We conjecture there are at least two reasons: (1) benchmark design, in that benchmarks are intentionally crafted that problems have a spread of difficulty without being too easy or too hard, and (2) selection bias, in that more interesting patterns such as power law scaling are more likely to garner more interest from the research community. \n\nDespite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws.\n\n## Impact Statement\nOur findings have important practical implications for the deployment of large language models, as they can help organizations more accurately forecast compute requirements and make informed trade-offs between model size, inference costs, and performance targets. The mathematical framework we develop could also generalize beyond language models to other domains where similar scaling phenomena emerge. While our work is primarily theoretical, we acknowledge that advances in language model capabilities can have broad societal impacts. We hope that better understanding these fundamental scaling behaviors will help the research community develop more efficient and reliable AI systems. \n\nof",
            "reference_string": "[276580891 | Schaeffer et al. | 2025 | Citations: 4]"
        },
        {
            "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
            "venue": "",
            "year": 2025,
            "reference_count": 34,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.11484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "26339093",
                    "name": "Yige Xu"
                },
                {
                    "authorId": "2325014301",
                    "name": "Xu Guo"
                },
                {
                    "authorId": "49512311",
                    "name": "Zhiwei Zeng"
                },
                {
                    "authorId": "2238949687",
                    "name": "Chunyan Miao"
                }
            ],
            "abstract": "Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.",
            "corpus_id": 278714770,
            "sentences": [
                {
                    "corpus_id": "278714770",
                    "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
                    "text": "Test-time scaling (TTS) has emerged as a pivotal strategy in enhancing the performance of LLMs by allocating additional computational resources during inference. This approach shifts the traditional emphasis from extensive pretraining to optimizing inference-time computation, enabling models to tackle complex tasks more effectively. Following Muennighoff et al. [2025] and Zhang et al. [2025], we classify test-time scaling methods into: (1) Parallel Scaling [Wang et al., 2023, Brown et al., 2024, Snell et al., 2024, Liu et al., 2025], where parallel computes multiple reasoning chains independently, (2) Sequential Scaling [Madaan et al., 2023, Chen et al., 2024, Muennighoff et al., 2025], where computes a longer reasoning chain and generates the chain sequentially, and (3) Hybrid Scaling [Yao et al., 2023, Gandhi et al., 2024, Wang et al., 2025], where combines the parallel scaling and sequential scaling methods. In this paper, we mainly focus on parallel test-time scaling, which can be adopt on large-scale LLMs efficiently. \n\nAs conclued by Zhang et al. [2025], parallel scaling improves test-time performance by generating multiple reasoning chains in parallel, and then aggregating them together to the final answer. Early evidence that sampling multiple reasoning chains and voting improves robustness came from Self-Consistency (SC) [Wang et al., 2023], inspiring subsequent studies on how many chains to sample for a fixed compute envelope [Snell et al., 2024]. Li et al. [2025] suggest that the chance of finding the correct answer improves while increasing the number of generated responses, which is empirically summarized by a log-linear scaling law [Brown et al., 2024]. Despite the effectiveness of these approaches, the majority of existing parallel test-time scaling methods rely on discrete token-by-token generation, which imposes inherent constraints and limits their expressiveness.",
                    "score": 0.5724663210515659,
                    "section_title": "Test-Time Scaling",
                    "char_start_offset": 5749,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 1038
                        },
                        {
                            "start": 1041,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1914
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 461,
                            "end": 479,
                            "matchedPaperCorpusId": "247595263"
                        },
                        {
                            "start": 628,
                            "end": 648,
                            "matchedPaperCorpusId": "257900871"
                        },
                        {
                            "start": 648,
                            "end": 667,
                            "matchedPaperCorpusId": "258059885"
                        },
                        {
                            "start": 797,
                            "end": 814,
                            "matchedPaperCorpusId": "258762525"
                        },
                        {
                            "start": 1352,
                            "end": 1371,
                            "matchedPaperCorpusId": "247595263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.806640625
                }
            ],
            "relevance_judgement": 0.806640625,
            "relevance_judgment_input_expanded": "# Title: SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning\n# Venue: \n# Authors: Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao\n## Abstract\nTest-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.\n## Test-Time Scaling\nTest-time scaling (TTS) has emerged as a pivotal strategy in enhancing the performance of LLMs by allocating additional computational resources during inference. This approach shifts the traditional emphasis from extensive pretraining to optimizing inference-time computation, enabling models to tackle complex tasks more effectively. Following Muennighoff et al. [2025] and Zhang et al. [2025], we classify test-time scaling methods into: (1) Parallel Scaling [Wang et al., 2023, Brown et al., 2024, Snell et al., 2024, Liu et al., 2025], where parallel computes multiple reasoning chains independently, (2) Sequential Scaling [Madaan et al., 2023, Chen et al., 2024, Muennighoff et al., 2025], where computes a longer reasoning chain and generates the chain sequentially, and (3) Hybrid Scaling [Yao et al., 2023, Gandhi et al., 2024, Wang et al., 2025], where combines the parallel scaling and sequential scaling methods. In this paper, we mainly focus on parallel test-time scaling, which can be adopt on large-scale LLMs efficiently. \n\nAs conclued by Zhang et al. [2025], parallel scaling improves test-time performance by generating multiple reasoning chains in parallel, and then aggregating them together to the final answer. Early evidence that sampling multiple reasoning chains and voting improves robustness came from Self-Consistency (SC) [Wang et al., 2023], inspiring subsequent studies on how many chains to sample for a fixed compute envelope [Snell et al., 2024]. Li et al. [2025] suggest that the chance of finding the correct answer improves while increasing the number of generated responses, which is empirically summarized by a log-linear scaling law [Brown et al., 2024]. Despite the effectiveness of these approaches, the majority of existing parallel test-time scaling methods rely on discrete token-by-token generation, which imposes inherent constraints and limits their expressiveness.",
            "reference_string": "[278714770 | Xu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Archon: An Architecture Search Framework for Inference-Time Techniques",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15254, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283840040",
                    "name": "Jon Saad-Falcon"
                },
                {
                    "authorId": "2298071127",
                    "name": "Adrian Gamarra Lafuente"
                },
                {
                    "authorId": "2322445633",
                    "name": "Shlok Natarajan"
                },
                {
                    "authorId": "2322449133",
                    "name": "Nahum Maru"
                },
                {
                    "authorId": "2322445297",
                    "name": "Hristo Todorov"
                },
                {
                    "authorId": "2179104354",
                    "name": "E. Guha"
                },
                {
                    "authorId": "2336877105",
                    "name": "E. K. Buchanan"
                },
                {
                    "authorId": "2322453298",
                    "name": "Mayee Chen"
                },
                {
                    "authorId": "2820009",
                    "name": "Neel Guha"
                },
                {
                    "authorId": "2287938277",
                    "name": "Christopher R\u00e9"
                },
                {
                    "authorId": "1861312",
                    "name": "Azalia Mirhoseini"
                }
            ],
            "abstract": "Inference-time techniques, such as repeated sampling or iterative revisions, are emerging as powerful ways to enhance large-language models (LLMs) at test time. However, best practices for developing systems that combine these techniques remain underdeveloped due to our limited understanding of the utility of each technique across models and tasks, the interactions between them, and the massive search space for combining them. To address these challenges, we introduce Archon, a modular and automated framework for optimizing the process of selecting and combining inference-time techniques and LLMs. Given a compute budget and a set of available LLMs, Archon explores a large design space to discover optimized configurations tailored to target benchmarks. It can design custom or general-purpose architectures that advance the Pareto frontier of accuracy vs. maximum token budget compared to top-performing baselines. Across instruction-following, reasoning, and coding tasks, we show that Archon can leverage additional inference compute budget to design systems that outperform frontier models such as OpenAI's o1, GPT-4o, and Claude 3.5 Sonnet by an average of 15.1%.",
            "corpus_id": 272827424,
            "sentences": [
                {
                    "corpus_id": "272827424",
                    "title": "Archon: An Architecture Search Framework for Inference-Time Techniques",
                    "text": "Language models [22,40,50,53] have transformed the field of artificial intelligence across a vast number of domains and tasks. LLMs are pretrained on substantial amounts of textual data before being further aligned with human preferences through instruction fine-tuning [10,57], direct policy optimization (DPO) [44], Kahneman-Tversky optimization (KTO) [16], reinforcement learning from AI feedback (RLAIF) [5], and other techniques. As language models continue to gain improved abilities with further scaling of data, parameters, and compute [17,26], the cost of developing new LLMs is ever increasing, requiring the curation of trillions of new tokens as well as substantial GPU-hours for pretraining. Furthermore, as the current state-of-the-art in LLMs are primarily closed-source APIs, such as OpenAI's GPT-4o [40], Google's Gemini [51] and Anthropic's Claude [2], it is difficult to effectively explore and push the frontier of existing LLMs without being able to manipulate the parameters of these closed-source models and employing techniques such as continual pretraining [25], instruction fine-tuning [57], data mixing [61], chain-of-thought [58], among others. A number of recent works show the effectiveness of scaling compute at test time [6,7,24,47]. In particular, Large Language Monkeys [6] characterizes inference-time scaling laws, showing a log-linear relationship between coverage-the fraction of problems solved by at least one attempt-and the number of samples drawn from the model across a broad range of reasoning tasks and LLMs. We build on the inference-time compute scaling approaches and propose Archon, a formal framework for applying inference-time techniques, combining the strengths of multiple pretrained LLMs (Section 3.2), and exploring different inference-time architectures with ITAS (Section 3.4).",
                    "score": 0.5857518997358231,
                    "section_title": "Scaling Laws of Language Models",
                    "char_start_offset": 7781,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 1172
                        },
                        {
                            "start": 1173,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1554
                        },
                        {
                            "start": 1555,
                            "end": 1836
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80419921875
                }
            ],
            "relevance_judgement": 0.80419921875,
            "relevance_judgment_input_expanded": "# Title: Archon: An Architecture Search Framework for Inference-Time Techniques\n# Venue: arXiv.org\n# Authors: Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, E. Guha, E. K. Buchanan, Mayee Chen, Neel Guha, Christopher R\u00e9, Azalia Mirhoseini\n## Abstract\nInference-time techniques, such as repeated sampling or iterative revisions, are emerging as powerful ways to enhance large-language models (LLMs) at test time. However, best practices for developing systems that combine these techniques remain underdeveloped due to our limited understanding of the utility of each technique across models and tasks, the interactions between them, and the massive search space for combining them. To address these challenges, we introduce Archon, a modular and automated framework for optimizing the process of selecting and combining inference-time techniques and LLMs. Given a compute budget and a set of available LLMs, Archon explores a large design space to discover optimized configurations tailored to target benchmarks. It can design custom or general-purpose architectures that advance the Pareto frontier of accuracy vs. maximum token budget compared to top-performing baselines. Across instruction-following, reasoning, and coding tasks, we show that Archon can leverage additional inference compute budget to design systems that outperform frontier models such as OpenAI's o1, GPT-4o, and Claude 3.5 Sonnet by an average of 15.1%.\n## Scaling Laws of Language Models\nLanguage models [22,40,50,53] have transformed the field of artificial intelligence across a vast number of domains and tasks. LLMs are pretrained on substantial amounts of textual data before being further aligned with human preferences through instruction fine-tuning [10,57], direct policy optimization (DPO) [44], Kahneman-Tversky optimization (KTO) [16], reinforcement learning from AI feedback (RLAIF) [5], and other techniques. As language models continue to gain improved abilities with further scaling of data, parameters, and compute [17,26], the cost of developing new LLMs is ever increasing, requiring the curation of trillions of new tokens as well as substantial GPU-hours for pretraining. Furthermore, as the current state-of-the-art in LLMs are primarily closed-source APIs, such as OpenAI's GPT-4o [40], Google's Gemini [51] and Anthropic's Claude [2], it is difficult to effectively explore and push the frontier of existing LLMs without being able to manipulate the parameters of these closed-source models and employing techniques such as continual pretraining [25], instruction fine-tuning [57], data mixing [61], chain-of-thought [58], among others. A number of recent works show the effectiveness of scaling compute at test time [6,7,24,47]. In particular, Large Language Monkeys [6] characterizes inference-time scaling laws, showing a log-linear relationship between coverage-the fraction of problems solved by at least one attempt-and the number of samples drawn from the model across a broad range of reasoning tasks and LLMs. We build on the inference-time compute scaling approaches and propose Archon, a formal framework for applying inference-time techniques, combining the strengths of multiple pretrained LLMs (Section 3.2), and exploring different inference-time architectures with ITAS (Section 3.4).",
            "reference_string": "[272827424 | Saad-Falcon et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Scaling Law Hypothesis for Multimodal Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 12,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.06754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320938961",
                    "name": "Qingyun Sun"
                },
                {
                    "authorId": "2320723029",
                    "name": "Zhen Guo"
                }
            ],
            "abstract": "We propose a scaling law hypothesis for multimodal models processing text, audio, images, and video within a shared token and embedding space. Our framework predicts model performance based on modality-specific compression and tokenization efficiency, extending established scaling laws from text-based decoder models to mixed-modality systems. We explore whether leveraging more training data in multiple modalities can reduce the size of the multimodal model, enabling efficient deployment on resource-constrained devices.",
            "corpus_id": 272593336,
            "sentences": [
                {
                    "corpus_id": "272593336",
                    "title": "Scaling Law Hypothesis for Multimodal Model",
                    "text": "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance. The initial scaling laws proposed by OpenAI [1] suggested that larger models are more sample-efficient, leading to the creation of massive models like GPT-3. However, subsequent research from DeepMind, notably the Chinchilla study [2], revealed that many large models were undertrained. Their findings indicated that smaller models trained on more data could outperform larger models when the compute budget is held constant. Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8]. \n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]. This approach introduces bits per character (BPC) as a performance metric that reflects the model's compression efficiency [9]. BPC has been shown to correlate linearly with model performance across various modalities (Figure 1). This perspective reveals a linear relationship between BPC and the logarithm of compute used, which can be formalized as: \n\nwhere N is the number of training tokens, and P is the number of model parameters (Figure 2). \n\nThis unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant. In multimodal systems, diverse types of data, such as text, audio, images and video, are processed through specialized tokenization techniques, each with varying levels of compression efficiency. \n\nText is tokenized using methods like Byte Pair Encoding (BPE) [10], which offers relatively stable compression efficiency.",
                    "score": 0.6682560799268944,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 1006
                        },
                        {
                            "start": 1007,
                            "end": 1142
                        },
                        {
                            "start": 1145,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1679
                        },
                        {
                            "start": 1682,
                            "end": 1775
                        },
                        {
                            "start": 1778,
                            "end": 1983
                        },
                        {
                            "start": 1984,
                            "end": 2179
                        },
                        {
                            "start": 2182,
                            "end": 2304
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.802734375
                }
            ],
            "relevance_judgement": 0.802734375,
            "relevance_judgment_input_expanded": "# Title: Scaling Law Hypothesis for Multimodal Model\n# Venue: arXiv.org\n# Authors: Qingyun Sun, Zhen Guo\n## Abstract\nWe propose a scaling law hypothesis for multimodal models processing text, audio, images, and video within a shared token and embedding space. Our framework predicts model performance based on modality-specific compression and tokenization efficiency, extending established scaling laws from text-based decoder models to mixed-modality systems. We explore whether leveraging more training data in multiple modalities can reduce the size of the multimodal model, enabling efficient deployment on resource-constrained devices.\n## Introduction\nScaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance. The initial scaling laws proposed by OpenAI [1] suggested that larger models are more sample-efficient, leading to the creation of massive models like GPT-3. However, subsequent research from DeepMind, notably the Chinchilla study [2], revealed that many large models were undertrained. Their findings indicated that smaller models trained on more data could outperform larger models when the compute budget is held constant. Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8]. \n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]. This approach introduces bits per character (BPC) as a performance metric that reflects the model's compression efficiency [9]. BPC has been shown to correlate linearly with model performance across various modalities (Figure 1). This perspective reveals a linear relationship between BPC and the logarithm of compute used, which can be formalized as: \n\nwhere N is the number of training tokens, and P is the number of model parameters (Figure 2). \n\nThis unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant. In multimodal systems, diverse types of data, such as text, audio, images and video, are processed through specialized tokenization techniques, each with varying levels of compression efficiency. \n\nText is tokenized using methods like Byte Pair Encoding (BPE) [10], which offers relatively stable compression efficiency.",
            "reference_string": "[272593336 | Sun et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 18,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257764593",
                    "name": "Kevin Lin"
                },
                {
                    "authorId": "2355866239",
                    "name": "Charlie Snell"
                },
                {
                    "authorId": "2356041105",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2257345100",
                    "name": "Charles Packer"
                },
                {
                    "authorId": "68974952",
                    "name": "Sarah Wooders"
                },
                {
                    "authorId": "2055174324",
                    "name": "Ion Stoica"
                },
                {
                    "authorId": "2258073138",
                    "name": "Joseph Gonzalez"
                }
            ],
            "abstract": "Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to\"think\"offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.",
            "corpus_id": 277857467,
            "sentences": [],
            "relevance_judgement": 0.80029296875,
            "relevance_judgment_input_expanded": "# Title: Sleep-time Compute: Beyond Inference Scaling at Test-time\n# Venue: arXiv.org\n# Authors: Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, Ion Stoica, Joseph Gonzalez\n## Abstract\nScaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to\"think\"offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.\n",
            "reference_string": "[277857467 | Lin et al. | 2025 | Citations: 8]"
        },
        {
            "title": "Training Language Models to Reason Efficiently",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 39,
            "citation_count": 72,
            "influential_citation_count": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.04463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2138481119",
                    "name": "Daman Arora"
                },
                {
                    "authorId": "2288537572",
                    "name": "Andrea Zanette"
                }
            ],
            "abstract": "Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models. In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.",
            "corpus_id": 276235717,
            "sentences": [],
            "relevance_judgement": 0.798828125,
            "relevance_judgment_input_expanded": "# Title: Training Language Models to Reason Efficiently\n# Venue: arXiv.org\n# Authors: Daman Arora, Andrea Zanette\n## Abstract\nScaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models. In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.\n",
            "reference_string": "[276235717 | Arora et al. | 2025 | Citations: 72]"
        },
        {
            "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294718538",
                    "name": "Longfei Yun"
                },
                {
                    "authorId": "2152482391",
                    "name": "Yonghao Zhuang"
                },
                {
                    "authorId": "2294813888",
                    "name": "Yao Fu"
                },
                {
                    "authorId": "2243336934",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2294828709",
                    "name": "Hao Zhang"
                }
            ],
            "abstract": "Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss. We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training. On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget.",
            "corpus_id": 268875826,
            "sentences": [
                {
                    "corpus_id": "268875826",
                    "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
                    "text": "This paper studies the problem of how to scale the number of experts in the fast-developing MoE large language models.We first extend the scaling law, originally developed for dense transformer LLMs, to the context of MoEs, establishing a new relation between the validation loss and the number of experts, the number of training tokens, and the model size.We then discuss the need and the unique challenge to additionally consider inference efficiency when scaling MoEs.Our findings provide new insights on how to appropriately scale MoE models under compute constraints.",
                    "score": 0.6119916831427209,
                    "section_title": "Conclusion",
                    "char_start_offset": 23396,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 118,
                            "end": 357
                        },
                        {
                            "start": 357,
                            "end": 471
                        },
                        {
                            "start": 471,
                            "end": 572
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.796875
                }
            ],
            "relevance_judgement": 0.796875,
            "relevance_judgment_input_expanded": "# Title: Toward Inference-optimal Mixture-of-Expert Large Language Models\n# Venue: arXiv.org\n# Authors: Longfei Yun, Yonghao Zhuang, Yao Fu, Eric P. Xing, Hao Zhang\n## Abstract\nMixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss. We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training. On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget.\n## Conclusion\nThis paper studies the problem of how to scale the number of experts in the fast-developing MoE large language models.We first extend the scaling law, originally developed for dense transformer LLMs, to the context of MoEs, establishing a new relation between the validation loss and the number of experts, the number of training tokens, and the model size.We then discuss the need and the unique challenge to additionally consider inference efficiency when scaling MoEs.Our findings provide new insights on how to appropriately scale MoE models under compute constraints.",
            "reference_string": "[268875826 | Yun et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Temporal Scaling Law for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.17785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2249971338",
                    "name": "Yizhe Xiong"
                },
                {
                    "authorId": "2298904872",
                    "name": "Xiansheng Chen"
                },
                {
                    "authorId": "2299108794",
                    "name": "Xin Ye"
                },
                {
                    "authorId": "2298921971",
                    "name": "Hui Chen"
                },
                {
                    "authorId": "1818920",
                    "name": "Zijia Lin"
                },
                {
                    "authorId": "2298903058",
                    "name": "Haoran Lian"
                },
                {
                    "authorId": "2293626051",
                    "name": "Jianwei Niu"
                },
                {
                    "authorId": "2242661989",
                    "name": "Guiguang Ding"
                }
            ],
            "abstract": "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.",
            "corpus_id": 269449894,
            "sentences": [
                {
                    "corpus_id": "269449894",
                    "title": "Temporal Scaling Law for Large Language Models",
                    "text": "The concept of scaling laws for language models was proposed by (Kaplan et al., 2020). Their study revealed that the test loss for generative transformer models scales as a power-law with model size, dataset size, and the amount of compute used for training. Building upon this foundational study (Kaplan et al., 2020), further research has expanded the concept of scaling laws across diverse problem settings (Hernandez et al., 2021) and model architectures (Cherti et al., 2023;Aghajanyan et al., 2023). For instance, (Hernandez et al., 2021) has investigated scaling laws for transfer learning, and discovered that the effective data transferred for a model trained on natural language to a model trained on held-out data also follows a power-law. In the scope of multimodal models, (Cherti et al., 2023) found that scaling behavior differs when trained on different task-dependent datasets in different tasks, and (Aghajanyan et al., 2023) organized a scaling law which unified the contributions and interactions of different modalities during multi-modal pre-training. \n\nDespite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training. By studying the loss behavior on different token positions, we introduce a more accurate temporal scaling law for LLM test loss, allowing for precise tracking and prediction of LLM performance during the pre-training stage. \n\n3 Temporal Scaling Law",
                    "score": 0.5902978990149376,
                    "section_title": "Scaling Laws for Language Models",
                    "char_start_offset": 8141,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 86
                        },
                        {
                            "start": 87,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 1073
                        },
                        {
                            "start": 1076,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1795
                        },
                        {
                            "start": 1798,
                            "end": 1820
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 459,
                            "end": 480,
                            "matchedPaperCorpusId": "254636568"
                        },
                        {
                            "start": 480,
                            "end": 504,
                            "matchedPaperCorpusId": "255570036"
                        },
                        {
                            "start": 786,
                            "end": 807,
                            "matchedPaperCorpusId": "254636568"
                        },
                        {
                            "start": 918,
                            "end": 943,
                            "matchedPaperCorpusId": "255570036"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79296875
                }
            ],
            "relevance_judgement": 0.79296875,
            "relevance_judgment_input_expanded": "# Title: Temporal Scaling Law for Large Language Models\n# Venue: arXiv.org\n# Authors: Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Jianwei Niu, Guiguang Ding\n## Abstract\nRecently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.\n## Scaling Laws for Language Models\nThe concept of scaling laws for language models was proposed by (Kaplan et al., 2020). Their study revealed that the test loss for generative transformer models scales as a power-law with model size, dataset size, and the amount of compute used for training. Building upon this foundational study (Kaplan et al., 2020), further research has expanded the concept of scaling laws across diverse problem settings (Hernandez et al., 2021) and model architectures (Cherti et al., 2023;Aghajanyan et al., 2023). For instance, (Hernandez et al., 2021) has investigated scaling laws for transfer learning, and discovered that the effective data transferred for a model trained on natural language to a model trained on held-out data also follows a power-law. In the scope of multimodal models, (Cherti et al., 2023) found that scaling behavior differs when trained on different task-dependent datasets in different tasks, and (Aghajanyan et al., 2023) organized a scaling law which unified the contributions and interactions of different modalities during multi-modal pre-training. \n\nDespite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training. By studying the loss behavior on different token positions, we introduce a more accurate temporal scaling law for LLM test loss, allowing for precise tracking and prediction of LLM performance during the pre-training stage. \n\n3 Temporal Scaling Law",
            "reference_string": "[269449894 | Xiong et al. | 2024 | Citations: 10]"
        },
        {
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 50,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34920835",
                    "name": "Ayan Sengupta"
                },
                {
                    "authorId": "2345922770",
                    "name": "Yash Goel"
                },
                {
                    "authorId": "2249914540",
                    "name": "Tanmoy Chakraborty"
                }
            ],
            "abstract": "Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.",
            "corpus_id": 276421468,
            "sentences": [
                {
                    "corpus_id": "276421468",
                    "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                    "text": "Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03. \n\n(13) \n\nThese findings indicate that hybrid approaches combining model size optimization with test-time computation strategies may offer the best trade-off for organizations deploying language models. Practitioners should assess the complexity of their target tasks and consider adaptive computation strategies that adjust inference resources based on problem difficulty. This approach can lead to significant cost savings while maintaining performance, particularly for routine tasks that do not require the full capacity of larger models. \n\nRQ5. How does scaling fine-tuning parameters affect performance on downstream tasks? Hernandez et al. (2021) proposed scaling laws for transfer by fine-tuning decoder-only transformer models on python code. They introduced a concept of effective data transferred D t , i.e., the amount of additional python data that a model of the same size trained on only python would have needed to achieve the same loss on python as a model pretrained on language, as a function of fine tuning data D f . The law is given as : \n\nAlong the same tracks, Lin et al. (2024a) proposed a rectified scaling law given by Equation 15. They introduced the term pre-learned data size D l that indicates how much amount of downstream data a model has learned from pre-training: \n\nwhere D is the fine-tuning data size and B, E, \u03b2 are fitting parameters. Abnar et al. (2021) predicted, downstream error e DS for image recognitions tasks on ViTs and ResNets as a function of upstream error e U S , given by the equation: \n\nThis was further explored by Mikami et al. (2021), modelling pre-training data size, consisting of syntheic dataset to predict downstream error with following equation: \n\nwhere D f denotes fine-tuning data, X denotes either LLM model size or pretraining data size or PEFT parameter size, and A, E, \u03b1, \u03b2 are fitting parameters. \n\nRQ5.",
                    "score": 0.5964691863084732,
                    "section_title": "D Additional research questions and guidelines",
                    "char_start_offset": 41734,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 444
                        },
                        {
                            "start": 447,
                            "end": 451
                        },
                        {
                            "start": 454,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 986
                        },
                        {
                            "start": 989,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1503
                        },
                        {
                            "start": 1506,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1742
                        },
                        {
                            "start": 1745,
                            "end": 1817
                        },
                        {
                            "start": 1818,
                            "end": 1982
                        },
                        {
                            "start": 1985,
                            "end": 2153
                        },
                        {
                            "start": 2156,
                            "end": 2311
                        },
                        {
                            "start": 2314,
                            "end": 2318
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78857421875
                }
            ],
            "relevance_judgement": 0.78857421875,
            "relevance_judgment_input_expanded": "# Title: How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines\n# Venue: arXiv.org\n# Authors: Ayan Sengupta, Yash Goel, Tanmoy Chakraborty\n## Abstract\nNeural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.\n## D Additional research questions and guidelines\nFurther exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03. \n\n(13) \n\nThese findings indicate that hybrid approaches combining model size optimization with test-time computation strategies may offer the best trade-off for organizations deploying language models. Practitioners should assess the complexity of their target tasks and consider adaptive computation strategies that adjust inference resources based on problem difficulty. This approach can lead to significant cost savings while maintaining performance, particularly for routine tasks that do not require the full capacity of larger models. \n\nRQ5. How does scaling fine-tuning parameters affect performance on downstream tasks? Hernandez et al. (2021) proposed scaling laws for transfer by fine-tuning decoder-only transformer models on python code. They introduced a concept of effective data transferred D t , i.e., the amount of additional python data that a model of the same size trained on only python would have needed to achieve the same loss on python as a model pretrained on language, as a function of fine tuning data D f . The law is given as : \n\nAlong the same tracks, Lin et al. (2024a) proposed a rectified scaling law given by Equation 15. They introduced the term pre-learned data size D l that indicates how much amount of downstream data a model has learned from pre-training: \n\nwhere D is the fine-tuning data size and B, E, \u03b2 are fitting parameters. Abnar et al. (2021) predicted, downstream error e DS for image recognitions tasks on ViTs and ResNets as a function of upstream error e U S , given by the equation: \n\nThis was further explored by Mikami et al. (2021), modelling pre-training data size, consisting of syntheic dataset to predict downstream error with following equation: \n\nwhere D f denotes fine-tuning data, X denotes either LLM model size or pretraining data size or PEFT parameter size, and A, E, \u03b1, \u03b2 are fitting parameters. \n\nRQ5.",
            "reference_string": "[276421468 | Sengupta et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 698,
            "influential_citation_count": 60,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314917835",
                    "name": "C. Snell"
                },
                {
                    "authorId": "2315069693",
                    "name": "Jaehoon Lee"
                },
                {
                    "authorId": "2269023101",
                    "name": "Kelvin Xu"
                },
                {
                    "authorId": "2275526115",
                    "name": "Aviral Kumar"
                }
            ],
            "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a\"compute-optimal\"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
            "corpus_id": 271719990,
            "sentences": [],
            "relevance_judgement": 0.7880859375,
            "relevance_judgment_input_expanded": "# Title: Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters\n# Venue: arXiv.org\n# Authors: C. Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar\n## Abstract\nEnabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a\"compute-optimal\"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.\n",
            "reference_string": "[271719990 | Snell et al. | 2024 | Citations: 698]"
        },
        {
            "title": "Compression Scaling Laws:Unifying Sparsity and Quantization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 14,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1502248377",
                    "name": "Elias Frantar"
                },
                {
                    "authorId": "3399348",
                    "name": "Utku Evci"
                },
                {
                    "authorId": "2347195979",
                    "name": "Wonpyo Park"
                },
                {
                    "authorId": "2815290",
                    "name": "N. Houlsby"
                },
                {
                    "authorId": "3311387",
                    "name": "Dan Alistarh"
                }
            ],
            "abstract": "We investigate how different compression techniques -- such as weight and activation quantization, and weight sparsity -- affect the scaling behavior of large language models (LLMs) during pretraining. Building on previous work showing that weight sparsity acts as a constant multiplier on model size in scaling laws, we demonstrate that this\"effective parameter\"scaling pattern extends to quantization as well. Specifically, we establish that weight-only quantization achieves strong parameter efficiency multipliers, while full quantization of both weights and activations shows diminishing returns at lower bitwidths. Our results suggest that different compression techniques can be unified under a common scaling law framework, enabling principled comparison and combination of these methods.",
            "corpus_id": 276574918,
            "sentences": [
                {
                    "corpus_id": "276574918",
                    "title": "Compression Scaling Laws:Unifying Sparsity and Quantization",
                    "text": "Our work builds on and connects several lines of research around scaling laws, model compression techniques, and the intersection between them. \n\nScaling Laws for Language Models. The foundation of this work builds on established scaling laws for language models that characterize how performance improves with model size and training data. Kaplan et al. (2020) established the first comprehensive scaling laws showing that loss follows power law relationships with both parameters and data. Hoffmann et al. (2024) refined these results with the Chinchilla scaling laws, suggesting that previous models were over-parameterized and that parameters and data should be scaled roughly equally. Recent work has revealed additional nuances in scaling behavior -for example, when considering data redundancy (Muennighoff et al., 2023), or different model architectures (Clark et al., 2022). \n\nModel Compression and Sparsity. Parallel work has focused on making models more efficient through compression techniques. For sparsity, Frantar et al. (2024) established the first scaling laws characterizing how sparsity interacts with model and data scaling, showing that sparsity acts as a consistent multiplier on effective param-eter count. Their work demonstrated that optimal sparsity levels increase with longer training, as dense models hit diminishing returns. This report directly builds on this earlier work, studying how different representations affect scaling. \n\nQuantization for Language Models. Recent advances in quantization have enabled dramatically reduced precision while maintaining performance. Post-training quantization methods like GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2023) have shown strong results for inference. For quantization-aware training, BitNet (Wang et al., 2023) and follow-up work (Kaushal et al., 2024;Ma et al., 2024) demonstrated stable training with binary and ternary weights, although a precise comparison against dense model scaling is not possible in their setting given the different hyper-parameters used. This work complements these efforts by characterizing how quantization during training affects fundamental scaling behavior -showing for instance that weight-only quantization maintains strong parameter efficiency even at very low bitwidths, for both weights and activations.",
                    "score": 0.6995836782128677,
                    "section_title": "Related Work",
                    "char_start_offset": 11603,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 146,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 883
                        },
                        {
                            "start": 886,
                            "end": 917
                        },
                        {
                            "start": 918,
                            "end": 1007
                        },
                        {
                            "start": 1008,
                            "end": 1230
                        },
                        {
                            "start": 1231,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1460
                        },
                        {
                            "start": 1463,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 2053
                        },
                        {
                            "start": 2054,
                            "end": 2329
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 801,
                            "end": 827,
                            "matchedPaperCorpusId": "258888192"
                        },
                        {
                            "start": 862,
                            "end": 882,
                            "matchedPaperCorpusId": "246473179"
                        },
                        {
                            "start": 1022,
                            "end": 1043,
                            "matchedPaperCorpusId": "262013578"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77880859375
                }
            ],
            "relevance_judgement": 0.77880859375,
            "relevance_judgment_input_expanded": "# Title: Compression Scaling Laws:Unifying Sparsity and Quantization\n# Venue: arXiv.org\n# Authors: Elias Frantar, Utku Evci, Wonpyo Park, N. Houlsby, Dan Alistarh\n## Abstract\nWe investigate how different compression techniques -- such as weight and activation quantization, and weight sparsity -- affect the scaling behavior of large language models (LLMs) during pretraining. Building on previous work showing that weight sparsity acts as a constant multiplier on model size in scaling laws, we demonstrate that this\"effective parameter\"scaling pattern extends to quantization as well. Specifically, we establish that weight-only quantization achieves strong parameter efficiency multipliers, while full quantization of both weights and activations shows diminishing returns at lower bitwidths. Our results suggest that different compression techniques can be unified under a common scaling law framework, enabling principled comparison and combination of these methods.\n## Related Work\nOur work builds on and connects several lines of research around scaling laws, model compression techniques, and the intersection between them. \n\nScaling Laws for Language Models. The foundation of this work builds on established scaling laws for language models that characterize how performance improves with model size and training data. Kaplan et al. (2020) established the first comprehensive scaling laws showing that loss follows power law relationships with both parameters and data. Hoffmann et al. (2024) refined these results with the Chinchilla scaling laws, suggesting that previous models were over-parameterized and that parameters and data should be scaled roughly equally. Recent work has revealed additional nuances in scaling behavior -for example, when considering data redundancy (Muennighoff et al., 2023), or different model architectures (Clark et al., 2022). \n\nModel Compression and Sparsity. Parallel work has focused on making models more efficient through compression techniques. For sparsity, Frantar et al. (2024) established the first scaling laws characterizing how sparsity interacts with model and data scaling, showing that sparsity acts as a consistent multiplier on effective param-eter count. Their work demonstrated that optimal sparsity levels increase with longer training, as dense models hit diminishing returns. This report directly builds on this earlier work, studying how different representations affect scaling. \n\nQuantization for Language Models. Recent advances in quantization have enabled dramatically reduced precision while maintaining performance. Post-training quantization methods like GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2023) have shown strong results for inference. For quantization-aware training, BitNet (Wang et al., 2023) and follow-up work (Kaushal et al., 2024;Ma et al., 2024) demonstrated stable training with binary and ternary weights, although a precise comparison against dense model scaling is not possible in their setting given the different hyper-parameters used. This work complements these efforts by characterizing how quantization during training affects fundamental scaling behavior -showing for instance that weight-only quantization maintains strong parameter efficiency even at very low bitwidths, for both weights and activations.",
            "reference_string": "[276574918 | Frantar et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Unified Scaling Laws for Routed Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 69,
            "citation_count": 185,
            "influential_citation_count": 21,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.01169, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31993415",
                    "name": "Aidan Clark"
                },
                {
                    "authorId": "40550616",
                    "name": "Diego de Las Casas"
                },
                {
                    "authorId": "40895205",
                    "name": "Aurelia Guy"
                },
                {
                    "authorId": "1697879",
                    "name": "A. Mensch"
                },
                {
                    "authorId": "35550664",
                    "name": "Michela Paganini"
                },
                {
                    "authorId": "46616544",
                    "name": "Jordan Hoffmann"
                },
                {
                    "authorId": "2143374656",
                    "name": "Bogdan Damoc"
                },
                {
                    "authorId": "3135881",
                    "name": "Blake A. Hechtman"
                },
                {
                    "authorId": "2072572294",
                    "name": "Trevor Cai"
                },
                {
                    "authorId": "148016269",
                    "name": "Sebastian Borgeaud"
                },
                {
                    "authorId": "47568983",
                    "name": "George van den Driessche"
                },
                {
                    "authorId": "2143538252",
                    "name": "Eliza Rutherford"
                },
                {
                    "authorId": "4629007",
                    "name": "T. Hennigan"
                },
                {
                    "authorId": "2124469920",
                    "name": "Matthew G. Johnson"
                },
                {
                    "authorId": "2143434227",
                    "name": "Katie Millican"
                },
                {
                    "authorId": "51042571",
                    "name": "Albin Cassirer"
                },
                {
                    "authorId": "2115601070",
                    "name": "Chris Jones"
                },
                {
                    "authorId": "118801223",
                    "name": "Elena Buchatskaya"
                },
                {
                    "authorId": "2508525",
                    "name": "D. Budden"
                },
                {
                    "authorId": "2175946",
                    "name": "L. Sifre"
                },
                {
                    "authorId": "2217144",
                    "name": "Simon Osindero"
                },
                {
                    "authorId": "1689108",
                    "name": "O. Vinyals"
                },
                {
                    "authorId": "34269227",
                    "name": "Jack W. Rae"
                },
                {
                    "authorId": "152585800",
                    "name": "Erich Elsen"
                },
                {
                    "authorId": "2645384",
                    "name": "K. Kavukcuoglu"
                },
                {
                    "authorId": "34838386",
                    "name": "K. Simonyan"
                }
            ],
            "abstract": "The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.",
            "corpus_id": 246473179,
            "sentences": [
                {
                    "corpus_id": "246473179",
                    "title": "Unified Scaling Laws for Routed Language Models",
                    "text": "The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.",
                    "score": 0.6454753927301228,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7783203125
                }
            ],
            "relevance_judgement": 0.7783203125,
            "relevance_judgment_input_expanded": "# Title: Unified Scaling Laws for Routed Language Models\n# Venue: International Conference on Machine Learning\n# Authors: Aidan Clark, Diego de Las Casas, Aurelia Guy, A. Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, T. Hennigan, Matthew G. Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, D. Budden, L. Sifre, Simon Osindero, O. Vinyals, Jack W. Rae, Erich Elsen, K. Kavukcuoglu, K. Simonyan\n## Abstract\nThe performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.\n",
            "reference_string": "[246473179 | Clark et al. | 2022 | Citations: 185]"
        },
        {
            "title": "gzip Predicts Data-dependent Scaling Laws",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1471734043",
                    "name": "Rohan Pandey"
                }
            ],
            "abstract": "Past work has established scaling laws that predict the performance of a neural language model (LM) as a function of its parameter count and the number of tokens it's trained on, enabling optimal allocation of a fixed compute budget. Are these scaling laws agnostic to training data as some prior work suggests? We generate training datasets of varying complexities by modulating the syntactic properties of a PCFG, finding that 1) scaling laws are sensitive to differences in data complexity and that 2) gzip, a compression algorithm, is an effective predictor of how data complexity impacts scaling properties. We propose a new data-dependent scaling law for LM's that accounts for the training data's gzip-compressibility; its compute-optimal frontier increases in dataset size preference (over parameter count preference) as training data becomes harder to compress.",
            "corpus_id": 270063468,
            "sentences": [
                {
                    "corpus_id": "270063468",
                    "title": "gzip Predicts Data-dependent Scaling Laws",
                    "text": "Past work has established scaling laws that predict the performance of a neural language model (LM) as a function of its parameter count and the number of tokens it's trained on, enabling optimal allocation of a fixed compute budget. Are these scaling laws agnostic to training data as some prior work suggests? We generate training datasets of varying complexities by modulating the syntactic properties of a PCFG, finding that 1) scaling laws are sensitive to differences in data complexity and that 2) gzip, a compression algorithm, is an effective predictor of how data complexity impacts scaling properties. We propose a new data-dependent scaling law for LM's that accounts for the training data's gzip-compressibility; its compute-optimal frontier increases in dataset size preference (over parameter count preference) as training data becomes harder to compress.",
                    "score": 0.7827401046070721,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77685546875
                }
            ],
            "relevance_judgement": 0.77685546875,
            "relevance_judgment_input_expanded": "# Title: gzip Predicts Data-dependent Scaling Laws\n# Venue: arXiv.org\n# Authors: Rohan Pandey\n## Abstract\nPast work has established scaling laws that predict the performance of a neural language model (LM) as a function of its parameter count and the number of tokens it's trained on, enabling optimal allocation of a fixed compute budget. Are these scaling laws agnostic to training data as some prior work suggests? We generate training datasets of varying complexities by modulating the syntactic properties of a PCFG, finding that 1) scaling laws are sensitive to differences in data complexity and that 2) gzip, a compression algorithm, is an effective predictor of how data complexity impacts scaling properties. We propose a new data-dependent scaling law for LM's that accounts for the training data's gzip-compressibility; its compute-optimal frontier increases in dataset size preference (over parameter count preference) as training data becomes harder to compress.\n",
            "reference_string": "[270063468 | Pandey | 2024 | Citations: 11]"
        },
        {
            "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 101,
            "influential_citation_count": 15,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114841965",
                    "name": "Pranjal Aggarwal"
                },
                {
                    "authorId": "2129663",
                    "name": "S. Welleck"
                }
            ],
            "abstract": "Reasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''-that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy. We release code and models at https://www.cmu-l3.github.io/l1",
            "corpus_id": 276813519,
            "sentences": [
                {
                    "corpus_id": "276813519",
                    "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning",
                    "text": "Test-Time Scaling in Large Language Models. Increasing test-time computation has consistently been shown to improve performance in complex reasoning tasks, mathematical problem-solving, and code generation (Wu et al., 2024;Wang et al., 2023;Wei et al., 2023;DeepSeek-AI et al., 2025;Snell et al., 2024). Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning chains or longer ones (Wu et al., 2024;Snell et al., 2024;OpenAI et al., 2024a). Prominent approaches include parallel sampling of multiple reasoning paths (Wang et al., 2023;Aggarwal et al., 2023), tree-based search (Yao et al., 2023;Wu et al., 2024;Xin et al., 2024), and iterative refinement techniques (Welleck et al., 2023;Madaan et al., 2023;Snell et al., 2024;Welleck et al., 2024). Recent reasoning language models such as \"O1\" and \"R1\"-style models (OpenAI et al., 2024a;DeepSeek-AI et al., 2025) simplify test-time scaling by generating extended reasoning traces (longer chains-of-thought). Despite their promising results, these methods lack precise and dynamic control over the length of the generated reasoning chains, resulting in often suboptimal performance or unrealized potential efficiency gains. \n\nOur work complements and extends this line of research by enabling reasoning models to precisely control the length of generated outputs, thereby providing flexibility to calibrate inference compute based on task-specific requirements. \n\nLength Control in Large Language Models. Controlling the length of LLM-generated outputs is an important practical consideration across various generation tasks.",
                    "score": 0.587450317573555,
                    "section_title": "Related Work",
                    "char_start_offset": 4342,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 43
                        },
                        {
                            "start": 44,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 830
                        },
                        {
                            "start": 831,
                            "end": 1041
                        },
                        {
                            "start": 1042,
                            "end": 1256
                        },
                        {
                            "start": 1259,
                            "end": 1494
                        },
                        {
                            "start": 1497,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1658
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 616,
                            "end": 638,
                            "matchedPaperCorpusId": "258823191"
                        },
                        {
                            "start": 747,
                            "end": 769,
                            "matchedPaperCorpusId": "253244506"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7724609375
                }
            ],
            "relevance_judgement": 0.7724609375,
            "relevance_judgment_input_expanded": "# Title: L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning\n# Venue: arXiv.org\n# Authors: Pranjal Aggarwal, S. Welleck\n## Abstract\nReasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''-that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy. We release code and models at https://www.cmu-l3.github.io/l1\n## Related Work\nTest-Time Scaling in Large Language Models. Increasing test-time computation has consistently been shown to improve performance in complex reasoning tasks, mathematical problem-solving, and code generation (Wu et al., 2024;Wang et al., 2023;Wei et al., 2023;DeepSeek-AI et al., 2025;Snell et al., 2024). Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning chains or longer ones (Wu et al., 2024;Snell et al., 2024;OpenAI et al., 2024a). Prominent approaches include parallel sampling of multiple reasoning paths (Wang et al., 2023;Aggarwal et al., 2023), tree-based search (Yao et al., 2023;Wu et al., 2024;Xin et al., 2024), and iterative refinement techniques (Welleck et al., 2023;Madaan et al., 2023;Snell et al., 2024;Welleck et al., 2024). Recent reasoning language models such as \"O1\" and \"R1\"-style models (OpenAI et al., 2024a;DeepSeek-AI et al., 2025) simplify test-time scaling by generating extended reasoning traces (longer chains-of-thought). Despite their promising results, these methods lack precise and dynamic control over the length of the generated reasoning chains, resulting in often suboptimal performance or unrealized potential efficiency gains. \n\nOur work complements and extends this line of research by enabling reasoning models to precisely control the length of generated outputs, thereby providing flexibility to calibrate inference compute based on task-specific requirements. \n\nLength Control in Large Language Models. Controlling the length of LLM-generated outputs is an important practical consideration across various generation tasks.",
            "reference_string": "[276813519 | Aggarwal et al. | 2025 | Citations: 101]"
        },
        {
            "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
            "venue": "International Conference on Learning Representations",
            "year": 2022,
            "reference_count": 43,
            "citation_count": 27,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.00849",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.00849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2047360059",
                    "name": "Oren Neumann"
                },
                {
                    "authorId": "49009730",
                    "name": "C. Gros"
                }
            ],
            "abstract": "The recent observation of neural power-law scaling relations has made a significant impact in the field of deep learning. A substantial amount of attention has been dedicated as a consequence to the description of scaling laws, although mostly for supervised learning and only to a reduced extent for reinforcement learning frameworks. In this paper we present an extensive study of performance scaling for a cornerstone reinforcement learning algorithm, AlphaZero. On the basis of a relationship between Elo rating, playing strength and power-law scaling, we train AlphaZero agents on the games Connect Four and Pentago and analyze their performance. We find that player strength scales as a power law in neural network parameter count when not bottlenecked by available compute, and as a power of compute when training optimally sized agents. We observe nearly identical scaling exponents for both games. Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models. We find that the predicted scaling of optimal neural network size fits our data for both games. This scaling law implies that previously published state-of-the-art game-playing models are significantly smaller than their optimal size, given the respective compute budgets. We also show that large AlphaZero models are more sample efficient, performing better than smaller models with the same amount of training data.",
            "corpus_id": 252683098,
            "sentences": [
                {
                    "corpus_id": "252683098",
                    "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
                    "text": "3.1 LANGUAGE MODEL SCALING LAWS Kaplan et al. (2020) showed that the cross-entropy loss of autoregressive Transformers (Vaswani et al., 2017) scales as a power law with model size, dataset size and compute. These scaling laws hold when training is not bottlenecked by the other two resources. Specifically, the model size power law applies when models are trained to convergence, while the compute scaling law is valid when training optimal-sized models. By combining these laws a power-law scaling of optimal model size with compute is obtained, with an exponent derived from the loss scaling exponents. These exponents, later recalculated by Hoffmann et al. (2022), tend to fall in the range [0, 1].",
                    "score": 0.6402662391751535,
                    "section_title": "BACKGROUND",
                    "char_start_offset": 5944,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 206
                        },
                        {
                            "start": 207,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 454
                        },
                        {
                            "start": 455,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 701
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77197265625
                }
            ],
            "relevance_judgement": 0.77197265625,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws for a Multi-Agent Reinforcement Learning Model\n# Venue: International Conference on Learning Representations\n# Authors: Oren Neumann, C. Gros\n## Abstract\nThe recent observation of neural power-law scaling relations has made a significant impact in the field of deep learning. A substantial amount of attention has been dedicated as a consequence to the description of scaling laws, although mostly for supervised learning and only to a reduced extent for reinforcement learning frameworks. In this paper we present an extensive study of performance scaling for a cornerstone reinforcement learning algorithm, AlphaZero. On the basis of a relationship between Elo rating, playing strength and power-law scaling, we train AlphaZero agents on the games Connect Four and Pentago and analyze their performance. We find that player strength scales as a power law in neural network parameter count when not bottlenecked by available compute, and as a power of compute when training optimally sized agents. We observe nearly identical scaling exponents for both games. Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models. We find that the predicted scaling of optimal neural network size fits our data for both games. This scaling law implies that previously published state-of-the-art game-playing models are significantly smaller than their optimal size, given the respective compute budgets. We also show that large AlphaZero models are more sample efficient, performing better than smaller models with the same amount of training data.\n## BACKGROUND\n3.1 LANGUAGE MODEL SCALING LAWS Kaplan et al. (2020) showed that the cross-entropy loss of autoregressive Transformers (Vaswani et al., 2017) scales as a power law with model size, dataset size and compute. These scaling laws hold when training is not bottlenecked by the other two resources. Specifically, the model size power law applies when models are trained to convergence, while the compute scaling law is valid when training optimal-sized models. By combining these laws a power-law scaling of optimal model size with compute is obtained, with an exponent derived from the loss scaling exponents. These exponents, later recalculated by Hoffmann et al. (2022), tend to fall in the range [0, 1].",
            "reference_string": "[252683098 | Neumann et al. | 2022 | Citations: 27]"
        },
        {
            "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling",
            "venue": "",
            "year": 2025,
            "reference_count": 39,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.05449, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2278582297",
                    "name": "Weizhe Chen"
                },
                {
                    "authorId": "2256845651",
                    "name": "Sven Koenig"
                },
                {
                    "authorId": "1796375",
                    "name": "B. Dilkina"
                }
            ],
            "abstract": "Recent reasoning models, such as OpenAI's O1 series, have demonstrated exceptional performance on complex reasoning tasks and revealed new test-time scaling laws. Inspired by this, many people have been studying how to train models to achieve effective self-evaluation and self-correction to further enable the scaling paradigm. However, less studied is how to efficiently scale test-time compute from a fixed model, and this remains a challenge. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving at test time, which can also subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how systematically triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.",
            "corpus_id": 276249712,
            "sentences": [
                {
                    "corpus_id": "276249712",
                    "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling",
                    "text": "Since ChatGPT, large language models (LLMs) have been a rapidly evolving domain that tries to solve problems be-yond traditional language tasks like summarization or question answering (Chen et al., 2023;Yao et al., 2023;Chen et al., 2024b;d). Significantly, the newly released Ope-nAI O1 has demonstrated its strong capability in complex problem-solving through its detailed reasoning steps before outputting the final answer (Jaech et al., 2024). Since then, many researchers have studied how to replicate success from an open-source perspective and how to train models that are even better at efficiently solving problems that still remain unsolvable by the current LLMs (Huang et al., 2024;Zeng et al., 2024;DeepSeek-AI et al., 2025). \n\nThe remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities. Moreover, this paradigm not only enhances inference-time efficiency but also facilitates the generation of high-quality synthetic data, which can be leveraged to train the next generation of models and evaluate their performance (Guan et al., 2025). \n\nA key insight in this line of research is that high-quality data capable of eliciting self-reflection in LLMs is crucial for enhancing their reasoning capabilities. Notably, o1-like models demonstrate improved performance when longer reasoning chains are utilized (Huang et al., 2024;DeepSeek-AI et al., 2025).",
                    "score": 0.5721188390177964,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 738
                        },
                        {
                            "start": 741,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1456
                        },
                        {
                            "start": 1457,
                            "end": 1617
                        },
                        {
                            "start": 1618,
                            "end": 1867
                        },
                        {
                            "start": 1870,
                            "end": 2034
                        },
                        {
                            "start": 2035,
                            "end": 2180
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 204,
                            "end": 221,
                            "matchedPaperCorpusId": "252762395"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77197265625
                }
            ],
            "relevance_judgement": 0.77197265625,
            "relevance_judgment_input_expanded": "# Title: Iterative Deepening Sampling as Efficient Test-Time Scaling\n# Venue: \n# Authors: Weizhe Chen, Sven Koenig, B. Dilkina\n## Abstract\nRecent reasoning models, such as OpenAI's O1 series, have demonstrated exceptional performance on complex reasoning tasks and revealed new test-time scaling laws. Inspired by this, many people have been studying how to train models to achieve effective self-evaluation and self-correction to further enable the scaling paradigm. However, less studied is how to efficiently scale test-time compute from a fixed model, and this remains a challenge. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving at test time, which can also subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how systematically triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.\n## Introduction\nSince ChatGPT, large language models (LLMs) have been a rapidly evolving domain that tries to solve problems be-yond traditional language tasks like summarization or question answering (Chen et al., 2023;Yao et al., 2023;Chen et al., 2024b;d). Significantly, the newly released Ope-nAI O1 has demonstrated its strong capability in complex problem-solving through its detailed reasoning steps before outputting the final answer (Jaech et al., 2024). Since then, many researchers have studied how to replicate success from an open-source perspective and how to train models that are even better at efficiently solving problems that still remain unsolvable by the current LLMs (Huang et al., 2024;Zeng et al., 2024;DeepSeek-AI et al., 2025). \n\nThe remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities. Moreover, this paradigm not only enhances inference-time efficiency but also facilitates the generation of high-quality synthetic data, which can be leveraged to train the next generation of models and evaluate their performance (Guan et al., 2025). \n\nA key insight in this line of research is that high-quality data capable of eliciting self-reflection in LLMs is crucial for enhancing their reasoning capabilities. Notably, o1-like models demonstrate improved performance when longer reasoning chains are utilized (Huang et al., 2024;DeepSeek-AI et al., 2025).",
            "reference_string": "[276249712 | Chen et al. | 2025 | Citations: 1]"
        },
        {
            "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258718382",
                    "name": "Prasanna Mayilvahanan"
                },
                {
                    "authorId": "2179052180",
                    "name": "Thadd\u00e4us Wiedemer"
                },
                {
                    "authorId": "2345922971",
                    "name": "Sayak Mallick"
                },
                {
                    "authorId": "2261937896",
                    "name": "Matthias Bethge"
                },
                {
                    "authorId": "40634590",
                    "name": "Wieland Brendel"
                }
            ],
            "abstract": "Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.",
            "corpus_id": 276421795,
            "sentences": [
                {
                    "corpus_id": "276421795",
                    "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
                    "text": "Scaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget (Kaplan et al., 2020;Hoffmann et al., 2022;Grattafiori et al., 2024). Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs). FLOPs account for both parameter count and the number of training tokens. While useful for pretraining, scaling laws do not capture how well a model ultimately performs on downstream tasks (Gadre et al., 2024;Schaeffer et al., 2024;Du et al., 2025). Consequently, multiple works have begun to investigate downstream scaling laws: Scaling laws that directly predict downstream loss from FLOPs (Schaeffer et al., 2024;Gadre et al., 2024). \n\nFigure 1. LLMs' loss-to-loss scaling follows power laws primarily shaped by the choice of pretraining data and tokenizer. Using Llama trained on FineWeb-Edu as a baseline, we intervene on various factors to assess their impact on train-to-test loss scaling. Changing the pretraining data has the largest effect, followed by the choice of tokenizer. Switching the architecture, e.g., from Llama to Mamba, has limited impact, while factors like model size, context length, and optimizer settings exert little-to-no influence. Brandfonbrener et al. (2024) show that downstream scaling laws can be decomposed into compute-to-train-loss scaling laws and (train)-loss-to-(test)-loss scaling laws. The combination of compute-to-loss and loss-to-loss scaling laws enables efficient and accurate prediction of a model's downstream performance.",
                    "score": 0.6836335026047238,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 613
                        },
                        {
                            "start": 614,
                            "end": 800
                        },
                        {
                            "start": 803,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1493
                        },
                        {
                            "start": 1494,
                            "end": 1637
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76416015625
                }
            ],
            "relevance_judgement": 0.76416015625,
            "relevance_judgment_input_expanded": "# Title: LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws\n# Venue: arXiv.org\n# Authors: Prasanna Mayilvahanan, Thadd\u00e4us Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel\n## Abstract\nScaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.\n## Introduction\nScaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget (Kaplan et al., 2020;Hoffmann et al., 2022;Grattafiori et al., 2024). Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs). FLOPs account for both parameter count and the number of training tokens. While useful for pretraining, scaling laws do not capture how well a model ultimately performs on downstream tasks (Gadre et al., 2024;Schaeffer et al., 2024;Du et al., 2025). Consequently, multiple works have begun to investigate downstream scaling laws: Scaling laws that directly predict downstream loss from FLOPs (Schaeffer et al., 2024;Gadre et al., 2024). \n\nFigure 1. LLMs' loss-to-loss scaling follows power laws primarily shaped by the choice of pretraining data and tokenizer. Using Llama trained on FineWeb-Edu as a baseline, we intervene on various factors to assess their impact on train-to-test loss scaling. Changing the pretraining data has the largest effect, followed by the choice of tokenizer. Switching the architecture, e.g., from Llama to Mamba, has limited impact, while factors like model size, context length, and optimizer settings exert little-to-no influence. Brandfonbrener et al. (2024) show that downstream scaling laws can be decomposed into compute-to-train-loss scaling laws and (train)-loss-to-(test)-loss scaling laws. The combination of compute-to-loss and loss-to-loss scaling laws enables efficient and accurate prediction of a model's downstream performance.",
            "reference_string": "[276421795 | Mayilvahanan et al. | 2025 | Citations: 3]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.",
            "score": 0.9696707021503846,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8837890625
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e.,\"Chinchilla optimal\"regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.",
            "score": 0.9476544960333888,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "The prosperity of large language models (LLMs) has necessitated the development of scaling laws (Kaplan et al., 2020) to optimize the tradeoff between increasing model size and expanding training data within finite computational resources.Scaling laws empirically study the correlation between model performance and factors including the number of parameters, training tokens, and FLOPs.Previous works (Kaplan et al., 2020;Henighan et al., 2020;Hoffmann et al., 2022;Clark et al., 2022) have established power laws to describe \u22c6 Equal contribution.The corresponding author (Email: zhongyiran@gmail.com).\n\nthese scaling trends.Experiments are typically conducted on smaller models with relatively low training costs.From these observations, regression models are derived to guide the scaling of parameters, data, and computational resources.Establishing these scaling laws is crucial before expanding language models to the scale of LLMs, ensuring predictable results under controllable training costs.Scaling laws have guided the success of many recent LLMs, such as Chinchilla (Hoffmann et al., 2022) and GPT-4 (OpanAI, 2023).It is noteworthy that existing scaling laws are predominantly established for traditional softmax attention transformers (Vaswani et al., 2017).\n\nLinear complexity language models (Katharopoulos et al., 2020;Qin et al., 2022b;Choromanski et al., 2021;Zheng et al., 2022Zheng et al., , 2023;;Hua et al., 2022;Liu et al., 2022;Qin et al., 2023bQin et al., , 2024c,d;,d;Gu et al., 2021aGu et al., , 2020Gu et al., , 2022;;Fu et al., 2022;Qin et al., 2023a;Fu et al., 2023;Orvieto et al., 2023;Qin et al., 2023dYang et al., 2023;Gu and Dao, 2023;Dao and Gu, 2024;Sun et al., 2023), have emerged as a promising alternative to traditional transformers in causal language modeling.",
            "score": 0.8834162753486547,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 548
                },
                {
                    "start": 548,
                    "end": 603
                },
                {
                    "start": 605,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1127
                },
                {
                    "start": 1127,
                    "end": 1271
                },
                {
                    "start": 1273,
                    "end": 1801
                }
            ],
            "ref_mentions": [
                {
                    "start": 1248,
                    "end": 1270,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460693359375
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.",
            "score": 0.870959457044782,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "271270413",
            "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
            "text": "Scaling laws consider a computational budget, C, which is measured in FLOPs. The goal is to optimally allocate the compute budget to model parameters N and the number of training tokens D [30,6,26,44]. It can be formulated as: \n\nFollowing Radford et al. [51], the loss function is typically the language modeling loss when evaluating language models, which can be written as: \n\nwhere p(w i |w 1:i\u22121 , V ) is the output probability of word w i given the context w 1:i\u22121 and the tokenizer with vocabulary size V . Generally, the lower L indicates better performance of the language model. However, due to its dependency on V , L cannot be used to compare language models with different vocabulary sizes. Thus, we propose an adaptation later in \u00a72.2. Fitting scaling laws generally requires various models trained for different configurations [23]. A common approach is to select several compute budgets and train models with varying N and D for each budget to find the best one, i.e. the one with the lowest loss (\"IsoFLOPs\") [26]. Using fitting techniques we can then estimate a function that maps from the compute budget to the optimal allocation to N and D.",
            "score": 0.8565465052881007,
            "section_title": "Scaling law",
            "char_start_offset": 6697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1158
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 200,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "278310893",
            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
            "text": "The development of neural scaling laws has provided a foundational framework for understanding the performance trajectory of large language models (LLMs). These laws (Kaplan et al., 2020;Hoffmann et al., 2022) describe how model performance improves predictably with increases in parameters, dataset size, and compute resources. Initially, scaling laws seemed to offer a clear roadmap for the continuous and predictable advancement of LLMs. However, as the field has evolved -evidenced by the sharp rise in the number of proposed scaling laws between 2020 and 2024, as shown in Figure 1 -significant limitations and challenges have become apparent. These emerging concerns cast doubt on the long-term viability and effectiveness of scaling laws as the primary strategy for advancing AI.  their reliance on simplified power law relationships to predict model performance. While these laws capture broad trends, they often fail to account for nuanced factors that influence real-world outcomes. For instance, the assumption that increasing dataset size or compute will indefinitely yield proportional performance improvements ignores diminishing returns observed in practice (Diaz & Madaio, 2024). As models scale, the marginal gains from additional data and compute tend to decrease, leading to inefficient resource allocation (Muennighoff et al., 2023). \n\nAnother limitation of neural scaling laws is their emphasis on uniform scaling across model size, data, and compute. Jin et al. (2023) demonstrated that different abilities, such as fact recall and in-context learning, degrade at different rates under pruning or downsizing. This variability suggests that scaling laws may not provide a one-size-fits-all solution and that alternative approaches are needed to address diverse performance requirements. By focusing exclusively on scaling, researchers risk missing opportunities to develop more efficient and specialized models tailored to specific applications. \n\nNeural scaling laws also neglect the broader implications of scaling on energy consumption and environmental sustainability. The computational requirements for training large-scale models are immense, resulting in significant carbon emissions (Faiz et al., 2024). Recent studies showed that the energy efficiency of compute-intensive tasks varies based on factors such as hardware configuration, server location, and training duration (Zhang et al., 2023).",
            "score": 0.8326311829420312,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2232
                },
                {
                    "start": 2233,
                    "end": 2425
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447021484375
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Training large language models is expensive.Furthermore, training high-quality models requires a complex recipe of algorithmic techniques and training data.To reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate their efficacy to larger model and data regimes via scaling laws.With reliable extrapolation, it is possible to quickly iterate at small scale and still pick the method that will perform best for the final large training run.Indeed, this workflow has become commonplace for training state-of-the-art language models like Chinchilla 70B [45], PaLM 540B [19], GPT-4 [76], and many others.We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training.We are able to extrapolate, in both N and M , the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law.(right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss.We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction.We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute.For this figure, we train all models on RedPajama [112].\n\nDespite their importance for model development, published scaling laws differ from the goals of training state-of-the-art models in important ways.For instance, scaling studies usually focus on the compute-optimal training regime (\"Chinchilla optimality\" [45]), where model and dataset size are set to yield minimum loss for a given compute budget.However, this setting ignores inference costs.As larger models are more expensive at inference, it is now common practice to over-train smaller models [113].Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets.",
            "score": 0.8218023613064185,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 44,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1527
                },
                {
                    "start": 1529,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2034
                },
                {
                    "start": 2034,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "Scaling laws in large language models aim for an ideal balance between increasing the number of parameters and enlarging the training corpus, given limited computation resources (Kaplan et al., 2020;Henighan et al., 2020;Hernandez et al., 2021;Hoffmann et al., 2022;Clark et al., 2022).The initial scaling laws (Kaplan et al., 2020) use the test-time cross-entropy loss as a regression target to investigate its power-law correlations with model size, dataset size and training computation budget.Hoffmann et al. ( 2022) use three approaches to find the optimal model size and dataset size given a fixed computation budget.By 1) freezing model size and varying number of training tokens, 2) fixing FLOPs and changing model sizes and dataset sizes and 3) directly solving a constrained optimization equation, they conclude that models and the training corpus should be scaled equally when enlarging computing resources.They use the revised scaling law to train a compute-optimal model, Chinchilla, that stands out across various benchmarks.Other works extend scaling laws to multiple modalities (Henighan et al., 2020), mixture of expert models (Clark et al., 2022) and reinforcement learning (Hilton et al., 2023).Recently, Su et al. (2024);Bi et al. (2024) studied the influence of additional factors such as learning rate, context length, and batch size on the scaling-law coefficients.(Isik et al., 2024) studies scaling laws of downstream task performance in a transfer learning setting for the machine translation task.",
            "score": 0.8137489545236722,
            "section_title": "Related work",
            "char_start_offset": 22035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1524
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "276580891",
            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
            "text": "This work advances our mathematical understanding of how and why language model performance improves with additional inference compute through repeat sampling. By establishing rigorous theoretical foundations for these empirically-observed power laws, our work provides practitioners with principled ways to understand and predict model performance when scaling inference compute. The distributional perspective we develop explains previously puzzling deviations from power law scaling and enables more efficient estimation of scaling parameters. \n\nTwo related questions are why such distributional structure exists in the single-attempt success rates and whether one should expect such structure to appear in future benchmarks. We conjecture there are at least two reasons: (1) benchmark design, in that benchmarks are intentionally crafted that problems have a spread of difficulty without being too easy or too hard, and (2) selection bias, in that more interesting patterns such as power law scaling are more likely to garner more interest from the research community. \n\nDespite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws.",
            "score": 0.810227628116417,
            "section_title": "Discussion and Future Directions",
            "char_start_offset": 19791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 2199
                },
                {
                    "start": 2200,
                    "end": 2308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "264172438",
            "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
            "text": "Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models. \n\nTo study the scaling law of binarized Transformer, we start by plotting the scaling curve of both BitNet and the FP16 Transformer baseline against the parameter count. We fix the number of training tokens and vary the model sizes. Figure 3 shows that the loss scaling of BitNet is similar to the FP16 Transformer, which follows a power-law. We then fit the scaling law with an irreducible loss term: To evaluate whether the scaling law can accurately predict the loss, we choose the models from 125M to 6.7B to fit the parameters in the power-law and use the law to predict the loss of 13B and 30B. It shows that the fitted scaling law predicted BitNet's loss with high accuracy. Besides, the gap between BitNet and FP16 Transformer becomes smaller as the model size grows. \n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once. We estimate the energy consumption as in Section 2.3. Figure 3 shows the scaling curve against the inference energy cost at 7nm process nodes. It proves that BitNet has much higher scaling efficiency. Given a fixed computation budget, BitNet achieves a significantly better loss. Meanwhile, the inference cost is much smaller to get the same performance as the FP16 models.",
            "score": 0.7984850104899606,
            "section_title": "Inference-Optimal Scaling Law",
            "char_start_offset": 13008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2216
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "266818336",
            "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
            "text": "Research on scaling laws (Hestness et al., 2017) predates the emergence of large language models. Scaling laws (Henighan et al., 2020;Hoffmann et al., 2022;Kaplan et al., 2020) suggest that model performance can be predictably improved with increases in compute budget , model scale , and data scale . When model scale  is represented by model parameters and data scale  by the number of tokens,  can be approximated as  = 6 . Therefore, how to optimize the allocation between model and data scales when increasing the compute budget is also a crucial research objective in scaling laws. \n\nThe development of LLMs (Dai et al., 2019;Radford et al., 2019), with larger models achieving unexpected and significant performance improvements, has brought scaling laws research to a new peak. Results in scaling laws demonstrate that expanding the compute budget continues to yield significant benefits, which further encourages the increase in model scales (Brown et al., 2020;Smith et al., 2022). \n\nHowever, as shown in Table 4, early works (Hoffmann et al., 2022;Kaplan et al., 2020) on the optimal model/data scaling-up allocation strategy have shown varying conclusions, raising doubts about the general applicability of scaling laws. Moreover, these studies often lacked a complete description of hyperparameter settings, leaving it uncertain whether models under different compute budgets reached optimal performance. Therefore, we revisit scaling laws in this section to address these uncertainties and ensure we are on the right path to efficiently scaleup compute, which reflects the long-term perspective and is key to developing continuously improving models. \n\nTo ensure that models under different compute budgets can achieve optimal performance, we first studied the scaling laws of hyperparameters. Empirically, it has been observed that the optimal values of most parameters during training do not change when varying compute budgets. Therefore, these parameters are consistent with those outlined in Section 2.3 and remain unchanged across different compute budgets. However, the hyperparameters that have the most significant impact on performance, namely batch size and learning rate, were re-examined.",
            "score": 0.7928344859408867,
            "section_title": "Scaling Laws",
            "char_start_offset": 13043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 632,
                    "end": 653,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51123046875
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "Training Inference-Optimal Language Models Chinchilla (Hoffmann et al., 2022) explored the scaling law to determine the training-optimal amounts for parameters and training data under a training FLOP budget. On the other hands, modern LLMs are increasingly interested on inference-optimal models. Some practitioners use much more data than the Chinchilla recommendation to train small models due to their high inference efficiency (Qwen Team, 2024;Allal et al., 2025;Sardana et al., 2024). Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2024) is a promising direction. \n\nFurther Theoretical Analysis for Parallel Scaling Laws One of our contributions is quantitatively computing the impact of parameters and computation on model capability. Although we present some theoretical results (Proposition 1), the challenge of directly modeling DIVERSITY limits us to using extensive experiments to fit parallel scaling laws. Why the diversity is related to log P, is there a growth rate that exceeds O(log P), and whether there is a performance upper bound for P \u226b 8, remain open questions.",
            "score": 0.7870669596041316,
            "section_title": "Discussion and Future Work",
            "char_start_offset": 32802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 488,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 605,
                    "end": 622,
                    "matchedPaperCorpusId": "271601023"
                },
                {
                    "start": 622,
                    "end": 641,
                    "matchedPaperCorpusId": "278498044"
                },
                {
                    "start": 918,
                    "end": 940,
                    "matchedPaperCorpusId": "266693796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "270063468",
            "title": "gzip Predicts Data-dependent Scaling Laws",
            "text": "Past work has established scaling laws that predict the performance of a neural language model (LM) as a function of its parameter count and the number of tokens it's trained on, enabling optimal allocation of a fixed compute budget. Are these scaling laws agnostic to training data as some prior work suggests? We generate training datasets of varying complexities by modulating the syntactic properties of a PCFG, finding that 1) scaling laws are sensitive to differences in data complexity and that 2) gzip, a compression algorithm, is an effective predictor of how data complexity impacts scaling properties. We propose a new data-dependent scaling law for LM's that accounts for the training data's gzip-compressibility; its compute-optimal frontier increases in dataset size preference (over parameter count preference) as training data becomes harder to compress.",
            "score": 0.7827401046070721,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "270045432",
            "title": "Scaling Laws for Discriminative Classification in Large Language Models",
            "text": "Given our two-stage training pipeline, our goal in studying scaling laws is to predict how well a model adapted to a domain using a language modeling objective (stage 1) will perform when discriminatively fine-tuned as a classifier (stage 2).We evaluate this scaling behavior using various metrics that offer insights into performance, efficiency, and behavior across different scales, including FLOPs (Floating Point Operations): The number of floating point operations executed during the language modeling stage.It provides insights into the computational complexity of the LLM and how it scales with model and dataset size.\n\nLanguage Modeling Loss: The cross entropy between the model's predicted next token and the actual next token in training data, directly optimized by the language modeling objective.Monitoring the loss ensures that the model is converging toward an optimal language model of the data.Figure 3 depicts properties of the scaling laws observed in our experiments.In Figure 3 (a) we plot classification loss as a function of language modeling FLOPs for each of the model sizes we adapted.We observe overlap across the three model sizes tested such that for a given compute budget such as 10 18 FLOPs, larger models exhibit lower training loss, suggesting their ability to learn complex patterns and structure within the training data.While we do not fit a formal scaling law here, the observation is consistent with compute optimal language modeling [16,18] applied to discriminative finetuning.\n\nIn Figure 3 (b), we plot the classification loss as a function of number of tokens seen during our domain adaptation stage.We observe scaling as a function of number of training tokens with larger models exhibiting lower classification loss for the same number of training tokens.We also observe a linear relationship between supervised classification loss and the number of training tokens across model sizes.These results suggest that even the largest model we trained will still benefit from more data.We find this encouraging because it means we are able to produce better classifiers when discriminatively fine-tuned, despite our practical parameterlimitations [15,18].\n\nWe relate language modeling loss to classification loss in Figure 3  (c).We observe a linear relationship between language modeling loss during domain adaptation and classification loss during discriminative fine-tuning across model sizes.",
            "score": 0.7779431507800711,
            "section_title": "EXPERIMENTS 5.1 Offline Training and Evaluation",
            "char_start_offset": 19942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 627
                },
                {
                    "start": 629,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 988
                },
                {
                    "start": 988,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1519
                },
                {
                    "start": 1521,
                    "end": 1644
                },
                {
                    "start": 1644,
                    "end": 1801
                },
                {
                    "start": 1801,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2195
                },
                {
                    "start": 2197,
                    "end": 2270
                },
                {
                    "start": 2270,
                    "end": 2436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "We consider the problem of compute-optimal language model training: given a compute budget C, we wish to predict how to best allocate it across model size (in parameters) and dataset size (in tokens). With pretraining budgets ever-increasing, compute-optimal scaling is a question of paramount importance. In their seminal work, Kaplan et al. [30] proposed a scaling law predicting that the optimal ratio of tokens to parameters decays as a power of C. This scaling law was influential in determining the size of GPT-3 and several subsequent models [12,51,43,32,47,62,52]. However, Hoffmann et al. [25] challenged its validity, arguing instead that the optimal token-to-parameter ratio should be approximately independent of C, and that contemporary models had too many parameters relative to their number of training tokens. Based on this prediction, they trained a 67B parameters model called Chinchilla and which outperformed larger models with a similar compute budget. \n\nWhile Hoffmann et al. [25] and subsequent work [55,56,17,26,15] established that following the Hoffmann et al. scaling law leads to better performance than Kaplan et al. scaling, it is still important to understand why the two works arrived at different conclusions. Is the difference due to architecture, training setup, pretraining data, results analysis, or perhaps something else entirely? The answer could teach us important lessons on how to correctly predict and perform model scaling. \n\nHoffmann et al. [25] hypothesize that the scaling law discrepancy is due to Kaplan et al. [30] not tailoring the learning rate schedule for each token budget separately. While they demonstrate that mismatched learning rate decay results in a higher loss, they do not show it leads to a different compute-optimal scaling law. We further discuss Hoffmann et al. [25]'s hypothesis in Appendix A. To the best of our knowledge, this hypothesis is the only explanation offered in the literature so far. \n\nOur contribution. In this work, we uncover three factors contributing to the discrepancy, and disprove Hoffman et al.'s hypothesis about the role of learning rate decay; Figure 1 illustrates our main  Cosine decay (no tuning) results.",
            "score": 0.774957756532119,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1831,
                    "end": 1835,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "268820276",
            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
            "text": "Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields (e.g., programming and creative writing). However, training large language models requires significant pretraining computation costs on large-scale training data. To reduce the computation costs, scaling law (Kaplan et al., 2020) is proposed to illustrate the powerlaw relationship between pretraining loss and computational effort, which has provided valuable insights into model optimization with minimal computational cost. Recently, several findings, such as those exploring the phenomena of emergence (Wei et al., 2022) and the Broken Neural Scaling Laws (Caballero et al., 2023), indicate these laws might not fully capture model capabilities, particularly for downstream tasks. Therefore, it is important to expand and refine our evaluative frameworks.",
            "score": 0.7731024235975239,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Scaling laws have become a fundamental aspect of modern AI development, especially for large language models (LLMs). In recent years, researchers have identified consistent relationships between model size, dataset volume, and computational resources, demonstrating that increasing these factors leads to systematic improvements in performance. These empirical patterns have been formalized into mathematical principles, known as scaling laws, which provide a framework for understanding how * Equal contribution the capabilities of neural networks evolve as they grow. Mastering these laws is crucial for building more powerful AI models, optimizing efficiency, reducing costs, and improving generalization. \n\nThe study of neural scaling laws gained prominence with the foundational work of Kaplan et al. (2020), who demonstrated that model performance follows a power-law relationship with respect to size, data, and compute. Their findings suggested that larger language models (LMs) achieve lower loss when trained on sufficiently large datasets with increased computational resources. Later, Hoffmann et al. (2022) refined these ideas, introducing the notion of compute-optimal scaling, which revealed that training a moderate-sized model on a larger dataset is often more effective than scaling model size alone. However, recent studies (Muennighoff et al., 2023;Caballero et al., 2023;Krajewski et al., 2024) have challenged the universality of these laws, highlighting cases where sparse models, mixture-of-experts architectures, and retrievalaugmented methods introduce deviations from traditional scaling patterns. These findings suggested that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies. \n\nDespite the growing importance of scaling laws, existing research remains fragmented, with limited synthesis of theoretical foundations, empirical findings, and practical implications. Given the rapid evolution of this field, there is a need for a structured analysis that consolidates key insights, identifies limitations, and outlines future research directions. While theoretical studies have established the mathematical principles governing scaling, their real-world applications, such as efficient model training, optimized resource allocation, and improved inference strategies, are less explored.",
            "score": 0.7705305336677908,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "269899695",
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "text": "Language model (LM) scaling plays a central role in discussions of model capabilities and affects everything from the tasks they can perform to the effectiveness of post-training techniques such as Chain-of-Thought [99]. Due to this importance, understanding and predicting LM behaviors across scales, benchmarks, and algorithmic interventions is a major question for many researchers and engineers. Machine learning researchers may wish to understand whether their proposed algorithmic interventions remain effective in the face of future model scaling, while engineers and benchmark builders may wish to understand whether complex capabilities such as agentic abilities will scale predictably in the same way as existing LM benchmarks. \n\nScaling laws [6,36,37,44,65] have been powerful tools for understanding the scaling trend of LMs, which have shown that LMs follow a precise power-law relationship between compute measures (such as training FLOPs) and downstream capabilities ranging from perplexity [37,44] to benchmark performance [34,35]. This power-law relationship has been used in a variety of waysincluding hyperparameter and architecture selection [9,37,44] as well as model capability forecasting [25,66,67]. Unfortunately, scaling analyses remain uncommon in many benchmarking and posttraining studies, as most researchers do not have the compute resources to build scaling laws from scratch, and open models are trained at too few scales (3)(4)(5) for reliable scaling predictions. training compute to downstream capabilities (dashed line) by hypothesizing the existence of a lowrank space of LM capabilities that have a log-linear relationship with compute (center), and can be extracted directly from standardized LM benchmarks (left). This enables us to get low-cost, high-resolution scaling predictions of LMs' complex downstream capabilities from their observable standard benchmark metrics using nearly 100 publicly accessible LMs (left to right). \n\n(e.g., agentic or \"emergent\" behaviors) and post-training interventions, can be done using a lower-cost, higher-resolution, and broader-coverage alternative to the standard approach of training (or using) a single family of LMs across compute scales.",
            "score": 0.769814897823177,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "269042762",
            "title": "Rho-1: Not All Tokens Are What You Need",
            "text": "Scaling laws guide us in discovering the impact of factors such as parameter count, data size, and compute on language model performance and behavior. These studies usually focus on predicable scaling though power law [Kaplan et al., 2020, Hernandez et al., 2021], optimal resource allocation [Hoffmann et al., 2022], downstream tasks [Wei et al., 2022b, Isik et al., 2024, Gadre et al., 2024], architectures [Tay et al., 2022], memorization [Tirumala et al., 2022, Carlini et al., 2022, Henighan et al., 2023, Biderman et al., 2024], and repeating data [Hernandez et al., 2022, Muennighoff et al., 2024, Xue et al., 2024]. Most scaling laws on model performance study cross-entory loss on all training tokens, while we focus on the tokens loss of desired distributions.",
            "score": 0.7688980955070785,
            "section_title": "B.4 Scaling Laws",
            "char_start_offset": 27682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 770
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 372,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 442,
                    "end": 464,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 509,
                    "end": 533,
                    "matchedPaperCorpusId": "258291763"
                },
                {
                    "start": 603,
                    "end": 622,
                    "matchedPaperCorpusId": "258833284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390869140625
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "While neural scaling laws precede the advent of large language models [24,45], breakthroughs in model [57] and data [42,44] scaling allowed Kaplan et al. [30] to demonstrate the dramatic possibility of unbounded improvement with scale, triggering an explosion in the literature on the topic. Here we focus on the relatively fewer works that tackle optimal resource allocation under a compute constraint. \n\nFor language modeling, Hu et al. [26] and DeepSeek [15] repeat subsets of the analyses in Hoffmann et al. [25] and derive compute optimal scaling laws. Employing Approach 3 of Hoffmann et al. [25] (see also [9]), Hu et al. [26] find that, for their models, optimal scaling favors larger token-toparameter ratios than in Hoffmann et al. [25] and in our results. They attribute this difference to modeling improvements since [25] and argue the same holds for Llama 2 [56]. However, our setup incorporates most of the advances in Llama 2 and still produces power laws very close to Hoffmann et al. [25]. Like us, DeepSeek [15] perform hyperparameter tuning and use isoFLOP analysis to determine compute-optimal model sizes on multiple datasets. While they arrive at an exponent on the order of Hoffmann et al. [25] for the main dataset they study, they report a higher exponent a = 0.578 for OpenWebText2 (i.e., predicting lower token-parameter-ratio at scale), which they attribute to the superior quality of the dataset. We also study this dataset but arrive much closer to the Hoffmann et al. scaling law. We conjecture the larger exponent might be due to repeating training data, which likely occurred in their experiment given the dataset's limited size and their compute budget. Settling these discrepancies could be a source of further valuable lessons on optimal model scaling. \n\nRecent work also studies compute-bounded scaling laws beyond the compute-optimal regime.",
            "score": 0.7658035926033597,
            "section_title": "Related work",
            "char_start_offset": 22294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6416015625
        },
        {
            "corpus_id": "273850391",
            "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
            "text": "Scaling laws [KMH + 20] for large language models (LLMs) have proven to be valuable for predicting how model capabilities grow with increasing data, compute, and model size. However, inference-time scaling-which focuses on the value of investing in additional computation at model run-time-is a promising new area of study. We see opportunities for innovating on how best to guide such run-time allocations for advancing efficiency, accuracy, and reasoning abilities. We review in this section promising directions in this emerging area of research and development, partly framed by our experimental results and prior research in AI on reasoning, ensembling, and control of the nature and extent of inference.",
            "score": 0.7652103430898813,
            "section_title": "Directions with LLM Run-Time Strategies",
            "char_start_offset": 19823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 709
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85986328125
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets.However, practitioners usually turn to benchmark performance, not loss, to compare models.\n\nIn this paper, we conduct an extensive set of experiments to address both scaling in the over-trained regime and benchmark performance prediction.\n\nMotivated by the practice of training beyond compute-optimality, we first investigate whether scaling follows reliable trends in the over-trained regime.We notice, as implied by Hoffmann et al. [45], for a set of models of different sizes trained with a constant ratio of tokens to parameters, models' reducible loss L \u2032 [43,45] follows a power law (L \u2032 = \u03bb \u2022 C \u2212\u03b7 ) in the amount of training compute C. We find that as one increases the ratio of tokens to parameters, corresponding to more over-training, the scaling exponent \u03b7 remains about the same, while the scalar \u03bb changes.We explain our observations by reparameterizing existing scaling laws in relation to the amount of over-training.\n\nTo establish empirically that scaling extrapolates in the over-trained regime, we further experiment with a testbed of 104 models, trained from scratch on three different datasets: C4 [27,88], RedPajama [112], and RefinedWeb [82].We find that scaling laws fit to small models can accurately predict the performance of larger models that undergo more over-training.Figure 1 (left) illustrates our main over-training result, where we invest 2.4e19 FLOPs to extrapolate the C4 validation performance of a 1.4B parameter model trained on 900B tokens, which requires 300\u00d7 more compute to train.\n\nIn addition to over-training, we also investigate if scaling laws can predict the performance of a model on downstream tasks.We establish a power law relationship between language modeling perplexity and the average top-1 error on a suite of downstream tasks.While it can be difficult to predict the error on individual tasks, we find it possible to predict aggregate performance from a model's perplexity among models trained on the same training data.",
            "score": 0.7641613795467055,
            "section_title": "Introduction",
            "char_start_offset": 2049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 263
                },
                {
                    "start": 265,
                    "end": 411
                },
                {
                    "start": 413,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1106
                },
                {
                    "start": 1108,
                    "end": 1338
                },
                {
                    "start": 1338,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1697
                },
                {
                    "start": 1699,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 1958
                },
                {
                    "start": 1958,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "The work [126] presents a thorough study of the empirical scaling laws of transformer-based large language models. The authors observe that model performance (objective function ) primarily depends on three factors: the number of model parameters  , dataset size , and the computing budget for training. They demonstrate a power-law relationship between model performance (measured in objective function, ) and these factors. For instance, they found that the relationship between performance and dataset size can be represented as () \u2248 (5.4 \u00d7 10 13 /) 0.095 . This formula suggests that as the dataset size increases, the model's performance improves following a specific pattern. While theoretical generalization bounds may suggest a similar power-law relationships, they generally do not provide specific coefficients like those identified in Kaplan et al.'s work [126]. This specificity is crucial for accurately predicting model performance. Additionally, the study highlights that transformers, known for their effective handling of long-range data dependencies, tend to outperform Long Short-Term Memory networks (LSTMs) [106] as they scale. This observation underscores the potential of transformers in large-scale language processing tasks. \n\nCompute-Optimal Models via Scaling Law. When working within a fixed computational budget, it is crucial to find the right balance between model size ( ) and dataset size (). This is where the scaling law curve, ( , ), becomes a vital tool. It helps determine the most effective trade-off between these two factors. The scaling law, as observed in [126], was instrumental in designing GPT-3, a 175 billion parameter language model. Interestingly, GPT-3 was trained on fewer tokens than was typical for its time [23]. Different forms of the scaling law curve have led to the development of diverse models, as seen in subsequent studies [9,25,107]. A notable application of these predicted scaling laws is found in [107]. Their research revealed that many previously trained LLMs, including Gopher [216], could have achieved better performance within the same compute budget. They demonstrated this by training a smaller model, Chinchilla, with 70 billion parameters, which outperformed the larger 280 billion parameter Gopher model [216] while using a similar compute budget. \n\nScaling Law for Transfer Learning.",
            "score": 0.7623943837166824,
            "section_title": "Scaling Law",
            "char_start_offset": 15063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2325
                },
                {
                    "start": 2328,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 1128,
                    "end": 1133,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1762,
                    "end": 1766,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1886,
                    "end": 1889,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 1889,
                    "end": 1892,
                    "matchedPaperCorpusId": "253117181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "269457365",
            "title": "More Compute Is What You Need",
            "text": "Large language model pre-training has become increasingly expensive, with most practitioners relying on scaling laws to allocate compute budgets for model size and training tokens, commonly referred to as Compute-Optimal or Chinchilla Optimal. In this paper, we hypothesize a new scaling law that suggests model performance depends mostly on the amount of compute spent for transformer-based models, independent of the specific allocation to model size and dataset size. Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets, and (b) assuming the exhaustion of available web datasets, scaling the model size might be the only way to further improve model performance.",
            "score": 0.7615405106108435,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "270068102",
            "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
            "text": "However, they did not investigate the relation of weight averaging to compute optimality and its implications for scaling experiments. \n\nScaling Law Experiments for Neural Language Models. Kaplan et al. (2020) were the first to establish scaling laws for language models by training a suite of models for a fixed token count. Important to our work, Hoffmann et al. (2022) revise their laws and demonstrate specific methods to establish laws, notably training a family of models for different cosine lengths. The subsequent models like LLama and LLama2 (Touvron et al., 2023a,b) further improve performance of smaller models by training beyond the Chinchilla optimal point, motivated by lower inference costs (Gadre et al., 2024;De Vries, 2023;Sardana & Frankle, 2023). Recent works (Muennighoff et al., 2023;Bi et al., 2024;Goyal et al., 2024) highlight how data repetition and quality affect the scaling behavior, which suggests that scaling laws should be updated more frequently. However, these works do not consider efficient experiments for scaling laws, which is the focus of our work.",
            "score": 0.755324918105523,
            "section_title": "Related work",
            "char_start_offset": 25271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1091
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 371,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.671875
        },
        {
            "corpus_id": "269899695",
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "text": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.",
            "score": 0.7551384511725324,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "257913333",
            "title": "Eight Things to Know about Large Language Models",
            "text": "Scaling law results (Kaplan et al., 2020;Brown et al., 2020;Hoffmann et al., 2022) have been a major driving factor in the recent surge of research and investment into LLMs (Ganguli et al., 2022a). Scaling laws allow us to precisely predict some coarse-but-useful measures of how capable future models will be as we scale them up along three dimensions: the amount of data they are fed, their size (measured in parameters), and the amount of computation used to train them (measured in FLOPs). These results thereby allow us to make some key design decisions, such as the optimal size of a model given some fixed resource budget, without extremely expensive trial and error. \n\nOur ability to make this kind of precise prediction is unusual in the history of software and unusual even in the history of modern AI research. It is also a powerful tool for driving investment since it allows R&D teams to propose model-training projects costing many millions of dollars, with reasonable confidence that these projects will succeed A scaling law result for one measure of language model performance, showing a consistent trend as the amount of computation used to train a model is scaled up 10,000,000,000\u00d7 times from a small prototype system to GPT-4. \n\nat producing economically valuable systems. \n\nConcretely, consider these three superficially very different systems: OpenAI's original GPT can perform simple text-labeling tasks but cannot generally produce coherent text (Radford et al., 2018). GPT-2 adds the ability to produce text of reasonably high quality, as well as a limited ability to follow simple instructions (Radford et al., 2019). GPT-3 is the first modern general-purpose LLM, and is practically useful across a wide range of language tasks. The designs of these three models hardly differ at all. Instead, the qualitative differences between them stem from vast differences in scale: Training GPT-3 used roughly 20,000\u00d7 more computation than training the original GPT (Sevilla et al., 2022), as well as significantly more data and parameters.",
            "score": 0.7469181852673857,
            "section_title": "LLMs predictably get more capable with increasing investment, even without targeted innovation",
            "char_start_offset": 1569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 60,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 60,
                    "end": 82,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 173,
                    "end": 196,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 1471,
                    "end": 1493,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1621,
                    "end": 1643,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57275390625
        },
        {
            "corpus_id": "273507997",
            "title": "A Simple Model of Inference Scaling Laws",
            "text": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.",
            "score": 0.7464773222835295,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7841796875
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "Large language models (LLMs) have substantial training and inference compute and energy costs (Knight, 2023;Pope et al., 2022). Training computation costs are primarily determined by the size of the model and the amount of data seen during training (Hoffmann et al., 2022). For state-ofthe-art models with tens of billions of parameters trained on trillions of tokens, training costs can easily exceed millions of dollars. Similarly, inference costs depend on the size of the model and the volume of user queries over the lifetime Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). of the model. This volume can be significant; demand for popular models can exceed billions of tokens per day (OpenAI & Pilipiszyn, 2021;Shazeer & Freitas, 2022). \n\nAccounting for both training and inference, how does one minimize the cost required to produce and serve a high quality model? \n\nRecent studies have proposed scaling laws, empirical formulas that estimate how changes in model and training data size impact model quality (Kaplan et al., 2020;Hoffmann et al., 2022). Hoffmann et al. (2022) is perhaps the most influential of these works, finding that to scale language models most efficiently, parameters and tokens should grow approximately linearly. The authors applied this scaling law to train a 70B parameter model (dubbed Chinchilla) that outperformed much larger and more expensive models such as GPT-3. As a result, many subsequent LLMs have been trained following the Chinchilla scaling laws (Dey et al., 2023;Muennighoff et al., 2023). \n\nHowever, the Chinchilla scaling laws only account for the computational costs of training. By contrast, the Llama 2 family of models were trained on 2 trillion tokens and the Llama 3 family of models were trained on 15 trillion tokens, which is far more data than the Chinchilla scaling laws would deem \"optimal\" (Touvron et al., 2023a;b;AI@Meta, 2024).",
            "score": 0.7431607158895432,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1982
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
            "score": 0.7368656151730907,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "275336968",
            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
            "text": "The study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
            "score": 0.7339258260535553,
            "section_title": "Related Work",
            "char_start_offset": 4305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 1803,
                    "end": 1806,
                    "matchedPaperCorpusId": "6519532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "269605352",
            "title": "Quantifying the Capabilities of LLMs across Scale and Precision",
            "text": "The availability of extensive data and substantial computational resources enable the pretraining of Large Language Models (LLMs) at an unprecedented scale.The increase in scale (e.g., the amount of compute budget for training, model parameters, etc.), according to a wider belief, can lead to unpredictable improvements in the performance and sampling efficiency on a broad spectrum of downstream tasks (Wei et al., 2022a;Kaplan et al., 2020;Radford et al., 2019;Devlin et al., 2018).As these models are scaled up, they begin to show emerging abilities (Wei et al., 2022a,b).With such an increase in size, LLMs can generate human-like text and excel in complex tasks that require creativity (Min et al., 2021;Kasneci et al., 2023;Yang et al., 2023b).The alignment with methods like Reinforcement Learning from Human Feedback (RLHF) further enables a higher level of customization with human needs, thereby raising the potential applications and utility of such LLMs (Ouyang et al., 2022;Korbak et al., 2023).As these models continue to improve with scale, it has now become a standard practice to train models with billions or even trillions of parameters (K\u00f6pf et al., 2023;Balagansky and Gavrilov, 2023;Yang et al., 2023a).\n\nContrary to the previous view that model performance enhances with scale which is also referred to as the scaling laws, a few studies argue that improvements do not linearly correlate with an increase in the number of parameters for certain tasks (Ganguli et al., 2022;Wei et al., 2022a;Lin et al., 2021).It also remains uncertain to what extent scaling laws apply across various downstream tasks.Moreover, achieving performance with scale carries a significant computational cost and carbon footprint.For instance, it is estimated that training GPT-3 with 175 billion parameters requires nearly 1300 megawatt-hours of electricity (Patterson et al., 2021) and would take almost 288 years with a single NVIDIA V100 GPU (Narayanan et al., 2021).",
            "score": 0.7315217752692768,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 485
                },
                {
                    "start": 485,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1226
                },
                {
                    "start": 1228,
                    "end": 1533
                },
                {
                    "start": 1533,
                    "end": 1625
                },
                {
                    "start": 1625,
                    "end": 1730
                },
                {
                    "start": 1730,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 464,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 710,
                    "end": 731,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 967,
                    "end": 988,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 988,
                    "end": 1008,
                    "matchedPaperCorpusId": "257020046"
                },
                {
                    "start": 1475,
                    "end": 1497,
                    "matchedPaperCorpusId": "246867298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1763916015625
        },
        {
            "corpus_id": "276574659",
            "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
            "text": "Following cluster analysis, we compute evaluation metrics of small models within each cluster and conduct separate extrapolation curve fitting procedures. Small models are trained with the same ratio of training tokens to Compute Per Token (CPT). We propose a scaling law for downstream task performance, supported by theoretical analysis, which allows us to derive prediction formulas for performance scaling within clusters of tasks that share similar difficulty features. The fitting process initially excludes outlier samples, focusing only on the clustered sample set. For each cluster identified in the previous step, we compute accuracy metrics across small models, yielding an expected accuracy array for each cluster. By fitting these accuracy values against the computational costs of small models, we derive the expected accuracy-to-compute curve for each cluster. \n\nWe derive the fitting formula for the downstream task scaling law based on the following three assumptions: \n\n1. The relationship between the answer loss and the compute follows a power law, which generalizes the power law in loss prediction into (Question, Answer) format data. 2. For task samples with a finite set of answers, the model gives a random guess choice if it cannot accurately solve it. 3. The task passrate is defined as the product of the predicted probabilities for each token, implying that each task sample has a unique answer, and the model outputs the answer only without any intermediate reasoning progress. \n\nNote that these assumptions may not perfectly hold in practice, we provide additional discussions on Assumption 3 in Section 6. Under the above assumptions we can derive the scaling law for downstream task performance. \n\nProposition 1 (Scaling Law for Downstream Task Performance). Given a language model trained with computational budget C, and a set of downstream tasks P , under the following assumptions: \n\nThe expected accuracy on tasks P can be modeled as: \n\nwhere: \n\n\u2022 g represents the random guess performance floor; \n\n\u2022 1 \u2212 g represents the maximum achievable performance improvement; \n\n\u2022 a, b, c are positive constants; \n\n\u2022 \u00b5 = 1 #P (q,ans)\u2208P loss ans ; \n\nWe outline the key proof intuition here, with detailed proofs provided in Appendix B. Like existing approaches [19], we aim to establish the relationship between loss ans and model passrate, leveraging loss power-law scaling to derive a scaling formula for downstream task passrate metrics.",
            "score": 0.7303916888927189,
            "section_title": "Fitting",
            "char_start_offset": 12405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 1972
                },
                {
                    "start": 1975,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2034
                },
                {
                    "start": 2037,
                    "end": 2103
                },
                {
                    "start": 2106,
                    "end": 2139
                },
                {
                    "start": 2142,
                    "end": 2173
                },
                {
                    "start": 2176,
                    "end": 2466
                }
            ],
            "ref_mentions": [
                {
                    "start": 2287,
                    "end": 2291,
                    "matchedPaperCorpusId": "263672005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "276580569",
            "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
            "text": "Inference strategies have been a long-studied topic in the field of language processing. Traditional approaches include greedy decoding (Teller, 2000), beam search (Graves, 2012), and Best-of-N. \n\nRecent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored.",
            "score": 0.7293059125233033,
            "section_title": "Test-Time Scaling",
            "char_start_offset": 6996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 847
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "273323177",
            "title": "Scaling Laws for Predicting Downstream Performance in LLMs",
            "text": "Large language models (LLMs) form the basis for numerous real-world applications (Brown et al., 2020;Jiang et al., 2023;Xu et al., 2024;Hadi et al., 2023) and scaling laws analysis serves as the foundation for LLMs development (Kaplan et al., 2020;Bahri et al., 2024). The key idea of scaling laws involves training a sequence of language models (LMs) to gather data (e.g., expended compute and corresponding model performance). This data is then used to build a predictive model that estimates the performance of a substantially larger target LLM (Su et al., 2024;Hoffmann et al., 2022). \n\nPrevious efforts focus on predicting the target LLM's pre-training loss and establish a power-law relation between the computational resource expended (e.g., floating-point operations per second (FLOPs)) and the final loss achieved (Kaplan et al., 2020;Muennighoff et al., 2024;Henighan et al., 2020). Further, we aim to predict the downstream performance in pre-trained LLMs (i.e., zero-or few-shot evaluation) to more accurately reflect the primary concerns regarding their capabilities. The critical challenge is the emergent abilities in LLMs, which states that LLMs only exceed random performance when the FLOPs expended during training surpass task-specific thresholds (Wei et al., 2022a). Supposing a task threshold of F c , typical methods require training N LMs, expending total FLOPs F t = N i=1 FLOPs i > N \u00d7 F c , to obtain N effective data points, thereby necessitating significant computational resources. Fig. 1 demonstrates that the sampling LMs require more than 5 \u00d7 10 20 FLOPs to perform better than random on most benchmarks, with only three data points available to fit the predictive curve across these benchmarks. Hu et al. (2023) address this challenge by significantly increasing the sampling times to compute the PassUntil of a task, basically increasing the \"metric resolution\" to enable the abilities to emerge earlier (i.e., reducing F c ).",
            "score": 0.7280018868291898,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 101,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 248,
                    "end": 267,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 844,
                    "end": 869,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "270123337",
            "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model",
            "text": "For a broad spectrum of factors x, scaling laws (Kaplan et al., 2020;Henighan et al., 2020;Hoffmann et al., 2022) indicate that their impact on the loss L of a pre-trained model follows a power law relationship. Here, x may represent model sizes, quantities of training data, or training steps, with parameters to be determined. Previous research (Alabdulmohsin et al., 2022;OpenAI, 2023;Bi et al., 2024;Su et al., 2024;Xiong et al., 2024) highlights the impressive predictive power of scaling laws. Notably, fitting this relationship to a set of smaller models, training datasets, or computational resources enables precise extrapolation to predict the test loss for much larger cases across several orders of magnitude. This capability allows practitioners to estimate the performance of a pre-trained larger language model without incurring the substantial cost of completing extensive training runs. However, the scaling law for synthesized long-context data remains unexplored despite its importance for long-context modeling. Therefore, using our Quest method, we investigate the scaling laws of synthesized long-context data across various model sizes and confirm the predictability of the Quest method.",
            "score": 0.7245777251100833,
            "section_title": "Scaling Laws",
            "char_start_offset": 7729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1210
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "276574659",
            "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
            "text": "Loss scaling laws provide a systematic framework for understanding the relationship between computational resources, data, model size, and the final performance of LLMs. Early work by Kaplan et al. [21] demonstrates that the pre-training loss of LLMs follows a power-law relationship with the compute (the number of floating-point operations) used in training. Subsequent studies extend these findings to other domains, such as computer vision [41], graph learning [24] and vision-language models [2,16]. Recent research has also explored scaling laws in specific contexts, such as fine-tuning [17,34], vocabulary size optimization [33], retrieval-augmented models [30], and hyperparameter tuning [23,40]. These studies highlight the broad applicability of scaling laws and their potential to guide the efficient allocation of computational resources.",
            "score": 0.7217078046804154,
            "section_title": "Loss Scaling Laws",
            "char_start_offset": 3392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 851
                }
            ],
            "ref_mentions": [
                {
                    "start": 444,
                    "end": 448,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 497,
                    "end": 500,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "260498358"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "276903172",
            "title": "AGI, Governments, and Free Societies",
            "text": "Rapid improvements in models have surpassed the expectations of many observers and academics in the field, largely due to the sheer scale of the data and computing resources available to train deep neural network models. \n\nScaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters. 4  short, bigger is better--at least so far--thanks to a combination of technological advancements and increased resource allocation, leading to rapid progress in AI capabilities. The compute required for language models to reach a set performance threshold has halved approximately every eight months since 2012. In addition to significant algorithmic innovations, such as the Transformer architecture, many recent performance gains stem primarily from the ability to scale up models and datasets to unprecedented sizes. This has been made possible by rapid growth in computing power, which has allowed the rate of improvement to substantially exceed hardware gains from Moore's Law. 5 untless recent advances by labs, academic institutions, and open source communities reflect growing investment in the field and a diversification of approaches and algorithms. For example, new systems from Google DeepMind have recently solved four out of six problems from last year's International Mathematical Olympiad, matching the competition's silver medalist for the first time.6",
            "score": 0.7172247620167215,
            "section_title": "II. The Age of Artificial Intelligence",
            "char_start_offset": 5600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2349
                },
                {
                    "start": 2350,
                    "end": 2559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
            "score": 0.7163580906245168,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "Scaling laws. Recent research on scaling laws has established that model performance follows predictable power-law relationships with respect to the number of parameters, the size of the training dataset, and the available compute (Hestness et al., 2017;Rosenfeld et al., 2020). The seminal work by Kaplan et al. (2020) demonstrates that the test loss of language models decays as a function of model size and data in a highly regular manner. Subsequent studies refine these initial observations and extend them into more diverse settings (Hoffmann et al., 2022;Alabdulmohsin et al., 2022;Muennighoff et al., 2023;Lin et al., 2024;Goyal et al., 2024b). However, most of these existing works are primarily focused on the training regime. Inference strategies and inference-time compute utilization in LLM problem-solving. A variety of inference strategies have been developed to generate sequences with a trained model (Welleck et al., 2024). Deterministic methods such as greedy decoding and beam search (Teller, 2000;Graves, 2012) find highly probable sequences which typically have decent quality but lacks diversity. Sampling algorithms (e.g., temperature sampling (Ackley et al., 1985)) can produce a diverse set of results which are then aggregated to achieve higher accuracy (e.g., via the self-consistency approach (Wang et al., 2023a)). Recent methods combine search algorithms with LLMs, including breadth-first or depth-first search (Yao et al., 2023), Monte-Carlo Tree Search (MCTS) (Zhang et al., 2023;Zhou et al., 2024;Liu et al., 2024;Choi et al., 2023), and guided beam search (Xie et al., 2023). Several prior studies also find that LLM problem-solving performance can be improved by outputting \"dummy\" tokens at inference time (Goyal et al., 2024a;Pfau et al., 2024).",
            "score": 0.7147803423060922,
            "section_title": "RELATED WORKS",
            "char_start_offset": 7022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 277,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 562,
                    "end": 589,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 589,
                    "end": 614,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 614,
                    "end": 631,
                    "matchedPaperCorpusId": "267411718"
                },
                {
                    "start": 631,
                    "end": 651,
                    "matchedPaperCorpusId": "269033049"
                },
                {
                    "start": 918,
                    "end": 940,
                    "matchedPaperCorpusId": "270703266"
                },
                {
                    "start": 1168,
                    "end": 1189,
                    "matchedPaperCorpusId": "12174018"
                },
                {
                    "start": 1322,
                    "end": 1342,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1443,
                    "end": 1461,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1494,
                    "end": 1514,
                    "matchedPaperCorpusId": "257427177"
                },
                {
                    "start": 1532,
                    "end": 1549,
                    "matchedPaperCorpusId": "262824527"
                },
                {
                    "start": 1549,
                    "end": 1567,
                    "matchedPaperCorpusId": "264128245"
                },
                {
                    "start": 1592,
                    "end": 1610,
                    "matchedPaperCorpusId": "258426922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "268351635",
            "title": "Unraveling the Mystery of Scaling Laws: Part I",
            "text": "A wide range of studies have shown that the performance of a language model exhibits a notable growth pattern as the number of parameters and data size increase, following a power-law relationship [HNA + 17, KMH + 20, HKK + 20, CDLCG + 22, ZKHB22, GSH23, BSA + 23].This scaling law plays a fundamental role in the development of large language models, enabling us to estimate optimal configurations of large models from the training logs of much smaller models [TDR + 22, HBM + 22].As mentioned in the GPT-4 technical report [AAA + 23], some aspects of GPT-4's performance can be accurately predicted based on models trained with no more than 1/1,000th the compute of GPT-4.By properly utilizing the scaling law, we avoid the need to perform extensive model-specific tuning on large models.\n\nThe original scaling law paper by OpenAI presented the formulas of scaling laws and illustrated how they could aid in determining optimal training configurations [KMH + 20].Nonetheless, the presented formulas are based on static exponents estimated from their specific experiment setup.The full details on how to derive the constant terms in the scaling-law formulas for a new experiment setup (model architecture, tokenization, data distribution, etc) remain undisclosed.Furthermore, [KMH + 20] only conducted experiments with models containing up to 1.5B parameters, a size significantly smaller than that of contemporary large language models.There have been subsequent works that study scaling laws on larger models [CDLCG + 22, IPH + 24].Some have drawn different conclusions from the original scaling-law paper, casting doubt to the general applicability of scaling laws.For example, [HBM + 22] claimed that the training data size should be scaled much more than the recommendation in [KMH + 20].[BCC + 24] suggested that the optimal batch size depends only on the compute budget rather than the loss value.\n\nIn this paper, we revisit the scaling-law formulas proposed by [KMH + 20], confirming that they remain generally applicable when scaling the model size up to 33B.",
            "score": 0.714371969940762,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 790
                },
                {
                    "start": 792,
                    "end": 965
                },
                {
                    "start": 965,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1905
                },
                {
                    "start": 1907,
                    "end": 2069
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.564453125
        },
        {
            "corpus_id": "271270413",
            "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
            "text": "Language models The Transformer [71] has proven to be a scalable architecture for language models, especially large language model (LLMs) [11,14,52,47,20,29,53,70,73,41,8,4,38,25,62,67,7,39,32,88]. These models typically acquire a deep understanding of language enabling them to perform multiple tasks after a pre-training period and an optional fine-tuning period. Their capabilities include code generation [33,3,43,87,86], mathematical reasoning [76,5], question answering [48,45] among others. Given the expensive deployment costs required by the language models, various techniques can be adopted for efficient inference [64,65,78,74,36]. In our work, we pre-train large language models from scratch on English corpora and focus on their validation loss and downstream performance after training. \n\nScaling laws Scaling laws aim to develop a predictive framework to find the best allocation of compute resources to maximize model performance. Besides language models, they have been studied in other domains [40,68,13]. For language models, Kaplan et al. [30] show that performance improves as a power law with more compute allocated to both parameters or data. Hoffmann et al. [26] show that the compute allocation of parameters and data should be scaled equally. Other work considers various cases such as downstream performance [23,28,55], inference time [57] or data constraints [44,80]. However, the effect of vocabulary size has generally been ignored previously.",
            "score": 0.714038298060786,
            "section_title": "Related work",
            "char_start_offset": 26150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1474
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 169,
                    "end": 171,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "259370805"
                },
                {
                    "start": 476,
                    "end": 480,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "259858812"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 1388,
                    "end": 1392,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 1392,
                    "end": 1395,
                    "matchedPaperCorpusId": "258833284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "252089870",
            "title": "A Review of Sparse Expert Models in Deep Learning",
            "text": "The cross-entropy loss of dense neural language models was shown to scale as a power-law (i.e. l(x) = (c/x) \u03b1 for a variable x) with respect to the model parameter count, amount of data, and compute budget when not constrained by the other two factors (Kaplan et al., 2020). The power law coefficients were later corrected in Hoffmann et al. (2022), which demonstrated that computeoptimal models required a closer balance of data and parameter scaling. In contrast, early research in sparse expert models scaled heuristically -achieving strong empirical results -but without careful characterization of the scaling laws. Further, several works highlighted discrepancies between upstream (e.g. pre-training) and downstream (e.g. fine-tuning) behavior (Fedus et al., 2021;Artetxe et al., 2021), further complicating the understanding and explanation of sparse expert models.",
            "score": 0.7098945774388584,
            "section_title": "SCALING PROPERTIES OF SPARSE EXPERT MODELS",
            "char_start_offset": 11046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 872
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484130859375
        },
        {
            "corpus_id": "274436125",
            "title": "Scaling Law for Language Models Training Considering Batch Size",
            "text": "Large language models (LLMs) have made remarkable advances in recent years, with scaling laws playing a critical role in this rapid progress. In this paper, we empirically investigate how a critical hyper-parameter, i.e., the global batch size, influences the LLM training prdocess. We begin by training language models ranging from 125 million to 2.6 billion parameters, using up to 300 billion high-quality tokens. Through these experiments, we establish a basic scaling law on model size and training data amount. We then examine how varying batch sizes and learning rates affect the convergence and generalization of these models. Our analysis yields batch size scaling laws under two different cases: with a fixed compute budget, and with a fixed amount of training data. Extrapolation experiments on models of increasing sizes validate our predicted laws, which provides guidance for optimizing LLM training strategies under specific resource constraints.",
            "score": 0.708878419132696,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61669921875
        },
        {
            "corpus_id": "277940458",
            "title": "Scaling Laws for Data-Efficient Visual Transfer Learning",
            "text": "Early studies [5,17,37] explore the relationship between model performance and scale. Although the concept of scaling laws is not initially formalized, these pioneering works provide crucial insights and lay the groundwork for understanding the predictability of model performance as scale increases. \n\nIn NLP, the concept of scaling laws is widely recognized largely due to the works of [15,16,20]. These studies formally establish a quantitative relationship between model size, dataset size, and training compute for transformer models. Their findings have not only provided a foundation for subsequent research, but have also inspired deeper investigations into the fundamental principles governing scaling behavior in language models. \n\nBeyond empirical exploration of neural scaling laws across different models and tasks, many researchers have sought to challenge these patterns by refining model training strategies or data sampling approaches in hopes of breaking conventional scaling laws. [42] proposes a novel approach to *breaking* the neural scaling law by initializing large models with the weights of smaller pretrained models, thus improving data efficiency. [39] has further shown that model shape, rather than model size, plays a more crucial role in downstream NLP tasks and has introduced a scaling strategy called DeepNarrow, which reduces parameter count while improving training speed and performance.",
            "score": 0.7083978218285262,
            "section_title": "Scaling Laws in NLP",
            "char_start_offset": 6545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1425
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 17,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "260816728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54833984375
        },
        {
            "corpus_id": "270226169",
            "title": "Zyda: A 1.3T Dataset for Open Language Modeling",
            "text": "Over the last five years, large language models (LLMs) have been undergoing an extremely rapid growth in scale, cost, and capabilities (Vaswani et al., 2017;Radford et al., 2019;Brown et al., 2020;Team et al., 2023;Achiam et al., 2023;Sevilla et al., 2022). This development was fueled by the LLM scaling laws (Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022) that established a relationship between the attainable loss and model size, dataset size and compute budget based on systematic experiments. Highlighting how performance increases with model size, these scaling laws provide guidance for how to optimally allocate resources for model size and dataset size given a fixed compute budget and provide concrete and fairly accurate predictions about the final loss and downstream capabilities of these models. Specifically, the \"Chinchilla scaling laws\" (Hoffmann et al., 2022) suggest that equal scaling of parameters and data is required to train a \"compute-optimal\" model. However, as models become increasingly widely deployed, the majority of the total FLOPs are spent in inference, and not in pretraining. Therefore, the focus has been shifting towards \"inference-optimal\" models, which are much smaller and are trained on significantly more tokens than the Chinchilla scaling laws would recommend (Touvron et al., 2023;Jiang et al., 2023). These smaller models require significantly fewer forward-pass FLOPs and significantly less GPU VRAM for inference, which has caused them to become extremely important in the open-source LLM community, where the ability of a model to fit inside the VRAM of consumer GPUs is extremely important. \n\nThese trends have resulted in a significant increase in the ratio of total number of training tokens to model parameter count. State-of-the-art LLMs went from a 300B:175B ratio with GPT3 and a 540B:760B ratio with PaLM to a 12T:132B ratio with DBRX (MosaicML, 2024) and a 15T:8B ratio with Llama3.",
            "score": 0.7076353387869798,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 157,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 157,
                    "end": 178,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 178,
                    "end": 197,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 235,
                    "end": 256,
                    "matchedPaperCorpusId": "246822642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "While neural scaling laws have provided valuable insights into model performance, their current formulations often fail to account for recent advancements in architecture, data efficiency, and inference strategies. The following directions highlight key areas where scaling laws should be adapted to improve their predictive power and practical utility. \n\nInference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications. \n\nCompute-optimal model selection: Scaling laws should not only predict performance improvements but also guide model selection given a fixed compute budget. Future work should explore multiobjective optimization frameworks that balance performance, energy efficiency, and cost to drive more sustainable AI development. \n\nEfficient data scaling and pruning: The optimization of model scaling necessitates a shift from volume-based to quality-focused data selection. Future frameworks should prioritize informative examples and integrate diversity metrics to enhance generalization, moving beyond simple dataset expansion.",
            "score": 0.7062814132481757,
            "section_title": "Future recommendations",
            "char_start_offset": 25490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1371
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "275993956",
            "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
            "text": "Training Scaling in LLM and DiT Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34]. Furthermore, extensive experiments on scaling both denoising backbones and training sets reveal that increasing transformer blocks is more parameter-efficient than increasing channel numbers for improving text-image alignment. The quality and diversity of the training set prove more crucial than mere dataset size [33]. These findings provide valuable insights for determining optimal model architectures and data requirements in both domains. \n\nInference Scaling Law Recent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship. Building upon this, self-consistency approaches demonstrated that sampling multiple reasoning paths and selecting the most consistent answer can substantially improve model accuracy [53]. This was further enhanced by progressive-hint prompting techniques [54], achieving significant gains on various reasoning benchmarks. Recent theoretical work [55] shows that smaller models paired with advanced inference algorithms can outperform larger models under the same computation budget. However, studies on compound inference systems [56] reveal that increasing LLM calls shows non-monotonic behavior, performing better on \"easy\" queries but worse on \"hard\" ones. These findings collectively demonstrate the importance of optimizing inference strategies rather than simply scaling up model size or increasing the sampling budget. Concurrent works [37,38] have also independently explored and validated the effectiveness of inference scaling in diffusion models.",
            "score": 0.7004899592904988,
            "section_title": "A. Full Related Work",
            "char_start_offset": 25352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "268875775"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "268875775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "276574918",
            "title": "Compression Scaling Laws:Unifying Sparsity and Quantization",
            "text": "Our work builds on and connects several lines of research around scaling laws, model compression techniques, and the intersection between them. \n\nScaling Laws for Language Models. The foundation of this work builds on established scaling laws for language models that characterize how performance improves with model size and training data. Kaplan et al. (2020) established the first comprehensive scaling laws showing that loss follows power law relationships with both parameters and data. Hoffmann et al. (2024) refined these results with the Chinchilla scaling laws, suggesting that previous models were over-parameterized and that parameters and data should be scaled roughly equally. Recent work has revealed additional nuances in scaling behavior -for example, when considering data redundancy (Muennighoff et al., 2023), or different model architectures (Clark et al., 2022). \n\nModel Compression and Sparsity. Parallel work has focused on making models more efficient through compression techniques. For sparsity, Frantar et al. (2024) established the first scaling laws characterizing how sparsity interacts with model and data scaling, showing that sparsity acts as a consistent multiplier on effective param-eter count. Their work demonstrated that optimal sparsity levels increase with longer training, as dense models hit diminishing returns. This report directly builds on this earlier work, studying how different representations affect scaling. \n\nQuantization for Language Models. Recent advances in quantization have enabled dramatically reduced precision while maintaining performance. Post-training quantization methods like GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2023) have shown strong results for inference. For quantization-aware training, BitNet (Wang et al., 2023) and follow-up work (Kaushal et al., 2024;Ma et al., 2024) demonstrated stable training with binary and ternary weights, although a precise comparison against dense model scaling is not possible in their setting given the different hyper-parameters used. This work complements these efforts by characterizing how quantization during training affects fundamental scaling behavior -showing for instance that weight-only quantization maintains strong parameter efficiency even at very low bitwidths, for both weights and activations.",
            "score": 0.6995836782128677,
            "section_title": "Related Work",
            "char_start_offset": 11603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 146,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 801,
                    "end": 827,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 862,
                    "end": 882,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 1022,
                    "end": 1043,
                    "matchedPaperCorpusId": "262013578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "On the left, the training curves for models ranging from 70M to 7B parameters are displayed.From these curves, we extract the envelope of minimum loss per FLOP, using these data points to estimate the optimal model size (center) for a specified compute budget, and the optimal number of training tokens (right).\n\nattention transformers.For a comprehensive analysis, we compare the scaling behavior of downstream task performance across different architectures.As outlined in Section 3, each model is evaluated in terms of linguistic proficiency, knowledge retention, and information retrieval and generation.Our findings reveal that linear complexity models exhibit similar scaling trends to conventional transformer-based models and consistently outperform LLaMA in cross-domain perplexity and average accuracy in commonsense reasoning under the same FLOPs budget but demonstrate weakness in retrieval tasks.\n\nOur contributions are summarized as follows:\n\n\u2022 We disclose scaling laws for linear complexity language models, focusing on three different architectures.Reveling the training loss L, model size N and dataset size D have powerlaw relationships with computation budget C. \u2022 Our experiments showcase the advantage of linear complexity language models over traditional transformers on linguistic proficiency while inferior performance in retrieval tasks.\n\n\u2022 We analyze the scaling trends for downstream task performance and observe the correlation of performance with computation budget.\u2022 For linear models, aspect ratio (model dimension / number of layers) and context length affect model capacity.This is contradictory to previous scaling laws, where model shape makes a negligible impact.\u2022 For linear models, data-dependent decay is beneficial in retrieval tasks and is not significantly different from data-independent decay in other tasks.",
            "score": 0.6992084347193858,
            "section_title": "Introduction",
            "char_start_offset": 2781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 92,
                    "end": 311
                },
                {
                    "start": 313,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 909
                },
                {
                    "start": 911,
                    "end": 955
                },
                {
                    "start": 957,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1362
                },
                {
                    "start": 1364,
                    "end": 1495
                },
                {
                    "start": 1495,
                    "end": 1607
                },
                {
                    "start": 1607,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "262152780",
            "title": "Neural scaling of deep chemical models",
            "text": "For large language and computer vision models trained to convergence with sufficient model parameters and/or data, performance is characterized by empirical scaling laws where the loss scales as a power law 18 of the form \n\nfor coefficient \u03b1, scaling exponent \u03b2 and resource R. R is the number of model parameters, dataset size or compute. \u03b2 measures the slope of the power law and indicates the scaling efficiency of the model with respect to a scaling factor, R. The power-law trends break down in 'resolution limited' regimes 34 , indicating that the model (dataset) size is insufficient for the given amount of data (model parameters). \n\nNeural scaling presents a best-case scenario for model pre-training loss improvements with increasing resources, and allows for optimal allocation of fixed budgets, for example, to decide whether longer training, more data or larger models will be most efficient for improving pre-training loss. Comparing neural-scaling exponents also provides a fundamental metric for measuring resource efficiency across model architectures. Investigations into neural scaling in the NLP domain have revealed general conclusions about overfitting, sensitivity to architectural choices, transfer learning and sample efficiency 18 . These factors are equally or more important in scientific deep learning applications, where rapid advances are being made in specialized architecture development, and it is often unclear how architectures will perform beyond the small benchmark datasets that are commonly available in scientific settings.",
            "score": 0.6990109745984516,
            "section_title": "Neural scaling",
            "char_start_offset": 24239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1564
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "We study the relationship between task performance and the amount of compute expended during inference for various model sizes, model families, and inference strategies, to form empirical inference scaling laws. These relationships let us reason about compute-optimal inference: inference configurations that give the best performance at a given compute budget. \n\nOur results lead to three main takeaways. First, we find that using a smaller model and generating more tokens in an inference strategy often outperforms using a larger model at a fixed compute budget. This has implications for models deployed in the real world, where inference compute is constrained in various ways. Specifically, it is potentially beneficial to deploy smaller models with more sophisticated inference strategies for better cost-performance trade-off. Second, we show that in the limit of infinite compute (allocated by drawing more samples), sampling-based majority voting strategies inevitably saturate to a distribution that depends on the underlying generation policy. Hence, it is of interest to alter the sampling distribution by designing an alternative inference strategy. Third, we design such an inference strategy-the novel REBASE tree search-and find it is Pareto optimal, in that it achieves the best performance across all tested compute budgets. Notably, it outperforms commonly used weighted majority voting and MCTS methods that have attracted much interest and widespread use. This finding not only shows the strength of REBASE, but also indicates that there is large headroom to improve language model performances via inference-time algorithms. \n\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning, acting, and planning in language models. In Proceedings of the 41st International Conference on Machine Learning, pp. 62138-62160, 2024.",
            "score": 0.6967606130216627,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 26183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "Scaling laws predict the loss of a target machine learning model by extrapolating from easier-to-train models with fewer parameters or smaller training sets. This provides an efficient way for practitioners and researchers alike to compare pretraining decisions involving optimizers, datasets, and model architectures. Despite the widespread use of scaling laws to model the dynamics of language model training, there has been little work on understanding how to best estimate and interpret them. We collect (and release) a large-scale dataset containing losses and downstream evaluations for 485 previously published pretrained models. We use these to estimate more than 1000 scaling laws, then derive a set of best practices for estimating scaling laws in new model families. We find that fitting scaling laws to intermediate checkpoints of training runs (and not just their final losses) substantially improves accuracy, and that -- all else equal -- estimates of performance are generally most accurate when derived from other models of similar sizes. However, because there is a significant degree of variability across model seeds, training multiple small models is sometimes more useful than training a single large one. Moreover, while different model families differ scaling behavior, they are often similar enough that a target model's behavior can be predicted from a single model with the same architecture, along with scaling parameter estimates derived from other model families.",
            "score": 0.6962850379623172,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "Scaling laws of neural networks (Hestness et al., 2017;Rosenfeld et al., 2020) have been established across a range of domains, including language modeling (Kaplan et al., 2020;Hoffmann et al., 2022;OpenAI, 2023), image modeling (Henighan et al., 2020;Yu et al., 2022;Peebles & Xie, 2023), video modeling (Brooks et al., 2024), reward modeling (Gao et al., 2023), and board games (Jones, 2021). These studies have demonstrated how model performance is influenced by both the size of the model and the amount of training compute. However, there is limited knowledge on how varying the compute during inference affects model performance after the model has been trained. \n\nTo improve the task performance of large language models (LLMs), inference techniques typically involve additional compute as a performance maximization step at inference time (Nye et al., 2021;Wei et al., 2022;Wang et al., 2023b;Yao et al., 2023;Chen et al., 2024b). The computational cost of these techniques must be taken into account for compute-optimal inference. For example, Monte Carlo Tree Search (MCTS) may improve task performance, but it potentially requires much more compute than simply sampling solutions multiple times (Jones, 2021). Generally speaking, we need a comprehensive understanding of how various inference-time methods (e.g., best-of-n, majority voting (Wang et al., 2023a;Li et al., 2023)) trade off between performance and cost. To improve our understanding, this paper presents a thorough empirical evaluation with careful analysis over various configurations of representative LLMs and inference algorithms. \n\nSpecifically, we explore how to select an optimal size for the language model and an effective inference strategy (e.g., greedy search, majority voting, best-of-n, weighted voting, and their treesearch variants) to maximize performance (i.e., accuracy) with a given compute budget. We control the inference compute (FLOPs) of a fixed model by generating more tokens through the language model1 , sampling further candidate solutions, and ranking them with a reward model.",
            "score": 0.6949184040428467,
            "section_title": "INTRODUCTION",
            "char_start_offset": 609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 78,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 252,
                    "end": 268,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 268,
                    "end": 288,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 344,
                    "end": 362,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 865,
                    "end": 882,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 882,
                    "end": 901,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 901,
                    "end": 918,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 918,
                    "end": 937,
                    "matchedPaperCorpusId": "267740392"
                },
                {
                    "start": 1351,
                    "end": 1371,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1371,
                    "end": 1387,
                    "matchedPaperCorpusId": "259370847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "We observe no signs of deviation from straight power-law trends at large values of compute, data, or model size. Our trends must eventually level off, though, since natural language has non-zero entropy. \n\nIndeed, the trends for compute-efficient training described in this section already contain an apparent contradiction. At scales several orders of magnitude above those documented here, the performance predicted by the L(C min ) scaling law decreases below what should be possible given the slow growth in training data with compute. This implies that our scaling laws must break down before this point, but we conjecture that the intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language models reach maximal performance. \n\nSince the amount of data used by compute-efficient training grows slowly with the compute budget, the performance predicted by L(C min ) eventually hits a lower bound set by the L(D) power law (see Figure 15). Let us work this out in more detail. \n\nTo keep overfitting under control, the results of Section 4 imply that we should scale the dataset size as D \u221d N 0.74 \u221d C 0.54 min (6.6) where we have used the compute-efficient N (C min ) from Figure 14. though the numerical values are highly uncertain, varying by an order or magnitude in either direction depending on the precise values of the exponents from the power-law fits. The most obvious interpretation is that our scaling laws break down at or before we reach this point, which is still many orders of magnitude away in both compute and model size. \n\nOne might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model size beyond N * without qualitatively different data requirements, perhaps this means that once we reach C * min and N * , we have extracted all of the reliable information available in natural language data. In this interpretation, L * would provide a rough estimate for the entropy-per-token7 of natural language. In this scenario, we would expect the loss trend to level off at or before L * . \n\nWe can guess at the functional form of L(C min ) as it levels off by considering a version of our training dataset with added noise.",
            "score": 0.6944257492893581,
            "section_title": "Contradictions and a Conjecture",
            "char_start_offset": 33407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1590
                },
                {
                    "start": 1593,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2098
                },
                {
                    "start": 2101,
                    "end": 2233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.",
            "score": 0.6939979566771646,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439208984375
        },
        {
            "corpus_id": "276580891",
            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
            "text": "Scaling behaviors of large neural language models have surprised and fascinated engineers, scientists and society alike (Hestness et al., 2017;Kaplan et al., 2020;Brown et al., 2020a;Hoffmann et al., 2022;Ganguli et al., 2022;Sorscher et al., 2022;Wei et al., 2022b;Schaeffer et al., 2023;OpenAI et al., 2024), shaping engineering, economic  Brown et al. (2024) found the negative log average pass rate \u2212 log(pass D @k) at solving mathematical problems scales polynomially (i.e., as a power law) with the number of independent attempts per problem k. Bottom: Hughes et al. (2024) similarly found the negative log average attack success rate \u2212 log(ASR D @k) when jailbreaking multimodal language models scales polynomially with the number of jailbreak attempts per prompt. Should such power law scaling be expected? From where do large language monkeys obtain their power (laws)? and governmental interests in frontier AI systems (Bommasani et al., 2021;Eloundou et al., 2023;Anderljung et al., 2023;Wang et al., 2023;Reuel et al., 2024;Besiroglu et al., 2024a;Maslej et al., 2024). For a more thorough exposition of relevant literature, please see Related Work (Section 6). \n\nOne direction of renewed interest is inference-time compute scaling, whereby compute is controllably increased at infer- The \u2212 log(pass D @k) scales as a power law with the number of attempts per problem k (left). This arises from a combination of two factors: (1) for each problem, \u2212 log(pass i @k) scales exponentially with k (center), and (2) the distribution (over problems in the dataset) of single-attempt success rates pass i @1 itself has a left power-law tail of small values (right). \n\nence to improve the performance of a model, e.g., Pachocki et al. (2024).",
            "score": 0.6899513601694998,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 183,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 205,
                    "end": 226,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 226,
                    "end": 248,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 266,
                    "end": 289,
                    "matchedPaperCorpusId": "258418299"
                },
                {
                    "start": 999,
                    "end": 1017,
                    "matchedPaperCorpusId": "260384616"
                },
                {
                    "start": 1036,
                    "end": 1060,
                    "matchedPaperCorpusId": "254823341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326171875
        },
        {
            "corpus_id": "278165731",
            "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance",
            "text": "Performance Estimation via Scaling Laws. Early studies on large language models (LLMs) have shown that core metrics like cross-entropy loss exhibit approximate power-law relationships with key design factors such as model size, dataset scale, and overall computational budget (Kaplan et al., 2020;Hoffmann et al., 2022;Hernandez et al., 2022;Gordon et al., 2021). While these classical \"scaling laws\" provide valuable insights into how performance grows with parameters and tokens, their estimates typically assume a single model family or a narrowly defined training setup. Consequently, scaling relationships learned from one architecture (e.g., Transformer-based language models) may not directly generalize to another. \n\nRecent work expands scaling laws beyond a single family. For instance, Ye et al. (2023) investigate whether past performance records from multiple LLM variants can predict new model settings; Owen (2024) show that aggregated benchmark scores (e.g., BIG-Bench) follow smooth scaling trends across diverse model families, yet individual tasks are more difficult to predict. Ruan et al. (2024) explore observational rather than purely experimental scaling, leveraging over a hundred pretrained LLMs to fit parametric curves without training each from scratch. These lines of research challenge the assumption that scaling laws transfer seamlessly across all model families. Indeed, Choshen et al. (2025) highlight large predictive errors when applying scaling trends derived from one family to a different, structurally distinct family. Overall, while scaling laws are fundamental to LLM research, their reliance on homogeneous or near-homogeneous settings can limit predictive accuracy, especially when addressing newly merged or fine-tuned models for which classical scaling metrics (e.g., total FLOPs or training tokens) are not strictly comparable. \n\nPredicting Downstream Task Performance from Observational Data. A growing body of work aims to bypass some limitations of traditional scaling laws by using observational or empirical records from existing models. Rather than modeling only cross-entropy loss, these methods directly estimate how well a new or partially evaluated LLM will perform on downstream benchmarks. One common strategy is to reduce the high cost of comprehensive benchmarking by sampling only a small subset of instances.",
            "score": 0.6899203967707679,
            "section_title": "Related Work",
            "char_start_offset": 4326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1874
                },
                {
                    "start": 1877,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 319,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 342,
                    "end": 362,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 796,
                    "end": 812,
                    "matchedPaperCorpusId": "258865485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65380859375
        },
        {
            "corpus_id": "274762856",
            "title": "Memory Layers at Scale",
            "text": "Language model scaling laws (Kaplan et al., 2020) study the empirical performance of language models as they are scaled in compute, data, and parameter size. Scaling laws are typically formulated in terms of training/test log likelihood, which is generally believed to correlate well with downstream performance. Scaling plots on downstream tasks are also not without precedent (Brown et al., 2020), but have sometimes been shown to exhibit non-linear behaviour and phase transitions (Wei et al., 2022;Ganguli et al., 2022). Nevertheless, given a well behaved metric (such as task likelihood loss), most tasks exhibit smooth improvements with scaling (Schaeffer et al., 2023). Kaplan et al. (2020) showed that performance scales log-linearly with compute and parameter size across a wide range of architecture hyper-parameters, such as model depth and width. It has been difficult to find architectures which substantially deviate from these laws. Mixture-of-experts (MOE) (Shazeer et al., 2017;Lepikhin et al., 2020) is a notable exception. MOE adds extra parameters to the model without increasing the computation budget. While scaling laws for MOE also mostly focus on training perplexity, gains transfer well to downstream applications, as evidenced by the popularity of MOE architectures in recent state-of-the-art model families (Jiang et al., 2024;OpenAI et al., 2024;Team et al., 2024). Nevertheless, scaling laws for specific task families and capabilities like factuality remain understudied. \n\nLike MOE, memory augmented models also aim to augment the parameter space of the model without adding significant computational cost. Memory networks were proposed initially in (Weston et al., 2015), and with end-to-end training in (Sukhbaatar et al., 2015). Neural Turing Machines (Graves et al., 2014(Graves et al., , 2016) ) combine external trainable memory with other components to build a neural trainable computer. Product-key networks (Lample et al., 2019) were introduced to make the memory lookup more efficient and scalable.",
            "score": 0.6888046695987274,
            "section_title": "Related work",
            "char_start_offset": 3364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 502,
                    "end": 523,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 973,
                    "end": 995,
                    "matchedPaperCorpusId": "12462234"
                },
                {
                    "start": 1787,
                    "end": 1807,
                    "matchedPaperCorpusId": "205251479"
                },
                {
                    "start": 1807,
                    "end": 1832,
                    "matchedPaperCorpusId": "205251479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "274280709",
            "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens",
            "text": "Our projection indicates potential challenges for the future application of low-bit quantization. \n\n\u2022 We release all the 1500+ quantized checkpoints used in this work to facilitate future research on this problem. \n\n2 Preliminary: Scaling Laws for Large Language Models \n\nScaling laws for large language models (Kaplan et al., 2020;Hoffmann et al., 2022) are crucial for understanding how these models' performance improves with increased scale, including the number of parameters and training tokens: \n\nNumber of Parameters LLMs' performance typically follows a power-law improvement as the number of parameters increases, allowing larger models to better fit and generalize on the same dataset: \n\nwhere L(N ) is the loss function4 dependent on N (the number of non-embedding parameters), a is a constant (i.e., coefficient), \u03b1 is the scaling exponent, and \u03f5 represents the error term. This relationship indicates larger models are generally more capable of capturing the complexities of language, leading to better generalization and lower loss. \n\nTraining Tokens More training tokens also boost performance in a power-law fashion, enabling models to capture language complexities more effectively: \n\nwhere D denotes the number of training tokens, b is a constant (i.e., coefficient) and \u03b2 is the scaling exponent for training tokens. More training tokens enhance an LLM's ability to learn and generalize, allowing it to achieve better language modeling performance with lower loss. \n\nWhen scaling both the number of parameters N and the amount of training data D simultaneously, the scaling law can be expressed as a function that accounts for the combined effects of both: \n\nThis scaling law allows us to estimate the performance of language models at unprecedented scales of model size and training data effectively before conducting actual training runs. \n\n3 Scaling Laws for Low-bit Quantization \n\nIn this section, we propose scaling laws for low-bit quantization. Unlike the scaling laws discussed in Section 2, the focus here is on understanding how quantization-induced degradation (QiD) changes when low-bit quantization is applied to LLMs of varying training scales.",
            "score": 0.6884542100147188,
            "section_title": "Introduction",
            "char_start_offset": 3807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 213
                },
                {
                    "start": 216,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1676
                },
                {
                    "start": 1679,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "However, these works have not explored the scaling law of parallel computation from the perspective of model capacity. As we discuss in Appendix A, we find that the specific differentiation technique had a minimal impact, and the key factor is the scaling in parallel computation. \n\nScaling Laws for Language Models Many researchers explore the predictable relationships between LLM training performance and various factors under different settings, such as the number of parameters and data (Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022;DeepSeek-AI, 2024;Frantar et al., 2024), data repetition cycles (Muennighoff et al., 2023;Hernandez et al., 2022), data mixing (Ye et al., 2025;Que et al., 2024), and fine-tuning (Zhang et al., 2024a). By extending the predictive empirical scaling laws developed from smaller models to larger models, we can significantly reduce exploration costs. \n\nRecently, some studies have investigated the scaling effects during inference (Sardana et al., 2024), noting a log-linear relationship between sampling number and performance (Brown et al., 2024;Snell et al., 2025). But they are limited to certain application scenarios. Our work extends the Chinchilla scaling law (Hoffmann et al., 2022) by introducing the intrinsic quantitative relationship between parallel scaling and parameter scaling. Existing literature has also identified a power-law relationship between the number of ensembles and loss in model ensemble scaling laws (Lobacheva et al., 2020a), which can be considered a special case of Proposition 1 when \u03c1 = 0.",
            "score": 0.6876753435846442,
            "section_title": "Inference-Time Scaling",
            "char_start_offset": 31191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1580
                }
            ],
            "ref_mentions": [
                {
                    "start": 621,
                    "end": 647,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 684,
                    "end": 701,
                    "matchedPaperCorpusId": "268681464"
                },
                {
                    "start": 736,
                    "end": 757,
                    "matchedPaperCorpusId": "268032247"
                },
                {
                    "start": 985,
                    "end": 1007,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 1102,
                    "end": 1121,
                    "matchedPaperCorpusId": "278498044"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61962890625
        },
        {
            "corpus_id": "268351635",
            "title": "Unraveling the Mystery of Scaling Laws: Part I",
            "text": "Essentially, scaling laws [KMH + 20] reveal how to predict the validation/test loss3 of a given model, which can be the final loss when the model is trained to converge, or the loss at a certain step amid training.Scaling laws have been proven to be widely valid across a variety of likelihood-based tasks [HKK + 20].By adhering to scaling laws, researchers can uncover patterns in how changes in model parameters and training data impact the overall effectiveness of large language models before actually training them.\n\nNotation To enhance the clarity of explanations, we use the following notations throughout the paper, most of which are adapted from [KMH + 20]:\n\n\u2022 L -the cross entropy loss in nats averaged over the tokens in a context\n\n\u2022 N -the number of model parameters, excluding all vocabulary and positional embeddings\n\n\u2022 B -the batch size\n\n\u2022 B crit -the critical batch size defined in [MKAT18].Training at the critical batch size provides a roughly optimal compromise between time and compute efficiency\n\n\u2022 E -amount of processed tokens\n\n\u2022 E min -an estimate of the minimum amount of processed tokens needed to reach a given value of the loss.This is also the number of training steps that would be used if the model were trained at a batch size much smaller than the critical batch size\n\n\u2022 S -number of training steps\n\n\u2022 S min -an estimate of the minimal number of training steps needed to reach a given value of the loss.This is also the number of training steps that would be used if the model were trained at a batch size much greater than the critical batch size\n\n\u2022 N c , S c , B * , \u03b1 c , \u03b1 S , \u03b1 B -constant terms in scaling-law formulas that need to be estimated\n\nGoal The original scaling-law paper is comprehensive, encompassing a wide range of content.To emphasize the key aspects useful for pre-training large language models, this paper concentrates on estimating the following three functions, which serve as foundations of scaling laws.Using these three functions, we are able to accurately predict the training behavior of large language models before the training starts: 3 Deriving Scaling Laws \"Scaling laws are decided by god; The constants are determined by members of the technical staff\"\n\n-Sam Altman",
            "score": 0.6838564765140702,
            "section_title": "Preliminary",
            "char_start_offset": 5519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 520
                },
                {
                    "start": 522,
                    "end": 666
                },
                {
                    "start": 668,
                    "end": 741
                },
                {
                    "start": 743,
                    "end": 830
                },
                {
                    "start": 832,
                    "end": 851
                },
                {
                    "start": 853,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1016
                },
                {
                    "start": 1018,
                    "end": 1049
                },
                {
                    "start": 1051,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1300
                },
                {
                    "start": 1302,
                    "end": 1331
                },
                {
                    "start": 1333,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1580
                },
                {
                    "start": 1582,
                    "end": 1683
                },
                {
                    "start": 1685,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 1964
                },
                {
                    "start": 1964,
                    "end": 2223
                },
                {
                    "start": 2225,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 898,
                    "end": 906,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.623046875
        },
        {
            "corpus_id": "276421795",
            "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
            "text": "Scaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget (Kaplan et al., 2020;Hoffmann et al., 2022;Grattafiori et al., 2024). Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs). FLOPs account for both parameter count and the number of training tokens. While useful for pretraining, scaling laws do not capture how well a model ultimately performs on downstream tasks (Gadre et al., 2024;Schaeffer et al., 2024;Du et al., 2025). Consequently, multiple works have begun to investigate downstream scaling laws: Scaling laws that directly predict downstream loss from FLOPs (Schaeffer et al., 2024;Gadre et al., 2024). \n\nFigure 1. LLMs' loss-to-loss scaling follows power laws primarily shaped by the choice of pretraining data and tokenizer. Using Llama trained on FineWeb-Edu as a baseline, we intervene on various factors to assess their impact on train-to-test loss scaling. Changing the pretraining data has the largest effect, followed by the choice of tokenizer. Switching the architecture, e.g., from Llama to Mamba, has limited impact, while factors like model size, context length, and optimizer settings exert little-to-no influence. Brandfonbrener et al. (2024) show that downstream scaling laws can be decomposed into compute-to-train-loss scaling laws and (train)-loss-to-(test)-loss scaling laws. The combination of compute-to-loss and loss-to-loss scaling laws enables efficient and accurate prediction of a model's downstream performance.",
            "score": 0.6836335026047238,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1637
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76416015625
        },
        {
            "corpus_id": "259341682",
            "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens",
            "text": "An important property of large language models is that the loss scales as a power law with compute. \n\nTo verify whether LONGNET still follows the similar scaling law, we train a series of models with different model sizes, from 125 million to 2.7 billion parameters. The 2.7B model is trained with 300B tokens, while the rest digest about 40B tokens. Figure 7(a) plots the scaling curve of LONGNET regarding the compute. We compute the perplexity on the same test set. for scaling the language models. Additionally, the scalability and the efficiency are both obtained by LONGNET.",
            "score": 0.6834231895479581,
            "section_title": "Scaling up Model Size",
            "char_start_offset": 14644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 580
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407958984375
        },
        {
            "corpus_id": "266818336",
            "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
            "text": "Nonetheless, research on scaling laws is of utmost importance, considering that the current open-source models are merely at the initial stage of Artificial General Intelligence (AGI) development. In addition, early works (Hoffmann et al., 2022;Kaplan et al., 2020) reached varying conclusions on the scaling of model and data with increased compute budgets and inadequately addressed hyperparameter discussions. In this paper, we extensively investigate the scaling behavior of language models and apply our findings in two widely used large-scale model configurations, namely 7B and 67B. Our study aims to lay the groundwork for future scaling of open-source LLMs, paving the way for further advancements in this domain. Specifically, we first examined the scaling laws of batch size and learning rate, and found their trends with model size. Building on this, we conducted a comprehensive study of the scaling laws of the data and model scale, successfully revealing the optimal model/data scaling-up allocation strategy and predicting the expected performance of our large-scale models. Additionally, during development, we discovered that the scaling laws derived from different datasets show significant differences. This suggests that choice of dataset remarkably affects the scaling behavior, indicating that caution should be exercised when generalizing scaling laws across datasets. \n\nUnder the guidance of our scaling laws, we build from scratch open-source large language models, and release as much information as possible for community reference. We collect 2 trillion tokens for pre-training, primarily in Chinese and English. At the model level, we generally followed the architecture of LLaMA, but replaced the cosine learning rate scheduler with a multi-step learning rate scheduler, maintaining performance while facilitating continual training. We collected over 1 million instances for supervised fine-tuning (SFT) (Ouyang et al., 2022) from diverse sources. This paper shares our experiences with different SFT strategies and findings in data ablation techniques. Additionally, we have utilized direct preference optimization (DPO) (Rafailov et al., 2023) to improve the conversational performance of the model. \n\nWe conduct extensive evaluations using our base and chat models. The evaluation results demonstrate that DeepSeek LLM surpasses LLaMA-2 70B across various benchmarks, particularly in the fields of code, mathematics, and reasoning.",
            "score": 0.683015602350553,
            "section_title": "Introduction",
            "char_start_offset": 1814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2233
                },
                {
                    "start": 2236,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2466
                }
            ],
            "ref_mentions": [
                {
                    "start": 1936,
                    "end": 1957,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32470703125
        },
        {
            "corpus_id": "268358466",
            "title": "Algorithmic progress in language models",
            "text": "The field of language modeling has seen rapid advances, with recent large language models (LLMs) demonstrating strong performance in domains such as programming (Y.Li et al. 2022;Leblond et al. 2023), mathematics (Cobbe et al. 2021;Trinh et al. 2024), and a wide range of standardized tests (OpenAI 2023).These capabilities have enabled LLMs to support a range of commercial and scientific applications (Kaddour et al. 2023).\n\nA key driver of this progress has been algorithmic improvements, which result in more efficient use of resources such as compute and training data.These include changes in model architectures, optimization algorithms, and software frameworks.Many surveys of progress in language modeling describe specific innovations in detail, such as the transformer architecture, layer normalization, IO-aware exact attention algorithms such as FlashAttention, positional embeddings such as RoPE, and innovations in the attention mechanism such as multi-query attention (Zhao et al. 2023;Jing and Xu 2019;Sun, X. Luo, and M. Y. Luo 2022;Huang and Chang 2022;Mialon et al. 2023;Shazeer 2019).In addition, data quality improvements, such as training on high-quality textbook examples (Gunasekar et al. 2023) and data pruning (Sorscher et al. 2022;Marion et al. 2023), can enable LLMs to be trained on substantially smaller datasets.\n\nThe rapid scaling of compute for training language models (Sevilla et al. 2022), coupled with insights from scaling laws (Hoffmann et al. 2022;Kaplan et al. 2020), suggests that a substantial portion of the improvement in language model capabilities can be attributed to the increased use of computational resources.The key question we wish to answer is thus: How much of recent progress in language models has come from algorithmic improvements during pre-training, and how much has been from scaling up models and datasets?Related questions have been investigated in other domains of scientific computing, such as linear programming, SAT solvers, and computer chess, among others (see Figure 1).",
            "score": 0.6811898763952711,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 425
                },
                {
                    "start": 427,
                    "end": 574
                },
                {
                    "start": 574,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1344
                },
                {
                    "start": 1346,
                    "end": 1662
                },
                {
                    "start": 1662,
                    "end": 1871
                },
                {
                    "start": 1871,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 250,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 1237,
                    "end": 1259,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1489,
                    "end": 1508,
                    "matchedPaperCorpusId": "210861095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4140625
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "As a result, optimizing this metric is often a key focus in the development of more efficient LLMs. Recent research in predicting large language model (LLM) performance has concentrated on understanding the scaling law [126]. This law delineates how LLM performance is influenced by factors such as model architecture, neural model size, computing power for training, and available data. The concept of scaling law, rooted in statistical mechanics approaches for predicting model generalization, has a rich history dating back to the early 1990s [10,17,96,240]. Its relevance has been reinvigorated recently in the context of modern deep learning models [9, 25, 102-104, 107, 126, 192, 226, 254, 266, 268]. This section will delve into the latest advancements and insights in the scaling law as applied to LLMs, highlighting how these models evolve and perform under varying conditions.",
            "score": 0.6811368115274135,
            "section_title": "Number of Parameters.",
            "char_start_offset": 14161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 886
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "17577797"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "28307549"
                },
                {
                    "start": 556,
                    "end": 560,
                    "matchedPaperCorpusId": "7394722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "267547740",
            "title": "A Resource Model For Neural Scaling Law",
            "text": "This conjecture might be equivalent to the optimal allocation of neurons subject to the finite model capacity, specifically the number of neurons in each layer. Combining the single task scaling law and the homogeneous growth conjecture, we can derive (and indeed observe) that the scaling law of the overall composite task can emerge, i.e., the total loss \u2113 total is inversely proportional to the total number of neurons N total , i.e., \u2113 total \u221d N \u22121 total . \u2022 The implication for neural scaling laws in large language models (Section 4): For a finitewidth neural network trained for complex tasks which contains many subtasks, the optimal loss scales with the total resource is \u2113 \u221d N \u22121 . Under a few reasonable assumptions we have N p \u221d N 3 , so \u2113 \u221d N \u22121/3 p (N p is the number of model parameters), agreeing with the compute-optimal scaling law of Chinchilla models where they observed \u2113 \u221d N \u22120.34 p (Hoffmann et al., 2022).",
            "score": 0.677388249369016,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 929
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1312255859375
        },
        {
            "corpus_id": "258486871",
            "title": "GPT-2 Metadata Pretraining Towards Instruction Finetuning for Ukrainian",
            "text": "The power laws suggest that the returns from increasing model, data, or compute diminish after a certain point. Caballero et al. (2022) present a smoothly broken neural scaling law equation, suggesting a scaling speedup laying further ahead past the currently accepted inflection region. Sorscher et al. (2022) suggest a way to beat scaling laws by using careful data selection methods on vision tasks. These ideas give us the insight that we should use the biggest models possible for our compute budget. \n\nIt's not only compute that's important. While the work of Radford et al. (2019) discovered prompts that drove the model to perform tasks like sum-marization, Schick and Sch\u00fctze (2021) introduce pattern-exploiting training that reformulates sentences into cloze tasks on purpose. It is beneficial to curate examples of natural language instructions to save compute. \n\nInstruction finetuning datasets, such as The Flan Collection, released by Longpre et al. (2023), curate massive amounts of task-specific datasets and provide a pipeline to reformulate tasks into natural language using seqio introduced in Roberts et al. (2022). Flan T5 demonstrates that you can achieve higher performance on multiple NLP tasks at once with smaller models in 1.5B-11B range using such data curation methods. These ideas inspire us to leverage metadata and attempt to formulate NLP tasks using natural language. \n\nTechniques like sequence length warmup (Li et al., 2022), gradient clipping (Graves, 2013) enable training stability. Dettmers et al. (2022) enable memory savings by quantizing gradient statistics. Katharopoulos et al. (2020) explore a recurrent formulation of attention with lower computational complexity, and Schlag et al. (2021) view it as fast weight programmers improving capacity of attention in the recurrent setting. Tillet et al. (2019) provide a programming language to implement high performing kernels quickly. Dao et al. (2022) demonstrate how to significantly speed up computation of self-attention and allow much larger context sizes than 1024 or 2048 tokens.",
            "score": 0.6765657131576476,
            "section_title": "Introduction",
            "char_start_offset": 2054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 135,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 288,
                    "end": 310,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1113,
                    "end": 1134,
                    "matchedPaperCorpusId": "247942109"
                },
                {
                    "start": 1443,
                    "end": 1460,
                    "matchedPaperCorpusId": "252917575"
                },
                {
                    "start": 1522,
                    "end": 1544,
                    "matchedPaperCorpusId": "238408308"
                },
                {
                    "start": 1602,
                    "end": 1629,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1716,
                    "end": 1736,
                    "matchedPaperCorpusId": "235377069"
                },
                {
                    "start": 1830,
                    "end": 1850,
                    "matchedPaperCorpusId": "252907428"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1279296875
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "Recent work on compute-optimal language models [42] shows that many previously trained large language models (LLMs, which we define as having more than one billion parameters) could have attained better performance for a given compute budget by training a smaller model on more data. Notably, the 70-billion parameter Chinchilla model [42] outperforms the 280-billion parameter Gopher model [89] while using a similar compute budget by being trained on four times more data. Extrapolating these laws for compute allocation (hereafter \"Chinchilla scaling laws\") to a 530 billion parameter model, such as the under-trained MT-NLG model [99], would require training on a massive 11 trillion tokens, corresponding to more than 30 terabytes of text data. For most languages, available data is several orders of magnitude smaller, meaning that LLMs in those languages are already data-constrained. Villalobos et al. [112] estimate that even high-quality English language data will be exhausted by the year 2024 given the Chinchilla scaling laws and the trend of training ever-larger models. This motivates the question [112,81]: what should we do when we run out of data? \n\nIn this work we investigate scaling large language models in a data-constrained regime, and whether training an LLM with multiple epochs of repeated data impacts scaling. Using multiple epochs is, of course, standard in machine learning generally; however, most prior large language models have been trained for a single epoch [51,15] and some work explicitly advocates against reusing data [40]. An exception is the recent Galactica models [108] that were trained for 4.25 epochs and exhibit continually decreasing validation loss and improving downstream performance throughout training. However, the experiments of Galactica do not compare this setup to an alternative non-dataconstrained model trained for one epoch on unique data. Without this comparison, it is difficult to quantify the trade-off between additional compute versus additional data collection. \n\nOur main focus is to quantify the impact of multiple epochs in LLM training such that practitioners can decide how to allocate compute when scaling models. Toward this end, we assembled a battery of empirical training runs of varying data and compute constraints.",
            "score": 0.6756565744086038,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 1499,
                    "end": 1502,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390869140625
        },
        {
            "corpus_id": "268358466",
            "title": "Algorithmic progress in language models",
            "text": "Using a dataset of over 200 language model evaluations spanning 2012-2023 evaluated on Wikitext and Penn Treebank, we find that the compute required to reach a fixed performance threshold has halved approximately every 8 months.This is much faster than the rate associated with Moore's law and many other domains of computing.While algorithmic innovations have occurred rapidly, compute scaling has expanded by over a million-fold in this same period, exceeding the gains from algorithms and constituting the predominant source of performance improvements in recent years.\n\nOverall, our work provides a quantitative estimate of the rapid pace of progress in language modeling.It also reveals the dominant role of scale rather than algorithms for recent gains.Future work could benefit from extending this analysis to additional, specific benchmarks and more closely examining the impact of data quality improvements and the gains from additional specific innovations.Despite its limitations, this research demonstrates the valuable insights that can be gained from a detailed statistical analysis of extensive datasets of machine learning results.By identifying the main drivers of performance improvements, this work lays the groundwork for further exploration and understanding of these trends in the field.",
            "score": 0.6685928380326073,
            "section_title": "Conclusion",
            "char_start_offset": 36806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 572
                },
                {
                    "start": 574,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 759
                },
                {
                    "start": 759,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "272593336",
            "title": "Scaling Law Hypothesis for Multimodal Model",
            "text": "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance. The initial scaling laws proposed by OpenAI [1] suggested that larger models are more sample-efficient, leading to the creation of massive models like GPT-3. However, subsequent research from DeepMind, notably the Chinchilla study [2], revealed that many large models were undertrained. Their findings indicated that smaller models trained on more data could outperform larger models when the compute budget is held constant. Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8]. \n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]. This approach introduces bits per character (BPC) as a performance metric that reflects the model's compression efficiency [9]. BPC has been shown to correlate linearly with model performance across various modalities (Figure 1). This perspective reveals a linear relationship between BPC and the logarithm of compute used, which can be formalized as: \n\nwhere N is the number of training tokens, and P is the number of model parameters (Figure 2). \n\nThis unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant. In multimodal systems, diverse types of data, such as text, audio, images and video, are processed through specialized tokenization techniques, each with varying levels of compression efficiency. \n\nText is tokenized using methods like Byte Pair Encoding (BPE) [10], which offers relatively stable compression efficiency.",
            "score": 0.6682560799268944,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2179
                },
                {
                    "start": 2182,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "277780691",
            "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
            "text": "There is a body of literature that explicitly separates or conceptually distinguishes syntax and knowledge models within language models. Here we only mention a few representive ones. (Dyer et al., 2016) proposed a RNN-based model that learn syntactic structures (in the form of parse trees) alongside the generation of words. They did not explicitly incorporate a separate knowledge model. (Kusner et al., 2017) proposed Grammar Variational Autoencoder combining variational autoencoders (VAEs) with formal grammars to generate syntactically valid structured data, explicitly separates the syntactic and semantic elements. (Konstas et al., 2017) proposed the neural Abstract Meaning Representation model that, in some pipelines, first generates a syntactic skeleton, then integrates semantic content from AMR. \n\nScaling Laws: The study of neural scaling laws began with the observation that the population loss of trained deep neural networks follows a power-law relationship with respect to dataset size and model parameters. Early work by Rosenfeld et al. (2019) introduced a joint error function that captured these dependencies, laying the groundwork for empirical analyses. Henighan et al. (Henighan et al., 2020) subsequently expanded scaling laws to a broader range of architectures and tasks, while Kaplan et al. (Kaplan et al., 2020) demonstrated their robustness at vastly larger scales, showing that the loss scales as L \u221d (N/N min ) \u2212\u03b1 N (D/D min ) \u2212\u03b1 D where N is the number of parameters, D is the dataset size, and \u03b1 N , \u03b1 D are scaling exponents. The Chinchilla study (Hoffmann et al., 2022) later refined this framework by fitting their scaling law, and then identifying the compute-optimal frontier, showing that many prior models were undertrained, and demonstrating that scaling both parameters and data in tandem yields superior performance under fixed compute budgets. On the theoretical front, Bahri et al. (Bahri et al., 2024) distinguished between variance-limited and resolution-limited regimes, identifying four distinct scaling behaviors. Sharma et al. (Sharma & Kaplan, 2020) then linked scaling exponents to the intrinsic dimension of data manifolds, highlighting the role of data geometry in performance.",
            "score": 0.6674975041307363,
            "section_title": "Syntax-Knowledge Modeling of Language:",
            "char_start_offset": 35166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "1949831"
                },
                {
                    "start": 391,
                    "end": 412,
                    "matchedPaperCorpusId": "7648414"
                },
                {
                    "start": 624,
                    "end": 646,
                    "matchedPaperCorpusId": "8066499"
                },
                {
                    "start": 1931,
                    "end": 1951,
                    "matchedPaperCorpusId": "231918701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "270703043",
            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
            "text": "Chinchilla's scaling law [10], one of these recommendations, states that the optimal model size and the number of tokens for training a language model should scale equally for compute-optimal training under a given computational budget. In [10], it is further proved that current large language models are significantly undertrained due to the recent focus on scaling language models while keeping the amount of training data constant. A smaller model trained with more high-quality data can thus achieve better performance than its larger counterparts with the same computing budget.",
            "score": 0.6670285097818413,
            "section_title": "Large language models",
            "char_start_offset": 13650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 584
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 240,
                    "end": 244,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52001953125
        },
        {
            "corpus_id": "272144447",
            "title": "A long-context RNA foundation model for predicting transcriptome architecture",
            "text": "In our study, we conducted a comprehensive scaling law analysis to compare the performance of various model architectures under a compute-optimized protocol. The primary objective was to determine the optimal balance between model size and dataset size to maximize the efficiency of computational resources. Our experiments explored model sizes ranging from 0.5 million to 40 million parameters, maintaining a fixed context length of 65,536 tokens. To ensure the reliability of our results, we gradually reduced the learning rate and repeated the experiments until convergence was achieved. Model performance was evaluated using the perplexity metric, which measures the model's accuracy in predicting the next token in a sequence-a lower perplexity indicates better performance. \n\nTo carry out the scaling law analysis, we defined a series of compute budgets ranging from to FLOPs. For each budget, we calculated the FLOPs required to process a fixed input size, representing the \"cost\" of running the model. We then identified the compute-optimal allocation by selecting a range of model sizes and determining the number of training tokens needed to fully utilize the compute budget. To pinpoint the optimal compute allocation, we fitted a second-order polynomial to the relationship between log model size and perplexity. The minimum point of this function indicated the compute-optimal point, which signifies the most efficient allocation of model size and training tokens for a given compute budget. This methodical approach enabled us to predict performance trends as model size, dataset size, and compute resources scale, offering valuable insights into the relative efficiencies of different architectures. For further details, please refer to (7,31). \n\nTo further evaluate the performance of LoRNA SH , after identifying the optimal architecture-comprising 13 Hyena layers interleaved with 3 attention layers (totaling 16 layers)-we compared it against two other configurations: an All-Hyena model and an All-Attention model. The All-Hyena model replaces the attention layers with Hyena layers, resulting in 16 Hyena layers similar to the HyenaDNA architecture (6). Conversely, the All-Attention model replaces the Hyena layers with attention layers, resulting in 16 attention layers akin to the GPT architecture (32).",
            "score": 0.6668138617972319,
            "section_title": "Scaling Laws and Comparison with All-Hyena and All-Attention Architectures",
            "char_start_offset": 41212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 1752,
                    "end": 1755,
                    "matchedPaperCorpusId": "268064306"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "268358466",
            "title": "Algorithmic progress in language models",
            "text": "This is not a question that our model equips us to answer.It is therefore important to note that our measured reductions in compute requirements pertain to both algorithmic improvements and data quality improvements, the relative contributions of which could be a subject of future research.\n\n\u2022 Reliance on the Chinchilla scaling law.The scaling law from which our model is derived applies to dense transformers following a GPT-3 architecture (Hoffmann et al. 2022;Rae et al. 2021).However, we use this scaling law to model algorithmic improvements in different transformer architectures, recurrent neural networks, etc. Future algorithms might also follow different scaling laws (e.g.GPT-4 is rumored to be a mixture of experts).However, we believe it is likely that our core results should still hold: for one, neural scaling is not a phenomenon restricted to transformers (e.g. it is known to happen in RNNs as well, see Kaplan et al. 2020).We find that a wide range of statistical model structures provide consistent estimates, and that alternative methods of estimating pre-training algorithmic progress also give similar results (see e.g.appendix A), so it is probable that our core results are robust to the use of the scaling law from Hoffmann et al. 2022.\n\n\u2022 Limited insight about future progress.While the results from this paper could be used to inform one about future progress in language modeling, our paper focuses on historical improvements.Future rates of progress could be slower (e.g. if one thinks that historical progress consisted of picking \"low hanging-fruit\"), but they could potentially also be faster (e.g.due to increased research interest and investment).Expectations about future progress need to account for factors such as these, which we do not discuss in depth for the most part.",
            "score": 0.666722322530227,
            "section_title": "Limitations",
            "char_start_offset": 34979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 58,
                    "end": 291
                },
                {
                    "start": 293,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 685
                },
                {
                    "start": 685,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1264
                },
                {
                    "start": 1266,
                    "end": 1306
                },
                {
                    "start": 1306,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1684
                },
                {
                    "start": 1684,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 924,
                    "end": 943,
                    "matchedPaperCorpusId": "210861095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "269033049",
            "title": "Scaling Laws for Data Filtering\u2014Data Curation Cannot be Compute Agnostic",
            "text": "Some other works include Yu et al. (2023) which uses a mixture of rules and Xu et al. (2023) which uses similarity with downstream metadata.\n\nIn this work, we highlight why data filtering cannot be agnostic to training compute and how the ordering varies as one changes the training paradigm.In fact, we showcase LAION filtering (used to train state-of-the-art OpenCLIP models ) can even underperform no-filtering or training on the raw common crawl under certain settings.\n\nScaling Laws in Language Modeling One of the most salient trends in recent deep learning research is the observation that neural network performance tends to improve predictably with increases in model size, data size, and computation.In the domain of language modeling, such observations have been systematized into a set of principles known as scaling laws.Kaplan et al. (2020) conducted a comprehensive study on scaling laws for neural language models.They observed that, given fixed computational budgets, there exists an optimal model size, training data size, and training time.Interestingly, the triple (model size, data size, batch size) corresponding to the state of the art tends to scale in lockstep, reinforcing the intuition that larger models require more data and more computation to be trained effectively.This observation is corroborated by Hernandez et al. (2021);Hoffmann et al. (2022) who delve deeper into training compute-optimal language models and highlight the importance of balancing computation with model and data sizes.Sardana and Frankle (2023) propose modifications to incorporate the inference cost into the scaling laws.Bahri et al. (2021); Hutter (2021) theoretically study neural scaling laws.\n\nMost closely related to our work, Muennighoff et al. (2023) show that training on tokens beyond four epochs yields negligible gains compared to training on new language data due to diminishing utility.However, they do not consider the case of different data quality pools.In this work, we how that mixture of data pools cannot be modeled with an effective dataset size formulation of Muennighoff et al. (2023).Crucially, one needs to model a decay in utility factor (the scaling parameter b in y = an b ) as well.",
            "score": 0.6666662772214149,
            "section_title": "Related Work",
            "char_start_offset": 8767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 142,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 473
                },
                {
                    "start": 475,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 834
                },
                {
                    "start": 834,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1059
                },
                {
                    "start": 1059,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1628
                },
                {
                    "start": 1628,
                    "end": 1703
                },
                {
                    "start": 1705,
                    "end": 1906
                },
                {
                    "start": 1906,
                    "end": 1977
                },
                {
                    "start": 1977,
                    "end": 2115
                },
                {
                    "start": 2115,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "The concept of scaling laws involves four key factors: loss L, specifically the cross-entropy loss in a causal language modeling setting; model size N , which is determined by the number of model parameters; dataset size D, calculated as the number of training tokens; and computation budget C, represented by the total FLOPs used for training.N opt and D opt are the optimal model size and dataset size given a fixed computation budget.\n\nInitially, we establish power law equations between L and C. In this analysis, we adopt the approach introduced by (Hoffmann et al., 2022), treating the training loss as an unbiased estimate of the test loss.Subsequently, based on the fitted curve, we ascertain the optimal loss for specific FLOPs, enabling us to obtain coefficients for N opt \u221d C a and D opt \u221d C b .When modeling the scaling trend of loss against factors such as N , D, and C, the original scaling laws (Kaplan et al., 2020) utilize the power function L(X) = (X 0 /X) \u03b1 X , where X represents the factor of interest.Subsequent studies (Henighan et al., 2020;Clark et al., 2022;Hoffmann et al., 2022;Gao et al., 2024) employ a more general power-law plus constant form, L(X) = \u03f5 + (X 0 /X) \u03b1 X , to achieve improved fitting.Here, the constant \u03f5 is interpreted as irreducible loss or the entropy of natural text (Hoffmann et al., 2022).In our case, we have simplified all forms of the power law and unified them into L(X) = \u03b2 X X \u03b1 X , which allows for a more intuitive comparison of the scaling capabilities of different models based on coefficients \u03b1 X and \u03b2 X .",
            "score": 0.6660798285412085,
            "section_title": "Scaling laws",
            "char_start_offset": 13844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 437
                },
                {
                    "start": 439,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Understanding the scaling laws of neural models is crucial for optimizing performance across different domains. We predominantly explore the scaling principles for language models, extending to other modalities such as vision and multimodal learning. We also examine scaling behaviors in domain adaptation, inference, efficient model architectures, and data utilization. We highlight the taxonomy tree of scaling laws research in Figure 2. As highlighted in Figure 1, neural scaling laws have been proposed predominantly for pre-training and fine-tuning scaling of large neural models. Among the models studied, as highlighted in Figure 3a, decoder-only Transformers dominate the subject, followed by vision transformers (ViT) and Mixture-of-Experts (MoE). \n\nThe most common neural scaling laws take the form of power laws (Equation 1), where the model's loss (L) or performance metric assumes to follow a predictable relationship with different scaling variables, \n\nwith appropriate scaling parameters \u03b2 i and fitting parameters \u03b1 i for different scaling parameter P i . Figure 3b highlights that the number of model parameters and data size are the most common used scaling factors. The exact forms of all the scaling laws are highlighted in Table 7 of Appendix B. Among all the tasks, Figure 3c suggests that language generation is the most common task used for developing these scaling laws, where the training cross-entropy loss is widely used to fit the laws. Based on the values obtained empirically, the scaling laws are fitted with non-linear optimization, most commonly by running algorithms like least square and BFGS (Broyden-Fletcher-Goldfarb-Shanno). Statistical methods like goodness-of-fit metrics are used to validate the correctness of the fitted curves. We elaborate on the evaluation of neural scaling laws in Appendix A. \n\nIn the following sections, we review the existing literature on neural scaling across various domains. Kaplan et al. (2020) suggested that larger LMs improve performance by reducing loss through power-law scaling. However, this view evolved when studies showed that many large models were undertrained, and data scaling plays an equally crucial role in compute efficiency (Hoffmann et al., 2022). More recent breakthroughs challenged traditional scaling assumptions.",
            "score": 0.6658041940491299,
            "section_title": "Taxonomy of neural scaling laws",
            "char_start_offset": 3460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2310
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "263834967",
            "title": "Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining1",
            "text": "Within AI/ML applications, Nature Language Processing (NLP) is used to parse vast amounts of literature in all languages and also enable computer-aided translation between the different languages. These AI/ML methods depend on training on a large corpus, namely significant amounts of data using words, phrases, part-of speech requirements, existing collections of text from academic journals, books, social network websites, Wikipedia, and Common Crawl (crawling the web). Although not all of these are curated web sites, applications of current NLP methods for reproducing texts have achieved reasonable accuracies [13]. \n\nTo estimate the training cost for these NLP algorithms, the energies based on the number of floating-point operations are used as in the precursor work [3]. Other analysis has assessed that the number of floating-point operations needed for largescale models should scale with the number of tokens (a metric used for the training effort in these models) [14][15][16]. The initial training estimates for the number of floating-point operations range from 6 x 10 18 to 3 x 10 24 , exceeding conservative estimates from the previous work [3]. As before, assuming the energy per instruction (EPI) to range from 1 x 10 -12 Joule/FPI (FP16) (lower bound) to 1 x 10 -11 (FP64) (upper bound) representing two cases, the energy required varies from 6 million joules to 30 trillion joules. Clearly, these highlight the large energy budgets required for training, consistent with published findings on the large energy costs for training these algorithms [17]. Although the energy cost of inference is spread over its usage, the time for inference is longer than the time for training. Retraining a large model is less compute-intensive, but given the power law nature of large language models [18] and new words and colloquial language regularly entering the corpus, the retraining can also become computationally energyintensive over time. \n\nFor inference, the energy estimates are based on the usage patterns of Large Language Models [19][20]. The bounds are calculated assuming ten million queries/day, each query limited to five hundred words.",
            "score": 0.6648227468872534,
            "section_title": "A. AI/ML for Natural Language Processing",
            "char_start_offset": 9534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1955
                },
                {
                    "start": 1958,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 777,
                    "end": 780,
                    "matchedPaperCorpusId": "253237070"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1160,
                    "end": 1163,
                    "matchedPaperCorpusId": "253237070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "253237527",
            "title": "A Solvable Model of Neural Scaling Laws",
            "text": "Large language models with a huge number of parameters, when trained on near internet-sized number of tokens, have been empirically shown to obey neural scaling laws: specifically, their performance behaves predictably as a power law in either parameters or dataset size until bottlenecked by the other resource. To understand this better, we first identify the necessary properties allowing such scaling laws to arise and then propose a statistical model \u2013 a joint generative data model and random feature model \u2013 that captures this neural scaling phenomenology. By solving this model in the dual limit of large training set size and large number of parameters, we gain insight into (i) the statistical structure of datasets and tasks that lead to scaling laws, (ii) the way nonlinear feature maps, such as those provided by neural networks, enable scaling laws when trained on these datasets, (iii) the optimality of the equiparameterization scaling of training sets and parameters, and (iv) whether such scaling laws can break down and how they behave when they do. Key findings are the manner in which the power laws that occur in the statistics of natural datasets are extended by nonlinear random feature maps and then translated into power-law scalings of the test loss and how the finite extent of the data\u2019s spectral power law causes the model\u2019s performance to plateau. ? Equal contribution. ar X iv :2 21 0. 16 85 9v 1 [ cs .L G ] 3 0 O ct 2 02 2",
            "score": 0.6640308995858237,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28369140625
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "Our first experimental setting considers scaling in a setting where all models have the same data constraint. For these experiments, the unique training data budget D C is fixed at either 100M, 400M or 1.5B tokens. For each data budget, we train a set of language models with increasing amounts of compute that is allocated to either more parameters or more epochs on the unique training data. \n\nFigure 3 (left) shows the main results for scaling with 100M unique tokens2 (see Appendix C for 400M and 1.5B tokens). For 100M tokens, the corresponding one-epoch compute-optimal model  according to scaling laws from [42] has U N of approximately 7M parameters (see Appendix B for the scaling coefficients we use). Results show that more than a 50% reduction in loss can be attained by training for several epochs (R D > 0) and increasing model size beyond what would be compute-optimal for 100M tokens (R N > 0). We find the best loss to be at around 20-60\u00d7 more parameters and epochs, which corresponds to spending around 7000\u00d7 more FLOPs. These results suggest that one-epoch models significantly under-utilize their training data and more signal can be extracted by repeating data and adding parameters at the cost of sub-optimal compute utilization. \n\nFigure 3 (right) shows the predicted contours created by fitting our data-constrained scaling laws on 182 training runs. In the single-epoch case (R D = 0) with near compute-optimal parameters (R N = 0) our scaling equation ( \u00a73.1) reduces to the Chinchilla equation. In this case, both formulas predict the optimal allocation of compute to parameters and data to be the same, resulting in overlapping efficient frontiers. As data is repeated for more than a single epoch, our fit predicts that excess parameters decay faster in value than repeated data (R * N < R * D ). As a result, the dataconstrained efficient frontier suggests allocating most additional compute to more epochs rather than more parameters. This contrasts the Chinchilla scaling laws [42], which suggest equally scaling both. However, note that they do not repeat the entire training data and their parametric fit explicitly relies on the assumption that models are trained for a single epoch only.",
            "score": 0.6634271086638064,
            "section_title": "Results: Resource Allocation for Data-Constrained Scaling",
            "char_start_offset": 11698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59326171875
        },
        {
            "corpus_id": "258832459",
            "title": "RWKV: Reinventing RNNs for the Transformer Era",
            "text": "Scaling laws (Kaplan et al., 2020;Henighan et al., 2020;Hoffmann et al., 2022;Muennighoff et al., 2023) in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors. These factors can include the model size (N ), dataset size (D), or the optimally allocated compute budget (C min ). Scaling laws are important for two primary reasons: they allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation (Black et al., 2022;Le Scao et al., 2022) and the contexts in which they fail provides rich feedback on important areas for future research (Wei et al., 2022a;Biderman et al., 2023a). \n\nPrevious work on scaling laws for RNNs has claimed that LSTMs do not strictly follow the same log-log linear scaling that transformers do (Kaplan et al., 2020). We train 45 RWKV models for a variety of pairs (dataset, parameters) and find that RWKV does follow the same general form of the scaling law that is well established for transformers. Figure 4 shows our results for loss as a function of compute, with the linear fit to the Pareto optimal points holding an r 2 value of 0.994. Even when we extrapolate our curve an additional order of magnitude (blue), we find an extremely good fit with an r 2 of 0.875.",
            "score": 0.6630916701936683,
            "section_title": "Scaling Laws",
            "char_start_offset": 13013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6923828125
        },
        {
            "corpus_id": "273653892",
            "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
            "text": "In general, a scaling law includes four parameters, which also characterize a language model: (1) Parameters count N . The number of parameters of an LLM is often associated with the number of transformer layers and the hidden size, except for some MoE LLMs. (2) Data size D. In LLM, this refers to the number of tokens for training. (3) Computation cost C. This is typically measured in terms of time and computational resources. (4) Loss L. The performance of training is usually evaluated by the training loss. There are two representative scaling laws for transformer LLMs. The Kaplan scaling law Proposed by Kaplan [48], the law examines the statistical relations between the parameters C, N, D and L over a wide range of values, models and data tokens. The relationships can be expressed through the following equations: \n\nLpDq \" \n\nLpCq \" \n\nwhere the loss L is influenced by parameters N , D, and C, shedding light on decision-making processes when computational resources are limited. The Chinchilla scaling law Proposed by DeepMind [49], the law provides guidelines for compute-optimal training of LLMs, specifically when computational resources are limited. Through rigorous experiments spanning a wide range of model sizes from 70M to 16B and dataset sizes from 5B to 500B tokens, they derived a scaling law with different coefficients compared to Kaplan's, as shown below: \n\nwhere E denotes the loss of an ideal generative process on the test data. Furthermore, claimed by the research, the constants in this formula are \u03b1 \" 0.34, \u03b2 \" 0.28, A \" 406.4, B \" 410.7, L 0 \" 1.69. Moreover, there is a general constraint that model the relationship between C and pN, Dq: C \" 6N D, which means that it costs six FLOPs per parameter to train one token. Thus, the optimal selection of model size and data size can be determined and expressed as: \n\nFrom the equations, scaling laws can guide decisions regarding model size. Given a fixed compute budget (e.g., 100K GPU hours), they enable predictions on whether a smaller model trained for a longer duration or a larger model trained for a shorter time would yield better performance.",
            "score": 0.6630220979426722,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 13627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "257833842",
            "title": "BloombergGPT: A Large Language Model for Finance",
            "text": "is low. To provide externally comparable results, we developed a few-shot strategy for FLUE, but also decided to augment the publicly available evaluation tasks with company-internal benchmarks.\n\nModel Size. Large language model training remains expensive in terms of the computational cost and human effort to assemble data and train the model. Determining the optimal amount of training data and model shape and size for the best utilization of resources becomes important. Kaplan et al. (2020) first studied the dependence of language model performance on architecture, parameter size, compute power, and dataset size. They reported that the number of model parameters, the dataset size, and the amount of compute improves performance on the autoregressive language modeling objective smoothly according to the power law. A similar investigation by Hernandez et al. (2021) into data transfer for differing distributions found that this also follows a power law. Moving beyond studying the effect on loss, Rae et al. (2021) analyzed the effect of scale on undesirable properties such as bias and toxicity by training a wide range of model sizes.\n\nComparing model architectures, Levine et al. (2020) studied the scaling of models that use self-attention and derived guidelines for depth-to-width allocation. Tay et al. (2021) reported that model shape (depth-width ratio) impacted performance on downstream tasks even if it had minimal impact on the pretraining objective. Tay et al. (2022a) further studied the effect of scaling for different model architectures and showed that architecture choice is pertinent when scaling and that the vanilla transformer architecture scales best.\n\nOf particular importance to this work is the study of Hoffmann et al. (2022), who investigated the effect of model size and the number of training tokens on the performance of a model given a fixed compute budget. They posited that existing large language models were undertrained and that model size and the number of training tokens should be scaled equally. They demonstrated this hypothesis through Chinchilla, a model significantly smaller, yet higher performing, than most of the largest LLMs. These findings opened the door for \"Chinchilla optimal\" training of smaller models that achieve strong performance, and for which inference can be run much more efficiently than for their larger counterparts. These findings led us to consider a nearly Chinchilla-optimal model",
            "score": 0.6627656213072316,
            "section_title": "Related Work",
            "char_start_offset": 70997,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 496,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1180,
                    "end": 1200,
                    "matchedPaperCorpusId": "219965648"
                },
                {
                    "start": 1741,
                    "end": 1763,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406982421875
        },
        {
            "corpus_id": "253553203",
            "title": "Galactica: A Large Language Model for Science",
            "text": "The idea of \"scaling laws\" was put forward by , who demonstrated evidence that loss scales as a power-law with model size, dataset size, and the amount of training compute. The focus was on upstream perplexity, and work by Tay et al. (2022a) showed that this does not always correlate with downstream performance. Hoffmann et al. (2022) presented new analysis taking into account the optimal amount of data, and suggested that existing language models were undertrained: \"Chinchilla scaling laws\". This work did not take into the account of fresh versus repeated tokens. In this work, we show that we can improve upstream and downstream performance by training on repeated tokens.",
            "score": 0.6596653420605555,
            "section_title": "Scaling Laws",
            "char_start_offset": 2912,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 314,
                    "end": 336,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "268358466",
            "title": "Algorithmic progress in language models",
            "text": "Given that our core model is similar to previously proposed language model scaling laws, we can compare our estimates to see how well they correspond to prior work.In particular, the estimates for \u03b1 param and \u03b2 data in  One way to understand this discrepancy is to consider the scaling laws on the same plot, shown in Figure 9.Here we observe that the scaling laws strongly diverge for compute values below around 10 18 to 10 19 FLOP (and the same is true for values greater than 10 22 FLOP).However, between these two regimes the scaling laws appear much more similar in slope.\n\nThis observation suggests the possibility that the discrepancy in estimated scaling exponents is due to the range of fitted data.Indeed, around 80% of our models with known training compute estimates lie between \u223c 4 \u00d7 10 17 FLOP and 10 22 FLOP.This suggests that a large fraction of our data lies within the regime where it is hard for our model to distinguish between the exponents from Hoffmann et al. 2022 andKaplan et al. 2020.\n\nAnother possible explanation for this discrepancy that we considered is that it is due to the omission of an irreducible loss term in our core model, resulting in an omitted variable bias.However we do not put much weight on this explanation for our fits-in our robustness check using models with an irreducible loss term (see section H), we obtain very similar scaling exponents to those obtained in our core model.",
            "score": 0.6583938242452838,
            "section_title": "C.1 Comparing our estimates to earlier work",
            "char_start_offset": 42223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 492
                },
                {
                    "start": 492,
                    "end": 578
                },
                {
                    "start": 580,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 1011
                },
                {
                    "start": 1013,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 992,
                    "end": 1010,
                    "matchedPaperCorpusId": "210861095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5703125
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "Linear complexity language models (Katharopoulos et al., 2020;Qin et al., 2022b;Choromanski et al., 2021;Zheng et al., 2022Zheng et al., , 2023;;Hua et al., 2022;Liu et al., 2022;Qin et al., 2023bQin et al., , 2024c,d;,d;Gu et al., 2021aGu et al., , 2020Gu et al., , 2022;;Fu et al., 2022;Qin et al., 2023a;Fu et al., 2023;Orvieto et al., 2023;Qin et al., 2023dYang et al., 2023;Gu and Dao, 2023;Dao and Gu, 2024;Sun et al., 2023), have emerged as a promising alternative to traditional transformers in causal language modeling.However, the scalability of these models remains uncertain, which limits their applicability to large language models.To address this concern, in this paper we have developed pre-training scaling laws for efficient large language models.Following the approach outlined in (Hoffmann et al., 2022), we have used the training loss as a regression target to establish power law equations against FLOPs and infer the optimal model size and dataset size under constant computation budgets for linear complexity models.Our study focuses on investigating three efficient architectures, as detailed in Section 2: TNL (Qin et al., 2024c), HGRN2 (Qin et al., 2024e), and cosFormer2 (Qin et al., 2022b(Qin et al., , 2023c(Qin et al., , 2024a)).Additionally, LLaMA (Touvron et al., 2023b) is used as a baseline to represent softmax In the master row, we present predicted training curves for various architectures, with each subsequent row representing a different architecture.On the left, the training curves for models ranging from 70M to 7B parameters are displayed.",
            "score": 0.6569083361909878,
            "section_title": "Introduction",
            "char_start_offset": 1288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1260
                },
                {
                    "start": 1260,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 123,
                    "matchedPaperCorpusId": "248085050"
                },
                {
                    "start": 123,
                    "end": 145,
                    "matchedPaperCorpusId": "248085050"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3291015625
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "Substantial effort and cost are required to train even a single large language model (LLM). 1 There is thus an acute need for efficient decision-making aids that can evaluate the effectiveness of proposed changes to language models' architecture or training data without full-scale training runs. While there is a large body of work that motivates or evaluates these changes using small models (Warstadt et al., 2023;Hillier et al., 2024), synthetic tasks (Aky\u00fcrek et al., 2024;Wortsman et al., 2023) or theory (Jelassi et al., 2024), one of the most important tools for current practitioners is the estimation of scaling laws for LLMs (Ivgi et al., 2022;Dubey et al., 2024). \n\nA scaling law extrapolates the performance of a target model from the performance of a set of models with fewer parameters or smaller training sets. Typically, this extrapolation requires models to belong to the same model family, differing only in parameter count and training set size, but using the same architecture and training distribution. A high-quality scaling law accurately predicts the target model's test performance (Rosenfeld et al.;Kaplan et al., 2020;Hoffmann et al., 2022). \n\nMost past work describing and characterizing scaling laws has begun by exhaustively training models in a family across a full range of dataset sizes and parameter counts. One question that has received comparatively little attention is how, when training a new LLM, a practitioner with limited computational resources should choose which small-scale models to train in order to best estimate a target model's final performance. This paper offers a practical guide to when, and how, to use small models to efficiently obtain meaningful predictions about large models' behaviormaximizing prediction reliability while minimizing the budget for preliminary experimentation, which necessarily involves tradeoffs between the number of preliminary models trained, the size of the largest preliminary model, and size of the dataset used to train it. 1 Code, data and full numbers are found in our repository 1 arXiv:2410.11840v1 [cs.LG] 15 Oct 2024 arXiv Version",
            "score": 0.6561856907756557,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 417,
                    "matchedPaperCorpusId": "266182457"
                },
                {
                    "start": 478,
                    "end": 500,
                    "matchedPaperCorpusId": "262824542"
                },
                {
                    "start": 1108,
                    "end": 1126,
                    "matchedPaperCorpusId": "203592013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380859375
        },
        {
            "corpus_id": "263672005",
            "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
            "text": "Large Language Models (LLMs) (Devlin et al., 2018;Raffel et al., 2020;Brown et al., 2020;Chowdhery et al., 2022) have become a center of interest among AI researchers recently. These models, trained on expansive datasets and furnished with an enormous number of parameters, have demonstrated unparalleled proficiency across diverse domains, such as text generation (Dubois et al., 2023), code completion (Chen et al., 2021;Rozi\u00e8re et al., 2023), and academic test (Hendrycks et al., 2020). \n\nThe impressive success of these LLMs depends heavily on scaling up the model parameters and pre-training data volume. It has been consistently observed that, when considering a continuum of models with nearly identical architectures, larger models coupled with increased pre-training corpora consistently yield diminished training loss. This observation has been mathematically formalized as the scaling law of loss (Kaplan et al., 2020;Henighan et al., 2020), which states that the reducible loss achieved by the model in the log scale is linear to the model size in the log scale. Scaling law has provided guidance for the scientific scaling of LLMs, including determining the balance of the model size and pre-training data size (Hoffmann et al., 2022;Muennighoff et al., 2023). This has transformed what was once a somewhat blind scaling process into a methodology underpinned by empirical assurance. Nonetheless, such beneficial scaling law yield predictions solely on the loss, not extending to the real task performance encountered in practice. This divergence establishes a substantial gap in a comprehensive scaling-up methodology (Ganguli et al., 2022). space, which displays strong linearity, supporting the task scaling law (Eq.( 3)). \n\nThe challenge in extending loss caling law to task performance predominantly stems from the discontinuity observed in task performance during scaling. Language models below a certain size yield trivial performance, i.e., random guessing on multiple choices or zero scores on generation tasks. However, when the model size surpasses a certain threshold, a distinct surge in performance appears, which leads to substantially non-trivial performance.",
            "score": 0.6557719097096708,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 70,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 70,
                    "end": 89,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1632,
                    "end": 1654,
                    "matchedPaperCorpusId": "246867298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.286376953125
        },
        {
            "corpus_id": "266755802",
            "title": "TinyLlama: An Open-Source Small Language Model",
            "text": "Recent progress in natural language processing (NLP) has been largely propelled by scaling up language model sizes (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a,b). Large Language Models (LLMs) pre-trained on extensive text corpora have demonstrated their effectiveness on a wide range of tasks (OpenAI, 2023;Touvron et al., 2023b). Some empirical studies demonstrated emergent abilities in LLMs, abilities that may only manifest in models with a sufficiently large number of parameters, such as few-shot prompting (Brown et al., 2020) and chain-of-thought reasoning (Wei et al., 2022). Other studies focus on modeling the scaling behavior of LLMs (Kaplan et al., 2020;Hoffmann et al., 2022). Hoffmann et al. (2022) suggest that, to train a compute-optimal model, the size of the model and the amount of training data should be increased proportionally. This provides a guideline on how to optimally select the model size and allocate the amount of training data when the compute budget is fixed. \n\nAlthough these works show a clear preference on large models, the potential of training smaller models with larger datasets remains under-explored. Instead of training compute-optimal language models, Touvron et al. (2023a) highlight the importance of the inference budget, instead of focusing solely on training compute-optimal language models. Inference-optimal language models aim for optimal performance within specific inference constraints. This is achieved by training models with more tokens than what is recommended by the scaling law (Hoffmann et al., 2022). Touvron et al. (2023a) demonstrates that smaller models, when trained with more data, can match or even outperform their larger counterparts. Also, Thadd\u00e9e (2023) suggest that existing scaling laws (Hoffmann et al., 2022) may not predict accurately in situations where smaller models are trained for longer periods. \n\nMotivated by these new findings, this work focuses on exploring the behavior of smaller models when trained with a significantly larger number of tokens than what is suggested by the scaling law (Hoffmann et al., 2022).",
            "score": 0.6557661215504083,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 135,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 534,
                    "end": 554,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 586,
                    "end": 604,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 688,
                    "end": 710,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 712,
                    "end": 734,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1562,
                    "end": 1585,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1785,
                    "end": 1808,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3857421875
        },
        {
            "corpus_id": "273098522",
            "title": "Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices",
            "text": "In Figure 9, we show our findings in Section 4 translate to the more standard GPT-2 evaluation with a longer sequence length of 512 and its original vocabulary of 50,257 tokens. We train models with L = 12 layers up to the GPT-2 Small [20] size by increasing width d. We use Adam with a base learning rate of 0.002 for a L = 3, d = 256 dense model, which is scaled to other models via \u00b5P. Since the language modeling head contains a significant fraction of the parameters for models of this scale, we replace all layers, including the head, with Einsums. Qualitatively, Figure 9 differs from Figure 4 in two ways: 1) the scaling laws are less power law like and show some curvature on a log-log scale, and 2) BTT with \u03bd > 0 seems to perform better than \u03bd = 0. We believe 1) is due to the increased context length and vocabulary size, making the loss no longer follow a clean power law at the small scales we tested [13,10]. This was an important motivation for performing experiments with a smaller vocabulary size and context length in Section 4. Similarly, we believe the increased vocabulary size and context length contributed to 2), as a larger \u03bd implies at small scales a higher fraction of compute are in the transformer blocks rather than the language modeling head, which likely improves performance. By contrast, in our setup in Section 4, the model dimension d dominates the vocabulary size and context length, leading to less significant finite-size effects. \n\n\u03b1\u03b3 coincide with hj. In Figure 10, we show doing so slightly improves compute efficiency by an average of 17% over naively replacing all attention and FFN matrices with BTT, which corresponds to \u03b8 = (1/2, 0, 1/2, 0, 1/2, 1/2, 0) in Section 4.",
            "score": 0.6551059536453085,
            "section_title": "D Experiments D.1 GPT-2 with Original Vocabulary and Longer Context",
            "char_start_offset": 33984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1715
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2015380859375
        },
        {
            "corpus_id": "259837466",
            "title": "Large Language Models",
            "text": "Natural language tasks are very complex, and benchmarks constructed from real world data cannot be used directly in theoretical considerations. For this purpose one generally defines \"toy worlds\" and generates synthetic data. The possibilities are endless, but some which have been used are arithmetic problems (decimal arithmetic; modular arithmetic), game play, solving systems of equations, and parsing formal languages. A particularly interesting task is linear regression [48]; since this is the prototypical case of statistical inference, a system which learns to do it can be said to be \"learning how to learn.\" \n\nComing to scaling laws, denote the model size (number of parameters) as P and the dataset size (number of tokens in the corpus) as D, then there are two general regimes. If we hold one of these (say P ) fixed and take the other (say D) to infinity, then a law of large numbers applies and L \u223c 1/D. On the other hand, if we take one parameter very large and study the dependence on the other, nontrivial power law scaling can emerge. In principle one can get different exponents for D and P , suggesting the ansatz \n\nwhere L is test loss Eq. 3 computed in an optimally regularized model. 24 This is a good fit to Figure 2. While in Figure 2 the two exponents appear to differ, there is not really convincing evidence that this is significant. Before working hard on this, one should ask if there is any way to control the many choices involved, so as to define universal exponents. One context in which this can be studied systematically is transfer learning, by distinguishing the dependence on the pretraining and fine tuning datasets [55]. Another relevant and practical question is whether one can prune the dataset to improve the scaling. It is intuitively plausible and can be shown in examples that sets of data items are worth more if they are diverse than if they are similar. The challenge is to find simple ways to quantify this similarity; in [126] many proposals are studied. \n\nScaling laws can arise in many ways, not specific to language models. One hypothesis is that the data lies on a low dimensional submanifold in a higher dimensional space. 25",
            "score": 0.655039258999046,
            "section_title": "Phenomenology of language models",
            "char_start_offset": 30134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2008
                },
                {
                    "start": 2011,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.248291015625
        },
        {
            "corpus_id": "274150498",
            "title": "Scaling Laws for Online Advertisement Retrieval",
            "text": "The neural scaling laws, describing how neural network performance changes with key factors (e.g. model size, dataset size, computational cost), have been discovered in various research areas [8,15,18,19,23,32]. Early research [14] shows that the neural network performance is predictable when scaling training data size in various tasks such as neural machine translation and language modeling. Kaplan et al.(2020) [19] further empirically verify the scaling law of Transformer architecture in language modeling, regarding the key factors (model size, data size, training cost) and training performance (PPL). Inspired by the scaling law, researchers extend the size of pre-trained language models and further empirically verify the scaling law by training GPT-3 [3]. This wave of enthusiasm has led to the creation of GPT-3.5 and GPT-4 [1], ushering in a new era of NLP research and applications. \n\nBased on the scaling laws, the optimal key factors of the model can be determined under given constraints, thus guiding us in model design and resource allocation. Recommendation and advertising systems are mature commercial applications that prioritize ROI, making it highly valuable to explore whether there exists scaling laws for recommendation and advertising models. Due to the lack of a thriving community and open data ecosystem similar to NLP, research on model scaling is relatively scarce in the recommendation and advertisement areas. Early studies primarily gave some qualitative conclusions about model scaling [23,32]. Fang et al.(2024) [8] first attempt to give a quantitative scaling law of model performance and the amount of training data and model parameters based on public information retrieval benchmarks, and give the practice to solve the optimal amount of data and model parameters under a given total training resource. \n\nHowever, there is still a lack of literature on scaling law research in real-world online recommendation and advertising systems. We attribute this to two main challenges: 1) For commercial systems, the scaling law should describe the relationship between business revenue and machine costs, rather than the relationship between computing volume, data volume, and offline metrics as seen in traditional research.",
            "score": 0.6524548903034827,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "268723791"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "244477730"
                },
                {
                    "start": 1526,
                    "end": 1530,
                    "matchedPaperCorpusId": "244477730"
                },
                {
                    "start": 1535,
                    "end": 1552,
                    "matchedPaperCorpusId": "268723791"
                },
                {
                    "start": 1553,
                    "end": 1556,
                    "matchedPaperCorpusId": "268723791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "276812901",
            "title": "Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions",
            "text": "We define scaling laws here as a relationship between the number of parameters N and the number of tokens D of a language model family, and the expected language-modelling loss at convergence L(N, D).2 Importantly, these laws are typically examined while holding all other factors constant: keeping the same model architecture, training data, and model parameters. Originally, Kaplan et al. (2020) showed that over a wide range of transformer-based models, this relationship can be expressed as a power law: \n\nLater, Hoffmann et al. (2022a) introduced a similar law, which differed in the coefficients fitted, but was also based on a power law. However, scaling laws are not absolute, and the exact functional form and fitted coefficients may depend on the architecture type, size range (Pearce and Song, 2024), or other considerations such as inference costs. See ( \u00a76.2) for further discussion.",
            "score": 0.6521627188465499,
            "section_title": "Definition",
            "char_start_offset": 3556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 896
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "276574659",
            "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
            "text": "Large Language Models (LLMs) have emerged as transformative technologies in natural language understanding, generation, and reasoning [1,14,5]. Their impressive success heavily relies on scaling model parameters and pre-training data, with training loss empirically following a power-law relationship with compute [18,21]. However, this reduction in training loss primarily reflects an indomain compression effect and does not necessarily indicate improved out-of-domain generalization or downstream performance-the factor of primary concern in practice. Specifically, performance scaling of downstream tasks aims to predict the accuracy of the target LLM on downstream tasks using metrics from smaller models. Our objective is to develop a prediction method that works reliably across a diverse range of downstream tasks, minimizing the worst-case prediction error. \n\nDespite extensive efforts, a reliable scaling law for downstream tasks remains elusive. One line of work attempts to extrapolate large-model performance by modeling the performance-loss relationship [6,13,8,38,26], but this often fails to capture the emergent behaviors of LLMs and the mismatch between in-domain loss and downstream metrics [42]. Another line of research focuses on direct extrapolation of performance-compute relationship [1,19], yet the uneven difficulty distribution across different evaluation samples undermines its accuracy. We observe that different evaluation samples actually follow distinct performance scaling patterns, and thus applying a single extrapolation formula to the entire evaluation set is suboptimal. We give the detailed analysis in Section 3. \n\nTo address these challenges, we propose a new performance scaling law, derived from the existing loss scaling law [21], specifically applicable to evaluation subsets that exhibit consistent performance scaling patterns. Building on the performance scaling law, we develop a Clustering-On-Difficulty (COD) multi-stage framework for predicting downstream performance. Specifically, we first cluster tasks by their difficulty features, and then filter out clusters that lack valid extrapolation patterns. Next, we fit the performance-compute relationships in the remaining clusters under our performance scaling law, extrapolate the performance of large models within these clusters, and finally map the aggregated predictions to the complete task set.",
            "score": 0.6517974289222049,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 1210,
                    "end": 1214,
                    "matchedPaperCorpusId": "231991101"
                },
                {
                    "start": 1312,
                    "end": 1315,
                    "matchedPaperCorpusId": "263672005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "232335877",
            "title": "UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark",
            "text": "Scaling Laws In contemporary machine learning, simple methods that scale often outperform complex ones (Sutton 2019). Accordingly, recent years have seen a sharp rise in compute used by state-of-the-art methods (Amodei and Hernandez 2018). Performance gains from increasing data, parameters, and training are not only reliable, but empirically predictable (Hestness et al. 2017;Sun et al. 2017;Rosenfeld et al. 2020;Kaplan et al. 2020). For example, Sun et al. (2017) found that models need exponential data for improvements in accuracy. 6 These observations, that scaling is reliable, predictable, and critical to the current successes, motivate our focus on evaluation based on cost-benefit trade-offs, i.e. the cost equivalent curve.\n\nCommonsense Benchmarks Rapid progress in modeling has led to a major challenge for NLP: the creation of suitable benchmarks. Neural models often cue off statistical biases and annotation artifacts to solve datasets without un-4 While sequential training for the RAINBOW tasks would likely yield the best results, it would have required much more compute. 5 All tasks use accuracy for evaluation except WINOGRANDE which uses area under the dataset size-accuracy learning curve. 6 Eventually, models saturate and need super-exponential data.\n\nderstanding tasks (Gururangan et al. 2018 Transfer Learning Semi-supervised and transfer learning have grown into cornerstones of NLP. Early work learned unsupervised representations of words (Brown et al. 1992;Mikolov et al. 2013), while more recent work employs contextualized representations from neural language models (Peters et al. 2018). Radford et al. (2018) demonstrated that language models could be fine-tuned directly to solve a wide-variety of tasks by providing the inputs encoded as text, while Devlin et al. (2019) and others improved upon the technique (Yang et al. 2019;Liu et al. 2019b;Lan et al. 2019). Most relevant to this work, Raffel et al. (2019) introduced T5 which built off previous work to reframe any NLP task as text-to-text, dispensing with the need for taskspecific model adaptations.",
            "score": 0.6513555674066107,
            "section_title": "Related Work",
            "char_start_offset": 17627,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 394,
                    "matchedPaperCorpusId": "6842201"
                },
                {
                    "start": 450,
                    "end": 467,
                    "matchedPaperCorpusId": "6842201"
                },
                {
                    "start": 1471,
                    "end": 1490,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 1490,
                    "end": 1510,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 1602,
                    "end": 1622,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1789,
                    "end": 1809,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1849,
                    "end": 1867,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27294921875
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "A scaling law estimates the loss of a costly model by training cheaper ones (see Fig. 2) which share a pretraining procedure and differ by some hyperparameters, typically model size (#params) and number of tokens seen during training (#toks). A scaling law is a function that predicts a target model's loss on held-out data when setting the value of one hyperparameter (Kaplan et al., 2020) or both (Rosenfeld et al.;Hoffmann et al., 2022). Comparing laws' predictions about different pretraining choices (e.g. data Ge et al., 2024) allows informed decisions about which large-scale model to train. \n\nMod el #pa ram s \n\nFigure 2: Illustration of a scaled family, an estimated scaling law, and its prediction error for a target model. \n\nA scaling law also enables finding the optimal choice of hyperparameters under computational constraints on pretraining (Hoffmann et al., 2022) or inference (Touvron et al., 2023;Sardana et al.). \n\nFormally, we will call a model f any single concrete neural language model with a specific set of parameters. Different seeds, or even different checkpoints from the same training run, correspond to different models. We define a scaled model family f as a set of models, with each f \u2208 F differing only in size #params(f ) and number of tokens #toks(f ). \n\nThere are two specific subsets of scaled model families that will be useful in our experiments. First, the maximal parameter family max #params (F ) contains only models in F with the largest number of parameters. Formally, define m = max f \u2208F #params(f ); then max # params(F ) =  (3) the size of the largest model trained (expressed as a scale-up factor-the ratio between the target model and the largest model in F train ). In (a), all laws are estimated from four models. In (c) all laws use the full corpus. Orange lines show iso-FLOP contours (sets of scaling laws whose training sets require the same computational cost to produce). \n\nrepresent the most efficient ways to obtain 15%, 10% and 5% ARE.",
            "score": 0.6499984018106564,
            "section_title": "DEFINING A SCALING LAW",
            "char_start_offset": 4633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 417,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 915,
                    "end": 930,
                    "matchedPaperCorpusId": "266693796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45458984375
        },
        {
            "corpus_id": "268691910",
            "title": "Mechanistic Design and Scaling of Hybrid Architectures",
            "text": "To investigate the link between MAD synthetics and real-world scaling, we execute the largest scaling law analysis on emerging architectures to date, training over 500 language models between 70 million and 7 billion parameters with different architectures. Our protocol builds and expands on compute-optimal scaling laws for LSTMs and Transformers [1,22,2]. Our findings show that hybrid architectures improve on all scaling measures, resulting in lower pretraining losses at different floating point operation (FLOP) compute-budgets at the compute-optimal frontier 1 . We also verify new architectures to be more robust to large pretraining runs outside the efficient frontier e.g., smaller models trained for significantly more tokens, which make up a majority of training settings in practice due to inference cost considerations [23]. \n\nHybridization insights at scale Building on our scaling law analysis, we investigate hybridization schedules and model topology. Our findings uncover optimal hybridization ratios for attention [6], Hyena [10], and Mamba [12] mixtures, as well as the respective placement of these layers in an architecture. \n\nState-optimal scaling laws The size of the state -the analog of kv-caches in standard Transformers [24] of emerging convolutional and recurrent primitives [10,12] plays a central role in MAD and our scaling analysis, as it determines inference efficiency, memory cost, and provably has a direct effect on recall capabilities [17]. We introduce a state-optimal scaling analysis, with the objective of estimating how perplexity scales with the state dimension of different model architectures. We find hybrid architectures to balance the trade-off between compute requirements, state dimension, and perplexity. \n\nNew state-of-the-art architectures Leveraging MAD and new computational primitives, derived from the insights developed in this work, we design new state-of-the-art hybrid architectures, outperforming the best Transformer, convolutional, and recurrent baselines (Transformer++ [4], Hyena, Mamba) with a reduction of up to 20% in perplexity for the same compute budget.",
            "score": 0.6497310186516603,
            "section_title": "Scaling laws of emerging architectures",
            "char_start_offset": 4487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 1035,
                    "end": 1038,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "270620190",
            "title": "Reconciling Kaplan and Chinchilla Scaling Laws",
            "text": "Following early works formalizing how language models improve with parameters, data, and training compute (Rosenfeld et al., 2019;Kaplan et al., 2020;Hoffmann et al., 2022), there has been investigation into whether these scaling laws arise in other domains (Henighan et al., 2020), and to explain their existence from a theoretical standpoint (Hutter, 2021;Maloney et al., 2022;Bahri et al., 2024). \n\nCloser in spirit to our paper are several concurrent works that have investigated the influence of various design decisions on scaling law analyses. Su et al. (2024) revisit the methodology used to find scaling coefficients. H\u00e4gele et al. (2024) found that multiple independent cosine schedules could be reproduced more efficiently through a constant learning rate with multiple short decays, or stochastic weight averaging. \n\nOur finding is subtly different; a simple fixed learning rate will recover very similar compute-parameter scaling coefficients as multiple cosine schedules. Bi et al. (2024) study the effect of various hyperparameters on scaling laws. They observe that different text datasets produce slightly different optimal coefficients, with 'cleaner' data leading to more parameter-hungry scaling behavior, which they speculate could partially explain the difference between Kaplan and Chinchilla coefficients. \n\nPorian et al. ( 2024) provide a concurrent work with the same objective as our paper -explaining the differences between the Kaplan and Chinchilla coefficients. Through a set of large-scale experiments reproducing Kaplan's study, they determine that responsibility for the discrepancy can be attributed, in decreasing order of significance, to; 1) Kaplan counting non-embedding rather than total compute. 2) Kaplan using a fixedlength warmup period that was too long for smaller models, making them appear less efficient. 3) Kaplan not fully tuning optimization hyperparameters. We see these findings as complimentary to our own. We have been able to identify the primary 'first-order' reason using only information that was publicly available in the two papers, with a fully analytical approach. (Tiny-scale experiments were run post-hoc as verification.) This illustrates the promise of applying mathematical approaches to the empirical science of scaling.",
            "score": 0.649278391661456,
            "section_title": "Related work",
            "char_start_offset": 16226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2290
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 398,
                    "matchedPaperCorpusId": "231918701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3837890625
        },
        {
            "corpus_id": "263672005",
            "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
            "text": "The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ``emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy with theoretically infinite resolution, through massive sampling in the decoding phase. With PassUntil, we conduct a quantitative investigation into the scaling law of task performance. The investigation contains two parts. Firstly, a strict task scaling law that is not conventionally known to exist, is identified, enhancing the predictability of task performances. Remarkably, we are able to predict the performance of the 2.4B model on code generation with merely 0.05\\% deviation before training starts, which is the first systematic attempt to verify predictable scaling proposed by GPT-4's report. Secondly, we are able to study emergent abilities quantitatively. We identify a kind of accelerated emergence whose scaling curve cannot be fitted by standard scaling law function and has a increasing speed. We then examine two hypothesis and imply that the ``multiple circuits hypothesis'' might be responsible for the accelerated emergence.",
            "score": 0.6482076799720662,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "Predicting the scaling behavior of large models is critical when deciding on training resources. Specifically, two questions are of interest: (Allocation) What is the optimal balance of resources? (Return) What is the expected value of additional resources? For scaling LLMs, the resource is compute (measured in FLOPs), and it can be allocated to training a larger model or training for more steps. 1 The metric used to quantify progress is the model's loss on held-out data, i.e. the ability to predict the underlying data as measured in the model's cross-entropy [2,42]. We aim to minimize the loss (L) subject to a compute resource constraint (C) via optimal allocation to N and D as: Currently, there are established best practices for scaling LLMs. Return follows a power-law: loss scales as a power-law with the amount of compute used for training [39,46,6,35,7,41]. Allocation is balanced: resources are divided roughly equally between scaling of parameters and data [42]. These scaling laws were established empirically by training LLMs and carefully extrapolating behavior. \n\nChinchilla [42] uses three methods for making scaling predictions: \n\n\u2022 (Fixed Parameters) Train with a fixed model size but on varying amounts of data. \n\n\u2022 (Fixed FLOPs) Train with fixed computation while parameters and training tokens vary. \n\n\u2022 (Parametric Fit) Derive and fit a formula for the loss. \n\nFor the parametric fit, the loss (L) is a function of parameters (N ) and training tokens (D): \n\nWhere {A, \u03b1, B, \u03b2, E} are learned variables fit using the training runs from the first two approaches [42]. Using these learned variables, they propose calculating the optimal allocation of compute (C) to N and D as follows: \n\nwhere G = \u03b1A \u03b2B \n\nThese methods lead to the conclusion that \u03b1 \u2248 \u03b2 and hence N and D should be scaled proportionally for compute-optimal training. As loss can be an imperfect proxy for performance on natural language tasks [123,97,105], they also validate their conclusions on various downstream tasks.",
            "score": 0.6479442603566711,
            "section_title": "Background",
            "char_start_offset": 4342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 569,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 867,
                    "end": 869,
                    "matchedPaperCorpusId": "246608156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "The results for L(C min ) and the allocations can be predicted from the L(N, S min ) equation obtained in Section 5. Given our equation for L(N, S min ), we can substitute S min = Cmin 6N B and then find the minimum of the loss as a function of N , while fixing the training compute. We carry out this procedure in detail in Appendix B, where we also provide some additional predictions. \n\nFor the loss as a function of training compute, we predict that \n\nwhere \n\nin excellent agreement with the exponent of Figure 13. We also predict that \n\nwhich also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive framework for the performance of language modeling. \n\nThe intersection point is sensitive to the precise power-law parameters \n\nFigure 15 Far beyond the model sizes we study empirically, we find a contradiction between our equations for L(C min ) and L(D) due to the slow growth of data needed for compute-efficient training. The intersection marks the point before which we expect our predictions to break down. The location of this point is highly sensitive to the precise exponents from our power-law fits.",
            "score": 0.645958584167857,
            "section_title": "Predictions from L(N, S min )",
            "char_start_offset": 32213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1158
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "246473179",
            "title": "Unified Scaling Laws for Routed Language Models",
            "text": "The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.",
            "score": 0.6454753927301228,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "In Figure 14 (left), we see that as one increases the amount of compute, it is possible to get better fits with lower relative error.In Figure 14 (right), we see a similar trend as one increases the number of data points used to fit a scaling law.Blue stars indicate the configurations from Table 1, which provide accurate predictions relative to the general trends-hinting at their usefulness for our investigation.In Figures 15 and 16 we repeat the compute analysis comparing trade-offs for loss prediction and error prediction for our RedPajama 1.4B parameter, 900B token and 6.9B parameter, 138B token runs respectively.We find that less compute is generally necessary to construct a loss scaling law that achieves the same relative error as that of an error prediction scaling law.\n\nOn compute-optimal token multipliers.We consider 20 tokens per parameter as close to compute-optimal for our experiments.Here we investigate, using different approaches, what the compute-optimal token multipliers are for each dataset-assuming one should scale number of parameter and training tokens equally as Hoffmann et al. [45] suggest.\n\nTurning to Figure 9, we notice that there are many multipliers, between 10 and 80 that yield models close to the frontier.Hence, empirically, it appears choices within this range should be suitable for the optimal token multiplier.\n\nWe can also compute an optimal token multiplier using the coefficients in Table 6.Based on Hoffmann et al. [45]'s Equation ( 4) and the assumption that \u03b1 = \u03b2, we write,\n\nTo compute M * = D * /N * , we then have,\n\nUsing the values from   4).Recall that the C4 training set is English-filtered.Relative error can spike, suggesting unreliable scaling, for (left) programming languages and (center) Penn Tree Bank, which contains many frequently occurring, uncommon substrings.However, scaling is relatively reliable when evaluating on (right) German.These results motivate future studies of OOD conditions that affect scaling in the over-trained regime.observation in Figure 9, which suggests M = 5 is already too small to give points on the Pareto frontier.We hypothesize this mismatch arises because we fit our scaling laws using models with M \u2265 20.",
            "score": 0.6450309631676524,
            "section_title": "3.6%",
            "char_start_offset": 40138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 786
                },
                {
                    "start": 788,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1128
                },
                {
                    "start": 1130,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1361
                },
                {
                    "start": 1363,
                    "end": 1445
                },
                {
                    "start": 1445,
                    "end": 1531
                },
                {
                    "start": 1533,
                    "end": 1574
                },
                {
                    "start": 1576,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1836
                },
                {
                    "start": 1836,
                    "end": 1910
                },
                {
                    "start": 1910,
                    "end": 2013
                },
                {
                    "start": 2013,
                    "end": 2118
                },
                {
                    "start": 2118,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330078125
        },
        {
            "corpus_id": "270703266",
            "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
            "text": "One of the most striking findings in modern research on large language models (LLMs) is that, given a model and dataset of sufficient scale, scaling up compute at training time leads to better final results (Kaplan et al., 2020;Hoffmann et al., 2022).However, there is also another lesser-mentioned scaling phenomenon, where adopting more sophisticated methods or scaling compute at inference time (Jones, 2021)",
            "score": 0.6438978997865911,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 411
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 250,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "252683098",
            "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
            "text": "3.1 LANGUAGE MODEL SCALING LAWS Kaplan et al. (2020) showed that the cross-entropy loss of autoregressive Transformers (Vaswani et al., 2017) scales as a power law with model size, dataset size and compute. These scaling laws hold when training is not bottlenecked by the other two resources. Specifically, the model size power law applies when models are trained to convergence, while the compute scaling law is valid when training optimal-sized models. By combining these laws a power-law scaling of optimal model size with compute is obtained, with an exponent derived from the loss scaling exponents. These exponents, later recalculated by Hoffmann et al. (2022), tend to fall in the range [0, 1].",
            "score": 0.6402662391751535,
            "section_title": "BACKGROUND",
            "char_start_offset": 5944,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 701
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "275336720",
            "title": "The interplay between domain specialization and model size",
            "text": "Scaling laws for language models have often focused on finding the optimal model size and token count for training from scratch. However, achieving this optimal balance requires significant compute resources due to the extensive data demands when training models from randomly-initialized weights. Continued pretraining offers a cost-effective alternative, leveraging the compute investment from pretrained models to incorporate new knowledge without requiring extensive new data. Recent findings suggest that data quality influences constants in scaling laws, thereby altering the optimal parameter-token allocation ratio. Building on this insight, we investigate the interplay between domain specialization and model size during continued pretraining under compute-constrained scenarios. Our goal is to identify an optimal training regime for this scenario and detect patterns in this interplay that can be generalized across different model sizes and domains. To compare general and specialized training, we filtered a web-based dataset to extract data from three domains: legal, medical, and accounting. We pretrained models with 1.5B, 3B, 7B, and 14B parameters on both the unfiltered and filtered datasets, then evaluated their performance on domain-specific exams. Results show that as model size increases, specialized models outperform general models while requiring less training compute. Additionally, their growing compute efficiency leads to reduced forgetting of previously learned knowledge.",
            "score": 0.6399620404193198,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "257038048",
            "title": "Scaling Laws for Multilingual Neural Machine Translation",
            "text": "Over the past few years, scaling has emerged as a popular and effective way to improve the performance of neural networks (Brown et al., 2020;Chowdhery et al., 2022;Lepikhin et al., 2020). Given the costs associated with training large neural models, much work has gone into understanding their scaling properties and predicting the 1 Google Research 2 Carnegie Mellon University 3 Instituto Superior T\u00e9cnico. Correspondence to: Patrick Fernandes <pfernand@cs.cmu.edu>. evolution of their performance with scale through scaling laws. Such scaling laws have been instrumental in guiding the model development efforts across a variety of domains such as computer vision (Zhai et al., 2022), language modeling (Kaplan et al., 2020;Hoffmann et al., 2022), and neural machine translation (Ghorbani et al., 2022). Despite these impressive developments, most of the scaling laws studies available in the literature only focus on single-task, single-language models. On the contrary, current massive neural models are often trained to solve more than one task across one or more modalities & languages (Chowdhery et al., 2022;Sanh et al., 2022;Reed et al., 2022). This disconnect from the current research frontier limits the applicability of scaling laws in guiding model development decisions. In particular, currently available scaling laws studies are unable to inform the decision process on balancing the different tasks effectively at training time. Without such guidance, practitioners often have to rely on cumbersome and costly approaches such as approximate grid search to inform their decision-making; such approaches quickly become infeasible as the problem scale grows. \n\nIn this paper, we take the initial step towards developing a quantitative understanding of the scaling behavior for multitask models. We choose multilingual neural machine translation (MNMT) as the setup for this initial study.",
            "score": 0.6395842820038479,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 142,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 142,
                    "end": 165,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 668,
                    "end": 687,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1094,
                    "end": 1118,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1118,
                    "end": 1136,
                    "matchedPaperCorpusId": "239009562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342529296875
        },
        {
            "corpus_id": "270620190",
            "title": "Reconciling Kaplan and Chinchilla Scaling Laws",
            "text": "Our analysis uses information and data from the Chinchilla and Kaplan studies to estimate the scaling laws that would emerge if the Chinchilla relationship had been expressed in terms of N \\E & C \\E , and this had been done over the smaller model sizes used in Kaplan, as summarized in Figure 1. \n\nWe will see that for large N T , N E becomes a negligible portion of the model's parameters and compute cost. Hence in the large parameter regime the two coefficients directly conflict with each other. At smaller values of N T , N E is not negligible (this is the regime considered in Kaplan's study -768 to 1.5B parameters). We find that at the smaller end of this range, the relationship between N * \\E & C \\E is not in fact a power law. However, fitting a \"local\" power law at this small scale, produces a coefficient that is close to Kaplan's, and hence roughly reconciles these two results. \n\nOur approach in Section 3 is broken down as follows. Section 4 experimentally verifies our analysis by training a set of language models at tiny scale and conducting scaling law analyses under various settings. Simply changing the basis N T to N \\E produces coefficients inline with Chinchilla and Kaplan respectively, while multiple token budgets and decay schedules does not. \n\nSection 5 presents a second, related contribution. We reconcile differences in the relationship between loss and compute proposed by the two studies. We leverage a similar analysis as above, but Step 3 & 4 are now performed with respect to the relationship between the optimal loss L * \\E , and compute C \\E . So again, we begin from Chinchilla data, and correct for the exclusion of embedding parameters and compute, combined with the smaller model sizes used in Kaplan's study, and additionally a differing choice of fitting function. Through these corrections, we are able to roughly recover Kaplan's compute-loss coefficient, and hence reconcile the two studies.",
            "score": 0.6391801071456813,
            "section_title": "Analysis Overview",
            "char_start_offset": 3194,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1942
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43701171875
        },
        {
            "corpus_id": "270869927",
            "title": "Collaborative Performance Prediction for Large Language Models",
            "text": "Large Language Models (LLMs) (Brown et al., 2020;Ouyang et al., 2022) have emerged as one of the most important AI research powered by largescale parameters, high computational resources, and massive training data. With the substantial increase in model sizes, the evaluation cost of LLMs' performance becomes even more significant. For example, testing a single LLM on certain benchmarks often requires $10K+ and 4K+ GPU hours (Liang et al., 2023). Therefore, understanding the behaviors and predicting the capabilities of LLMs across scales under various tasks becomes a vital question (Ganguli et al., 2022a;Owen, 2024;Finnveden, 2020;Hu et al., 2024) for both researchers and engineers. \n\nScaling laws (Kaplan et al., 2020;Hoffmann et al., 2022;Hernandez et al., 2022;Gordon et al., 2021;Bahri et al., 2024;Muennighoff et al., 2023) have been powerful tools for predicting the capabilities of LLMs. It indicates a power-law correlation between the model performance and design factors such as computational measure (FLOPs) utilized during training. Although the scaling law was originally proposed as a strong intuitive guide for designing LLM, researchers (Hu et al., 2024;Ruan et al., 2024;Isik et al., 2024) have extended its utility into predicting model performances on various metrics, such as BLEU in Machine Translation, and different tasks. These works can accurately predict model performances by utilizing the similarity within each model family, e.g., models within each family are usually trained on the same dataset. However, there are several issues rooted in their methods: the performance prediction 1) requires transparent design factors that consume substantial training resources to fit the curve, 2) is only tailored to a certain model family and a specific task metric, and 3) neglects the connections among different models and tasks. \n\nThe aforementioned limitations motivate us to design more effective methods for predicting the performance of LLMs on downstream tasks. Two observations sparked our attention. Firstly, A strong similarity exists between model families, e.g.LLama-family and GPT family.",
            "score": 0.6390415915669913,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 69,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models. Gadre et al. [17] directly predict the loss and downstream performance for models trained past the point of compute optimality, and Muennighoff et al. [37] model joint compute-data bottlenecks. All three works rely on the Hoffmann et al. law as a reference point, with [17,37] baking it to their parametric forms. \n\nCompute-optimal scaling is studied beyond the language domain, particularly in vision. Henighan et al. [23] study autoregressive modeling for a variety of tasks and find scaling laws roughly consistent with the Kaplan et al. [30] adjusted scaling law (with exponent a = 0.73). That work shares the methodological issues described in the top row of Figure 1 (FLOP count and long warmup), but performs hyperparameter tuning for smaller scale models; in Appendix H we reach similar results when doing the same. Zhai et al. [59] characterize the compute-efficient frontier of Vision Transformers (ViTs), while Cherti et al. [13] studies compute constrained scaling of CLIP models. However, they do not offer a power law for scaling model size with compute. Alabdulmohsin et al. [4] tackle model design under an inference compute constraint by fitting multi-term parametric forms to obtain predictions for the optimal ViT shape. Goyal et al. [20] point out an intricate interplay between data filtering and compute constraints. Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28].",
            "score": 0.6390254465973226,
            "section_title": "Related work",
            "char_start_offset": 24085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 636,
                    "end": 639,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 1199,
                    "end": 1203,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1299,
                    "end": 1303,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 1453,
                    "end": 1456,
                    "matchedPaperCorpusId": "258832817"
                },
                {
                    "start": 1616,
                    "end": 1620,
                    "matchedPaperCorpusId": "269033049"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise\"precision-aware\"scaling laws for both training and inference. We propose that training in lower precision reduces the model's\"effective parameter count,\"allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
            "score": 0.6388677463636746,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "258865485",
            "title": "How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench",
            "text": "Scaling Laws and Emergent Abilities. Pretraining scale is critical to language model capabilities. Research on scaling laws (Kaplan et al., 2020;Rae et al., 2021;Hoffmann et al., 2022) aims to categorize the relationship between pre-training compute, corpus size, model size and the test loglikelihood loss. Our work can be loosely considered as an extension to scaling laws, with three notable distinctions: \n\n(1) we focus on predicting downstream task performance; \n\n(2) we use model scale along with other experiment configuration information; \n\n(3) we mainly experiment with machine learning methods instead of explicit power laws. In this same vein, recent work has studied the effect of scale in a \"pre-train then fine-tune\" paradigm (Tay et al., 2022) and has explored non-monotonic scaling laws for complex scaling behaviors (Caballero et al., 2023). Another important observation about scale is that very large language models exhibit emergent abilities (Wei et al., 2022a), which are described as \"unpredictable.\" In this work we empirically examine this claim and quantify the prediction errors under various assumptions. \n\nBenchmarking for LLMs. Along with the development and scaling of LLMs, there are continuing efforts to create benchmarks that assess the capabilities of these models. One general trend for these benchmarks is transitioning from single-task (Bowman et al., 2015;Rajpurkar et al., 2016), to multi-task (Wang et al., 2018(Wang et al., , 2019)), and finally to massively multi-task (Hendrycks et al., 2021;Srivastava et al., 2023). However, due to budget or API constraints, models are typically evaluated on only a subset of the full range of available benchmarks. The selection is often made arbitrarily by the models' developers, making it challenging to compare models in a fair and holistic way (see Liang et al. 2023, Fig. 4). In response to this issue, we study the \"small-bench\" problem and hope it offers insights on efficient benchmarking of LLMs. \n\nPerformance Prediction.",
            "score": 0.6372397878052286,
            "section_title": "Related Work",
            "char_start_offset": 5018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1988
                },
                {
                    "start": 1991,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 740,
                    "end": 758,
                    "matchedPaperCorpusId": "260498358"
                },
                {
                    "start": 963,
                    "end": 982,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1375,
                    "end": 1396,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 1435,
                    "end": 1453,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1453,
                    "end": 1475,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "In this paper, we hypothesize that the effectiveness of CFG lies in its double computation. We further propose the following hypothesis: Hypothesis 1. Scaling parallel computation (while maintaining the nearly constant parameters) enhances the model's capability, with similar effects as scaling parameters. \n\nWe propose a proof-of-concept scaling approach called parallel scaling (PARSCALE) to validate this hypothesis on language models. The core idea is to increase the number of parallel streams while making the input transformation and output aggregation learnable. We propose appending P different learnable prefixes to the input and feeding them in parallel into the model. These P outputs are then aggregated into a single output using a dynamic weighted sum, as shown in Figure 1(1). This method efficiently scales parallel computation during both training and inference time by recycling existing parameters, which applies to various training algorithms, data, and tasks. \n\nOur preliminary theoretical analysis suggests that the loss of PARSCALE may follow a power law similar to the Chinchilla scaling law (Hoffmann et al., 2022). We then carry out large-scale pre-training experiments on the Stack-V2 (Lozhkov et al., 2024) and Pile (Gao et al., 2021) corpus, by ranging P from 1 to 8 and model parameters from 500M to 4.4B. We use the results to fit a new parallel scaling law that generalizes the Chinchilla scaling law, as depicted in Figure 1(2). It shows that parallelizing into P streams equates to scaling the model parameters by O(log P). Results on comprehensive tasks corroborate this conclusion. Unlike parameter scaling, PARSCALE introduces negligible parameters and increases only a little space overhead. It also leverages GPU-friendly parallel computation, shifting the memory bottleneck in LLM decoding to a computational bottleneck and, therefore, does not notably increase latency. For example, for a 1.6B model, when scaling to P = 8 using PARSCALE, it uses 22\u00d7 less memory increase and 6\u00d7 less latency increase compared to parameter scaling that achieves the same model capacity (batch size = 1, detailed in Section 3.3). Figure 1(3) illustrates that PARSCALE offers superior inference efficiency.",
            "score": 0.6360563346330244,
            "section_title": "Introduction",
            "char_start_offset": 1904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2230
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7099609375
        },
        {
            "corpus_id": "273507997",
            "title": "A Simple Model of Inference Scaling Laws",
            "text": "Advancements in deep learning have demonstrated that the performance of neural networks scales predictably as a function of model size, data size, and computational resources [Hestness et al., 2017, Kaplan et al., 2020a, Rosenfeld et al., 2020, Henighan et al., 2020a]. These trends, known as neural scaling laws, have motivated research into understanding how scaling influences model performance in a range of domains, in particular, Large Language Models (LLMs) [Brown et al., 2020, Hoffmann et al., 2022]. \n\nHowever, scaling during inference-the process by which a trained model makes predictions on new data-has received less attention. Recent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials. A natural question then arises: \n\nCan we interpret, or predict the inference scaling behavior of a model with repeated attempts? \n\nTo answer this question, we propose a simple toy model that isolates the inference scaling laws which dictate how certain performance metrics improve as a function of the number of inference attempts. Inspired by the work of Hutter [2021], which introduced a model to study scaling behavior for memorization and generalization, we devise a simple setting to capture the effect of repeated inference attempts, focusing on the coverage metric, also known as pass@k. \n\nIn this work, we present analytical predictions for coverage from a probabilistic perspective and demonstrate how inference improves with the number of repeated trials in a predictable way, which matches the observed",
            "score": 0.6353357268450773,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 243,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 484,
                    "end": 508,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "261951743",
            "title": "Baichuan 2: Open Large-scale Language Models",
            "text": "Neural scaling laws, where the error decreases as a power function of training set size, model size, or both, have enabled an assuring performance when training became more and more expensive in deep learning and large language models. Before training the large language models of billions of parameters, we first train some small-sized models and fit a scaling law for training larger models. \n\nWe launched a range of model sizes going from 10M to 3B, ranging from 1 1000 to 1 10 the size of the final model, and each of the model is trained for up to 1 trillion tokens, using consistent hyperparameters and the same data set sourced from Baichuan 2. Based on the final loss of different models, we can obtain a mapping from the training flops to the target loss. To fit the scaling law of the model, we employed the formula given by Henighan et al. (2020): \n\nwhere L \u221e is the irreducible loss and the first term is the reducible loss which is formulated as a power-law scaling term. C are training flops and the L C are final loss of the model in that flops. We used the curve_fit function from the SciPy4 library to fit the parameters. The final fitted scaling curve and the predicted 7 billion and 13 billion parameters model's final loss are shown in Figure 4. We can see that the fitted scaling law predicted Baichuan 2's final loss with high accuracy.",
            "score": 0.6344541884366952,
            "section_title": "Scaling Laws",
            "char_start_offset": 11273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1358
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45458984375
        },
        {
            "corpus_id": "258479686",
            "title": "Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs",
            "text": "A large body of work has looked at studying the impact of model scale on model capabilities along different dimensions. We summarize this work here. \n\nScaling laws and other benchmarking efforts. Recent work has proposed \"scaling laws\" (Kaplan et al., 2020), which show how model size affects the training and validation loss of these models by fitting a curve to dozens of training runs. While these scaling laws are instructive, we also care about the capabilities of models along other axes beyond validation loss (e.g., are models robust; do they exhibit stereotypes?). Moreover, large language models have been shown to exhibit emergent behavior that cannot easily be expressed as a continuous function of scale (Wei et al., 2022). Similarly, even though model size is used as a proxy for both training and inference runtime performance, it is not interpretable when trying to answer questions like \"Can model X meet a latency SLO of 100 milliseconds?\" or \"How much will it cost to use model X in this concrete application with the following characteristics?\". Consequently, we need to fall back on empirical analysis to fully understand the capabilities of these models. \n\nVarious empirical analyses that focus on quantifying the capabilities of LLMs along various dimensions, including papers introducing new models like PaLM (Chowdhery et al., 2022) and Gopher (Rae et al., 2021), and more ambitious benchmarking efforts like BIG-bench (Srivastava et al., 2022), also use model size as a proxy for scale and runtime performance. These comparisons are often useful within a family of models (e.g., OpenAI Instruct series of models), but are less useful when trying to compare model families. \n\nFloating-point operations and other proxy metrics for efficiency. The number of floating-point operations (FLOPs) required for the forward pass of a model has also often been used to estimate inference efficiency. While this is a fine approximation, it is not ideal for a couple different reasons. First, runtime does not correlate exactly with the number of FLOPs required (Scao et al., 2022).",
            "score": 0.6342611788963004,
            "section_title": "RELATED WORK",
            "char_start_offset": 32075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "273404213",
            "title": "Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens",
            "text": "Kaplan et al. (2020) empirically observed that, for language model, the validation loss scales as a power-law with model size, dataset size, and the amount of compute used for training. Hoffmann et al. (2022) discovered that contemporary LLMs are under-trained, and that for compute-optimal training, the model size and the number of training tokens should be scaled equally. Under the same compute budget while using 4x more more data, their 70B Chinchilla outperforms much larger models, such as the 530B Megatron-Turing NLG (Smith et al., 2022). Wei et al. (2022) found that larger models have emergent abilities that are not present in smaller models. These observations have inspired significant efforts to scale up language models to trillions of parameters (Achiam et al., 2023;Team et al., 2023;Dubey et al., 2024). \n\nScaling vision models. Similar scaling law has been obscure for computer vision. For recognition models (Tan, 2019;Zhai et al., 2022;He et al., 2022;Dehghani et al., 2023), scaling often comes with diminishing returns. For example, Dehghani et al. (2023) scale ViT up from 3B to 22B but only observed 0.25% accuracy increase in ImageNet linear probing. The scaling of generative models is more promising -DiT (Peebles & Xie, 2023) shows consistent improvement in generation quality when scaling up compute and model size (despite only up to 600M). The follow-up Sora (OpenAI, 2024) further shows the potential to scale up for video generation. In this paper, we perform a comprehensive empirical study of AR models for text-to-image generation.",
            "score": 0.6339813624653695,
            "section_title": "RELATED WORK",
            "char_start_offset": 7026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1570
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 959,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 959,
                    "end": 975,
                    "matchedPaperCorpusId": "243985980"
                },
                {
                    "start": 975,
                    "end": 997,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 1058,
                    "end": 1080,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 1235,
                    "end": 1256,
                    "matchedPaperCorpusId": "254854389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4951171875
        },
        {
            "corpus_id": "273233741",
            "title": "Scaling Laws For Diffusion Transformers",
            "text": "Scaling laws in large language models (LLMs) (Kaplan et al., 2020;Hestness et al., 2017;Henighan et al., 2020;Hoffmann et al., 2022) have been widely observed and validated, suggesting that pretraining performance follows a power-law relationship with the compute C. The actual compute could be roughly calculated as C = 6N D, where N is the model size and D is the data quantity. Therefore, determining the scaling law helps us make informed decisions about resource allocation to maximize computational efficiency, namely, figure out the optimal balance between model size and training data (i.e., the optimal model and data scale) given a fixed compute budget. However, scaling laws in diffusion models remain less explored. \n\nThe scalability has already been demonstrated in diffusion models, especially for diffusion transformers (DiT). Specifically, several prior works (Mei et al., 2024;Li et al., 2024) reveal that larger models always result in better visual quality and improved text-image alignment. However, the scaling property of diffusion transformers is clearly observed but not accurately predicted. Besides, the absence of explicit scaling laws also hinders a comprehensive understanding of how training budget relate to model size, data quantity, and loss. As a result, we cannot determine accordingly the optimal model and data sizes for a given compute budget and accurately predict training loss. Instead, heuristic configuration searches of models and data are required, which are costly and challenging to ensure optimal balance. \n\nIn this work, we characterize the scaling behavior of diffusion models for text-to-image synthesis, resulting in the explicit scaling laws of DiT for the first time. To investigate the explicit relationship between pretraining loss and compute, a wide range of compute budgets from 1e17 to 6e18 FLOPs are used. Models ranging from 1M to 1B are pretrained under given compute budgets. As shown in Fig. 1, for each compute budget, we can fit a parabola and extract an optimal point that corresponds to the optimal model size and consumed data under that specific compute constraint. Using these optimal configurations, we derive scaling laws by fitting a power-law relationship between compute budgets, model size, consumed data, and training loss.",
            "score": 0.6338628372848942,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 894,
                    "end": 910,
                    "matchedPaperCorpusId": "268875775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6416015625
        },
        {
            "corpus_id": "270045429",
            "title": "AstroPT: Scaling Large Observation Models for Astronomy",
            "text": "We find ourselves in a new era of connectionism.This era is dominated by large language models trained on web-scale data, with a rapid parameter growth fertilised by the discovery of predictable 'neural scaling laws'-power laws that can be used to estimate a model's performance given its parameter count and training set size (Cortes et al., 1993;Kaplan et al., 2020).In the language domain, the law that currently best describes model scaling was introduced in Hoffmann et al. (2022).This 'Chinchilla' scaling law established that for every additional neural parameter added to the network, around twenty additional textual data tokens must also be added to train a model in a training compute optimal regime.It is also often wise to 'overtrain' a model-or train at a token to parameter ratio larger than 20:1-which leads to a more performant model at a lower parameter count and therefore reduces compute cost at inference time.Overtraining is both environmentally and economically beneficial if a model's total lifetime compute costs are inference heavy (Touvron et al., 2023a;b;Zhang et al., 2024).The discovery of the Chinchilla scaling law, and the need to minimise compute cost at inference time means that we have nearly exhausted high-quality publicly available reserves of textual data for training large neural models (Sevilla et al., 2022;Xue et al., 2023).Some potential solutions to this 'token crisis' have already been investigated in the literature: for example, one can repeat training dataset epochs multiple times without significant performance degradation (Xue et al., 2023), or one can also turn to the use of synthetic data to generate tokens at scale (Silver et al., 2018;Gunasekar et al., 2023).We believe that multimodality can provide a further solution.\n\nLarge autoregressive models can process and digest tokens of different modalities (Reed et al., 2022).We can therefore build a model that is capable of learning from modalities that have an abundance of tokens, an abundance that is particularly notable in the observational sciences (Smith & Geach, 2023;Smith et al., 2023a).",
            "score": 0.6331755650277645,
            "section_title": "On 'Large Observation Models'",
            "char_start_offset": 32,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 48,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1103
                },
                {
                    "start": 1103,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1722
                },
                {
                    "start": 1722,
                    "end": 1783
                },
                {
                    "start": 1785,
                    "end": 1887
                },
                {
                    "start": 1887,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 348,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 1677,
                    "end": 1698,
                    "matchedPaperCorpusId": "54457125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "273662300",
            "title": "Pushing the Limits of All-Atom Geometric Graph Neural Networks: Pre-Training, Scaling and Zero-Shot Transfer",
            "text": "Neural Scaling Laws describe empirically-derived power-law relationships between model performance and various scaling factors. These laws have been instrumental in understanding and predicting the behavior of large language models. Kaplan et al. (2020) identified key power-law relationships between the pre-training loss L and several variables: \n\nwhere N is the number of model parameters, D is the dataset size, and C is the amount of compute. These laws have elucidated performance disparities between different neural architectures, such as Long Short-Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) and Transformers (Vaswani et al., 2017), particularly in terms of pre-training loss. Subsequent work by Hoffmann et al. (2022) and Muennighoff et al. (2024) further refined these relationships, considering factors like optimal batch size, compute-optimal training, and data constraint. As shown in Figure 5, we scanned combinations of five commonly used functionals (Zhao & Truhlar, 2008;Haoyu et al., 2016;Adamo & Barone, 1999;Lee et al., 1988) and basis sets (Weigend, 2006;Ditchfield et al., 1971;Dunning Jr, 1989), resulting in 25 distinct computational methods using PySCF package (Sun et al., 2020). The standard deviations of the HOMO-LUMO gap predictions were substantial: 12571\u00b11775, 8411\u00b11626, and 9333\u00b11758 meV for gdb1, gdb2, and gdb3, respectively. Such uncertainty from DFT calculations has been widely studied in chemistry community (Zhang & Musgrave, 2007), especially the accuracy of B3LYP functional used in QM9 dataset is non-ideal per current standard (Zhao & Truhlar, 2005;Goerigk & Grimme, 2011;Mardirossian & Head-Gordon, 2017). The plot shows the relationship between the number of layers and the pre-training loss, illustrating the initial rapid improvement followed by diminishing returns as depth increases.",
            "score": 0.6326372062556356,
            "section_title": "C NEURAL SCALING LAW ON LANGUAGE",
            "char_start_offset": 30136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 618,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 750,
                    "end": 775,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 985,
                    "end": 1007,
                    "matchedPaperCorpusId": "98119881"
                },
                {
                    "start": 1007,
                    "end": 1026,
                    "matchedPaperCorpusId": "262116371"
                },
                {
                    "start": 1026,
                    "end": 1047,
                    "matchedPaperCorpusId": "96110585"
                },
                {
                    "start": 1047,
                    "end": 1064,
                    "matchedPaperCorpusId": "45348446"
                },
                {
                    "start": 1080,
                    "end": 1095,
                    "matchedPaperCorpusId": "42762306"
                },
                {
                    "start": 1095,
                    "end": 1119,
                    "matchedPaperCorpusId": "96872397"
                },
                {
                    "start": 1119,
                    "end": 1136,
                    "matchedPaperCorpusId": "54183898"
                },
                {
                    "start": 1205,
                    "end": 1223,
                    "matchedPaperCorpusId": "211572993"
                },
                {
                    "start": 1467,
                    "end": 1491,
                    "matchedPaperCorpusId": "1516019"
                },
                {
                    "start": 1591,
                    "end": 1613,
                    "matchedPaperCorpusId": "24061290"
                },
                {
                    "start": 1613,
                    "end": 1636,
                    "matchedPaperCorpusId": "41029213"
                },
                {
                    "start": 1636,
                    "end": 1669,
                    "matchedPaperCorpusId": "103560395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1778564453125
        },
        {
            "corpus_id": "276422075",
            "title": "Factual Inconsistency in Data-to-Text Generation Scales Exponentially with LLM Size: A Statistical Validation",
            "text": "Scaling laws for LLMs describe how their performance scales with key factors such as model size (number of parameters) and training data size. Hestness et al. (2017) demonstrated that deep language models follow a power law scaling, laying the foundation for scaling law research. Kaplan et al. (2020) expanded this by systematically analyzing model size, data size, and computational efficiency, reinforcing the dominance of power law scaling in LLM performance. As research on scaling laws has expanded, various studies have explored their applications across different task domains, including close-ended text generation (Bansal et al., 2022) and open-ended text generation (Kaplan et al., 2020). Recent investigations have further examined scaling in diverse paradigms, such as sparse modeling (Frantar et al., 2024) and parameter-efficient fine-tuning (Zhang et al., 2024). Additionally, joint scaling lawssuch as additive and multiplicative formulationsare gaining prominence in multi-factor scaling setups (Hoffmann et al., 2022;Zhang et al., 2024). Scaling laws offer several key advantages, including optimizing hyperparameter tuning (Hendrycks, forthcoming), estimating training costs (H\u00e4gele et al., 2024), and setting realistic expectations for model performance (Hoffmann et al., 2022). A recent study by Bahri et al. (2024) further reinforces the theoretical foundations of scaling laws.",
            "score": 0.631969265564104,
            "section_title": "Related Work",
            "char_start_offset": 4488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1401
                }
            ],
            "ref_mentions": [
                {
                    "start": 624,
                    "end": 645,
                    "matchedPaperCorpusId": "246608156"
                },
                {
                    "start": 798,
                    "end": 820,
                    "matchedPaperCorpusId": "262013578"
                },
                {
                    "start": 1318,
                    "end": 1337,
                    "matchedPaperCorpusId": "231918701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "13750639",
            "title": "Assessing Language Models with Scaling Properties",
            "text": "The performance of language models has generally been evaluated with perplexity (Manning and Schutze 1999), which quantifies the predictive power of such models. Since the development of ngram models, state-of-the-art neural language models have radically reduced perplexity. Although perplexity is easy to compute and its signification is comprehensible, we consider it to have two main drawbacks. First, perplexity does not provide information on how a model is limited with respect to the important linguistic aspects, such as generalization of word concepts, syntactic structure, and long-term dependency. Although perplexity indicates the overall performance across various aspects, we want to know more specifically which aspects are challenging for the language models. Second, we cannot obtain a lower bound of perplexity in a dataset and thus cannot quantitatively recognize the difference of a model from natural language or an ideal language model that perfectly reproduces natural language. \n\nTo compensate for these drawbacks of perplexity, we propose to assess language models with a set of scaling properties exhibited by natural language. The assessment is conducted by investigating whether the text generated by a language model would exhibit these scaling properties. We presented this scheme in (Takahashi and Tanaka-Ishii 2017) in which the reproducibility of the character level neural language models are investigated. This assessment has two advantages over the perplexity measure. First, since the scaling properties are designed to measure some aspect underlying a sequence, they can provide information on how limited a model is in terms of that aspect. Second, since the scaling properties quantify the behavior of a data set in terms of exponents, we can evaluate how much the text generated from a model differs from natural language. \n\nThe scaling properties in this article are roughly categorized into two types: those related to word frequency distribution and those related to long memory. For the first category, we consider the wellknown Zipf's law and Heaps' law. For the second category, we consider properties that quantify the memory underlying a sequence. We then test whether texts generated from various language models exhibit every property.",
            "score": 0.6309609229300849,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1864
                },
                {
                    "start": 1867,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2197
                },
                {
                    "start": 2198,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 1315,
                    "end": 1348,
                    "matchedPaperCorpusId": "5682121"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224365234375
        },
        {
            "corpus_id": "258959511",
            "title": "Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale",
            "text": "Scaling Laws Kaplan et al. (2020) has demonstrated and popularized power law dependency between language model parameter count and per-plexity. This further motivated the existing trend for increasing model size (Brown et al., 2020;Chowdhery et al., 2022;Fedus et al., 2022). Investigation of smaller models has mostly focused on distillation (Sanh et al., 2019;Turc et al., 2019) and achieving the best performance given the parameter count. In contrast, we are interested in understanding at what scale pre-training works and the emergence of language model abilities to help downstream tasks. \n\nChanging pre-training data size via training token volume (P\u00e9rez-Mayos et al., 2021;Zhang et al., 2020), vocabulary size (Gowda and May, 2020), or the number of epochs (Voloshina et al., 2022) has also been explored for effects on language acquisition. These studies have generally demonstrated low-level linguistic tasks involving syntax require relatively little data volume compared to more complex tasks like WinoGrande (Sakaguchi et al., 2019). The relationship between model size, input data, and downstream performance remains the subject of much debate as to the nature of scaling laws. Hoffmann et al. (2022) concluded data size and model size should be scaled equally to achieve compute-optimal training for LLM's. Further credence to reducing LLM compute is lent by Sorscher et al. (2022), who found input data pruning can improve models to scale exponentially, and Chan et al. (2022), who showed LLM in-context learning derives from the Zipfian distribution of pre-training data. \n\nMost releavant to this work is Huebner et al. (2021) who found a small language model trained on child-directed speech can achieve comparable performance to larger LMs on a set of probing tasks. In contrast to them, we train multiple models, explore scaling in the low-compute region and evaluate on a filtered version of GLUE instead of a set of linguistic tests.",
            "score": 0.6284194454794746,
            "section_title": "Related Work",
            "char_start_offset": 3860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 274,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 656,
                    "end": 682,
                    "matchedPaperCorpusId": "237433614"
                },
                {
                    "start": 719,
                    "end": 740,
                    "matchedPaperCorpusId": "222141900"
                },
                {
                    "start": 766,
                    "end": 790,
                    "matchedPaperCorpusId": "250244076"
                },
                {
                    "start": 1022,
                    "end": 1046,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1375,
                    "end": 1397,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1475,
                    "end": 1493,
                    "matchedPaperCorpusId": "248665718"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19580078125
        },
        {
            "corpus_id": "268358466",
            "title": "Algorithmic progress in language models",
            "text": "We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 95% confidence interval of around 5 to 14 months, substantially faster than hardware gains per Moore's Law. We estimate augmented scaling laws, which enable us to quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period. Though limited by noisy benchmark data, our analysis quantifies the rapid progress in language modeling, shedding light on the relative contributions from compute and algorithms.",
            "score": 0.6271026064804077,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6220703125
        },
        {
            "corpus_id": "270063468",
            "title": "gzip Predicts Data-dependent Scaling Laws",
            "text": "We generate 6 datasets of varying complexity by modulating syntactic properties of a PCFG.For each of these datasets, we train LMs of 6 different sizes (4.4M to 1.4B parameters) and record results at 6 different train step counts (100K to 100M tokens).We then fit a scaling law for each dataset [Muennighoff et al., 2023], finding meaningful shifts in the law's parameters as syntactic complexity increases.Following prior work on entropy of formal grammars [Arora et al., 2022], we use median compressibility of each token sequence in a dataset as a measure of complexity that is straightforward to compute with gzip [Gailly and Adler, 1992].\n\nWe find that as the training data becomes less compressible (more complex), the scaling law's compute-optimal frontier gradually increases its preference for dataset size over parameter count.We then measure the compressibility of real-world code & natural language datasets, showing that the former is considerably more compressible and thus subject to a predictably different scaling law.With some napkin math, we estimate that we could reach the performance of StarCoder [Li et al., 2023] with 24% fewer FLOPs ($278,000 in H100 hours) since our compute-optimal scaling adjusts for data complexity.",
            "score": 0.6270713603152559,
            "section_title": "arXiv:2405.16684v1 [cs.CL] 26 May 2024",
            "char_start_offset": 2446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 90,
                    "end": 252
                },
                {
                    "start": 252,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 643
                },
                {
                    "start": 645,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1245
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 321,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 458,
                    "end": 478,
                    "matchedPaperCorpusId": "247940181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388916015625
        },
        {
            "corpus_id": "268064306",
            "title": "Sequence modeling and design from molecular to genome scale with Evo",
            "text": "Aiding our model design, we performed the first scaling laws analysis (to our knowledge) for DNA sequence modeling. The main objective of this type of analysis is to determine the relationship between training, architectural details, and performance metrics via a systematic experimental protocol (Hoffmann et al., 2022;Kaplan et al., 2020). Once a set of scaling laws is obtained, it can then be used as a guide to optimally scale training to larger models and datasets. \n\nHere, we compare different classes of architectures via a compute-optimal protocol, aimed at evaluating results on the compute-optimal frontier (Methods). We trained over 300 models across four architectures: Transformer++, Mamba, Hyena, and StripedHyena. Transformer++ is a state-of-the-art Transformer, and Mamba is a modern architecture using data-controlled state-space models (Gu and Dao, 2023). \n\nWe found Transformer++ to yield significantly worse perplexity (a measure of next token prediction quality) at all compute budgets (Figures 1G), a symptom of the inefficiency of the architecture at the byte resolution. State-space and deep signal processing architectures are observed to improve on the scaling rate over Transformer++, with Hyena and StripedHyena resulting in the best scaling rate. We observed stable training for StripedHyena throughout all the studied model sizes and learning rates during the scaling analysis. \n\nWe also compare architecture performance outside the compute-optimal frontier, namely with allocations of the computational budget that may be suboptimal. Performance outside the compute-optimal frontier is important in practice, as most models (including Evo) are trained for more tokens than recommended by compute-optimal scaling laws. We estimate 250 billion to be the compute-optimal number of tokens for Evo 7B given the FLOP budget, meaning the model was trained at a 17% offset from the compute-optimal model size during the initial 8192 sequence length pretraining phase of 300 billion tokens. Both Transformer++ and Mamba experienced numerical instability during training, and suffered from a sharper performance degradation of the scaling rate outside the compute-optimal frontier, in contrast to StripedHyena (further analysis in Figure S3).",
            "score": 0.6268877744304111,
            "section_title": "StripedHyena demonstrates favorable scaling laws on DNA sequence data",
            "char_start_offset": 9995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.102294921875
        },
        {
            "corpus_id": "256826923",
            "title": "The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment",
            "text": "Natural language processing systems have benefited from improvements in performance driven by scaling of training data and number of model parameters (Kaplan et al., 2020;Alabdulmohsin et al., 2022;Tay et al., 2021Tay et al., , 2022)). However, the accompanying increases in computation raise concerns as to the efficiency of these systems due to associated environmental costs of development and deployment (Schwartz et al., 2020;Strubell et al., 2019). \n\nIn particular, efficiency is especially important in inference settings where models are used repeatedly and at scale, in contrast to training which poses a single upfront computational cost. For example, Meta reports that inference workloads make up 70% of their AI power consumption, with the remaining 30% due to training and development (Wu et al., 2022), while Google attributes 60% of their ML energy consumption to inference (Patterson et al., 2022). Inference is also estimated to make up 80 to 90% of ML cloud computing demand (Barr, 2019;Leopold, 2019). In these settings, metrics of model speed such as latency and throughput are essential for inference workloads that are subject to real wall-clock time constraints such as real-time natural language generation and automated speech recognition (Reddi et al., 2020). These concerns have motivated research in designing more efficient neural network model architectures and faster hardware accelerators. In the past five years alone, the number of papers that mention the terms efficient or efficiency in top machine learning venues has grown by over 2.5x and even more so at venues in natural language processing, increasing by 8.3x in the same span. 1 This has spurred innovations in the design of efficient neural network architectures for language aiming to reduce the number of trainable model parameters, floating point or multiply-accumulate operations (MACs) (Iandola et al., 2020;Dai et al., 2020;Sun et al., 2020).",
            "score": 0.6259589998093678,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 198,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 408,
                    "end": 431,
                    "matchedPaperCorpusId": "198229505"
                },
                {
                    "start": 431,
                    "end": 453,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 798,
                    "end": 815,
                    "matchedPaperCorpusId": "240354766"
                },
                {
                    "start": 889,
                    "end": 913,
                    "matchedPaperCorpusId": "246840687"
                },
                {
                    "start": 1264,
                    "end": 1284,
                    "matchedPaperCorpusId": "207880425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3740234375
        },
        {
            "corpus_id": "276580891",
            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
            "text": "Our findings have important practical implications for the deployment of large language models, as they can help organizations more accurately forecast compute requirements and make informed trade-offs between model size, inference costs, and performance targets. The mathematical framework we develop could also generalize beyond language models to other domains where similar scaling phenomena emerge. While our work is primarily theoretical, we acknowledge that advances in language model capabilities can have broad societal impacts. We hope that better understanding these fundamental scaling behaviors will help the research community develop more efficient and reliable AI systems. \n\nof",
            "score": 0.6254129495633101,
            "section_title": "Impact Statement",
            "char_start_offset": 22120,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 693
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80908203125
        },
        {
            "corpus_id": "273228566",
            "title": "Scaling Laws For Mixed Qquantization",
            "text": "Enhanced language modelling performance at larger model sizes has led to interests in characterising scaling laws of LLM performance with respect to parameter count, training compute budget and number of training tokens. Kaplan et al. (2020) showed, though empirical analysis, that Transformer performance follows a power law trend with each of these factors. The authors proposed that for any 9\u00d7 increase in parameter count, dataset size should be increased by a factor of approximately 5\u00d7 to avoid a performance penalty, suggesting that higher performance gains are observed from scaling parameter count than dataset size under a fixed compute budget. Contrastingly, Hoffmann et al. (2022) argue that existing open weight LLMs are undertrained relative to their size, and parameter count should be increased in line with the number of training tokens. The findings from Pearce & Song (2024) later explained the discrepancy between Kaplan and Hoffman, reaffirming the validity of the Chincilla scaling laws and highlighting the need for high quality datasets for Language Model training. \n\nDespite the reconciliation of Kaplan's scaling laws, we see the continued trend of scaling LLM sizes, partly validated by the findings from Wei et al. (2022a) regarding the unpredictable emergence of abilities in larger LLMs. The authors characterise how LLM performance in few-shot tasks including arithmetic, question answering and multi-task language understanding emerges beyond certain size thresholds, despite not being observable in small models -for example, performance on arithmetic tasks from BIG-Bench is approximately random for GPT-3 models up to 13B parameters and LaMDA models up to 68B parameters, sharply rising thereafter. The authors note there are few compelling explanations for these emergence phenomena, although the required model depth for reasoning tasks may be correlated with the number of reasoning steps. At any rate, these findings raise the question of what emergent phenomena may observed for even larger models, and highlight the importance of understanding scaling laws across a wide range of model sizes, without extrapolating observations from small models.",
            "score": 0.6254104796546649,
            "section_title": "SCALING LAWS OF LARGE LANGUAGE MODEL TRAINING",
            "char_start_offset": 9839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43017578125
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "Our key findings for Transformer language models are are as follows: Figure 1 Language modeling performance improves smoothly as we increase the model size, datasetset size, and amount of compute 2 used for training. For optimal performance all three factors must be scaled up in tandem. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two. \n\nPerformance depends strongly on scale, weakly on model shape: Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embeddings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section 3) Smooth power laws: Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance must flatten out eventually before reaching zero loss. (Section 3) Universality of overfitting: Performance improves predictably as long as we scale up N and D in tandem, but enters a regime of diminishing returns if either N or D is held fixed while the other increases. The performance penalty depends predictably on the ratio N 0.74 /D, meaning that every time we increase the model size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4)",
            "score": 0.6248877473921475,
            "section_title": "Summary",
            "char_start_offset": 1267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1597
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "258959511",
            "title": "Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale",
            "text": "In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below $2.2 \\times 10^{15}$ FLOPs. We also find that adding layers does not always benefit downstream performance.",
            "score": 0.6246167872967656,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "271571035",
            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
            "text": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.",
            "score": 0.6232342499845855,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "273228196",
            "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
            "text": "The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size/learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.",
            "score": 0.6225578879406044,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "253018395",
            "title": "Transcending Scaling Laws with 0.1% Extra Compute",
            "text": "There has been significant interest in scaling of language models (Rae et al., 2021;Chowdhery et al., 2022;Brown et al., 2020). Scaling has inspired new research across multiple fronts, e.g., scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022;Tay et al., 2022a), emergent abilities (Wei et al., 2022a;Ganguli et al., 2022), reasoning capabilities (Wei et al., 2022b;Lewkowycz et al., 2022), inter alia. Generally, scaling laws predict a continued improvement in language model quality as we continue to scale up the computational budget (e.g., bigger models or more data). To date, most large language models that form the basis of scaling law research are trained almost exclusively as left-to-right causal language models (Kaplan et al., 2020;Hoffmann et al., 2022). \n\nThis paper proposes a new method to dramatically improve the scaling curves of large language models on downstream performance with a relatively tiny amount of additional computation cost. The key idea is to continue training an existing causal language model (Chowdhery et al., 2022) with a mixture of new objectives-specifically, the UL2 training objective mixture (Tay et al., 2022b). This restoration is expected to only cost roughly 0.1% to 1% of the original training FLOPs and requires no new data sources, making it highly efficient and convenient. We call this approach UL2R or UL2Restore. \n\nThe UL2 objective combines prefix language modeling and long-short span corruption (e.g., infilling) tasks (Raffel et al., 2019) that can be controlled at inference time using a mode switching prompt. Training a large language model with UL2 can be interpreted as teaching it to leverage bidirectional attention (i.e., PrefixLM) or leverage infilling-style pretraining that have been the foundation of language understanding (e.g., T5 (Raffel et al., 2019)).",
            "score": 0.6207332751120684,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1835
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 306,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 352,
                    "end": 371,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492431640625
        },
        {
            "corpus_id": "268691910",
            "title": "Mechanistic Design and Scaling of Hybrid Architectures",
            "text": "Finding 8: There exists a relation of the type P * \u221d M c between compute-optimal perplexity P * and total state size M , with c \u2248 \u22120.28 in our scaling experimental setup, consistent across all model architectures. The model class determines the offset of the state-optimal curve. Concretely, state-optimal scaling indicates that one may reach any target perplexity (up to saturation of compute-optimal scaling laws i.e., approaching entropy of text) with fixed-state architectures, by paying a FLOP cost multiplier that depends on the model class -training longer to maximize state utilization. Input-varying recurrences, multihead and striped hybrid architectures achieve a favourable trade-off between metrics, with comparable or improved compute-optimal perplexity to Transformers++ and a reduced total state dimension. \n\n4.3 Compute-optimal scaling at byte resolution 8 Accounting for state-optimality shifts the optimal ratio to 10%. Compute-optimal and state-optimal scaling on The Pile. We report total state dimension, fixed (recurrences) and dynamic (attention). All models are trained at sequence length 8k. We identify distinct regions in the state-optimal frontier, indicating that one may pay an additional FLOP cost to obtain the same perplexity with a state of smaller dimension, by using other classes of architectures. Scaling laws analysis primarily focus on sub-word level tokenization. With a new range of architectural options, we also explore computeoptimal scaling of a subset of architectures (Transformer++, Mamba, Hyena and StripedHyena) at byte resolution. We scale the models across FLOP budgets from 8e18 to 8e19 with model sizes from 6M to 1B parameters. The compute-optimal frontier is obtained using a similar protocol as outlined in Sec. C, with additional details and results shown in Sec. D.2. \n\nWe find attention-based models to yield significantly higher perplexity at all IsoFLOP groups, with alternative architectures outperforming Transformer++, including non-striped variants (Figure 4.4). These results show that model ranking varies significantly across domains and tokenization strategies.",
            "score": 0.619573563264493,
            "section_title": ".3, right).",
            "char_start_offset": 28178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2133
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18798828125
        },
        {
            "corpus_id": "276618147",
            "title": "(Mis)Fitting: A Survey of Scaling Laws",
            "text": "Finally, the variable must be fit with an objective and optimization method, which may in turn have their own initialization and hyperparameters to choose. \n\nTo aid scaling laws researchers in reporting details necessary to reproduce their work, we propose a checklist (Figure 1 -an expanded version may be found in Appendix B ). Based on this checklist, we summarize these details for all 51 papers in tabular form in Appendix C. We find that important details are frequently underreported, significantly impacting reproducibility, especially in cases where there is no code -only 19 of 42 papers surveyed have analysis code/code snippets available. Additionally, 23 (a little over half) of surveyed papers do not describe the optimization process, and 15 do not describe how training FLOPs or number of parameters are counted, which has been found to significantly change results (Porian et al., 2024). In addition, we fit our own power laws to further demonstrate how these choices critically impact the final scaling law (Section \u00a77). \n\nThe authors also claimed that the model size required to fit a given dataset grows log linearly. As for larger scale models, Amodei et al. (2016) observe a power-law WER improvement on increasing training data for a 38M parameter Deep Speech 2 model. Hestness et al. (2017) show similar power law relationships across several domains such as machine translation, language modeling, image processing and speech recognition. Moreover, they find that these exponential relationships found hold across model improvements. Kaplan et al. (2020) push the scale of these studies further, studying power laws for models up to 1.5B parameters trained on 23B tokens to determine the optimal allocation of a fixed compute budget. \n\nLater studies (Hoffmann et al., 2022;Hu et al., 2024) revisit this and find that Kaplan et al. (2020) greatly underestimate the amount of data needed to train models optimally, though major procedural differences render it challenging to attribute the source of this discrepancy. Since then, researchers have studied various aspects of scaling up language models.",
            "score": 0.6194669666149026,
            "section_title": "\u2022 Optimization Hyperparameters",
            "char_start_offset": 4399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 1166,
                    "end": 1186,
                    "matchedPaperCorpusId": "11590585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30224609375
        },
        {
            "corpus_id": "276776461",
            "title": "Scaling Law Phenomena Across Regression Paradigms: Multiple and Kernel Approaches",
            "text": "The scaling behavior of deep learning models concerning compute, sample size, and model size has been a central topic in machine learning research [HNA + 17, RRBS19, BMR + 20, HKK + 20, HBM + 22, ZKHB22, MRB + 23]. A pivotal contribution by [KMH + 20] revealed that generalization error in transformers decays as a power law with respect to these three factors. Their work provided joint formulas predicting how model performance improves with increased compute, data, and parameters. This finding highlighted the importance of scaling models and led to a surge in interest in large-scale models for NLP and other tasks. \n\nFurther research refined these scaling laws. For instance, [HBM + 22] proposed the Chinchilla law, which suggested that under compute constraints, balancing data size with model size yields optimal performance, contrasting with earlier models that focused primarily on increasing parameter counts. Meanwhile, [MRB + 23] introduced methods for data reuse across training passes, showing that efficient data handling can maintain performance while lowering the need for excessive compute resources. These empirical studies have provided clearer guidelines for allocating resources effectively when training large models. \n\nIn parallel, theoretical work has advanced understanding of these scaling laws. [SK20] demonstrated that, in regression, the generalization error scales as n \u22124/d , linking model performance to the intrinsic dimensionality of the data. Additionally, other researchers, such as [BDK + 21] and [BAP24], applied statistical physics to derive scaling laws for linear models, particularly under infinite parameter regimes. Theoretical contributions like these complement empirical findings and offer deeper insights into how models behave as they scale, shaping future directions in machine learning research.",
            "score": 0.619124575209897,
            "section_title": "Scaling Laws",
            "char_start_offset": 4507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1848
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44091796875
        },
        {
            "corpus_id": "277271576",
            "title": "Long Is More Important Than Difficult for Training Reasoning Models",
            "text": "Scaling laws have emerged as a crucial tool in the research of large language models (LLMs) for guiding model design and predicting performance. During the training phase, researchers [11] have identified that model performance scales predictably with model size, dataset size, and computational resources. In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters , and it is proposed that optimizing inference strategies can significantly improve inference accuracy. Our research introduces data length as a crucial scaling factor, demonstrating that the performance of LLMs in multi-step mathematical reasoning scales significantly with the length of reasoning chains.",
            "score": 0.6189412311883326,
            "section_title": "Scaling law on LLM",
            "char_start_offset": 23880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 865
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "260334352",
            "title": "A Theory for Emergence of Complex Skills in Language Models",
            "text": "Language modeling follows a statistical paradigm: occurences of linguistic units (say, words, sentences, paragraphs) are assumed to fit a statistical profile, and thus pieces of text from a sufficiently large and diverse corpus are assumed to be samples from a probability distribution Bengio et al. [2000]. Language models are trained to solve Next-word-prediction tasks: Given, say, the past 500 words2 in the text, predict the next word. Faced with such a task, humans may be able to give many completions, and the modeling assumes that frequencies of these completions can be modeled via probability. Thus the model M computes a probability Pr M [w i+1 | w 1 w 2 . . . w i ] for all possible words w i+1 's. The goodness of a model is computed by its cross entropy loss, which for a sequence of words w 1 w 2 . . . w t is: \n\nModels are trained by minimizing (via gradient descent) this training loss on a text corpus, and their goodness is computed by their test loss-evaluating the same loss expression on a held-out text from the same corpus. Often the training corpus is so large that the model trains only once (or a few times) on each piece of text, and by the end, the test loss on held-out text is almost the same as the training loss. \n\nScaling laws: These empirically-derived expressions describe how test cross entropy loss on heldout data scales (in experiments) with number of model parameters (N) and size of the dataset (D) Cortes et al. [1993], Hestness et al. [2017], Kaplan et al. [2020], Bahri et al. [2021]. For Chinchilla models Hoffmann et al. [2022] the law is as follows (precise constants of 2 hold only for the specific architecture and training strategy -even the constant A depends upon the tokenization): \n\nThis description of macro behavior using two basic parameters -reminiscent of 2nd Law of Thermodynamics-will help us circumvent the need for mechanistic understanding of training.",
            "score": 0.6188002714433768,
            "section_title": "Preliminaries",
            "char_start_offset": 5840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1736
                },
                {
                    "start": 1739,
                    "end": 1918
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 306,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 1442,
                    "end": 1462,
                    "matchedPaperCorpusId": "1364116"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30517578125
        },
        {
            "corpus_id": "277667736",
            "title": "On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition",
            "text": "Scaling laws have been observed in statistical learning since Cortes et al. [10] proposed power laws for model performance as a function of data size. However, scaling behaviour has only recently been extensively studied, with the availability of compute and internet-scale datasets. Notably, scaling laws have been extensively explored in large language model training [26,25,4,24,34,52], establishing a power law relationship between the number of tokens in the dataset and number of model parameters. Hoffmann et al., [25] formulated scaling laws for compute-optimal training, identifying over-training and under-training regimes for language models, given a compute budget. Furthermore, Hernandez et al. [24] formulated scaling laws for transfer learning, in which there is a predictable downstream performance in terms of the ratio between the amount of pretraining data and amount of fine-tuning data. In general, scaling laws fall under two broad categories of data scaling laws and model scaling laws. Some works [4,33] identified problems in which scaling up does not improve upon downstream performance. Caballero et al. [4] formulated a generalization of scaling laws and showed that a piecewise linear modelling of scaling is more appropriate when analyzing scaling across a wide range of orders of magnitude. \n\nAside from language modelling, scaling laws have been established for other domains as well, for example, for machine translation [2], masked image modelling [48], contrastive language-image learning [9] and recommendation models [41]. For computer vision [37], the need for scaling informed further architectural developments in vision transformers [16,1] for efficient distributed training. \n\nRegarding gait processing, scaling analysis has not been extensively performed to date. Several large-scale datasets have been proposed [58,57,11,18], but lack the magnitude and diversity for properly studying data scaling in realistic environments. Cosma et al., [12] combined multiple existing gait datasets into a larger set of 800k sequences for training an autoencoder-type model. However, they did not further explore the effect of the data scale. Previously, works in self-supervised learning have noticed improved downstream performance with increase in data scale [14,11], but the amount of data is relatively small to provide insights into scaling behaviour.",
            "score": 0.618196381809031,
            "section_title": "Related Work",
            "char_start_offset": 5675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 377,
                    "end": 379,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1021,
                    "end": 1024,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "259188012"
                },
                {
                    "start": 1131,
                    "end": 1134,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 1454,
                    "end": 1457,
                    "matchedPaperCorpusId": "246608156"
                },
                {
                    "start": 1482,
                    "end": 1486,
                    "matchedPaperCorpusId": "249538409"
                },
                {
                    "start": 1524,
                    "end": 1527,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 1554,
                    "end": 1558,
                    "matchedPaperCorpusId": "244477730"
                },
                {
                    "start": 1674,
                    "end": 1678,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 1678,
                    "end": 1680,
                    "matchedPaperCorpusId": "258832817"
                },
                {
                    "start": 1855,
                    "end": 1859,
                    "matchedPaperCorpusId": "266520895"
                },
                {
                    "start": 1859,
                    "end": 1862,
                    "matchedPaperCorpusId": "244906176"
                },
                {
                    "start": 1862,
                    "end": 1865,
                    "matchedPaperCorpusId": "252263212"
                },
                {
                    "start": 1865,
                    "end": 1868,
                    "matchedPaperCorpusId": "250089142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47900390625
        },
        {
            "corpus_id": "253237527",
            "title": "A Solvable Model of Neural Scaling Laws",
            "text": "models, gathering more data, or burning their FLOPS. Thus, given the breadth of this empirical investigation over a number of orders of magnitude, the existence of these neural scaling laws, as they've been dubbed, have led many to believe a scaling hypothesis [19]: performance on language modeling tasks can be made predictably good simply by taking current transformer models and continuing to scale up parameters, data, and compute.\n\nAfter such a study, a number of follow ups appeared showing even more general applicability and more detailed understanding [20][21][22][23] -and even improved performance scaling with data size from a power law to an exponential falloff with clever pruning [24]. 1 At the same time, autoregressive generative modeling with transformers has continued to be applied to broader AI tasks such as coding [27], quantitative reasoning [28], and even on the suite of computer vision tasks, with the advent of the Vision Transformer (ViT) [29] family of models.\n\nGiven the ever growing breadth of tasks that these models can accomplish [30] and given their continuing gains in performance as we engineer ever bigger models and scrape ever bigger datasets, it is increasingly important to understand the origin of these neural scaling laws. The set of important questions include:\n\n\u2022 What are the properties of datasets and tasks that lead to scaling laws?\n\n\u2022 Which classes of models support scaling laws when trained on these datasets?\n\n\u2022 How do scaling laws arise, or what mechanism leads to such predictive behavior?\n\n\u2022 Can this predictive behavior break down, and what happens in such regimes?\n\nAddressing these questions can not only help us improve our AI systems practically, but also help us better understand the structure of AI tasks -such as language modeling -that seem to require gigantic amounts of data to reach (approximate) human-level performance. 2 In this paper, we will provide some initial answers to these questions by jointly studying a generative data model and random feature model that together exhibit neural scaling laws analogous to those found in [18]. This provides a theoretical framework for deriving the observed phenomenology in a class of large-parameter and big-data models, just like the \"microscopic\" framework of statistical mechanics can be used to derive the \"macroscopic\" laws of therm",
            "score": 0.6181080811387453,
            "section_title": "Introduction",
            "char_start_offset": 1770,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "243865620"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328369140625
        },
        {
            "corpus_id": "273162494",
            "title": "Scaling Parameter-Constrained Language Models with Quality Data",
            "text": "In this work, we introduce a scaling law for language modeling systems, defined as \u011c(N, \n\nHere, \u011c(N, D) estimates accuracy, with N as model size and D as dataset size. Constants E, A, \u03b1, B, \u03b2, c1, and c2 are parameters to be determined. \n\nThe estimation of this scaling law constants involved analyzing a dataset of 210 data points, each representing different model and dataset sizes with corresponding training losses and accuracy scores. These estimation accounted for the refinement of the training data that incorporate additional factors such as diversity and syntheticity into the dataset size. Further, different transformations of the dataset size were included to determine how these factors could be integrated effectively. The accuracy of the model was then obtained for each of these refinements. This comprehensive dataset allowed for robust parameter estimation. Parameter estimation was achieved through nonlinear curve fitting, aiming to align the scaling law's predictions with observed training losses. The process included: During curve fitting, the goodness of fit was assessed using the R-squared value, which measures the proportion of variance in the observed data that is predictable from the model inputs. This iterative process of refinement and evaluation helped in achieving the best possible fit between the predicted and observed accuracies, enhancing the scaling law's ability to predict training losses across various settings. We stop the iteration at 200. This process refined the estimates of E, A, \u03b1, B, \u03b2, c1, and c2, enhancing the scaling law's ability to predict training losses across various settings, thus supporting efficient resource allocation and model design in language modeling. The refined constants provided a more accurate description of how training loss scales with changes in model size and dataset size, incorporating the effects of diversity and syntheticity through c1 and c2.",
            "score": 0.6176503681544189,
            "section_title": "D Scaling Law Constant Estimation",
            "char_start_offset": 22532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1935
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489501953125
        },
        {
            "corpus_id": "268358466",
            "title": "Algorithmic progress in language models",
            "text": "4.1 Summary of our findings This paper presents a comprehensive empirical analysis of algorithmic progress in language model pre-training from 2012 to 2023.By curating a dataset of over 200 language model evaluations on WikiText and Penn Treebank benchmarks, we quantify the relative contributions of compute scaling and algorithmic efficiency improvements to the overall performance gains.Our key findings are as follows:\n\nFirst, we estimate that the compute required to reach a set language modeling performance level has halved every 8-9 months on average since 2012.This rate significantly exceeds hardware gains per Moore's law and places language modeling among the fastest advancing domains in algorithmic progress, alongside computer vision and reinforcement learning.This supports the common intuition that language modeling is an unusually rapidly-advancing field of computer science.\n\nSecond, our work reveals that the majority of recent advancements in language modeling stem more from scaling models and datasets than from pre-training algorithmic innovations.A Shapley value-based analysis suggests that 60-95% of the performance gains stem from compute scaling, while algorithms contribute only 5-40%.\n\nThird, the introduction of the transformer architecture in 2017 was a major algorithmic advance, representing between 3x and 46x in compute-equivalent gain, which accounts for more than 10% of the algorithmic innovation in pretrained language models in the past decade.This highlights the significance of the transformer as a key architectural breakthrough in the field.",
            "score": 0.6169608363238839,
            "section_title": "Discussion and conclusion",
            "char_start_offset": 30932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 422
                },
                {
                    "start": 424,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 894
                },
                {
                    "start": 896,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1216
                },
                {
                    "start": 1218,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1588
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62646484375
        },
        {
            "corpus_id": "257985427",
            "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster",
            "text": "Recent research in large language models (LLMs) shows important advances that can improve LLM quality and efficiency. Scaling law studies show predictable and significant improvements in model performance by increasing model and dataset size (Hestness et al., 2017;Kaplan et al., 2020). Language models can also be improved just by training on more data (Hoffmann et al., 2022;Touvron et al., 2023). Recent works, such as Maximal Update Parameterization (\u00b5P), also show techniques to improve training stability and performance as models scale up (e.g., Bachlechner et al. (2020); Yang et al. (2021)). \n\nConcurrently with these advances, the research community has trained and released many open-source models. Models like GPT-J, GPT-NeoX, OPT, and Pythia have each held state-of-the-art accuracy for open source models for their size, and these models can be tested and used simply by downloading the pretrained weights (Wang & Komatsuzaki, 2021;Black et al., 2022;Zhang et al., 2022;Biderman et al., 2023). While these models are important contributions, they have not aimed to be compute-efficient. The research community needs more reproducible scaling efforts that can guide collective decisions about training large foundation models in a compute-efficient way. \n\nWe introduce Cerebras-GPT, our open effort to combine recent LLM efficient scaling techniques to produce compute-optimal pre-trained models and corresponding scaling laws. Cerebras-GPT is a family of GPT-3like models that we scale from 111M to 13B parameters. We train them on the open-source dataset, the Pile (Gao et al., 2020), following DeepMind's Chinchilla scaling rules (Hoffmann et al., 2022). Cerebras-GPT models show state-of-the-art training efficiency when targeting both upstream Pile evaluations as well as a suite of downstream tasks. Our largest model shows state-of-the-art performance on pre-training and most downstream tasks compared to other comparably-sized public models.",
            "score": 0.6164425375927368,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 580,
                    "end": 598,
                    "matchedPaperCorpusId": "263885307"
                },
                {
                    "start": 946,
                    "end": 965,
                    "matchedPaperCorpusId": "248177957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.391357421875
        },
        {
            "corpus_id": "270371652",
            "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
            "text": "Having developed an effective procedure to automatically scale the initialization and learning rates for structured layers, we now aim to understand how various structures compare in performance.\n\nWhen data is not a bottleneck, a neural network's test error or loss on a task follows a power law E \u221d P \u2212\u03b1 P if trained to (near) convergence, where P is the number of parameters and \u03b1 P is a constant (Kaplan et al., 2020;Hoffmann et al., 2022;Henighan et al., 2020).For dense models, compute per forward pass C \u221d P , so E \u221d C \u2212\u03b1 C for some constant \u03b1 C .We explore how different structures change how E scales with C, as P does not consistently relate to training or inference cost when varying the structure (Table 1).\n\nWe train all models for a fixed number of iterations T , so the total training compute C tot \u221d C. Thus, the scaling laws in C can differ from compute-optimal scaling laws, which require carefully optimizing the allocation of C tot \u221d CT between C and T (Kaplan et al., 2020;Hoffmann et al., 2022), which we leave to future work.\n\nTo compare multiple structures across compute scales, we conduct experiments primarily using MLPs and ViTs on CIFAR-10 and CIFAR-100.In Section 5, we present largerscale experiments on ImageNet and language modeling.With limited training data in CIFAR-10 and CIFAR-100, we apply heavy augmentation to alleviate over-fitting.The augmented training set is sufficiently large, resulting in relatively clean power-law scaling of training error with C. We extract these power law parameters, reflecting the expressivity afforded by each structure as a function of C, and visualize the scaling of test error with C, which is not well-described by a power law due to train-test discrepancy.",
            "score": 0.6152115280184831,
            "section_title": "Scaling Laws of Structured Matrices",
            "char_start_offset": 21342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 197,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 718
                },
                {
                    "start": 720,
                    "end": 1047
                },
                {
                    "start": 1049,
                    "end": 1182
                },
                {
                    "start": 1182,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1373
                },
                {
                    "start": 1373,
                    "end": 1732
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392822265625
        },
        {
            "corpus_id": "271270413",
            "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
            "text": "We find power law fits with respect to the optimal non-vocabulary parameters, vocabulary parameters, and the number of training characters, respectively. \n\nResults and Usage In Figure 5, we display the fitted power laws: N nv = 0.08 * C 0.50 , N v = 0.20 * C 0.42 and H = 6.42 * C 0.50 , where C is the FLOPs budget.The low RMSE and high R 2 values indicate the strength of our fit. Given a certain FLOPs budget, we can utilize the aforementioned relationships to obtain the optimal allocation (N nv , N v , H). We also draw the following conclusions: \n\n(1) LLMs are data-hungry. Compared to the non-vocabulary parameters N nv , practitioners should allocate more compute to the training data [80,44]. (2) Vocabulary parameters scale in a powerlaw relation with FLOPs (N v \u221d C 0.42 ). As models become more computationally intensive, a larger vocabulary enhances the model's ability to understand a more diverse array of text, and thus the vocabulary size is critical to scaling. (3) Vocabulary parameters N v should be scaled slower than non-vocabulary parameters N nv . This difference can be seen in their power law exponents, i.e. \u03b3 = 0.42/0.50 = 0.84 < 1. We hypothesize the reason is that: once a sufficiently rich embedding space is present via a large vocabulary, it is more critical to scale non-vocabulary parameters to learn the intricate syntactic and semantic structures of language via Transformer blocks.",
            "score": 0.614602232488708,
            "section_title": "Approach 1: Estimating power laws via IsoFLOPs",
            "char_start_offset": 16267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1419
                }
            ],
            "ref_mentions": [
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "258833284"
                },
                {
                    "start": 697,
                    "end": 700,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24560546875
        },
        {
            "corpus_id": "225094178",
            "title": "Scaling Laws for Autoregressive Generative Modeling",
            "text": "We have argued that a single neural architecture, the Transformer, can be applied to the generative modeling of images, videos, multimodal data, and math, along with language [KMH + 20, BMR + 20]. We identified common scaling laws for the loss achieved on all data modalities as a function of both model size and compute budget. As in the case of language, these results imply that larger models become more sample efficient. Furthermore, we found that in some important cases, finetuned performance on downstream tasks also follows similar scaling laws. This suggests that trends in the generative modeling loss translate into advantages in practical capabilities. \n\nA greater surprise was the approximately universal trend (figure 2) for optimal model size as a function of the training compute budget -we did not anticipate that the exponent N opt \u221d C 0.7 would be largely independent of the data distribution. This trend implies a dual trend for the number of tokens elapsed during optimized training, as a function of C or N , and leads to the conclusion that larger compute budgets should be 'spent' mostly on larger models, rather than much longer training runs. So this lesson from language modeling [KMH + 20] generalizes. These empirical regularities beg for theoretical explanation -why do these scaling relations hold? \n\nThe scaling laws also suggest a shift in perspective away from the particularities of neural architectures, loss functions, and training algorithms and towards the broader commonalities that appear when machine learning is studied across a large hierarchy of model, data, and compute scales. Work in ML often involves identifying specific deficiencies in current capabilities and remedying them through the alteration of models and algorithms. Perhaps many capabilities simply lie on a spectrum that can be continuously unlocked with increasing scale, as might be suggested by the metalearning capabilities of the GPT-3 model [BMR + 20]. \n\nWe also discussed some information theoretic implications of the scaling laws. Perhaps the most important point was that the two terms in equation (1.1) can be interpreted as the entropy of the true data distribution, and the KL divergence between that distribution and a given generative model.",
            "score": 0.6143777701725897,
            "section_title": "Discussion",
            "char_start_offset": 48412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "259202628",
            "title": "Self-Learning Monte Carlo with Equivariant Transformer",
            "text": "It is known that there is the empirical scaling laws for language model performance [30]. The authors in Ref. [30] claims that performance of language models improves smoothly as we increase the model size, dataset size, and amount of compute used for training and that empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two. We show that our model with attention blocks has also a similar scaling law. In the previous section, we discuss the layer-number dependence of the acceptance ratio (Fig. 4). With the use of Eq. ( 22), the MSE can be estimated from the average acceptance ratio: \n\nFigure 6 shows that there is a scaling law as a functional of the number of training parameters. Here, we set N eff MC = 800. Blue square corresponds to the estimated MSE for the linear model and red circles indicate MSE for L = 1, 2, \u2022 \u2022 \u2022 , 6 from the left. We fit only points for L \u2265 2 to guide eyes. \n\nWe remark that in the SLMC, new training data is always generated so that the dataset size is increasing during the SLMC. This means that large data requirement for training of Attention layers can be satisfied by self-training. Moreover, we can systematically increase the number of trainable parameters in our model with adding new attention layer without loosing performance. \n\nWe discuss the origin of the scaling law that we found. We note that the origin of the scaling law in large language models is not well understood. Although we do not know the direct origin of our scaling law, we show that the MSE of the effective model can be improved with increasing the number of the attention layers. We introduce the parameter space of the effective model with L attention layers P (L) . The parameter space with different number of the attention layers has the relation expressed as \n\nwhere P (0) is the parameter space of the linear model. \n\nLet us consider the model with layer L which has been optimized. In this case, the model loss has reached to the lower bound. And we add one more attention block with small random weights to the model. The model obtains additional capacity for lowering the loss.",
            "score": 0.6134174607499424,
            "section_title": "A. Scaling law",
            "char_start_offset": 16920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2168
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "We have observed consistent scalings of language model log-likelihood loss with non-embedding parameter count N , dataset size D, and optimized training computation C min , as encapsulated in Equations (1.5) and (1.6). Conversely, we find very weak dependence on many architectural and optimization hyperparameters. Since scalings with N, D, C min are power-laws, there are diminishing returns with increasing scale. \n\nWe were able to precisely model the dependence of the loss on N and D, and alternatively on N and S, when these parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude of overfitting, early stopping step, and data requirements when training large language models. So our scaling relations go beyond mere observation to provide a predictive framework. One might interpret these relations as analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way, independent of most of the details of its microscopic consituents. \n\nIt is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a maximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to test these relations on other domains, such as images, audio, and video models, and perhaps also for random network distillation. At this point we do not know which of our results depend on the structure of natural language data, and which are universal. It would also be exciting to find a theoretical framework from which the scaling relations can be derived: a 'statistical mechanics' underlying the 'thermodynamics' we have observed. Such a theory might make it possible to derive other more precise predictions, and provide a systematic understanding of the limitations of the scaling laws. \n\nIn the domain of natural language, it will be important to investigate whether continued improvement on the loss translates into improvement on relevant language tasks. Smooth quantitative change can mask major qualitative improvements: \"more is different\". For example, the smooth aggregate growth of the economy provides no indication of the specific technological developments that underwrite it. Similarly, the smooth improvements in language model loss may hide seemingly qualitative changes in capability.",
            "score": 0.6133082737962017,
            "section_title": "Discussion",
            "char_start_offset": 39222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76416015625
        },
        {
            "corpus_id": "272463550",
            "title": "An overview of domain-specific foundation model: key technologies, applications and challenges",
            "text": "Scaling law refers to the mathematical pattern about how the performance of a system changes as the scale of the system increases. In the field of AI, especially in the research and application of foundation models, scaling law describes a series of rules and phenomena about how model performance changes as the model scale expands, including the number of parameters, dataset size, computational resources. It uses quantitative analysis methods to reveal the intrinsic mechanism of the performance improvement of foundation models. \n\n[54] discussed how the inductive biases of different models affect the relationship between model scale expansion and performance. They found that model architecture is indeed one of the key factors affecting the benefits of model expansion. They also pointed out that although the standard transformer architecture may not always achieve the best performance, it does exhibit the best scalability. In the fields of computer vision [55] and natural language process [56], models based on the transformer architecture have shown an exponential relationship between model scale and model performance. \n\nBesides, [57] examined the impact of the number of downstream tasks and model scale on the performance of instruction fine-tuning. They fine-tuned the models on a wide variety of tasks by a multi-task joint training method. As a result, the language model could learn a broader language representation and knowledge, thereby enhancing its generalization ability on unseen tasks. During the joint training process, knowledge transfer between different tasks is promoted through parameter sharing, significantly improving the model's generalization ability and performance. In addition, joint training also reduces the time and computational resources required to train each task individually, improving training efficiency. This phenomenon of model performance improving as task diversity increases is a manifestation of the scaling law. [58] verified the phenomenon of model performance increasing with the number of tasks on the large-scale benchmark OPT-IML Bench. Additionally, there are also studies that have separately provided the model performance of natural language models [56] and autoregressive generative models of various modalities [59] at different scales. \n\nAlthough there is no unified form for the quantitative representation of the scaling law, it can be generally represented as an exponential relationship between the model loss function and the model's parameters, dataset size, and computational resources.",
            "score": 0.612944420242159,
            "section_title": "Scaling law",
            "char_start_offset": 14422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2309
                },
                {
                    "start": 2312,
                    "end": 2567
                }
            ],
            "ref_mentions": [
                {
                    "start": 968,
                    "end": 972,
                    "matchedPaperCorpusId": "235367962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3544921875
        },
        {
            "corpus_id": "277467695",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "text": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.",
            "score": 0.6128118588730337,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "270045836",
            "title": "SMART: Scalable Multi-agent Real-time Simulation via Next-token Prediction",
            "text": "Prior research [22,42] have established that scaling up large language models (LLMs) leads to a predictable decrease in test loss L. This trend correlates with parameter counts N , training tokens T , following a power-law: \n\nwhere X can be any of N , T . The exponent \u03b1 reflects the smoothness of power-law, and L denotes the reducible loss normalized by irreducible loss. The data sources for validating scaling laws are detailed in the A.3. Overall, we trained models across four sizes, ranging from 1M to 100M parameters, on a training set containing 2.2M scenarios (or 1B motion tokens under 0.5s agent motion tokenization). \n\nScaling laws with model parameters We investigate the test loss trend as the model size increases. \n\nWe assessed the final test cross-entropy loss L on the validation set of 100,000 traffic scenarios. The results are plotted in Figure 4, where we observed a clear power-law scaling trend for Loss L as a function of model size N . The power-law scaling laws can be expressed as: \n\nThese results verify the strong scalability of SMART, providing valuable insights into how model performance scales with dataset size.",
            "score": 0.6125978661868086,
            "section_title": "Scalability",
            "char_start_offset": 23088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "276449777",
            "title": "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking",
            "text": "Large language models (LLMs) (Anthropic, 2023;OpenAI, 2023;Touvron et al., 2023) have demonstrated remarkable performance across numerous natural language tasks. Recent studies (Fernandez et al., 2024;Hoffmann et al., 2022;Wang et al., 2024b) indicate that scaling laws for LLM parameters exhibit diminishing returns under constrained data availability and computational resource budgets. Scaling model parameters increases com-",
            "score": 0.6125524835617523,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 428
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312744140625
        },
        {
            "corpus_id": "268875826",
            "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
            "text": "This paper studies the problem of how to scale the number of experts in the fast-developing MoE large language models.We first extend the scaling law, originally developed for dense transformer LLMs, to the context of MoEs, establishing a new relation between the validation loss and the number of experts, the number of training tokens, and the model size.We then discuss the need and the unique challenge to additionally consider inference efficiency when scaling MoEs.Our findings provide new insights on how to appropriately scale MoE models under compute constraints.",
            "score": 0.6119916831427209,
            "section_title": "Conclusion",
            "char_start_offset": 23396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 118,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 572
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "276558383",
            "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale",
            "text": "Large Language Models (LLM) are becoming pervasive in many aspects of life. The performance of these models are dictated by several factors including the model architecture, model size, training data size as well as training data quality. \n\nHow much data should one use to train an LLM of certain size? The answer is typically governed by scaling laws -empirical formulas that estimate optimal models sizes and data sizes for a given compute budget. For instance, the widely adopted Chinchilla law [1] suggested a compute optimal token-to-parameter-ratio of roughly 20. However, recent state-of-the-art LLMs have been trained on far more data than what the scaling laws would deem as optimal. For instance, Llama3 family of models are trained on 15 trillion (15T) tokens (compared to 1.8T tokens for Llama2) [2,3], Gemma2 family of models are trained on 13T tokens [4], and Granite-3.0 family of models are trained on 12T tokens [5]. At the time of writing of this paper, the pre-training datasets for leading LLMs, such as Llama3 [2] and Mixtral [6], remain inaccessible to the public, with limited information available on their creation process. \n\nFigure 1: GneissWeb (\u223c10T tokens) outperforms state-of-the-art open-source datasets with 5T+ tokens. Specifically, we compare average scores on a set of 11 tasks with 18 variants (zero-shot and few-shot) for 1.4B parameter models (left) and 7B parameter models (right), trained on 350B tokens. We also compare with state-of-the-art existing models of roughly 1B parameter size. Models trained on GneissWeb (green) achieve higher performance than the models trained on other datasets (circles) and existing models (crosses). \n\nOpacity of the datasets used to train leading LLMs, has motivated the development of several open-source datasets [7,8,9,10,11].",
            "score": 0.611840105533742,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089599609375
        },
        {
            "corpus_id": "249674500",
            "title": "Emergent Abilities of Large Language Models",
            "text": "Language models have revolutionized natural language processing (NLP) in recent years. It is now well-known that increasing the scale of language models (e.g., training compute, model parameters, etc.) can lead to better performance and sample efficiency on a range of downstream NLP tasks (Devlin et al., 2019;Brown et al., 2020, inter alia). In many cases, the effect of scale on performance can often be methodologically predicted via scaling laws-for example, scaling curves for cross-entropy loss have been shown to empirically span more than seven orders of magnitude (Kaplan et al., 2020;Hoffmann et al., 2022). On the other hand, performance for certain downstream tasks counterintuitively does not appear to continuously improve as a function of scale, and such tasks cannot be predicted ahead of time (Ganguli et al., 2022). \n\nIn this paper, we will discuss the unpredictable phenomena of emergent abilities of large language models. Emergence as an idea has been long discussed in domains such as physics, biology, and computer science (Anderson, 1972;Hwang et al., 2012;Forrest, 1990;Corradini & O'Connor, 2010;Harper & Lewis, 2012, inter Training dataset size is also an important factor, but we do not plot capabilities against it because many language model families use a fixed number of training examples for all model sizes (Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022). Although we focus on training computation and model size here, there is not a single proxy that adequately captures all aspects of scale. For example, Chinchilla (Hoffmann et al., 2022) has one-fourth as many parameters as Gopher (Rae et al., 2021) but uses similar training compute; and sparse mixture-of-expert models have more parameters per training/inference compute than dense models (Fedus et al., 2021;Du et al., 2021). Overall, it may be wise to view emergence as a function of many correlated variables.",
            "score": 0.6110237953705706,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 311,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 595,
                    "end": 617,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1047,
                    "end": 1063,
                    "matchedPaperCorpusId": "2221052"
                },
                {
                    "start": 1063,
                    "end": 1082,
                    "matchedPaperCorpusId": "262482835"
                },
                {
                    "start": 1082,
                    "end": 1096,
                    "matchedPaperCorpusId": "39757717"
                },
                {
                    "start": 1342,
                    "end": 1362,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1566,
                    "end": 1589,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1814,
                    "end": 1830,
                    "matchedPaperCorpusId": "245124124"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271484375
        },
        {
            "corpus_id": "270063468",
            "title": "gzip Predicts Data-dependent Scaling Laws",
            "text": "Early work established that a neural network's test error is a power law of training dataset size [Cortes et al., 1993], model parameter count [Rosenfeld et al., 2019], and that the relationship holds over many orders of magnitude [Hestness et al., 2017].Kaplan et al. [2020] applied scaling laws to transformer-based language models and identified a compute-optimal frontier along which parameter & dataset size should be scaled.Hoffmann et al. [2022] propose the Chinchilla scaling laws, finding that Kaplan et al. [2020] and Rae et al. [2021] overparameterized their models and that the compute-optimal frontier requires parameter & dataset size to be scaled equally (rather than parameters scaling at 3x the rate of data).Sorscher et al. [2022] find that we can reach exponential (rather than power law) scaling on dataset size by pruning out redundant examples that do not provide much information to learn from.Liu and Tegmark [2023] identify a mechanistic explanation of why scaling follows a power law on model width.Aghajanyan et al. [2023] extend the Chinchilla scaling laws to a variety of modalities such as speech, image-text, and code.Caballero et al. [2023] propose a novel functional form for scaling laws that better models complex non-monotonic behavior and also apply it to several modalities including code.Both these works and Hoffmann et al. [2022] (in Appendix C) find that scaling behavior for code is different than for natural language-code is easier to learn and its compute-optimal frontier prefers parameters slightly over data.Bi et al. [2024] cursorily investigate scaling laws across datasets of different qualities, finding that cleaner & higher quality data results in the \"model scaling exponent increasing\" because \"high-quality data usually implies logical clarity and less predictive difficulty after sufficient training\".However, none of these works identify an underlying general principle that explains why scaling behavior varies across data modalities & complexities (or even just between code and natural language).",
            "score": 0.6103036993468136,
            "section_title": "Scaling Laws",
            "char_start_offset": 3721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1149
                },
                {
                    "start": 1149,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 119,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 726,
                    "end": 748,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1025,
                    "end": 1049,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 1149,
                    "end": 1172,
                    "matchedPaperCorpusId": "253117181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4619140625
        },
        {
            "corpus_id": "267406160",
            "title": "A Dynamical Model of Neural Scaling Laws",
            "text": "Large scale language and vision models have been shown to achieve better performance as the number of parameters and number of training steps are increased. Moreover, the scaling of various loss metrics (such as cross entropy or MSE test loss) has been empirically observed to exhibit remarkably regular, often power law behavior across several orders of magnitude (Hestness et al., 2017;Kaplan et al., Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 2020). These findings are termed \"neural scaling laws\". \n\nNeural scaling laws play a central role in modern deep learning practice, and have substantial implications for the optimal trade-off between model size and training time (Hoffmann et al., 2022), as well as architecture selection (Alabdulmohsin et al., 2023). Understanding the origin of such scaling laws, as well as their exponents, has the potential to offer insight into better architectures, the design of better datasets (Sorscher et al., 2022), and the failure modes and limitations of deep learning systems. Yet, many questions about neural scaling laws remain open. \n\nIn this paper, we introduce and analyze a solvable model which captures many important aspects of neural scaling laws. In particular, we are interested in understanding the following empirically observed phenomena: Test Loss Scales as a Power-law in Training Time and Model Size and Compute. In many domains of deep learning, the test loss of a model with N trainable parameters trained for t iterations has been found to scale as L(t, N ) \u2248 L 0 + a t t \u2212rt + a N N \u2212r N (Kaplan et al., 2020;Hoffmann et al., 2022). These scaling law exponents r t , r N generally depend on the dataset and architecture. We demonstrate scaling laws on simple vision and language tasks in Figure 1. The compute is proportional to the number of steps of gradient descent times the model size C \u221d N t. Setting N and t optimally gives that test loss scales as a power law in C. This is the compute optimal scaling law. \n\nCompute-Optimal Training Time and Model Size Scaling Exponents Are Different.",
            "score": 0.6092395133070976,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2072
                },
                {
                    "start": 2075,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 792,
                    "matchedPaperCorpusId": "49321232"
                },
                {
                    "start": 1025,
                    "end": 1048,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1667,
                    "end": 1689,
                    "matchedPaperCorpusId": "49321232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4765625
        },
        {
            "corpus_id": "275337234",
            "title": "Scaling Laws for Floating Point Quantization Training",
            "text": "Scaling laws of large language models (LLMs) could help developers effectively select superior parameter settings before experiments and accurately predict the model performance under different configurations. They are regarded as excellent guidance in LLM training. The widely acknowledged scaling law efforts such as Kaplan et al. (2020), Hoffmann et al. (2022), and Li et al. (2024) mainly concentrated on the central factors, i.e., model size and trained token size, which significantly impact the performance of LLMs. With the rapid growth of both model and data sizes, there has been increasing attention to the efficiency and cost of LLM training. Training and serving with lower precision becomes a popular solution. Currently, lots of representative LLMs were trained in BF16 and even lower precision (Dubey et al., 2024;Sun et al., 2024;Liu et al., 2024;Yang et al., 2024;Ma et al., 2024;Wang et al., 2023;Peng et al., 2023), aiming to balance effectiveness and efficiency. Compared to integer quantization, floating-point (FP) quantization can better maintain LLMs' accuracy at extremely lower bit rates and thus is often equipped in low-precision LLMs. Therefore, exploring the scaling laws of LLM performance under different low precision settings with FP quantization becomes essential to shed light on future low-precision LLM training. \n\nRecently, there was pioneer work that conducted in-depth analyses and explorations on the LLM's scaling laws for precision in both training and inference (Kumar et al., 2024), quantitatively measuring the degradation rules of post-train quantization and quantized training. This scaling law provides an appropriate conclusion explaining the potential damage of excessively increasing training data to low-precision LLMs' performance. However, Kumar et al. (2024) directly adopted the bit width as the precision in its low-precision scaling laws, which might lose finer-grained modeling of the relationship between various parameter settings related to the FP quantization and the final loss of LLMs. In practice, the key factors of FP quantization such as the exponent, mantissa, and the block size of scaling factors may have different impacts on the final loss.",
            "score": 0.6071070833085344,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 385,
                    "matchedPaperCorpusId": "269982407"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33837890625
        },
        {
            "corpus_id": "273695905",
            "title": "Does equivariance matter at scale?",
            "text": "Neural scaling laws The scaling of neural network performance as a function of model size or training steps has been studied extensively (Ahmad & Tesauro, 1988;Hestness et al., 2017;Rosenfeld et al., 2019;Henighan et al., 2020). Kaplan et al. (2020) first observed that the test loss of autoregressive language models follows a power law over many orders of magnitude. Hoffmann et al. (2022) improved the methodology further and found the \"Chinchilla\" scaling laws, which still serve as a reference point for many language models. In our quantitative analysis of compute scaling, we largely follow their approach. \n\nSeveral works have extended scaling laws from model size and training steps to other dimensions: Muennighoff et al. (2023) studied the effect of the training dataset size, which we also discuss, Alabdulmohsin et al. (2023) analyzed scaling of different architecture hyperparameters separately, and Jones (2021) investigated the scaling with problem complexity.",
            "score": 0.6065278959382124,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 1374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 976
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 160,
                    "matchedPaperCorpusId": "15615035"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "276259426",
            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
            "text": "Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time in language and multimodal systems. RINS is a particular form of recursive depth that significantly outperforms +55 other variants, including the recent\"repeat-all-over\"(RAO) strategy in Mobile LLM (Liu et al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior works, we carry out our comparisons on a compute-matched regime, and demonstrate that for a fixed model size and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. More importantly, with light-weight (linear) adapters (comprising<1% of model parameters) and stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled pretraining improves performance in language modeling even when recursive depth is not applied at inference time. This corresponds to improving performance on a training compute-, parameter-, and inference-matched regime, suggesting its potential as a viable component of LLM pretraining!",
            "score": 0.6063927664929138,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "Traditional scaling laws primarily focus on the relationship between computation power and training loss, typically measured by cross-entropy.However, this measure alone does not fully capture the capabilities of large language models ().To address this, we expand our investigation to include scaling laws that correlate computation power with validation perplexity and common sense reasoning (CSR) Both CSR scores and validation perplexity highlight the strong scaling potential of HGRN2, TNL, and cosFormer2 in addressing linguistic and knowledge-based tasks in downstream tasks.NIAH In evaluating the easy mode of the Needle in a Haystack (NIAH) task in 16K contexts, different architectures perform differently.Models with parameter sizes below 160M struggle to perform the tasks effectively.The vanilla transformer LLaMA maintains a success rate of about 50%.TNL begins to show results in NIAH only after reaching 1B parameters, achieving a maximum success rate of about 10%.Both HGRN2 and cosFormer2 start to display scaling capabilities in NIAH with over 410M parameters.Specifically, cosFormer2 achieves a maximum retrieval success rate of 25% in 4K context, while HGRN2 performs slightly better with 30% in 4.8K context.Linear complexity sequence models like cosFormer2 and HGRN2 tend to retrieve information from contexts shorter than their pre-training length of 8K.In terms of performance, the order is LLaMA > cos-Former2 = HGRN2 >> TNL.Additionally, these linear complexity sequence models require large parameter sizes to effectively handle NIAH tasks.SCROLLS Similar to NIAH, all architectures begin to effectively address the SCROLLS task starting at minimal 410 million parameters.All models with linear complexity sequences display a consistent scaling power comparable to LLaMA.TNL also requires a large parameter size (1B) for SCROLLS.The overall performance ranking is LLaMA and cosFormer2 at the top, followed by HGRN2, and then TNL.",
            "score": 0.6062451212552559,
            "section_title": "Downstream tasks",
            "char_start_offset": 16531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 797
                },
                {
                    "start": 797,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1079
                },
                {
                    "start": 1079,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1568
                },
                {
                    "start": 1568,
                    "end": 1700
                },
                {
                    "start": 1700,
                    "end": 1799
                },
                {
                    "start": 1799,
                    "end": 1857
                },
                {
                    "start": 1857,
                    "end": 1957
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60595703125
        },
        {
            "corpus_id": "273695905",
            "title": "Does equivariance matter at scale?",
            "text": "We model the scaling with compute quantitatively by fitting a scaling law to all of our experiments. Following Kaplan et al. (2020), we model the test loss L as a power law in the model parameters N and the training duration D, measured in tokens: \n\nHere A, B, E, \u03b1, \u03b2 are fit parameters. \n\nThe parameter E represents the irreducible loss that even a perfect model cannot eliminate. Unlike in language or image modelling tasks, there is no clear reason to expect such an irreducible error of practically relevant size for the deterministic physics task we use as a benchmark. We treat the choice of whether to include E as a fit parameter or fix it to zero as a hyperparameter and choose it through cross validation, as we will describe below. \n\nFor the scaling with the size of the training data set, we do not find a scaling law that convincingly describes our experiments. Our attempts at fitting Muennighoff et al.'s data-constrained scaling law (2023) to our data did not result in a good agreement. We therefore refrain from discussing the functional form for this direction of scaling, and will focus on scaling with compute for the remainder of this section. \n\nScaling-law fit Following Hoffmann et al. ( 2022), we fit the scaling-law parameters (A, B, E, \u03b1, \u03b2) separately for each architecture by minimizing the Huber loss (Huber, 1992) between the predicted and observed log loss values, experiments i \n\nHere \u03b4 is a hyperparameter, we choose it based on cross-validation, as we describe in a bit. We minimize this loss with the L-BFGS optimizer (Liu & Nocedal, 1989), starting multiple fits from a grid of initializations to avoid getting stuck in local minima.",
            "score": 0.6055252202157546,
            "section_title": "Scaling-law ansatz",
            "char_start_offset": 17351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1671
                }
            ],
            "ref_mentions": [
                {
                    "start": 1555,
                    "end": 1576,
                    "matchedPaperCorpusId": "5681609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2890625
        },
        {
            "corpus_id": "274514987",
            "title": "Establishing Task Scaling Laws via Compute-Efficient Model Ladders",
            "text": "We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. We train a set of small-scale\"ladder\"models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, we can predict the accuracy of both target models within 2 points of absolute error. We have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. We also find that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, we empirically show that our design choices and the two-step approach lead to superior performance in establishing scaling laws.",
            "score": 0.6054148259328852,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5751953125
        },
        {
            "corpus_id": "232428341",
            "title": "BASE Layers: Simplifying Training of Large, Sparse Models",
            "text": "Task We focus our experiments on language modelling, as recent work such as GPT3 (Brown et al., 2020) offers perhaps the clearest demonstration in machine learning of the power of large scale models. \n\nMetrics We focus exclusively on comparing compute efficiency, which we define as the best model performance (here, perplexity) that can be achieved by training with a given number of GPUs and wall-clock time. This metric is different from other commonly used metrics, such as sample efficiency (which measures the number of tokens the model trains on, but not the cost of processing samples) or FLOPefficiency (which measures the number of floating-point operations performed during training, but does not account for communication costs). As plentiful data is available for training language models, but computation is expensive, we believe that compute efficiency best captures the constraints of real world training. Therefore, we compare models using a fixed number of GPUs for the same runtime. \n\nTraining Hyperparameters We train all models for approximately 2.5 days. All models use similar hyperparameters of 2000 warm-up steps, and the Adam optimizer (Kingma & Ba, 2014). We tune learning rates for each model separately, and linearly decay the learning rate during training. Each worker processes two sequences of length 1024, and gradients are accumulated over 8 updates. We clip gradients if their l 2 norm exceeds 0.1 ( \u00a73). Learning rates are tuned in the range {0.5, 0.75, 1.0} \u00d7 10 \u22124 , taking the highest value that avoids divergence. \n\nHardware Unless otherwise stated, models are trained on 128 32GB V100 GPUs connected with Infiniband.2 \n\nData We train on a corpus of approximately 100B tokens, comprising the training corpus of RoBERTa (Liu et al., 2019), combined with the English portion of the CC100 corpus (Conneau et al., 2019). We use the byte-pair encoding (Sennrich et al., 2015) from GPT2 (Radford et al., 2019), which has a vocabulary of 51200.",
            "score": 0.6044261536959359,
            "section_title": "Experimental Setup",
            "char_start_offset": 12241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 1921,
                    "end": 1943,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282470703125
        },
        {
            "corpus_id": "273228196",
            "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
            "text": "The architecture of all models (including Dense Models and MoE Models) are shown in table 3 and  table 4, respectively. The number of layers, attention heads, hidden dimensions, and other relevant details are listed in the tables. \n\nDuring training, we employed the AdamW optimizer with parameters \u03b2 1 = 0.9 and \u03b2 2 = 0.95 for all models. Following the Chinchilla law (Hoffmann et al., 2022), we established a maximum learning rate of 1.5 \u00d7 10 \u22123 for smaller models and 2 \u00d7 10 \u22124 for larger ones. A cosine scheduler with a 10x learning rate decay was implemented throughout the training process. We applied Gaussian smoothing with a 10-step window length to enhance the training curve. \n\nSpecifically, we identified the best performance values within our hyperparameter range. The range of hyperparameter settings, including batch size and learning rate, was carefully selected for each model size to ensure optimal performance within the designated FLOP budget. Our observations indicate that performance tends to converge to optimal values around the neighborhood of the best settings, as illustrated in Figure 7. The dataset we used for the training of Dense Models and MoE Models is The Pile (Gao et al., 2020), which is a 825 GiB English text corpus consisting of 22 high-quality subsets. Due to the significant costs associated with training process, the scaling laws of large language models (LLMs) is crucial. Some previous studies (Kaplan et al., 2020;Bahri et al., 2021) have proposed a power-law relationship between loss and various factors like the number of non-embedding parameters, training tokens, and compute budget across different magnitudes. Notably, Kaplan et al. (2020) found that increasing the model size by 8 times only requires a roughly 5x increase in data to avoid penalties. In contrast to earlier findings, Hoffmann et al. ( 2022) implements optimized training configurations, which include the use of training tokens and learning rate schedules, and recommends scaling training tokens in proportion to model size. Additionally, research by Bi et al. (2024) explores the scaling laws of batch size and learning rate in relation to model scale (non-embedding FLOPs per token), offering a more precise estimation.",
            "score": 0.6039498299634752,
            "section_title": "A Experimental Settings",
            "char_start_offset": 19945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 391,
                    "matchedPaperCorpusId": "246473179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480712890625
        },
        {
            "corpus_id": "259837466",
            "title": "Large Language Models",
            "text": "Training can largely be done in parallel by dividing up this data, and the availability of large clusters of GPU-enabled servers at industrial labs and through cloud computing meant that sufficient computing resources were available in principle. However, the overall cost of training scales as (at least) the product of model size and dataset size, and this was becoming expensive. While the precise cost figures for the GPT series are not public, it is estimated that a single training run of the largest GPT-3 models cost tens of millions of dollars. To motivate and efficiently carry out such costly experiments, one needs some ability to predict in advance how changes in model and dataset size will affect the training methods (for example the optimal choice of learning rate) and performance. \n\nAn important advance in this direction was the observation of power law scaling in language model performance [67]. Figure 2 plots the test loss 17 against the logarithms of the sizes and compute resources used, and these straight lines correspond to a power law relation between size and perplexity. This scaling holds over many decades in model size and, while the exponents \u03b1 \u223c \u22120.076 to \u22120.095 are rather small, this is a strong argument that larger models will have better performance. These ideas were also used to determine optimal modeldataset size tradeoff [58] and the scaling of hyperparameters [141]. These results were a significant input into the decision to do this very expensive research. Now it should be realized that, while the measure being improved here is fairly objective, still there was no strong reason to think that improving it would lead to models with qualitatively new \"emergent\" capabilities. But it appears that this is what happened: GPT-3 and its fine-tuned cousins (such as Codex) were able to do tasks, such as write computer code from a natural language description, for which smaller models were almost worthless. 18 We will discuss more of this progress shortly, and speculate a bit in the conclusions. \n\nOne of the most interesting LLM phenomena is in-context learning, first discussed in the original GPT-3 paper [22].",
            "score": 0.6037951696506038,
            "section_title": "Language models",
            "char_start_offset": 22969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2045
                },
                {
                    "start": 2048,
                    "end": 2163
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.485595703125
        },
        {
            "corpus_id": "246823711",
            "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments",
            "text": "Moreover, in SQuAD 1.1, QNLI and MNLI, the predicted gap is very accurate. Overall, we conclude that when the goodness-of-fit, i.e., R 2 , is high enough, scaling laws present a viable approach for performing model selection without training a large model. We leave for future work to determine if predictions remain accurate when extrapolating over multiple orders-of-magnitude.\n\nComputational efficiency We have shown that for some NLP tasks, scaling laws can be an effective tool for model selection, and be further used for debugging convergence. However, applying them requires multiple runs across scales for uncertainty estimation and HPT. Thus, a key question is how much resources are saved with this effort.\n\nTo examine this, we perform a theoretical analysis of the FLOPs required to pretrain and finetune the small-scale models vs. the larger ones in our experimental setup, where we extrapolate to a model that is one order-of-magnitude larger. Following Kaplan et al. (2020), we estimate the number of FLOPs for the forward and backward passes with C \u2248 6N D where D is the total number of tokens observed. Assuming all models observe the same number of tokens (ignoring early stopping as it requires extra FLOPs for the evaluation set), the difference in computation arises solely from the number of parameters. For example, the total count of parameters of the 8 small-scale models in our setup is 16M while BERT-Base contains 85M, suggesting a 5x improvement. Extrapolating to larger scales will yield more substantial savings, but we did not pretrain and evaluate any larger models.\n\nIn practice, we observed that smaller scale models require more epochs during finetuning, increasing their token count, D. Still, even if we sum all FLOPs performed for HPT and finetuning over all scales and all 9 tasks, we empirically observed a 2.5x decrease in FLOP count compared to training the larger model. We do not compare runtime because different models were trained on different types of nodes, but we expect savings in terms of runtime to be even greater, as training multiple models is trivial to parallelize.\n\nOverall, one can expect compute savings of 2.5-5x when scaling to a model that is an order of magnitude larger,",
            "score": 0.6034243824180806,
            "section_title": "Model selection",
            "char_start_offset": 21124,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "270210711",
            "title": "Towards a theory of how the structure of language is acquired by deep neural networks",
            "text": "We proposed a conceptual framework for understanding the performance-vs.-data scaling laws of language models trained for next-token prediction. In our picture, increasing the number of data allows for the resolution of a longer range of correlations. These correlations, in turn, can be exploited to build a hierarchical representation of the data structure, the longer the range the deeper the representation. For our synthetic hierarchical data, the emergence of deeper representation results in a series of steps in the next-token prediction performance. These steps conspire to determine the scaling law, whose exponent depends on the dataset structure. This scenario is consistent with the empirical phenomenology of language models, including both the emergence of skills at specific training set sizes [18,43,44,45] and the steady improvement of overall performance [17]. To the best of our knowledge, this is the first theoretical description of scaling laws in a setting where learning data features is crucial, whereas previous works focused on kernel limits [46,47,48,49,50]. \n\nFurthermore, our analysis predicts a fundamental relationship between the effective context window captured by a language model trained with a finite training set and the decay of token-token correlations, which we confirmed empirically on two examples of text data. This finding suggests that the exponents entering scaling laws are influenced by the intrinsic properties of the data. On the one hand, our predictions can be tested on state-of-the-art LLMs trained on larger datasets. On the other hand, our framework can be extended to shed light on other aspects of scaling laws of high practical relevance, such as the role of the number of parameters and the behaviour of performance when the model size and the number of data are optimised under a fixed compute budget. \n\nLimitations. Our hierarchical model of data is limited by the context-free structure of the rules, which describes most, but not all, of the syntactic forms observed in natural languages [51]. Understanding the role of context-sensitive structures in language acquisition is a promising avenue for future research. In addition, the RHM assumes a fixed geometry of the data tree and the uniform probability and unambiguity of the production rules. These assumptions are not satisfied by real text data and are responsible for the stepwise behaviour of correlations in our model.",
            "score": 0.6033361414212663,
            "section_title": "Conclusions",
            "char_start_offset": 27096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2314
                },
                {
                    "start": 2315,
                    "end": 2445
                }
            ],
            "ref_mentions": [
                {
                    "start": 810,
                    "end": 814,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1070,
                    "end": 1074,
                    "matchedPaperCorpusId": "207063850"
                },
                {
                    "start": 1074,
                    "end": 1077,
                    "matchedPaperCorpusId": "260540101"
                },
                {
                    "start": 1080,
                    "end": 1083,
                    "matchedPaperCorpusId": "252355508"
                },
                {
                    "start": 2055,
                    "end": 2059,
                    "matchedPaperCorpusId": "222277837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.469970703125
        },
        {
            "corpus_id": "270045202",
            "title": "BiMix: A Bivariate Data Mixing Law for Language Model Pretraining",
            "text": "Chinchilla (Hoffmann et al., 2022) identified similar scaling patterns through rigorous experimentation and suggested a slightly different configuration for compute-optimal pretraining. The impactful GPT-4 model (OpenAI, 2024) validated the predictive accuracy of scaling laws and underscored their important role in the development of large language models. Concurrently, additional research efforts seek to elucidate the principles governing scaling laws (Sharma & Kaplan, 2022;Michaud et al., 2023) and to investigate scaling effects on downstream tasks (Tay et al., 2022;Isik et al., 2024;Caballero et al., 2023;Cherti et al., 2023). In the context of data mixtures, Ye et al. (2024) proposed a composite exponential law to capture the interactions among domains; yet, its scalability is challenged by increased complexity for expanding domain numbers, as compared in Appendix C. Our study is distinguished by two key aspects: First, we introduce a scalable mixing law that accurately captures the scaling behavior associated with the composition of training datasets, demonstrating the modeling ability to up to 22 domains. Second, the proposed bivariate mixing law jointly models two input variables, domain proportion and data volume, thereby offering broader applicability.",
            "score": 0.6013993254672516,
            "section_title": "RELATED WORK",
            "char_start_offset": 5412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1281
                }
            ],
            "ref_mentions": [
                {
                    "start": 457,
                    "end": 480,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 480,
                    "end": 501,
                    "matchedPaperCorpusId": "257687665"
                },
                {
                    "start": 557,
                    "end": 575,
                    "matchedPaperCorpusId": "260498358"
                },
                {
                    "start": 593,
                    "end": 616,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 616,
                    "end": 636,
                    "matchedPaperCorpusId": "254636568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414794921875
        },
        {
            "corpus_id": "257663521",
            "title": "Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense",
            "text": "Alongside the release of their GPT-3 model, OpenAI published a report detailing the scaling of large language models performance with its size, justifying the choice of the GPT-3 model size given the data they had access to at the time [Kaplan et al., 2020]. However, as we mentioned previously (section 1.6), these scaling laws have since been shown to underestimate the number of tokens needed to train the model to optimality [Ganguli et al., 2022, Hoffmann et al., 2022]. \n\nBased on these new scaling results, a new generation of LLMs has been developed and trained. While smaller in size than the GPT family, such LLMs have been shown to match and even exceed the capabilities of GPT family models 3x their size. The three most visible generative autoregressive models in this category are Google's Chinchilla Hoffmann et al. [2022], Facebook/Meta's LLaMA Touvron et al. [2023] and Anthropic's base LLM for Assistant and Claude, alluded to in Bai et al. [2022a,b]. Ranging in size between 52B parameters for Anthropic's base LLM and 70B parameters for Chinchilla, they all compare favorably to GPT3-175B while all being easier to deploy and run thanks to a smaller size. \n\nAs a side note, RoBERTa [Liu et al., 2019] and T5 [Raffel et al., 2020] models can also be argued to be compute-optimal as well. While they are not autoregressive and have not been designed for pure text generation, they have generative capabilities, and T5 is commonly used in this role. However, what makes them compute-optimal is the fact that they satisfy the empirical compute-optimal scaling rule of 10:1 between the training dataset size in tokens and the number of model parameters. Finally, for the same reason, GPT-J and GPT-neo models from EleutherAI can be considered as potentially compute-optimal.",
            "score": 0.601259994010791,
            "section_title": "Compute-Optimal Models",
            "char_start_offset": 44185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 450,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 1228,
                    "end": 1248,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24853515625
        },
        {
            "corpus_id": "266052386",
            "title": "Scaling Laws of Synthetic Images for Model Training \u2026 for Now",
            "text": "Scaling up model size, data amount, and training budget has unlocked new capabilities of deep models [13,18,55,59,91]. Recent studies [31,39] suggest the testing loss behaves as a power low with respect to each of these three resources when the other two are proper, in large language models (LLMs), machine translation [23], auto-regressive generative models [29], and transfer learning [30]. Similar behavior is observed in multi-modal models [2]. Chinchilla [35] suggests scaling up data propor-tionally to model size, to obtain compute-optimal LLMs. [3] propose to fit scaling laws by extrapolating training curves. Of particular interest, [74] theoretically shows one can break the power law with respect to data size with an ideal data pruning strategy. In this paper, we focus on the scaling behavior of synthetic data for training models.",
            "score": 0.601034164363033,
            "section_title": "Related Work",
            "char_start_offset": 7328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 846
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 114,
                    "end": 117,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 554,
                    "end": 557,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "250113273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.192138671875
        },
        {
            "corpus_id": "271270413",
            "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
            "text": "Large language models (LLMs) achieve remarkable performance by pre-training on vast text corpora using massive computational resources [47]. Extensive prior work on LLMs has focused on deriving so-called scaling laws: a set of empirical formulas to predict how model performance scales, mainly as computing floating-point operations (FLOPs), model parameters, and quantity of training data change [30,26,66,2,44,58]. These works show that power-law fits can effectively predict language modeling loss and by extension downstream performance [23,55]. However, these scaling laws usually disregard the impact of the vocabulary size. For example, in Kaplan et al. [30] only non-vocabulary parameters are considered in their predictive formula. This negligence has resulted in substantial variability in the vocabulary size of current LLMs. For instance, Llama2-7B employs a vocabulary size of 32K [70], while Gemma-7B [67] adopts a much larger vocabulary size of 256K despite both having a similar number of total parameters. This variability in vocabulary sizes across LLMs raises the research question: What is the compute-optimal vocabulary size for a LLM? \n\nThe relationship between non-vocabulary parameters N nv and the corresponding optimal vocabulary parameters N opt v follows a power law, where N opt v should be scaled slower than N nv as \u03b3 < 1. Empirical results align with predictions of our proposed approaches, with larger circles indicating higher loss values. Here V refers to the vocabulary size i.e. the number of distinct tokens. \n\nThe vocabulary size affects performance non-trivially. Intuitively, the optimal vocabulary size should neither be too large nor small. A larger vocabulary size improves tokenization fertility, i.e., splitting sentences into fewer tokens, thereby improving the tokenization efficiency. Additionally, a larger vocabulary enables the model to capture a wider range of concept. However, the risk of under-fitting for rare tokens increases with larger vocabulary sizes, especially in the data-constrained regime [44,72]. Thus, the optimal vocabulary size needs to be determined by taking the training data and the non-vocabulary parameters into account.",
            "score": 0.5993839980421743,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 407,
                    "matchedPaperCorpusId": "250920512"
                },
                {
                    "start": 407,
                    "end": 409,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 409,
                    "end": 412,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 2056,
                    "end": 2060,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363525390625
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "allowing us to predict loss scaling in many situations of practical interest. This functional form that posits bit precision and parameter count interchangeably contribute to a model's \"effective parameter count,\" N eff , and implementation details like which parts of a model are quantized to what precision, interact with loss scaling only through their effect on this quantity. \n\nOverall, we study the scaling of the effects of precision on loss as we vary data and parameters, both during and after training. We first study how the degradation induced by post-train quantization scales with parameters and data. We find that the degradation increases with data, so that for a fixed model, training on additional data after a certain point can be actively harmful if the model will be quantized after training. We then shift our focus to quantized training, examining both the quantization-aware-training (weights only) and low-precision training (weights, activations, attention all quantized) settings. Our scaling laws for pretraining suggest that the compute-optimal pretraining precision is in general independent of compute budget. Surprisingly, however, this independence ceases to be true if model size is constrained, in which case the compute-optimal precision grows slowly in compute. \n\nIn all, we pretrain a suite of 465 language models in 3 to 16 bit precisions, as well as post-train quantize each to multiple precisions. For a language model with N parameters, trained on D tokens with training precision P train , and post-train weight precision P post , we ultimately find a unified scaling law that takes the following form: (1) \n\nwhere A, B, E, \u03b1, \u03b2 are positive fitted constants, and \u03b4 PTQ refers to the loss degradation induced by post-training quantization before inference. Altogether, our results for post-train quantization illustrate how more pretraining FLOPs do not always lead to better models at inferencetime, and our results for low-precision pretraining suggest that both the standard practice of training models in 16-bit, and the race to extremely low (sub 4-bit) pretraining precision, may be suboptimal. \n\n2 Background, Related Work, and Setup \n\nNotation. Throughout, D denotes dataset size in tokens and N denotes model size in parameters.",
            "score": 0.599243290702668,
            "section_title": "Introduction",
            "char_start_offset": 1986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2143
                },
                {
                    "start": 2146,
                    "end": 2183
                },
                {
                    "start": 2186,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61962890625
        },
        {
            "corpus_id": "271909320",
            "title": "Scaling Law with Learning Rate Annealing",
            "text": "In recent years, large language models (LLMs) have garnered significant academic and industrial attention (Brown et al., 2020;Touvron et al., 2023). The scaling law suggests that the validation loss of language models follow a power-law pattern as model and data sizes increase (Hestness et al., 2017;Kaplan et al., 2020;Henighan et al., 2020). This law provides a powerful framework for forecasting LLM performances before large scale training by fitting losses at smaller scales (OpenAI, 2023;DeepSeek-AI, 2024;Dubey et al., 2024). Numerous studies have explored on the formulation to model the scaling effect of LLMs under various different settings (Bahri et al., 2021;Hernandez et al., 2021;Caballero et al., 2022;Michaud et al., 2023;Muennighoff et al., 2023). \n\nHowever, typical scaling law formulations focus only on the final loss at the end of training (Hoffmann et al., 2022). Specifically, previous approaches generally rely on a set of training runs and fit the scaling law curve solely on the final loss from each run. Essentially, the middle points with different degrees of LR annealing fail to follow typical scaling laws, which do not consider local loss drop brought by LR annealing. The previous approach under-utilizes the training compute and fails to capture the training dynamics within each run. Further, the application of scaling laws in LLM developments is limited since the loss curve through the whole training process is not modeled. An expressive formulation that models full loss curves enables prediction of future training dynamics and also offers insights on understanding the learning process of LLMs. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Step  and S 2 at the 20-th step of a cosine LR scheduler. S 1 is the forward area, i.e., sum of red grid areas, which can be approximately regarded as the total amount of movement for neural network parameters; S 2 is the decayed annealing area, i.e., weighted sum of blue grid areas, where lighter shades indicate smaller weights.",
            "score": 0.599160594601983,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 2023
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 126,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 719,
                    "end": 740,
                    "matchedPaperCorpusId": "257687665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Scaling models involves more than just increasing parameters; optimizing data mixtures, training duration, and vocabulary size also plays a crucial role in enhancing performance and efficiency. Data mixing laws allow AI practitioners to accurately predict optimal data compositions before training, leading to 27% fewer training steps without compromising accuracy (Ye et al., 2024). Techniques like REGMIX optimize data selection using proxy models and regression, reducing compute costs by 90% compared to manual data selection (Liu et al., 2024). Meanwhile, AUTOSCALE revealed that data efficiency depends on model scale, where highquality data like Wikipedia helps small models but loses effectiveness for larger models, which benefit from diverse datasets like CommonCrawl (Kang et al., 2024). For continual learning, the D-CPT Law provided a theoretical framework for balancing general and domain-specific data, guiding efficient domain adaptation and long-term model updates (Que et al., 2024). Additionally, Chinchilla scaling assumptions were challenged by evidence showing that training models for more epochs on limited data can outperform simply increasing model size (Muennighoff et al., 2023). Repeated data exposure remains stable up to 4 epochs, but returns diminish to zero after around 16 epochs, making longer training a more effective allocation of compute resources. Furthermore, the vocabulary scaling law suggested that as language models grow larger, their optimal vocabulary size should increase according to a power law relationship (Tao et al., 2024). Finally, knowledge capacity scaling laws established that language models store 2 bits of knowledge per parameter, meaning a 7B model can encode 14B bits of knowledge -surpassing English Wikipedia and textbooks combined (Allen-Zhu and Li, 2024). Table 5 summarizes the data scaling laws for developing neural models when data is not available in abundance.",
            "score": 0.5989670836420458,
            "section_title": "Data scaling laws",
            "char_start_offset": 18170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1935
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2391357421875
        },
        {
            "corpus_id": "253237527",
            "title": "A Solvable Model of Neural Scaling Laws",
            "text": "Large language models (LLMs) such as GPT-3 [1], LaMDA [2], and Palm [3] have made fantastic advances in the generation of language, so much so that they can convincingly write text that fools humans into thinking it's written by other humans. Built from the transformer architecture [4], these and similar dense LLMs [5][6][7][8] are \"large\" as in size, with Palm topping out at 540 billion parameters, and also \"large\" as in (big) data, with Chinchilla [8] trained on 1.4 trillion tokens. These regime that these models operate injointly large parameter and large data -differs from both the regime covered by classical statistical approaches to machine learning (see, e.g., [9]) -typically an underparameterized setting of large datasets and a fixed number of parameters and characterized by a biasvariance tradeoff -and the regime typically studied by modern theoretical approaches to deep learning [10][11][12][13][14][15][16] -an overparameterized setting of fixed datasets and a large number of parameters and characterized by interpolation [17] in which models memorize their training sets.\n\nInspired by the performance gains of the successive scaling up of LLMs, Ref. [18] comprehensively studied the test loss of such autoregressive transformer models trained on language model tasks across a large variety of model and dataset sizes. Impressively, they found that the overall performance can behave as a power law in any of parameters, dataset size, and compute, so long as the model isn't bottlenecked by any of the other two. (See, e.g., Fig. 1.) Moreover, by mapping the bottleneck and then jointly scaling parameters, data, and compute, practitioners can learn how to most efficiently apply their finite resources towards engineering bigger models, gathering more data, or burning their FLOPS. Thus, given the breadth of this empirical investigation over a number of orders of magnitude, the existence of these neural scaling laws, as they've been dubbed, have led many to believe a scaling hypothesis [19]: performance on language modeling tasks can be made",
            "score": 0.598284361321836,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "118117602"
                },
                {
                    "start": 910,
                    "end": 914,
                    "matchedPaperCorpusId": "13757156"
                },
                {
                    "start": 914,
                    "end": 918,
                    "matchedPaperCorpusId": "49321232"
                },
                {
                    "start": 918,
                    "end": 922,
                    "matchedPaperCorpusId": "62841516"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "203610462"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "198496504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4716796875
        },
        {
            "corpus_id": "271571035",
            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
            "text": "The ability of large language models (LLMs) to solve coding, mathematics, and other reasoning tasks has improved dramatically over the past several years [47,11,2,4]. Scaling the amount of training compute through bigger models, longer pre-training runs, and larger datasets has been a consistent driver of these gains [27,37,28]. \n\nIn contrast, a comparatively limited investment has been made in scaling the amount of computation used during inference. Larger models do require more inference compute than smaller ones, and prompting techniques like chain-of-thought [61] can increase answer quality at the cost of longer (and therefore more computationally expensive) outputs. However, when interacting with LLMs, users and developers often restrict models to making only one attempt when solving a problem.",
            "score": 0.5982004350658184,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72265625
        },
        {
            "corpus_id": "270063468",
            "title": "gzip Predicts Data-dependent Scaling Laws",
            "text": "In Tab. 1, we list the syntactic parameters for each grammar and the compression ratio we measured of token sequences sampled from the grammar.Observe that as non-terminals (grammatical categories), terminals (tokens), right-hand side options, and right-hand side length increase, the gzip-compressibility also increases (i.e. it becomes harder to compress).We plot these datasets alongside natural language & code in Fig. 1, showing how some PCFG datasets are more similar in complexity to code (the ones that are easier to compress) while others are more similar to natural language.To identify the scaling law for a dataset, we train a set of models of varying sizes (4.2M, 8.8M, 20.3M, 59.0M, 275.3M, 1.4B parameters; architectural specifics in Tab. 6) on varying size subsets of the data (100K, 1M, 5M, 20M, 50M, 100M tokens) and fit a power law on the resulting final losses of all training runs.Most experiments were run on a cluster of 4 Nvidia A100's with 80 GB VRAM each using PyTorch FSDP [Paszke et al., 2019, Zhao et al., 2023].We use a batch size of 32, single epoch training, the AdamW optimizer [Loshchilov and Hutter, 2017], and a learning rate starting from 5e\u22125 and cosine decayed over the number of train steps [Loshchilov and Hutter, 2016].\n\nAs intuitively expected, the more compressible a dataset is (lower compressibility ratio), the faster that models regardless of size will converge (Fig. 2).While this shows that we need more compute to model more complex datasets (which is in and of itself notable), we need more evidence to determine if the compute-optimal frontier directionally shifts based on data complexity.To establish such a non-trivial sensitivity of scaling laws to data complexity, we need to compute the law for each dataset and examine its fitted parameters.",
            "score": 0.5976650483315548,
            "section_title": "gzip-compressibility Measures Syntactic Complexity",
            "char_start_offset": 13153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1261
                },
                {
                    "start": 1263,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1643
                },
                {
                    "start": 1643,
                    "end": 1801
                }
            ],
            "ref_mentions": [
                {
                    "start": 1000,
                    "end": 1020,
                    "matchedPaperCorpusId": "202786778"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.184814453125
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "To compare the inference compute budgets of different models, we plot the figures with the number of FLOPs used per question during inference. We compute the inference FLOPs based on the commonly-used formula proposed by Kaplan et al. (2020). \n\nScaling law of compute-optimal inference for model size. Fig. 1 shows the relationship between inference compute and error rate for different model sizes. The error rate first decreases steadily and then starts to saturate. Initially, sampling many times from smaller models is compute-optimal. At larger compute budgets the larger models are preferable, since the performance of small models has saturated. As highlighted in the right panel of Fig. 1, the optimal model size varies based on the inference budget. We performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget. \n\nLlemma-7B achieves competitive accuracy to Llemma-34B with less compute. Fig. 4 and Fig. 5 shows the relationship between error rate and inference FLOPs for Llemma 7B and Llemma 34B using different inference strategies. Llemma-7B requires around 2\u00d7 less total FLOPs than Llemma-34B to achieve comparable accuracy. This held across inference strategies (sampling strategies, MCTS, REBASE) and tasks (MATH, GSM8K). This result suggests that, with the same training dataset and model family, generating more tokens with a suitable inference strategy using a smaller model can have more favorable cost-performance tradeoffs than using a larger model.",
            "score": 0.5966332554342604,
            "section_title": "COMPUTE-OPTIMAL MODEL SIZE",
            "char_start_offset": 21697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1717
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "270379481",
            "title": "Are Protein Language Models Compute Optimal?",
            "text": "While protein language models (pLMs) have transformed biological research, the scaling laws governing their improvement remain underexplored. By adapting methodologies from NLP scaling laws, we investigated the optimal ratio between model parameters and training tokens within a fixed compute budget. Our study reveals that pLM sizes scale sublinearly with compute budget, showing diminishing returns in performance as model size increases, and we identify a performance plateau in training loss comparable to the one found in relevant works in the field. Our findings suggest that widely-used pLMs might not be compute-optimal, indicating that larger models could achieve convergence more efficiently. Training a 35M model on a reduced token set, we attained perplexity results comparable to larger models like ESM-2 (15B) and xTrimoPGLM (100B) with a single dataset pass. This work paves the way towards more compute-efficient pLMs, democratizing their training and practical application in computational biology.",
            "score": 0.5964806460160232,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438232421875
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03. \n\n(13) \n\nThese findings indicate that hybrid approaches combining model size optimization with test-time computation strategies may offer the best trade-off for organizations deploying language models. Practitioners should assess the complexity of their target tasks and consider adaptive computation strategies that adjust inference resources based on problem difficulty. This approach can lead to significant cost savings while maintaining performance, particularly for routine tasks that do not require the full capacity of larger models. \n\nRQ5. How does scaling fine-tuning parameters affect performance on downstream tasks? Hernandez et al. (2021) proposed scaling laws for transfer by fine-tuning decoder-only transformer models on python code. They introduced a concept of effective data transferred D t , i.e., the amount of additional python data that a model of the same size trained on only python would have needed to achieve the same loss on python as a model pretrained on language, as a function of fine tuning data D f . The law is given as : \n\nAlong the same tracks, Lin et al. (2024a) proposed a rectified scaling law given by Equation 15. They introduced the term pre-learned data size D l that indicates how much amount of downstream data a model has learned from pre-training: \n\nwhere D is the fine-tuning data size and B, E, \u03b2 are fitting parameters. Abnar et al. (2021) predicted, downstream error e DS for image recognitions tasks on ViTs and ResNets as a function of upstream error e U S , given by the equation: \n\nThis was further explored by Mikami et al. (2021), modelling pre-training data size, consisting of syntheic dataset to predict downstream error with following equation: \n\nwhere D f denotes fine-tuning data, X denotes either LLM model size or pretraining data size or PEFT parameter size, and A, E, \u03b1, \u03b2 are fitting parameters. \n\nRQ5.",
            "score": 0.5964691863084732,
            "section_title": "D Additional research questions and guidelines",
            "char_start_offset": 41734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1982
                },
                {
                    "start": 1985,
                    "end": 2153
                },
                {
                    "start": 2156,
                    "end": 2311
                },
                {
                    "start": 2314,
                    "end": 2318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "237563187",
            "title": "Primer: Searching for Efficient Transformers for Language Modeling",
            "text": "In Section 4.1 of the main text, we reproduce the results of Kaplan et al. [9] and show that, at optimal parameter sizing, the relationship between language model quality and training compute follows a power law: l = ac \u2212k , where l is validation loss, c is training compute, and a and k are empirical constants. This is represented as a line in double log space (Figure 7): log l = \u2212k log c + log a. However, these lines are not the same for each architecture we compare. The lines are roughly parallel but shifted up and down. Thus, defining the shift between two architectures' lines as log b k , we can derive the relationship of their training costs as: \n\nwhere b is a consistent reduction factor regardless of l. Compute savings, s, for using a superior architecture can now be calculated as: \n\nPlugging this into the original power law relationship for c 1 we get: \n\nThus, the relationship between quality and compute savings yielded by an improved architecture also follows a power law with coefficient a 1 (1 \u2212 1/b) k . This relationship is intuitive when recognizing that the compute reduction factor b is consistent for all values of l and thus a power law investment of training compute with relation to l results in a power law savings with relation to l as well. \n\nA. 10  These results are run in the exact same configuration as Narang et al. [49], although we only run our models once, as MLM is not the focus of our work. * indicates rows that are taken from that study.",
            "score": 0.5960380592265097,
            "section_title": "A.9 Power Law Compute Savings Derivations",
            "char_start_offset": 35848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1486
                }
            ],
            "ref_mentions": [
                {
                    "start": 1282,
                    "end": 1284,
                    "matchedPaperCorpusId": "6287870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "272881081",
            "title": "Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale",
            "text": "We demonstrate that PROX could clearly improve data quality at scale only with acceptable extra computing. \n\nInference Time Scaling Recent trends in language models have begun to explore the potential of allocating additional computing at inference time, complementing the extensive computations already deviated to the pre-training and post-training phases. Several studies have demonstrated the potential of this approach, showing that smaller language models equipped with additional inference-time computing can perform comparably to, or even outperform, significantly larger models, evidenced across various domains, including code generation (Hassid et al., 2024;Brown et al., 2024), and math problem-solving (Snell et al., 2024;Wu et al., 2024). The significance of this approach has been further corroborated by OpenAI's latest o1 model release (OpenAI, 2024). While these studies focus on scaling computing on test time, our work demonstrates an alternative perspective on inference computing scaling. We advocate for allocating computing to refine pre-training corpora, particularly given that Internet-based corpora have been extensively utilized in language model pre-training. Our proposed PROX demonstrates remarkable gains in pre-training efficiency by investing moderately additional compute in the corpus refinement, facilitating more efficient and accessible development of LLMs.",
            "score": 0.595632403178872,
            "section_title": "RELATED WORKS",
            "char_start_offset": 29245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 109,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1397
                }
            ],
            "ref_mentions": [
                {
                    "start": 648,
                    "end": 669,
                    "matchedPaperCorpusId": "268819884"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "274131186",
            "title": "Character-level Tokenizations as Powerful Inductive Biases for RNA Foundational Models",
            "text": "In this section, we show several analyses on the factors that influence the performance of ChaRNABERT in order to identify the settings that provide the best performance of the model as well as to identify the optimal trade-off between performance and computational cost. \n\nAdvances in large language models (LLMs) have been driven by scaling up parameters, enhancing their applications in natural language processing (NLP), as detailed by the studies of (Kaplan et al., 2020) and (Hoffmann et al., 2022). While scaling studies have also progressed in fields like pLMs (Serrano et al., 2024), with research investigating model size effects, a comprehensive analysis of LLM scaling applied to RNA remains underexplored. Works such as RiNALMo (Peni\u0107 et al., 2024) have examined performance differences under variations of the parameter count, yet a detailed investigation of RNA-specific scaling laws is still absent. This analysis aims to bridge that gap, using ChaRN-ABERT as an initial model to guide future large-scale RNA LLM research. \n\nFirst, we explore how various learning rates and context window sizes affect ChaRNABERT's performance across model sizes, aiming to understand the impact of key hyperparameters. Next, we assess the model's efficiency with datasets of different sizes to evaluate the effects of data scaling. Finally, we analyze computational efficiency by measuring the impact of increased floating-point operations (FLOPs) on model improvements. Throughout, we follow (Hoffmann et al., 2022)'s scaling principles and compare tokenization strategies, highlighting the performance gains of using GBST over embeddings alone.",
            "score": 0.5954628485006236,
            "section_title": "Experiments",
            "char_start_offset": 26326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1646
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 591,
                    "matchedPaperCorpusId": "270379481"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.370361328125
        },
        {
            "corpus_id": "269983385",
            "title": "Neural Scaling Laws in Robotics",
            "text": "Neural scaling laws have driven significant advancements in machine learning, particularly in domains like language modeling and computer vision. However, the exploration of neural scaling laws within robotics has remained relatively underexplored, despite the growing adoption of foundation models in this field. This paper represents the first comprehensive study to quantify neural scaling laws for Robot Foundation Models (RFMs) and Large Language Models (LLMs) in robotics tasks. Through a meta-analysis of 327 research papers, we investigate how data size, model size, and compute resources influence downstream performance across a diverse set of robotic tasks. Consistent with previous scaling law research, our results reveal that the performance of robotic models improves with increased resources, following a power-law relationship. Promisingly, the improvement in robotic task performance scales notably faster than language tasks. This suggests that, while performance on downstream robotic tasks today is often moderate-to-poor, increased data and compute are likely to signficantly improve performance in the future. Also consistent with previous scaling law research, we also observe the emergence of new robot capabilities as models scale.",
            "score": 0.5953416637629325,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.311767578125
        },
        {
            "corpus_id": "273233741",
            "title": "Scaling Laws For Diffusion Transformers",
            "text": "Scaling Laws In this section, we investigate the scaling laws governing diffusion transformers, which describe the relationships between several key quantities: the objective function, model parameters, tokens, and compute. The objective measures discrepancy between the data and model's predictions. The number of parameters N reflects model's capacity, while tokens D denote the total amount of data (in tokens) processed during training. Compute C, typically measured in Floating Point Operations (FLOPs), quantifies the total computational resources consumed. In our experiments, the relationship between compute, tokens, and model size is formalized as C = 6N D, directly linking the number of parameters and tokens to the overall compute budget. \n\nBuilding on this, we hypothesize that power law equations can effectively capture the scaling relationships between these quantities. Specifically, we represent the optimal model size and token count as functions of the compute budget as follows: \n\nwhere N opt and D opt denote the optimal number of parameters and tokens for a given compute budget C, with a and b as scaling exponents that describe how these quantities grow with compute. \n\nTo empirically verify these scaling relationships, following Approach 2 in Hoffmann et al. (2022), we plot the isoFLOP figure to explore scaling laws. We select compute budgets [1e17, 3e17, 6e17, 1e18, 3e18, 6e18]. We change the In-context Transformers from 2 layers to 15 layers. For all experiments, we use AdamW (Kingma, 2014;Loshchilov, 2017) as the default optimizer. Except for the 1B parameter prediction experiments, we apply a constant learning rate of 1e-4. In the 1B parameter experiments, we train the model with a learning rate of 1e-4 for 80% of the total iterations, then decay it to 3.16e-5 for the next 10%, and finally to 1e-5 for the last 10%. This learning rate decay is crucial for larger models, leading to more accurate predictions.",
            "score": 0.5951155786360647,
            "section_title": "SCALING LAWS IN TRAINING DIFFUSION TRANSFORMERS",
            "char_start_offset": 14834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1951
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1688232421875
        },
        {
            "corpus_id": "247778764",
            "title": "Training Compute-Optimal Large Language Models",
            "text": "Recently a series of Large Language Models (LLMs) have been introduced (Brown et al., 2020;Lieber et al., 2021;Rae et al., 2021;Smith et al., 2022;Thoppilan et al., 2022), with the largest dense language models now having over 500 billion parameters. These large autoregressive transformers (Vaswani et al., 2017) have demonstrated impressive performance on many tasks using a variety of evaluation protocols such as zero-shot, few-shot, and fine-tuning. \n\nThe compute and energy cost for training large language models is substantial (Rae et al., 2021;Thoppilan et al., 2022) and rises with increasing model size. In practice, the allocated training compute budget is often known in advance: how many accelerators are available and for how long we want to use them. Since it is typically only feasible to train these large models once, accurately estimating the best model hyperparameters for a given compute budget is critical (Tay et al., 2021). Kaplan et al. (2020) showed that there is a power law relationship between the number of parameters in an autoregressive language model (LM) and its performance. As a result, the field has been training larger and larger models, expecting performance improvements. One notable conclusion in Kaplan et al. (2020) is that large models should not be trained to their lowest possible loss to be compute optimal. Whilst we reach the same conclusion, we estimate that large models should be trained for many more training tokens than recommended by the authors. Specifically, given a 10\u00d7 increase computational budget, they suggests that the size of the model should increase 5.5\u00d7 while the number of training tokens should only increase 1.8\u00d7. Instead, we find that model size and the number of training tokens should be scaled in equal proportions. \n\nFollowing Kaplan et al. (2020) and the training setup of GPT-3 (Brown et al., 2020), many of the recently trained large models have been trained for approximately 300 billion tokens (Table 1), in line with the approach of predominantly increasing model size when increasing compute.",
            "score": 0.595053238063128,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1792
                },
                {
                    "start": 1795,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 91,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 291,
                    "end": 313,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3974609375
        },
        {
            "corpus_id": "270870624",
            "title": "Information-Theoretic Foundations for Neural Scaling Laws",
            "text": "The analysis of Hoffmann et al. [2022] is somewhat generic rather than specialized to the particular neural network architecture used in that paper.In this paper, building on the work of Jeon and Van Roy [2022a,b], we develop rigorous information-theoretic foundations and use them to derive similar scaling laws.To keep things simple and concrete, we carry out the analysis with a particular data generating process for which neural networks are well-suited.The sorts of arguments developed by Hoffmann et al. [2022] are just as relevant to this context as they are to language models.Hoffmann et al. [2022] suggest that the compute optimal trade-off between parameter count and number of training tokens is linear, though the authors expressed some doubt and considered other possibilities that are near-linear as well.\n\nWe establish an upper bound on the minimal information-theoretically achievable expected error as a function of p and T and derive the relation required to minimize this bound for each compute budget.For large compute budgets, this relation is linear, as suggested by Hoffmann et al. [2022].\n\nOur main contributions include a first rigorous mathematical characterization of the compute-optimal efficient frontier for a neural network model and development of information-theoretic tools which enable that.A limitation of our analysis is in its simplified treatment of computational complexity as the product of the model and data set sizes; we do not assume any constraints on computation beyond those imposed by choices of p and T .In particular, we analyze, algorithms which carry out perfect Bayesian inference with respect to a model that is misspecificified due to its restricted size.While this abstracts away the details of practical training algorithms, empirical evidence suggests that our idealized framework leads to useful approximations [Zhu et al., 2022].In spite of these limitations, we hope our results set the stage for further mathematical work to guide hyperparameter selection when training large neural networks.",
            "score": 0.5948481101961984,
            "section_title": "Introduction",
            "char_start_offset": 2159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 459
                },
                {
                    "start": 459,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 821
                },
                {
                    "start": 823,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1114
                },
                {
                    "start": 1116,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1556
                },
                {
                    "start": 1556,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.169677734375
        },
        {
            "corpus_id": "276558421",
            "title": "The Relationship Between Reasoning and Performance in Large Language Models - o3 (mini) Thinks Harder, Not Longer",
            "text": "Large language models (LLMs) have evolved from handling basic natural language processing tasks to solving complex problems [1,2,3,4]. Scaling model size, data, and compute [5] has enabled larger models to develop richer internal representations [6,7] and emergent capabilities [8]. Recently, a new class of reasoning models has emerged that couples reinforcement learning with test-time compute scaling [9,10]. These models leverage reasoning tokens to guide the chain-of-thought process and maintain coherence throughout complex problem-solving tasks [11,12,13]. By explicitly optimizing the chain-of-thought in the reasoning tokens during training [14] and iteratively refining outputs at inference, these models achieve superior performance, including on challenging mathematical benchmarks [15,16]. Moreover, new test-time scaling laws demonstrate that longer reasoning-i.e. more reasoning tokens-yields log-linear performance gains [9]. \n\nIn this paper, we examine whether more capable models within a single family (o-series of OpenAI) require a longer chain-of-thought to achieve higher performance or if they can reason more effectively. By systematically comparing the number of tokens in the chain-of-thought generated by o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH dataset [17], we find that more proficient models (o1-mini vs. o3-mini (m)) do not generate longer reasoning chains Fig. 1 | Accuracy comparison of OpenAI models gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark. This figure displays the accuracy of gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark across disciplines and difficulty tiers. The gpt-4o model fails to attain 50% in any category and consistently lags behind the reasoning models. o1-mini significantly improves accuracy, reaching accuracies of 40-60% across all domains, while the o3-models surpass 50% accuracy in all categories.",
            "score": 0.5938573174505909,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1927
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 127,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 129,
                    "end": 131,
                    "matchedPaperCorpusId": "266223700"
                },
                {
                    "start": 131,
                    "end": 133,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "275133600"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61962890625
        },
        {
            "corpus_id": "258937784",
            "title": "A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning",
            "text": "For instance, Radford et al. [4] revealed that language models, when subjected to training on a novel dataset consisting of millions of webpages, exhibit an inherent ability to learn specific tasks without the need for any explicit supervision. They identified the significance of the language model's capacity in the success of zero-shot task transfer, with performance improving in a log-linear fashion across tasks. Their most extensive model, GPT-2, showcased an unparalleled performance on a majority of tested language modeling datasets, underscoring the potential of language processing systems that learn to perform tasks via naturally occurring demonstrations. \n\nSimilarly, Brown et al. [3] discovered that scaling up language models substantially enhances task-agnostic, few-shot, and GPT-3. They trained an autoregressive language model with a large parameter set, which was significantly larger than any previous non-sparse language model, and tested its performance in the few-shot setting. Despite acknowledging certain areas where GPT-3's few-shot learning struggles, their findings attest to the remarkable performance of GPT-3 across a range of NLP tasks. \n\nKaplan et al. [29] investigated the empirical scaling laws pertinent to language model performance on the cross-entropy loss, observing that the loss scales as a power-law with model, dataset size, and the amount of computation used in the training process. They noted the minimal effects of other features such as network width, despite the fact that larger models are significantly more sample-efficient. \n\nHowever, notwithstanding the compelling empirical results of these studies, there exists a conspicuous gap in our understanding of the model's behavior from a mathematical perspective. This gap is exactly what our research aspires to fill. We aim to provide a functional analysis of GPT, contributing to a deeper comprehension of the model's properties and elucidating the mechanisms that drive its performance.",
            "score": 0.5937597054351818,
            "section_title": "Definition 3 (Positional Encoding",
            "char_start_offset": 13114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31005859375
        },
        {
            "corpus_id": "275789021",
            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
            "text": "Our work examines sparse pre-training for large language models (LLMs) and presents a unified scaling law that effectively models both sparse and dense scaling. \n\nValue of sparse pre-training. Sparsely pre-trained LLMs match the final evaluation loss of dense models when their average parameter counts are the same (Figure 1). et al., 2023;Grattafiori et al., 2024) often train longer to reduce inference costs. Sardana et al. (2024) formalizes this practice by minimizing lifetime compute-the sum of training and inference compute. Applying the same analysis with our scaling law (see Appendix B), we find that training with less data can achieve the same target loss while reducing lifetime compute, compared with the prescription by Sardana et al. (2024). This surprising advantage stems from sparse pre-training's key feature: it decouples the average parameter count during training, which governs model quality, from the final parameter count after training, which determines inference compute. This decoupling enables a better balance between training efficiency and inference costs, making sparsity a crucial factor in designing compute-optimal training configurations for LLMs. \n\nCompression rate. Our findings suggest that up to a certain compression rate, sparse pre-training compresses the LLM during training without loss in quality. For a given sparse pre-training configuration, we define its compression rate as the ratio between two model sizes: the smallest dense model that matches the sparse model's evaluation loss (while keeping other factors like compute, data constant) and the final sparse model size. From our analysis, we know that a dense model needs as many parameters as the sparse model's average parameter count during pre-training to match its quality. Therefore, for a sparse pre-training configuration, its compression rate is also the ratio between average and final parameter counts. Within the LLMs we explore, we reach the maximum compression rate at 80% final sparsity, where our sparsity schedule results in an average parameter count of about 40% of the initial dense parameter count, yielding a 2x lossless compression rate. \n\nLimitation. We note that, due to the lack of adequate software and hardware support for executing matrix multiplications with unstructured sparsity, we are unable to demonstrate computational savings from sparse pre-training.",
            "score": 0.5936399507003169,
            "section_title": "CONCLUDING REMARK",
            "char_start_offset": 25384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 163,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2168
                },
                {
                    "start": 2171,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2396
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 434,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 737,
                    "end": 758,
                    "matchedPaperCorpusId": "266693796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "250113273",
            "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
            "text": "Recent work [1,2,3,4,5,6,7,8] has demonstrated that test loss L often falls off as a power law with different resources like model parameters (N ), number of training examples (P ), and amount of compute (C). However, the exponents \u03bd of these power laws are often close to 0, suggesting potentially inefficient use of resources. For example, for large models with lots of compute, so that the amount of training data constitutes a performance bottleneck, the loss scales as L \u2248 P \u2212\u03bd . Specifically for a large transformer based language model, \u03bd = 0.095, which implies an order of magnitude increase in training data drops cross-entropy loss by only about 0.6 nats ( Fig. 1 in [2]). In neural machine translation experiments \u03bd varies across language pairs from 0.35 to 0.48 (Table 1 in [5]). Interestingly, [8] explored a fixed computation budget C and optimized jointly over model size N and training set size P , revealing that scaling both N and P commensurately as C increases is compute optimal, and can yield smaller high performing models (trained on more data) than previous work. Nevertheless, for a transformer based language model, a 100\u00d7 increase in compute, corresponding to 10\u00d7 increases in both model size and training set size, leads to a drop in cross-entropy loss of only about 0.5 nats ( Fig. 2 in [8]). Similar slow scaling holds for large vision transformers where adding 2 billion pre-training images reduces ImageNet performance by a few percentage points ( Fig. 1 in [7]). While all of these results constitute significant improvements in performance, they do come at a substantial resource cost whose fundamental origin arises from power law scaling with small exponents. Recent theoretical works [23, 24,25] have argued that the power law exponent is governed by the dimension of a data manifold from which training examples are uniformly drawn. Here we explore whether we can beat power law scaling through careful data selection.",
            "score": 0.5935852572865321,
            "section_title": "Neural scaling laws and their potential inefficiency",
            "char_start_offset": 10264,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431396484375
        },
        {
            "corpus_id": "269899695",
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "text": "(e.g., agentic or \"emergent\" behaviors) and post-training interventions, can be done using a lower-cost, higher-resolution, and broader-coverage alternative to the standard approach of training (or using) a single family of LMs across compute scales. \n\nThe starting point of our work is the observation that there now exist hundreds of open models spanning a large range of scales and capabilities. While we cannot directly use these models for compute scaling laws (as the training compute efficiency varies widely across model families), we might hope that there exists a more general scaling law that holds across model families. In particular, we hypothesize that the downstream performance of an LM is a function of a low-dimensional space of capabilities (e.g., natural language understanding, reasoning, and code generation), and that model families vary only in the efficiency by which they convert training compute to these capabilities. If such a relationship held, it would imply that there is a log-linear relationship from low-dimensional capabilities to downstream capabilities across model families (which would allow us to build scaling laws that leverage all existing models), as well as a log-linear relationship between training compute and capabilities within each model family (as in standard compute scaling) (Fig. 1). \n\nThrough an analysis of existing standardized LM benchmarks (e.g., Open LLM Leaderboard [8]), we find a few such capability measures that have scaling law relationships with compute within model families (R 2 > 0.9) (Fig. 3), and with downstream metrics across model families. We call such scaling relationships observational scaling laws as they enable the predictions of complex downstream capabilities from simple observable quantities that we expect to scale with compute (like standardized benchmark performance). \n\nThe ability to build scaling laws across a large number of existing LMs from their standard benchmark metrics has significant advantages in cost, resolution, and coverage: Observational scaling incurs no training cost, while leveraging models spanning a much larger compute range than any single model family. It also significantly increases the resolution of scaling laws by virtue of using more models, which is useful for studying nearly discontinuous phenomena like \"emergent\" capabilities.",
            "score": 0.592653850483096,
            "section_title": "Introduction",
            "char_start_offset": 1988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2357
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "270379481",
            "title": "Are Protein Language Models Compute Optimal?",
            "text": "In this section we aim to solve the questions posed at the beginning of the manuscript through the proposed methods.\n\n3.1.What is the optimal ratio between the number of tokens and model parameters given a fixed compute budget?\n\nWe sought to derive power laws that describe the relationship between the computational budget, measured in FLOPs, and the optimal model size and training tokens for a range of model configurations.With this purpose, we systematically vary the number of training steps across different model parameter sizes for a later fitting of power laws to the resulting training loss curves and a final fitting of the parametric loss with the data obtained.\n\nFinding a power-law relationship between the parameter count, the number of tokens and computing budgets.By fitting power laws to the data of the most efficient models, we derived the relationships N opt \u221d C 0.27 and D opt \u221d C 0.71 .These relationships suggest that the optimal model size scales sublinearly with compute budget, while the optimal number of training tokens scales superlinearly, indicating that encoder-only, protein language models exhibit diminishing returns in training loss improvements with increasing model size.\n\nIntrigued by this findings we switched approaches to fit a parametric loss in order to obtain a more in depth explanation of the scaling parameters.   1 and 2), that the learning dynamics of all models for all token sets are lower bounded by the same loss value (around 2.45 ).\n\n3.2.Finding the optimal tradeoff between parameters and model size to efficiently reach the observed plateau.\n\nIn order to avoid any forced dynamics in the convergence of the losses upon training, we re-fitted a subset of the models with the different token splits used in the previous section 6 .We speculated that the learning rate decay upon tokens seen in training could interfere with standard training processes in pLMs.We trained and evaluated models with the following configurations: 5M, 15M, 35M, 50M, 100M, and 150M parameters, covering a broad spectrum of model complexities.With this configurations, we observed that all of the fitted models reach the same plateau at the same loss value, regardless of the learning rate decay strategy.The realization that all training configurations reached a common plateau led us to question whether there was an alternative approach to optimize training for pre-trained language models (pLMs).",
            "score": 0.5925940648751749,
            "section_title": "Experiments and Discussion",
            "char_start_offset": 4985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 118,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 227
                },
                {
                    "start": 229,
                    "end": 427
                },
                {
                    "start": 427,
                    "end": 675
                },
                {
                    "start": 677,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1211
                },
                {
                    "start": 1213,
                    "end": 1490
                },
                {
                    "start": 1492,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1601
                },
                {
                    "start": 1603,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1918
                },
                {
                    "start": 1918,
                    "end": 2079
                },
                {
                    "start": 2079,
                    "end": 2241
                },
                {
                    "start": 2241,
                    "end": 2436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.344482421875
        },
        {
            "corpus_id": "253237070",
            "title": "Trends in Energy Estimates for Computing in AI/Machine Learning Accelerators, Supercomputers, and Compute-Intensive Applications",
            "text": "Within AI/ML applications, Nature Language Processing (NLP) is used to parse vast amounts of literature in all languages and also enable computer-aided translation between the different languages. As opposed to linguistical methods, AI/ML methods depend on simpler analysis using words, phrases, part-of speech requirements, existing collections of text, and have been found to be very effective in applying these methods to reproduce texts with reasonable accuracies [4].   [19]. In contrast, geometric technology scaling takes more than twenty-four months, resulting in exponential increases of NLP computations ranging from 6 x 10 18 to 3 x 10 21 FLOPs for training a large model with over 100 billion NLP parameters [20,21]. In Figure 6, The main difference between training and inference is that the former needs large scale, higher precision (FP32 or FP16) computing upfront, while the latter is used routinely in applications many times over, generally at lower precision (FP16, INT16 or INT8). It is conceivable that the computational cost of training will be weighted upfront, while that of inference is spread over the entire time of application with comparable estimates, but further analysis is needed to quantify the differences between the two. Based on the analysis from the previous sections, we can roughly estimate the energy required for computing for a specific application. For illustration, we can assign EPI of 1 x 10 -12 Joule/FPI (FP16) (lower bound) to 1 x 10 -11 (FP64) (upper bound) representing two energy efficient floating-point cases from This energy at the lower bound is comparable to the average monthly electricity usage by the city of Atlanta or Los Angeles (~700 Kilowatt-hours in 2017) and at the upper bound compares with the monthly total electricity use by the top fifteen US cities combined (~9 Megawatt-hours in 2017). Clearly, even with the lower estimates, if the current trends were to persist, the energy consumption in training alone will more than off-set the energy efficiency from geometrical scaling or that due to architectures. In fact, more recent analysis suggests that the computational requirements for training NLP models may be many more orders of magnitude higher than the range we have addressed [21][22][",
            "score": 0.5918960530471782,
            "section_title": "D. Machine Learning in Natural Language Processing",
            "char_start_offset": 11883,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "7283070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2225341796875
        },
        {
            "corpus_id": "273162494",
            "title": "Scaling Parameter-Constrained Language Models with Quality Data",
            "text": "Recent advancements in language model (LM) development have been significantly influenced by the exploration of scaling laws, which articulate the relationship between training loss, dataset size, and the number of model parameters (Hestness et al., 2017;Kaplan et al., 2020;Aghajanyan et al., 2023). These scaling laws have been instrumental in predicting the computational resources necessary for training increasingly large models and have provided a framework for understanding how model performance scales with data and parameters (Hoffmann et al., 2022;Kaplan et al., 2020). However, these laws primarily focus on the quantity of data and model size, often underestimating the critical role of data quality in model generalization. \n\nIn this work, we challenge the prevailing focus 1 on merely increasing data volume and model size by emphasizing the importance of data quality, particularly in scenarios constrained by the number of model parameters. We argue that for sub-billion parameter models, the quality of data-or what we term as effective training tokens -plays a more decisive role in model performance than previously recognized. This perspective shifts the paradigm from a quantity-centric view to a quality-centric approach in the development of language models. Further, we provide qualitative measures of standard data refinement techniques including data sampling (Penedo et al., 2023;Wang et al., 2024;Albalak et al., 2024) and text synthesis (Liu et al., 2024), applied to a pretraining corpus such as Re-finedWeb (Penedo et al., 2023). This helps to formulate the relationship between the diversity and syntheticity of pretraining data in order to compute the number of effective training tokens, which evaluate the impact of data quality in terms of model size and the token number. Further, we conduct extensive experiments across eight different benchmarks to evaluate the impact of data refinement techniques which allow us to significantly outperform models trained on randomly selected data samples, across a spectrum of model sizes ranging from 25 million to 1.5 billion parameters. \n\nBy integrating the notion of effective token size into the scaling law formulation, we extend the existing scaling law formulation to better capture the nuances of data quality.",
            "score": 0.5911849735304625,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 299,
                    "matchedPaperCorpusId": "255570036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51416015625
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "Neural scaling laws The modern study of neural scaling laws began with Hestness et al. 2017, who observed a power law relationship between data size and error across image, language, and speech domains. Similarly, Rosenfeld et al. 2019 identified a power law relationship involving both model size and data. The work of Kaplan et al. 2020 andHoffmann et al. 2022 furthered this research in language modeling, emphasizing the implications for allocating compute budgets-a tradition we continue in this study. Hernandez et al. 2021 studied scaling laws for transfer in model size and fine-tuning data. Unlike their study, ours differs in three key ways. First, our study examines pre-training steps and fine-tuning data. Second, rather than studying the effective data transferred, our study directly investigates how test loss varies as a function of training inputs. These distinctions are crucial because they allow us to directly estimate the \"transfer gap\" between two distributions, enabling a more fine-grained analysis of the limits to pre-training. Finally, we examine transfer across multiple diverse datasets, not just from English to Python. \n\nAnother similar study is Mikami et al. 2021, which investigated how pre-training data affects transfer properties, similar to our approach. Unlike their study, ours involved training models to convergence on the fine-tuning data but not on the pre-training data, which we believe is a more realistic assumption in the large-data regime. \n\nTheir research also focused on image classification, whereas ours is focused on natural language processing. Moreover, we aim to measure the \"transfer gap\" between various distributions more directly and show how these gaps inform economic trade-offs when training large machine learning models, an aspect not addressed by Mikami et al. 2021. Nonetheless, we borrow the scaling law form from Mikami et al. 2021, which they postulated through theoretical analysis. For convenience and transparency, we detail how this scaling law can be derived in Appendix A. \n\nWhile our study focuses on transfer learning scaling laws between language datasets, Aghajanyan et al. 2023 investigated scaling laws for generative language models trained on multiple modalities. They modeled the individual contributions and interactions between modalities, identifying regimes of competition and synergy, where training on multiple modalities either negatively or positively affected performance, respectively.",
            "score": 0.5911391757690343,
            "section_title": "Related work",
            "char_start_offset": 5224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2051
                },
                {
                    "start": 2054,
                    "end": 2250
                },
                {
                    "start": 2251,
                    "end": 2483
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "273233741",
            "title": "Scaling Laws For Diffusion Transformers",
            "text": "Using these optimal configurations, we derive scaling laws by fitting a power-law relationship between compute budgets, model size, consumed data, and training loss. To evaluate the derived scaling laws, we 1 arXiv:2410.08184v1 [cs.CV] 10 Oct 2024 For each training budget, we train multiple models of varying sizes. A parabola is fitted to the loss for each training budget, and the minimum point on each parabola (represented by the purple dots) corresponds to the optimal allocation of model size and data for that specific budget. By identifying the model and data sizes at these optimal points, we can plot the scaling trends of model parameters, tokens, and training budgets. The powerlaw curves shown allow us to predict the optimal configurations for larger compute budgets, such as 1.5e21 FLOPs. extrapolate the compute budget to 1.5e21 FLOPs that results in the compute-optimal model size (approximately 1B parameters) and the corresponding data size. Therefore, a 1B-parameter model is trained under this budget and the final loss matches our prediction, demonstrating the effectiveness and accuracy of our scaling laws. \n\nTo make the best use of the scaling laws, we demonstrate that the generation performances (e.g., FID (Fr\u00e9chet Inception Distance)) also match the trend of pretraining loss. Namely, the synthesis quality also follows the power-law relationship with the compute budget, making it predictable. More importantly, this observation is transferable across various datasets. We conduct additional experiments on the COCO validation set (Lin et al., 2014), and the same scaling patterns hold, even when tested on out-of-domain data. Accordingly, scaling laws could serve as a predictable benchmark where we can assess the quality of both models and datasets at a significantly reduced computational cost, enabling efficient evaluation and optimization of the model training process. \n\nTo summarize, we at first confirm the presence of scaling laws in diffusion transformers during training, revealing a clear power-law relationship between compute budget and training losses. Next, we establish a connection between pretraining loss and synthesis evaluation metrics. Finally, We conducted preliminary experiments that demonstrate the potential of using scaling laws to evaluate both data and model performance.",
            "score": 0.5910736589698196,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 1562,
                    "end": 1580,
                    "matchedPaperCorpusId": "14113767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10797119140625
        },
        {
            "corpus_id": "275133390",
            "title": "Efficiently Scaling LLM Reasoning with Certaindex",
            "text": "Large language model (LLM) capabilities have advanced rapidly, evolving from basic language understanding to solving complex mathematical problems [5,38,47], generating and debugging code [20,52,53], and analyzing case law [25]. A key driver for this progress is the emergence of sophisticated inference-time algorithms that enhance LLM reasoning [16,48,51]. These \"LLM reasoning\" algorithms enable LLMs to reflect on their own outputs, explore different reasoning paths, and generate more robust and accurate answers to challenging questions. However, they also yield more output tokens, sometimes by orders of magnitude [9,38,47], driving up inference compute demands and response latencies. \u2020Equal contribution. \n\nAlthough reasoning algorithms are diverse, they share in common that they each admit a flexible trade-off between inference compute and output accuracy, known as inference-time scaling. For example, self-consistency (SC) [48] generates multiple solutions to a query and aggregates the final answer via majority voting. Higher accuracy might be achieved by generating more solutions, but at the cost of more GPU cycles. Similarly, Monte Carlo Tree Search (MCTS) [12,16] expands nodes of a solution tree, each representing a possible solution step. Expanding more nodes into a deeper or wider tree increases the likelihood of finding a correct solution but consumes more GPU resources to decode additional tokens. \n\nWhile reasoning algorithms and their inference-time scaling properties have been extensively studied in experimental settings, deploying them in real-world and multi-tenant environments raises additional challenges in resource allocation. First, different user queries may pose varying levels of difficulty to solve, resulting in different amounts of inference compute required. Second, assuming work-conservation, allocating more compute to one user query inevitably delays or reduces compute resources available for others. Third, the best end-to-end latency is achieved by allocating the least amount of time needed to reach a correct answer, and no more than that. All of these objectives call for adaptively allocating inference compute between different user queries to effectively balance accuracy and cost. \n\nFig. 1(a) illustrates the characteristics of inference-time scaling on GSM8K [7], a popular math reasoning benchmark.",
            "score": 0.5903725663750945,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2245
                },
                {
                    "start": 2248,
                    "end": 2365
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "269449894",
            "title": "Temporal Scaling Law for Large Language Models",
            "text": "The concept of scaling laws for language models was proposed by (Kaplan et al., 2020). Their study revealed that the test loss for generative transformer models scales as a power-law with model size, dataset size, and the amount of compute used for training. Building upon this foundational study (Kaplan et al., 2020), further research has expanded the concept of scaling laws across diverse problem settings (Hernandez et al., 2021) and model architectures (Cherti et al., 2023;Aghajanyan et al., 2023). For instance, (Hernandez et al., 2021) has investigated scaling laws for transfer learning, and discovered that the effective data transferred for a model trained on natural language to a model trained on held-out data also follows a power-law. In the scope of multimodal models, (Cherti et al., 2023) found that scaling behavior differs when trained on different task-dependent datasets in different tasks, and (Aghajanyan et al., 2023) organized a scaling law which unified the contributions and interactions of different modalities during multi-modal pre-training. \n\nDespite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training. By studying the loss behavior on different token positions, we introduce a more accurate temporal scaling law for LLM test loss, allowing for precise tracking and prediction of LLM performance during the pre-training stage. \n\n3 Temporal Scaling Law",
            "score": 0.5902978990149376,
            "section_title": "Scaling Laws for Language Models",
            "char_start_offset": 8141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1820
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 480,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 480,
                    "end": 504,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 786,
                    "end": 807,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 918,
                    "end": 943,
                    "matchedPaperCorpusId": "255570036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "268351635",
            "title": "Unraveling the Mystery of Scaling Laws: Part I",
            "text": "The significance of scaling laws extends beyond mere prediction of the loss trajectory.More importantly, they can aid in pinpointing the optimal experimental configuration without requiring extensive tuning on very large models, thereby transforming the training of large language models from an alchemy-like trialand-error process into a principled methodology.In this section, we highlight main benefits of scaling laws and discuss ways to further advance beyond them.\n\nDetermining B As long as all hyperparameters are well-tuned (especially the learning rate and regularization hyperparameters) and the number of training steps is sufficient, it is believed that the same final performance should be attainable using any batch size [SLA + 19], so the batch size mainly influences the training speed of language models.Often, when training large language models, the ideal batch size is suggested to be set as the largest batch size supported by the available hardware [GDG + 23], so as to maximize the training speed without considering the computational cost.In Eq 3.12, we show that the critical batch size with the optimal speed/computation trade-off can be analytically computed from the loss value.Under the guidance of this formula, we would be able to estimate the preferred batch size under any loss trajectory.Furthermore, this optimal batch size in Eq 3.12 is determined by equally minimizing the training time and required computation, as shown in Eq 3.9.In practice, if we would like to prioritize one over the other, we can follow the same process to derive the optimal batch size.By this means, we are able to obtain the optimal batch size based on our customized need in a systematic way.\n\nDetermining N and S In practice, we often opt for the largest affordable model size and train the model until convergence.Nevertheless, this simplistic approach can deviate significantly from the optimal configuration and result in substantial resource wastage.Scaling laws provide a principled approach to choosing the optimal model size N and number of training steps S given a fixed computational budget C8 .Given that Eq 3.13 already provides the precise relation between the loss L, batch size B, model size N and training steps S, we could find the model size that minimizes L under the critical batch size (B = B crit ).",
            "score": 0.590087735991722,
            "section_title": "Discussions",
            "char_start_offset": 19934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 87,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 470
                },
                {
                    "start": 472,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1322
                },
                {
                    "start": 1322,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1706
                },
                {
                    "start": 1708,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1969
                },
                {
                    "start": 1969,
                    "end": 2119
                },
                {
                    "start": 2119,
                    "end": 2335
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "274150480",
            "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
            "text": "Standard approaches to scaling laws attempt to fit a curve to the optimal number of model parameters N and training tokens D to minimize the pre-training loss under a given budget of FLOPs (Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022;Porian et al., 2024;Abnar et al., 2021;Maloney et al., 2022;Bordelon et al., 2024a). \n\nTo fit these curves, it is useful to specify a parametric form of the loss in terms of N and D. Hoffmann et al. (2022) assumes this curve takes the following form: \n\nThis formula is inspired by classical upper bounds on a loss decomposition that attributes error to Bayes risk (entropy), approximation error (from having finite parameters), and estimation error (from having finite data) (Bottou and Bousquet, 2007). \n\nOn the other hand Kaplan et al. (2020) instead assumes that: \n\n(2) \n\nBelow, we will advocate for a slightly different functional form that blends the two of these. \n\nRegardless of the functional form, scaling laws have been an integral part of the success of modern neural language models. Our work builds on the ideas originated in this line of work and extends them to consider how to translate scaling laws across data distributions.",
            "score": 0.5899495003521885,
            "section_title": "Scaling laws",
            "char_start_offset": 5048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396484375
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "Figure 1(3) illustrates that PARSCALE offers superior inference efficiency. \n\nFurthermore, we show that the high training cost of PARSCALE can be reduced by a two-stage approach: the first stage employs traditional training with most of the training data, and PARSCALE is applied only in the second stage with a small number of tokens. Based on this, we train 1.8B models with various P and scale the training data to 1T tokens. The results of 21 downstream benchmarks indicate the efficacy of this strategy. For example, when scaling to P = 8, it yields a 34% relative improvement for GSM8K and 23% relative improvement for MMLU using exactly the same training data. We also implement PARSCALE on an off-the-shelf model, Qwen-2.5 (Qwen Team, 2024), and demonstrate that PARSCALE is effective in both full and parameter-efficient fine-tuning settings. This also shows the viability of dynamic parallel scaling, which allows flexible adjustment of P during deployment while freezing the backbone weights, to fit different application scenerios. \n\nTable 1 compares PARSCALE with other mainstream scaling strategies. Beyond introducing an efficient scaling approach for language models, our research also tries to address a more fundamental question in machine learning: Is a model's capacity determined by the parameters or by the computation, and what is their individual contribution? Traditional machine learning models typically scale both parameters and computation simultaneously, making it difficult to determine their contribution ratio. The PARSCALE and the fitted parallel scaling law may offer a novel and quantitative perspective on this problem. \n\nTable 1: Comparisons of mainstream LLM scaling strategies. We subdivide parameter scaling into traditional Dense Scaling and Mixture-of-Expert (MoE) Scaling (Fedus et al., 2022) for comparison. Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference.",
            "score": 0.5897904642358729,
            "section_title": "Introduction",
            "char_start_offset": 4059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 78,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 1816,
                    "end": 1836,
                    "matchedPaperCorpusId": "231573431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "In this study, we presented an empirical analysis of scaling laws for transfer learning in transformer language models. By fitting a scaling law that incorporates terms for pre-training data and fine-tuning data size, we were able to measure the degree of transfer from pre-training to various downstream distributions. This scaling law provided a good fit across several language datasets, and reveals significant variation in the magnitude of the transfer gaps. \n\nA key positive result was that the standard errors for all parameter estimates were relatively tight, even though we used only 150 data points per fine-tuning dataset. Given that we achieved useful results without significant computing resources, we believe that our study has illustrated the potential for using scaling laws for transfer to precisely measure the transfer gap between distributions, even without expensive computing setups. \n\nOur work has important implications for the development of foundation models and achieving higher performance on complex tasks. When the transfer gap is sufficiently small, pre-training can become a highly cost-effective strategy for enhancing downstream performance. Conversely, for tasks with large transfer gaps, focusing resources on curating fine-tuning data may be more impactful. By accurately measuring transfer gaps across a wide range of domains, we can develop a clearer picture of the relative difficulty of achieving high performance on different tasks using deep learning and guide resource allocation accordingly. \n\nHowever, we also caution against interpreting these results too broadly. Since we did not examine the effect of model scale or model architecture on scaling laws for transfer, it is not straightforward to apply these scaling laws to models in completely different contexts. While we hope that this strand of research can be useful for helping researchers allocate resources between pre-training and fine-tuning to achieve high downstream performance, a more complete investigation is needed before this work can be applied directly to many applications. \n\nA More details about the scaling law for transfer \n\nTo speculate the form of the scaling law for transfer, we borrow from Mikami et al. 2021, who proposed plausible criteria for such a law, derived from their empirical analysis. It is important to note some key differences between their study and ours: they focused on L 2 loss in images, whereas our study deals with cross-entropy loss on mainly natural language data.",
            "score": 0.5897111361894045,
            "section_title": "Conclusion",
            "char_start_offset": 27685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2145
                },
                {
                    "start": 2148,
                    "end": 2324
                },
                {
                    "start": 2325,
                    "end": 2516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40283203125
        },
        {
            "corpus_id": "268681464",
            "title": "Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance",
            "text": "We briefly review the pretraining process of large language models and summarize key findings from neural scaling laws, then we formalize the problem we study. Further related works are in App. A. \n\nPretraining large language models. We consider the task of pretraining an autoregressive language model p \u03b8 via next-token predictions (Radford et al., 2018). The training dataset D train = {D i } M i=1 composes M domains with mixture proportions r \u2208 \u2206 M\u22121 . In each training step, the task first samples a batch of domain indices according to the mixture proportions and then sample sequences of L tokens from the sampled domains. Using the sampled data, it learns to optimize the negative log-likelihood of sampled data, i.e., \n\nTo evaluate the learned model, we compute the loss on validation data D val . \n\nScaling laws. For a wide range of factors x, scaling laws (Kaplan et al., 2020;Henighan et al., 2020;Hoffmann et al., 2022) show that their effect on the losses L follows power laws \n\nwhere c, k, and \u03b1 are parameters to fit and x can be model sizes, numbers of training data, training steps, and the amount of computation. Previous experience (Alabdulmohsin et al., 2022;OpenAI, 2023;Bi et al., 2024;Su et al., 2024) highlights the impressive predictability of scaling laws. Specifically, Eqn. 3 fitted on a collection of small models, training data, or computation can extrapolate to precisely predict the test loss of larger cases over orders of magnitudes. This enables practitioners to estimate the performance of a pretrained large language model without actually finishing the expensive runs. Recent development further shows various functional relationships between the performance of language models and a broader range of factors, including transfer learning (Hernandez et al., 2021), sparse architectures (Frantar et al., 2023), and repeated data (Muennighoff et al., 2024), consolidating the predictability of language model performance. \n\nProblem formalization. We study optimizing the mixture proportions of pretraining data for large language models.",
            "score": 0.588163613957909,
            "section_title": "Background",
            "char_start_offset": 5234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 911,
                    "end": 933,
                    "matchedPaperCorpusId": "2757547"
                },
                {
                    "start": 1153,
                    "end": 1181,
                    "matchedPaperCorpusId": "252220884"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "276574738",
            "title": "DISC: Dynamic Decomposition Improves LLM Inference Scaling",
            "text": "Scaling inference efficiency remains a fundamental challenge for large language models (LLMs). Many existing approaches improve inference by decomposing problems into smaller steps and systematically exploring different solutions (Feng et al., 2023;Zeng et al., 2024;Wu et al., 2024;Nori et al., 2024;Snell et al., 2024;Brown et al., 2024;Gandhi et al., 2024;Lee et al., 2025;Light et al., 2024a;Wang et al., 2025). Some decomposition methods rely on domain-specific heuristics and hand-crafted rules (Yao et al., 2024; Zelik-1 Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, USA 2 NEC Laboratories America, Princeton, NJ, USA 3 Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA 4 NEC Corporation, Tokyo, Japan. Correspondence to: Jonathan Light <jonathan.li.connect@gmail.com>, Wei Cheng <weicheng@neclabs.com>. arXiv preprint, 2025. Copyright 2025 by the author(s). man et al., 2023;Zhou et al., 2022). However, manually partitioning problems or designing task-specific heuristics is costly and lacks generalization. Moreover, identifying critical steps for an LLM can be non-trivial for humans. As shown in Sec. 3.5, LLMs may assign importance to seemingly trivial words (e.g., therefore or which), which, while counterintuitive to humans, play a crucial role in autoregressive generation (Lin et al., 2025). Other approaches employ fixed, uniform step sizes, such as token-or sentence-level decomposition (Feng et al., 2023;Guo et al., 2025). All these methods rely on static decomposition strategies, where step sizes are predefined or determined via heuristics. Such rigidity wastes compute on steps that are easy for the LLM (but potentially difficult for humans) while undersampling more challenging steps.",
            "score": 0.5877960865087538,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 414,
                    "matchedPaperCorpusId": "272423808"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28857421875
        },
        {
            "corpus_id": "239016819",
            "title": "PAGnol: An Extra-Large French Generative Model",
            "text": "Large pre-trained language models are the workhorses of modern Natural Language Processing (NLP). The use of scalable and efficient attention-based Transformers (Vaswani et al., 2017), rather than recurrent neural networks, has enabled increasingly large and capable models. Through self-supervised learning, these models learn contextual word embeddings, building a general representation of language. After this pre-training they can be fine-tuned to target specific tasks (e.g. classification, parsing, summarization). \n\nThree approaches dominate the field: (1) causal autoregressive decoder-only models, such as GPT (Radford et al., 2018), learning from a general language modelling tasks; (2) bidirectional encoder-only models, such as BERT (Devlin et al., 2018), learning from masked language modelling; \n\n(3) sequence-to-sequence models, such as BART (Lewis et al., 2019) or T5 (Raffel et al., 2020), combining both a bidirectional encoder and an autoregressive decoder, learning from a language denoising task. Encoder-only and sequence-to-sequence models excel in language understanding tasks, and have shadowed autoregressive models as a lesser option. \n\nAutoregressive models have been shown to predictably benefit from increased size (Kaplan et al., 2020;Henighan et al., 2020). Scaling laws establish a direct relationship between model size and end-task performance, justifying the training of increasingly large models (Brown et al., 2020;Zeng et al., 2021;Kim et al., 2021;Wei et al., 2021). These laws can also inform design decisions, helping practitioners use their available compute budget optimally. A significant finding has been that larger models are more sample and compute efficient: with a given compute budget, it is preferable to train a larger model significantly short of convergence than to train a smaller model to convergence.",
            "score": 0.5877571926814211,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 885,
                    "end": 906,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "276813519",
            "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning",
            "text": "Test-Time Scaling in Large Language Models. Increasing test-time computation has consistently been shown to improve performance in complex reasoning tasks, mathematical problem-solving, and code generation (Wu et al., 2024;Wang et al., 2023;Wei et al., 2023;DeepSeek-AI et al., 2025;Snell et al., 2024). Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning chains or longer ones (Wu et al., 2024;Snell et al., 2024;OpenAI et al., 2024a). Prominent approaches include parallel sampling of multiple reasoning paths (Wang et al., 2023;Aggarwal et al., 2023), tree-based search (Yao et al., 2023;Wu et al., 2024;Xin et al., 2024), and iterative refinement techniques (Welleck et al., 2023;Madaan et al., 2023;Snell et al., 2024;Welleck et al., 2024). Recent reasoning language models such as \"O1\" and \"R1\"-style models (OpenAI et al., 2024a;DeepSeek-AI et al., 2025) simplify test-time scaling by generating extended reasoning traces (longer chains-of-thought). Despite their promising results, these methods lack precise and dynamic control over the length of the generated reasoning chains, resulting in often suboptimal performance or unrealized potential efficiency gains. \n\nOur work complements and extends this line of research by enabling reasoning models to precisely control the length of generated outputs, thereby providing flexibility to calibrate inference compute based on task-specific requirements. \n\nLength Control in Large Language Models. Controlling the length of LLM-generated outputs is an important practical consideration across various generation tasks.",
            "score": 0.587450317573555,
            "section_title": "Related Work",
            "char_start_offset": 4342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 43
                },
                {
                    "start": 44,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1658
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 638,
                    "matchedPaperCorpusId": "258823191"
                },
                {
                    "start": 747,
                    "end": 769,
                    "matchedPaperCorpusId": "253244506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7724609375
        },
        {
            "corpus_id": "195345672",
            "title": "Evaluating Computational Language Models with Scaling Properties of Natural Language",
            "text": "In this section, we explain scaling properties, the statistical properties of natural language text that have a power-law form. One study on the statistics of natural language reported nine scaling laws (Altmann and Gerlach 2017). Four of them concern word formation and a network structure, which do not directly relate to language modeling. This leaves five scaling properties, which can be categorized into those for the vocabulary population and those for long memory. These properties are characterized by powerlaw functions, which involve a power exponent. The exponents of the scaling properties have the capability to characterize the degree of each property. They therefore serve to evaluate whether a language model has the same behavior as natural language text. Specifically, given a text generated by a language model, we set two levels of assessment for evaluation: Q1 Does the scaling property hold qualitatively? Q2 How does the exponent differ from that of the training data? \n\nAs revealed in the following sections, many models fail to satisfy even the first criterion, especially for long memory. For those models that do satisfy Q1, their exponents can be compared with those of the original text. \n\nHence, we propose the exponents of scaling properties as metrics to evaluate machine-generated text. Consider a power-law relation y \u221d z \u03ba for points (y 1 ,z 1 ),. . ., (y N ,z N ). These points (y i ,z i ) are calculated for any given text. Let c be the coefficient of the power law, and then the exponent \u03ba is estimated by the least-squares method: \u03b5(\u03ba, c) \n\nThe data points are regressed on a log-log scale. The regression method could be a problem if the errors between the data points and the fitting function are not Gaussian distributed. There are other proposed regression methods such as maximum likelihood estimation for Zipf's law (Clauset, Shalizi, and Newman 2009;Gerlach and Altmann 2013). In this article, however, because exponents obtained with the leastsquares method are effective in distinguishing machine-generated text from natural language text, and because this method has been a conventional standard, we adopt it for estimation.",
            "score": 0.5869674411099346,
            "section_title": "Scaling Properties of Natural Language for Evaluation",
            "char_start_offset": 9541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 1862,
                    "end": 1897,
                    "matchedPaperCorpusId": "9155618"
                },
                {
                    "start": 1897,
                    "end": 1922,
                    "matchedPaperCorpusId": "18208334"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "272827489",
            "title": "Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task",
            "text": "So far, we experimented with a scaling law formula based on the model size only, ignoring the training dataset size. Even if we just showed that lower perplexity/loss can be obtained with fewer data samples (in the case of the en-fr and en-de directions), larger training datasets still tend to increase the overall models' quality. But, it's also a waste of computing resources to train a model on more data than required, this is why modern language modeling scaling formula take into account both the number of trainable parameter and the training dataset size. Hence, we fitted multiple Chinchilla laws following the setting of Hoffmann et al. ( 2022), whose form is given below, on various combinations of input data to see if it can be used to reliably predict model performances. \n\nE, a, \u03b1, b and \u03b2 are variables fitted by minimizing the huber loss (with a delta value of 0.01) using the BFGS algorithm from SciPy (Virtanen et al., 2020) ; N and D are, respectively, the number of non-embedding parameters of the model and the number of training samples. More details are given in the original paper. As shown in Figure 6, the test loss of our translation models can be realistically described by the power law fitted on observations made on all our models (the purple dotted line). Furthermore, the general shape of the fitted curves is more stable, and thus more trustworthy. Indeed, the curve fitted on all models is very close to the one fitted without the 6.9B model, indicating that behaviors of larger models can be better estimated with this form of scaling law. However, as with the previous scaling law, the curve deviate from real observations when it is fitted on less data points. While it is not a surprising finding, it shows that scaling laws should not be trusted beyond a certain model size. However, we cannot provide a reasonable window in which the estimated loss is realistic. These experiments shows two things. First, the test loss of decoder-based translation models follows a scaling law similar to language modeling models, as the curves fitted on all data points matches the real observations.",
            "score": 0.586695880118663,
            "section_title": "Applying language modeling scaling law",
            "char_start_offset": 21652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2128
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "267499620",
            "title": "The Information of Large Language Model Geometry",
            "text": "Large language models (LLMs) have brought about a revolution in natural language processing, enabling significant breakthroughs across various tasks [8,20]. However, comprehending the underlying mechanisms that drive their impressive performance remains a challenging endeavor. One interesting phenomenon observed in LLMs is the scaling law [11,12], which shows that the performance of models follows a predictable power-law relationship with quantities like model parameters. While scaling laws have been empirically discovered in diverse domains, the theoretical understanding of it is rare [4,15]. The understanding of scaling laws in LLMs is of great importance as it can provide insights into the scalability, efficiency, and generalization capabilities of these models as they grow larger. \n\nTo begin unraveling the mysteries of scaling laws in LLMs, we conduct simulations [25] to analyze the representation entropy, revealing an interesting finding: a power law relationship may exist between representation entropy and model sizes. This discovery shows that LLM representation (geometry) has a deep connection with information theory. Building upon this insight, we propose a novel theoretical framework based on (conditional) entropy to provide a deeper understanding of the scaling law phenomenon. Our theory offers valuable insights into how information is encoded and utilized within LLMs as they scale. \n\nWhen dealing with scaling law, we consider each sentence as a whole, therefore focusing more on a \"macro\" behavior. When considering the \"micro\" structure, we shift our focus to the auto-regressive structure inherent in LLMs, specifically examining the relationship between the last token and the preceding context tokens. Our analysis establishes a compelling theoretical connection between the information gain brought by new tokens and ridge regression. This connection not only deepens our understanding of the auto-regressive nature of LLMs but also provides insights into how the addition of new tokens contributes to the overall information content of the models. Another critical aspect we explore is the effectiveness of token selection mechanisms within LLMs. Surprisingly, our analysis reveals that Lasso regression sometimes outperforms the closely related attention weights in identifying context tokens with high information content. \n\nFinally, we investigate whether a token embedding contains all the information from its preceding context. By comparing the result given by mean embedding and specific token embedding, we find that information is encoded among all tokens, not concentrated on specific tokens.",
            "score": 0.5865098179670873,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2366
                },
                {
                    "start": 2369,
                    "end": 2475
                },
                {
                    "start": 2476,
                    "end": 2644
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2880859375
        },
        {
            "corpus_id": "225094178",
            "title": "Scaling Laws for Autoregressive Generative Modeling",
            "text": "Instead of focusing on converged performance, one can study the loss L achieved with a finite training compute budget C when training with a large enough dataset to avoid overfitting. We define C theoretically rather than empirically, and approximate7 it as C \u2261 6N E where N is the non-embedding parameter count (model size) and E = SB is the total number of tokens processed during training (with S the number of parameter updates and B the batch size in tokens). The results for L(C) from a variety of model sizes are depicted in figure 5, along with the pareto-frontier of optimal loss for a given compute budget, and a power-law plus constant fit forced to lie below this frontier. Scaling laws with compute-Scaling laws with compute (total estimated floating point operations) for various domains, along with power-law plus constant fits (dashed). This is identical to figure 1, except that we do not subtract the fitted constant irreducible loss. Note that very small models underperform compared to the trends when they model images or videos with very large contexts. Note also that the largest language models [BMR + 20] were not trained to convergence. \n\nThe compute trends are most relevant for differentiating between the irreducible loss and reducible losses, since they avoid the issue of training to convergence, which makes the interpretation of L(N ) difficult. We display the reducible loss trends for L(C) in figure 1, and emphasize that these appear to be pure power-laws, even when the reducible loss is much smaller than the irreducible loss. \n\nWe can use the L(C) trends to estimate the model size N opt that optimizes the loss when training is constrained by a fixed compute8 budget C. For this purpose we select points on the convex hull of the loss versus compute frontier; these can be seen as blue points in figure 5. The results for all domains together appear in figure 2, while each domain is shown separately with individual fits in figure 16. In all cases we find that N opt (C) \u221d C \u03b2 can be fit with a pure power-law, with all exponents fairly close to \u03b2 \u223c 0.7.",
            "score": 0.5864891353439398,
            "section_title": "Compute Scaling and Optimal Model Sizes",
            "char_start_offset": 21358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5498046875
        },
        {
            "corpus_id": "272827424",
            "title": "Archon: An Architecture Search Framework for Inference-Time Techniques",
            "text": "Language models [22,40,50,53] have transformed the field of artificial intelligence across a vast number of domains and tasks. LLMs are pretrained on substantial amounts of textual data before being further aligned with human preferences through instruction fine-tuning [10,57], direct policy optimization (DPO) [44], Kahneman-Tversky optimization (KTO) [16], reinforcement learning from AI feedback (RLAIF) [5], and other techniques. As language models continue to gain improved abilities with further scaling of data, parameters, and compute [17,26], the cost of developing new LLMs is ever increasing, requiring the curation of trillions of new tokens as well as substantial GPU-hours for pretraining. Furthermore, as the current state-of-the-art in LLMs are primarily closed-source APIs, such as OpenAI's GPT-4o [40], Google's Gemini [51] and Anthropic's Claude [2], it is difficult to effectively explore and push the frontier of existing LLMs without being able to manipulate the parameters of these closed-source models and employing techniques such as continual pretraining [25], instruction fine-tuning [57], data mixing [61], chain-of-thought [58], among others. A number of recent works show the effectiveness of scaling compute at test time [6,7,24,47]. In particular, Large Language Monkeys [6] characterizes inference-time scaling laws, showing a log-linear relationship between coverage-the fraction of problems solved by at least one attempt-and the number of samples drawn from the model across a broad range of reasoning tasks and LLMs. We build on the inference-time compute scaling approaches and propose Archon, a formal framework for applying inference-time techniques, combining the strengths of multiple pretrained LLMs (Section 3.2), and exploring different inference-time architectures with ITAS (Section 3.4).",
            "score": 0.5857518997358231,
            "section_title": "Scaling Laws of Language Models",
            "char_start_offset": 7781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1836
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "275789021",
            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
            "text": "Neural scaling laws. Neural scaling laws provide a framework for understanding how neural networks' performance scales with parameters, data, and compute (Banko & Brill, 2001;Goodman, 2001;Ghorbani et al., 2022;Kaplan et al., 2020;Hoffmann et al., 2024;Bansal et al., 2022;Gordon et al., 2021). Kaplan et al. (2020) showed that for modern transformer-based language models, model loss decreases predictably with increasing model size, dataset size, and compute, following a power-law relationship. Hoffmann et al. (2024) later refined these insights by optimizing hyperparameter configurations, such as learning rate schedules, and proposed a new scaling law that emphasizes scaling training data more aggressively than Kaplan et al. (2020)'s original recommendations. Most relevant to our work are Rosenfeld et al. (2021) and Frantar et al. (2024). Rosenfeld et al. (2021) focused on small-scale CNNs for image classification, while Frantar et al. (2024) focused on transformer-based vision and language models; both modeled their respective performance as a function of model size and pruning configurations. Our work is different in that we unify the functional forms of scaling laws for both dense and sparse pre-training. This unification is partly enabled by our novel exploration of optimal hyperparameter configurations for sparse pre-training. \n\nAdditionally, our study is the largest-scale investigation of sparsely pretrained LLMs to date, with our largest model using over 5 times the compute of the largest model examined in prior work (Frantar et al., 2024). The largest model we investigate requires 4.5\u00d710 20 FLOPs training compute. \n\nPruning. Sparse pre-training involves pruning parameters in an LLM during the pre-training process.",
            "score": 0.5852905871655312,
            "section_title": "RELATED WORK",
            "char_start_offset": 3226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 175,
                    "matchedPaperCorpusId": "6645623"
                },
                {
                    "start": 189,
                    "end": 211,
                    "matchedPaperCorpusId": "237532682"
                },
                {
                    "start": 231,
                    "end": 253,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 253,
                    "end": 273,
                    "matchedPaperCorpusId": "246608156"
                },
                {
                    "start": 273,
                    "end": 293,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 498,
                    "end": 520,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 799,
                    "end": 822,
                    "matchedPaperCorpusId": "219792934"
                },
                {
                    "start": 827,
                    "end": 848,
                    "matchedPaperCorpusId": "260611140"
                },
                {
                    "start": 850,
                    "end": 873,
                    "matchedPaperCorpusId": "219792934"
                },
                {
                    "start": 934,
                    "end": 955,
                    "matchedPaperCorpusId": "260611140"
                },
                {
                    "start": 1549,
                    "end": 1571,
                    "matchedPaperCorpusId": "260611140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "230435736",
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
            "text": "We compute the test perplexity of the constituent datasets of the Pile using GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020), shown in Figure 2. We use all available versions of GPT-2, and all four versions of GPT-3 available via the OpenAI API. Because of the cost associated with using the OpenAI API, we evaluate on one-tenth of the respective test sets for most of the constituent datasets. We report the perplexity converted to bits per UTF-8 encoded byte (BPB). Importantly, we compute perplexity by evaluating each document independently within each dataset, as opposed to concatenating all documents as is common practice for computing perplexity on large corpora. \n\nFull details of the perplexity computation can be found in Appendix E.2. \n\nUnsurprisingly, larger language models generally attain lower perplexity compared to smaller models. \n\nRecent work has shown an increased focus on the empirical scaling laws of language models (Kaplan et al., 2020;Henighan et al., 2020). As such, we investigate the scaling law for the GPT-2 and GPT-3 families of models on perplexity evaluation on the Pile. The scaling law relation for the GPT-3 family of models is shown in Figure 2. 10 The line of best fit shown in the figure has a coefficient of -0.1674 and an intercept of 2.5516. Interestingly, while GPT-2 and GPT-3 were not trained on the Pile, there still appears to be a clear scaling law without diminishing returns. We hypothesize that this is due to the inherent generalization capability of these models. We leave a more rigorous analysis of zero-shot scaling laws to future work.",
            "score": 0.5851908676202002,
            "section_title": "Test Perplexity with GPT-2 and GPT-3",
            "char_start_offset": 15954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1608
                }
            ],
            "ref_mentions": [
                {
                    "start": 955,
                    "end": 976,
                    "matchedPaperCorpusId": "265038669"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2281494140625
        },
        {
            "corpus_id": "275606367",
            "title": "Computational Protein Science in the Era of Large Language Models (LLMs)",
            "text": "In the recent progress of AI, scaling law [81] stands out as a particularly instructive concept. As LLMs scale up in parameters, data, and computing, larger models acquire unprecedented emergent capabilities and demonstrate substantially improved performance. It is observed the development of pLMs follows this trend as well. For example, ESM-3 [30] is scaled up to 98 billion parameters, and xTrimoPGLM is trained at a massive scale of 100 billion parameters. No matter for big companies or academic groups, the high computational cost poses a tough issue, and the computational efficiency of pLMs should be improved. Therefore, it's in demand to understand the scaling behaviors for protein language modeling and formulate optimized computational schemes. Recently, there have been certain ongoing studies questioning whether pLMs are computeoptimal and reached the unanimous conclusion of nonoptimal [300,301], exploring how to scale down pLMs for better efficiency while maintaining their expressiveness [302,303], and implementing novel techniques like FlashAttention [304] to achieve efficient inference, training, and fine-tuning of pLMs [305,306]. In addition to these preliminary explorations, in the future, more efforts could be dedicated to implementing optimal models in the constraint of predetermined computation budgets.",
            "score": 0.5848435228534691,
            "section_title": "Computational Efficiency",
            "char_start_offset": 124220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1337
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "270963342"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "270392261"
                },
                {
                    "start": 1009,
                    "end": 1014,
                    "matchedPaperCorpusId": "272884725"
                },
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "274280378"
                },
                {
                    "start": 1074,
                    "end": 1079,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1146,
                    "end": 1151,
                    "matchedPaperCorpusId": "273638863"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373291015625
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "Irreducible loss \n\n(1) \n\nIn this scaling law, taken from Mikami et al. 2021, the transfer gap determines the efficiency of fine-tuning in the limit of infinite pre-training data. By measuring the transfer gap from inexpensive and abundant pretraining distributions to costly downstream distributions, we can predict how much pre-training might improve performance on a given downstream distribution. Thus, measuring the transfer gap across various tasks can meaningfully inform the relative difficulty of automating downstream tasks, although drawing firm conclusions about the difficulty of automation is premature given the limited data in this study. \n\nOur study relies heavily on the Pythia model suite (Biderman et al. 2023), providing a large set of transformer models trained on the Pile at various model sizes and checkpoints during training. We fine-tune a 2.8 billion parameter model on various downstream language datasets, including a mathematics dataset, a genetic sequence dataset, a statistics textbook, and a synthetic dataset of fictional biographies within a procedurally generated universe. After fitting the scaling law to empirical data, we demonstrate how this scaling law can be used to answer concrete questions, such as \"How valuable is it to collect more fine-tuning data for a given task, compared to scaling up pre-training data?\" and \"What is the transfer gap from one distribution to another?\". \n\nOur analysis is incomplete for several reasons. Firstly, we do not derive a scaling law in model size. 1 Secondly, our results are largely limited to language datasets and were obtained with a relatively modest compute budget. \n\nFor these reasons, we cannot necessarily extrapolate these results to much larger model sizes, of the type that exist at the current commercial frontier (e.g. OpenAI et al. 2024, Team, Anil, Borgeaud, Alayrac, et al. 2024). Nonetheless, we hope our study can serve as a limited foundation for further research, stimulating more investigations into empirical scaling laws for transfer learning, enhancing our understanding of the role of data in advancing AI progress, and providing deeper insights into the difficulty of automating various economically important tasks.",
            "score": 0.584075048377771,
            "section_title": "+ E",
            "char_start_offset": 2982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 19,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2225
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1624755859375
        },
        {
            "corpus_id": "271544155",
            "title": "Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies",
            "text": "Besides the performance of Large Language models, their computational cost, power consumption and environmental cost has also been discussed [4,16]. Although there are methods to measure the computational expense of running an LLM task, it is rare for LLM users who are conducting experiments on their models to report the computational cost at inference time [11,28]. This stands in contrast to many other areas in computer science where understanding (and accurately reporting) the computational cost of running an algorithm is critical for determining their scalability, practicality and for further development and improvement. \n\nA number of theoretical and empirical ways of measuring the cost of algorithms are well established including CPU time, clock cycles, time complexity analysis and space complexity analysis. For Large Language Models there are well established metrics for training including Floating Point Operations (FLOPs), parameter counts, training time and energy consumption. For open source models these metrics are usually published, however not all metrics are made public for state of the art closed models [30,39]. During inference, the metrics most commonly discussed are those relating to performance such as latency and throughput. In this paper, we suggest and implement a simple measure for the rough computational expense of calling an LLM; the number of completion tokens. This allows for a direct comparison of computational resources used when comparing different techniques and tasks for a given model. This however does not allow for a fair comparison between different models as models can have different numbers of parameters.",
            "score": 0.5838545801346913,
            "section_title": "LLM Computational Cost",
            "char_start_offset": 9375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1667
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "259360395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "267547740",
            "title": "A Resource Model For Neural Scaling Law",
            "text": "In previous sections, we demonstrated that \u2113 \u221d N \u22121 , i.e., the loss \u2113 is a scaling law against the number of allocated neurons N . Can we use this result as a starting point to derive the more useful neural scaling laws of large language models (LLMs)? Notice that there are two major gaps between our resource model and the LLM setup: (1) In LLM, the number of parameters N p is measured against instead of the number of allocated neurons N ; (2) In LLM, the depth is usually scaled with width, while our resource model (hypothesis 2) assumes only scaling up width. Below we aim to address these two gaps and derive a meaningful scaling law for LLMs. \n\nIn LLMs the empirical neural scaling law we care about is \u2113 \u221d N \u2212\u03b1 p where N p is the number of model parameters. How is N p related to N ? We aim to show that N p \u221d N 3 and finally derive \u03b1 = 1/3, with a number of reasonable assumptions5 : \n\nAssumption 1: The number of modules is proportional to the network width, i.e., N \u221d N width . This is aligned with the homogeneous growth conjecture. Redundant representations have been empirically observed in Liu & Tegmark (2023); Doimo et al. (2021). \n\nAssumption 2: The depth is scaled as the same factor as the width, i.e., N depth \u221d N width . The theoretical derivation of the optimal depth-width ratio (Levine et al., 2020) has been the foundation for best practices in LLM where the depth and width of the network are scaled by the same factor, e.g., in Hoffmann et al. (2022). \n\nAssumption 3: The number of parameters can be roughly estimated as N = N 2 width N depth . Most neurons of the neural network are in MLPs weights, while the number of other parameters are nearly negligible. The number of MLP weight parameters is simply N 2 width N depth . Combining all three assumptions, we have \n\ntogether with \u2113 \u221d N \u22121 immediately implies that \u2113 \u221d N \u22121/3 p . Pleasantly, this agrees with the empirical scaling law of Chinchilla models (Hoffmann et al., 2022) where they observed \u2113 \u221d N \u22120.34 p .",
            "score": 0.5825584743334866,
            "section_title": "EMERGENT NEURAL SCALING OF MODEL SIZE",
            "char_start_offset": 19688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09686279296875
        },
        {
            "corpus_id": "278481459",
            "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets",
            "text": "We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \\emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \\cite{salavati2024reducing} and subsets of the MMLU dataset \\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings.",
            "score": 0.5824421631845222,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.461181640625
        },
        {
            "corpus_id": "268064306",
            "title": "Sequence modeling and design from molecular to genome scale with Evo",
            "text": "We provide a summary of the steps involved in our scaling laws analysis. Quantifying scaling rates allows us to predict performance as model size, dataset size, and compute grow. \n\n1. Define a set of compute budgets to study. We use 8 \u00d7 10 18 , 2 \u00d7 10 19 , 4 \u00d7 10 19 and 8 \u00d7 10 19 FLOPS. 2. Calculate the FLOPS (floating point operations) required to process a fixed input size for the model architecture of interest (i.e. the \"cost\" of using the model). 3. Identify the model's compute-optimal allocation for each compute budget: \n\n(a) Select a wide range of possible model sizes, and calculate for each model size the corresponding number of tokens that need to be processed to reach the compute budget. Other hyperparameters are chosen according to Table S1. We generally observe minor changes to model topology (depth, width) to only minimally affect perplexity, aligning our results with the findings presented by Kaplan et al. (2020) for Transformers. (b) Train a model of each size and record its performance (e.g., in terms of perplexity). (c) Identify the optimal compute allocation: Following prior analysis, we fit a second-order polynomial as a function from (log) model size to perplexity, and extract obtained the compute-optimal point as its minimum. The compute-optimal point identifies the optimal allocation of model size and training tokens at the given compute budget. \n\nAfter deriving the compute-optimal scaling rates (Figure 1G), we compare architectures and compute optimal allocation of tokens and model size (Figure S5). In Figure S3, we also show rates for compute-suboptimal model sizes by architecture. In particular, we quantify the effect on perplexity scaling caused by a suboptimal allocation of compute budget to model or dataset size (e.g., training a smaller model for more tokens). We estimate the compute-optimal model size for each compute budget, then reduce it by a percentage (the offset). The corresponding perplexity is obtained via the IsoFLOP curves (Figure 1F).",
            "score": 0.5822603409149063,
            "section_title": "Scaling laws procedure",
            "char_start_offset": 67912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2110595703125
        },
        {
            "corpus_id": "268358466",
            "title": "Algorithmic progress in language models",
            "text": "Thus far, there have been few works investigating algorithmic progress in machine learning specifically.Notably, Hernandez and T. Brown (2020)  Across these benchmarks, state-of-the-art sample efficiency is found to improve at exponential rates, with doubling times ranging from 5 to 18 months depending on the domain and performance threshold.These rapid algorithmic improvements enable reaching a fixed level of performance with orders of magnitude fewer environment samples over time.Dorner finds that this progress is driven by factors such as better off-policy learning, model-based methods, auxiliary objectives, and explicit tuning for efficiency.\n\nMore recently, Erdil and Besiroglu (2022) propose an alternative approach to estimating algorithmic progress based on fitting a statistical model inspired by neural scaling laws, and use Shapley values-a technique from cooperative game theory-to determine the relative contributions of training compute and data to performance.They find that algorithmic improvements explain 25-70% of gains, with physical compute scaling accounting for 30-55% and data scaling contributing 10-30%, indicating algorithms and hardware contribute roughly equally.The majority of this algorithmic progress is \"compute-augmenting\", i.e. it enables the more efficient use of compute rather than data.According to their estimates, compute-augmenting algorithmic advances halve physical compute requirements for a certain performance level every 9 months, faster than hardware gains per Moore's law.\n\nEstimating the benefits of innovations in machine learning can be challenging, but in some cases the analysis is more straightforward.For example, consider recent work by Hoffmann et al. 2022 proposing an improved scaling law for training language models compared to the dominant understanding prescribed by Kaplan et al. 2020.By directly applying the new scaling law, we calculate it provides a 2\u00d7 to 4\u00d7 reduction in compute needed to reach a given loss target at the scale of current frontier LLMs, depending on the scale of the model (see Appendix B).",
            "score": 0.5819812599572649,
            "section_title": "Algorithmic progress in machine learning",
            "char_start_offset": 7679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 104,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 654
                },
                {
                    "start": 656,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1334
                },
                {
                    "start": 1334,
                    "end": 1531
                },
                {
                    "start": 1533,
                    "end": 1667
                },
                {
                    "start": 1667,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 1841,
                    "end": 1859,
                    "matchedPaperCorpusId": "210861095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "267034970",
            "title": "Molecular causality in the advent of foundation models",
            "text": "The scaling hypothesis posits that the performance of a model increases with the amount of data it is trained on. Recently, it has come to describe the idea that, given enough data, complex model behaviours can emerge. The enormous success of current Large Language Models has been attributed to scaling, with emergence of human-like language capabilities around the time of GPT-3. The ability to scale depends on several factors: the availability of data, parallelisation of training, adequate compute power with a parallel architecture, and a model architecture that can digest large amounts of data e ectively.",
            "score": 0.5817991952971578,
            "section_title": "Scaling hypothesis",
            "char_start_offset": 35259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 613
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21337890625
        },
        {
            "corpus_id": "263620702",
            "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
            "text": "Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs-despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies. In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of LLaMA-a recent state-of-the-art LLM-developed by Meta AI on two generations of popular GPUs (NVIDIA V100 & A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. We present the results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. To our knowledge, our work is the one of the first to study LLM inference performance from the perspective of computational and energy resources at this scale.",
            "score": 0.5817448707398427,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "247625205",
            "title": "What Language Model to Train if You Have One Million GPU Hours?",
            "text": "Recent years have seen the advent of large language models characterized by emergent capabilities (e.g., zero-shot generalization) arising from sheer scale alone (Radford et al., 2019;Brown et al., 2020). Scaling LLMs results in a predictable increase in performance: simple scaling laws connect the number of parameters, pretraining dataset size, and compute budget (Kaplan et al., 2020;Ganguli et al., 2022;Hoffmann et al., 2022), providing a clear Figure 1: Smooth scaling of language modeling loss as compute budget and model size increase. We observe a power-law coefficient \u03b1 C \u223c 0.046, in-line with Kaplan et al. (2020). We use this fit to estimate the optimal size and number of tokens to train on for the final model given the available budget. \n\npath towards more capable models. This paradigm shift has been fueled by the wide adoption of the Transformer (Vaswani et al., 2017), providing a scalable basis for practitioners to build upon. \n\nIn this paper, we design an architecture and training setup for a multilingual 100B+ parameters model (BLOOM, BigScience Workshop (2022)), seeking to best use a fixed 1,000,000 A100-hours budget. Because of the costs involved with training large language models, we cannot exhaustively explore the landscape of possible models. Instead, we position ourselves as practitioners exploring \"offthe-shelf\" solutions. We thus test promising additions to the Transformer to attempt to reproduce their findings in a controlled, large-scale setting. \n\nAlthough our main goal was to prepare the architecture and training setup of BLOOM, our findings are also valuable for practitioners building models in the 1-10B range, as they equally improve the performance of such smaller models. At variance with major works on large language models, we also make a significant effort towards reproducibility and openness: all of our pretrained models, code, and notes from our weekly meetings are made available. See Appendix A for the relevant links. \n\nContributions. We first study the impact of pretraining corpora, positional embeddings, activation functions, and embedding norm on zero-shot generalization.",
            "score": 0.5817375646731564,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 1984
                },
                {
                    "start": 1987,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 866,
                    "end": 888,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "255185900",
            "title": "Cramming: Training a Language Model on a Single GPU in One Day",
            "text": "Large-scale training of machine learning models with transformer architectures has lead to groundbreaking improvements in many sub-fields of natural language processing including language understanding and natural language generation (Vaswani et al., 2017;Dosovitskiy et al., 2021;Radford et al., 2019). The nowadays accepted (but historically surprising) key behavior of these systems is that they reliably scale -they continuously improve in performance when the number of model parameters and amount of data grow. These increases in performance are well-described by various power laws as studied by Kaplan et al. (2020). This sets up a dominant paradigm in which scaling is the key to performance improvement (Sutton, 2019). \n\nThe power of scale has set off a race to produce extremely large models, which in turn has created an environment where few researchers or practitioners feel that they are capable of training a language model. The original BERT model Devlin et al. (2019), which became a cornerstone transformer for many practical applications in natural language understanding, already required a significant amount of computation to train. Yet, the reproduction and improvements in Liu et al. (2019) further increased its performance by cranking up the level of computation by orders of magnitude. As these pre-trained checkpoints became popular for a range of downstream applications (Wolf et al., 2020), the competition for the largest language model became a focal point for industrial labs. This led to training runs that improved the performance of pretrained language models at the expense of computation at the zettaFLOP scale (Raffel et al., 2020;Yang et al., 2020;Zaheer et al., 2021) and later at the extremely large yottaFLOP scale (Brown et al., 2020;Black et al., 2022;Chowdhery et al., 2022;Rae et al., 2022). \n\nOur goal is to turn this trend on its head and investigate how to best scale down language model training and what trade-offs emerge when doing so: What downstream performance can be achieved by a modest researcher when training from scratch with a single GPU for a single day? \n\nThe ability to train a language model to the performance level of BERT with such modest resources has several interesting implications.",
            "score": 0.5809481037877617,
            "section_title": "SCALING UP AND SCALING DOWN",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 2119
                },
                {
                    "start": 2122,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 1759,
                    "end": 1779,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392578125
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "Power laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset size in density estimation [Was06] and in random forest models [Bia12] may be connected with our results. These models suggest that power-law exponents may have a very rough interpretation as the inverse of the number of relevant features in the data. Some early [BB01,Goo01] work found power-law scalings between performance and dataset size. More recent work [HNA + 17, HAD19] also investigated scaling between model size and data size; their work is perhaps the closest to ours in the literature8 . Note, however, that [HNA + 17] found super-linear scaling of dataset size with model size, whereas we find a sub-linear scaling. There are some parallels between our findings on optimal allocation of compute and [Kom19], including power-law learning curves. EfficientNets [TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent work [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and fits an ansatz similar to ours. \n\nEfficientNet [TL19] advocates scaling depth and width exponentially (with different coefficients) for optimal performance of image models, resulting in a power-law scaling of width as a function of depth. We find that for language models this power should be roughly one when scaling up (as width/depth should remain fixed). But more importantly, we find that the precise architectural hyperparameters are unimportant compared to the overall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles of shallower models, which could potentially explain this finding. Earlier work [ZK16] has compared width and depth, and found that wide ResNets can outperform deep ResNets on image classification. Some studies fix computation per data example, which tends to scale in proportion to the number of model parameters, whereas we investigate scaling with both model size and the quantity of training computation.",
            "score": 0.5802945699755421,
            "section_title": "Related Work",
            "char_start_offset": 36240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 173,
                    "matchedPaperCorpusId": "14463568"
                },
                {
                    "start": 366,
                    "end": 372,
                    "matchedPaperCorpusId": "6645623"
                },
                {
                    "start": 1736,
                    "end": 1742,
                    "matchedPaperCorpusId": "15276198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420166015625
        },
        {
            "corpus_id": "276961790",
            "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
            "text": "Used both to forecast performance for early pretraining decisions as well as decide on the optimal trade-off between parameters and pretraining dataset size given a particular compute budget, scaling laws (Kaplan et al., 2020, i.a.) have played an important role in the development of large language models (LLMs). Famously, with a series of experiments with models with different data/parameter trade-offs, Hoffmann et al. (2022) showed that previous LLMs were all erring on the side of too many parameters, causing a shift in the amount of training tokens to train LLMs. More recently, Dubey et al. (2024) used scaling laws not only to determine the optimal parameter count given their available compute budget, but also to forecast the impact of data selection decisions on evaluation scores. \n\nIn these works, the compute optima (COs), describing the optimal parameter count and number of training tokens, are selected based on aggregate performance estimators (APEs), in the form of negative log-likelihood (NLL) on a validation set not part of the pretraining corpus. Little is known, however, about whether the COs of individual skills such as mathematical reasoning, question answering (QA), or coding, align with these APE COs. While some studies use scaling laws to predict how downstream task performance improves with scale (e.g. Ye et al., 2025;Held et al., 2025), none of these studies cover whether COs themselves may be skill dependent. Is it possible that some skills are more data-hungry, whereas others benefit more from extra parameters? If so, how should that impact model training and training data selection? \n\nIn this paper, with an extensive set of experiments across 9 different compute scales and 2 skills as measured with 19 datasets across two different splits, we study exactly that. Specifically, we focus on the three research questions: \n\nR1. Are COs skill dependent? First, we consider how the IsoFLOP curves and corresponding COs for code1 -and knowledge-based skills compare to the APE COs, given a canonical datamix. Across the board, we find pronounced differences between the COs for these different skills: where knowledge QA tasks are capacity-hungry, code tasks instead prefer data.",
            "score": 0.580098768237414,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 1342,
                    "end": 1358,
                    "matchedPaperCorpusId": "268681464"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57568359375
        },
        {
            "corpus_id": "262064942",
            "title": "The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute",
            "text": "The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law. This is due to the improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws leads of both models results in an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the foundation for meaningful and reproducible language modelling research.",
            "score": 0.5797245727839626,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "262064942",
            "title": "The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute",
            "text": "While such comparisons offer insights, the best models are often also the most computeintensive ones Brown et al. (2020). With the rise of well-funded industry labs, it has become increasingly difficult for academic labs to do research at that scale. E.g., all publications which advance the state of the art on PG19 are from Google or Google Deepmind with model sizes of up to 1.3B parameters (Hutchins et al., 2022). Training such models requires dedicated servers with multiple state of the art accelerators training for several days just to reproduce the results. \n\nRecent work presents the idea of cramming experiments into a single day and a single consumer GPU (Geiping & Goldstein, 2023). In Section 3, we will also advocate for a shift away from unconstrained perplexity comparisons. While experiments offer valuable insights, they do not adequately account for the scalability factor, a key element in training large language models. The Languini benchmark, in an effort to demonstrate scalability, compares models based on different amounts of accelerator hours, resulting in a scaling plot or scaling law (Kaplan et al., 2020;Hoffmann et al., 2022). This approach seeks to provide a fair and meaningful comparison of language modelling research at varying compute scales, thereby promoting inclusivity for research groups with limited funding resources.",
            "score": 0.578963513363087,
            "section_title": "Existing Benchmarks",
            "char_start_offset": 13736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1365
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 417,
                    "matchedPaperCorpusId": "247451135"
                },
                {
                    "start": 668,
                    "end": 695,
                    "matchedPaperCorpusId": "255185900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.301513671875
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "Estimate scaling laws from intermediate checkpoints, not just fully trained models! Most past work on scaling behavior of language models (e.g., Gadre et al., 2024;Muennighoff et al., 2024) has trained a separate model for each value of #toks studied. This is based on the assumption that changes in the learning rate schedule, which depend on the size of the full dataset that will be used for training, render losses from intermediate checkpoints uninformative. \n\nHowever, some recent work has demonstrated the effectiveness of learning schedules that do not require prior access to the size of the training set (Hu et al., 2024), and some work has questioned whether careful choice of the learning rate decay is necessary for reliable scaling laws (Porian et al., 2024). Together, these findings motivate revisiting the assumption that only a single useful datapoint may be obtained from each training run. In the final portion of \u00a75.1, we observed the value of intermediate checkpoints when only a single #params family is used to fit a scaling law. We now test whether this finding extends to larger families-i.e. whether including intermediate checkpoints from all models in a model family reduces ARE. \n\nResults are shown in Fig. 4, which plots ARE for scaling laws estimated from data subsets of the form max #toks (F, q) for varying q. We find that including full training curves in scaling law estimation can predict losses well. In fact, relying merely on the end of training produces significantly worse performance across the board. Our remaining experiments thus fit scaling laws using all these intermediate checkpoints, and not final performance alone.",
            "score": 0.5783455191453162,
            "section_title": "I HAVE SOME DATA, WHAT PORTIONS SHOULD I USE?",
            "char_start_offset": 17498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1668
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 189,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "270702885",
            "title": "The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources",
            "text": "Several decisions made prior to model training can have significant impacts on the upstream and downstream environmental impact of a given model. Empirical scaling laws can be used to find the best allocation of resources. Kaplan et al. (2020);Hoffmann et al. (2022) estimate the optimal model size and training duration, given a training compute budget. And Aghajanyan et al. (2023) investigates the equivalent efficient compute allocation for multimodal settings. When working with text training data that is constrained, recent work explores how to allocate compute efficiently (Muennighoff et al., 2023b). For models frequently used downstream, it is important to consider the inference footprint and inference cost during model creation (Gadre et al., 2024b), to minimize the environmental impact of inference. For further resources and discussion, see 6.3.",
            "score": 0.5782338389409216,
            "section_title": "Effective use of resources",
            "char_start_offset": 58758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 862
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 383,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 581,
                    "end": 608,
                    "matchedPaperCorpusId": "10475843"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.220458984375
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "Language provides a natural domain for the study of artificial intelligence, as the vast majority of reasoning tasks can be efficiently expressed and evaluated in language, and the world's text provides a wealth of data for unsupervised learning via generative modeling. Deep learning has recently seen rapid progress in language modeling, with state of the art models [RNSS18, DCLT18, YDY + 19, LOG + 19, RSR + 19] approaching human-level performance on many specific tasks [WPN + 19], including the composition of coherent multiparagraph prompted text samples [RWC + 19]. \n\nOne might expect language modeling performance to depend on model architecture, the size of neural models, the computing power used to train them, and the data available for this training process. In this work we will empirically investigate the dependence of language modeling loss on all of these factors, focusing on the Transformer architecture [VSP + 17, LSP + 18]. The high ceiling and low floor for performance on language tasks allows us to study trends over more than seven orders of magnitude in scale. \n\nThroughout we will observe precise power-law scalings for performance as a function of training time, context length, dataset size, model size, and compute budget.",
            "score": 0.5778370583421552,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1254
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55126953125
        },
        {
            "corpus_id": "274150498",
            "title": "Scaling Laws for Online Advertisement Retrieval",
            "text": "The neural scaling laws, widely recognized in Natural Language Processing and Computer Vision areas, establish predictable relationships between model performance and key factors such as model size, dataset size, computational cost, and post-training error rates. Hestness et al.(2017) [14] introduced a unified scaling law applicable to machine translation, image classification, and speech recognition, noting that different tasks exhibit distinct scaling coefficients. Kaplan et al.(2020) [19] further elaborated on these laws, defining them through four parameters: model size (number of parameters), dataset size (number of tokens), computational cost (FLOPs), and training loss (perplexity per token). These relationships were empirically validated, including during the training of GPT-3 [3]. Subsequently, Hoffmann et al.(2022) [15] presented the Chinchilla Scaling Law, which differs somewhat from [19] because of their different training setups. In addition to estimating the training loss of the model, Isik et al.(2024) [18] further verified that there also exist scaling laws between the downstream task metrics and the model parameters. Caballero et al.(2023) [4] found that many scaling behaviors of artificial neural networks follow a smoothly broken power law (BNSL), verified on various NLP and CV tasks, covering a wide range of downstream tasks and model structures. \n\nIn the recommendation area, Shin et al.(2023) [23] and Zhang et al.(2024a) [32] conducted studies on whether there exist scaling laws for recommendation models and primarily provided qualitative conclusions. Fang et al.(2024) [8] first proposed a quantitative scaling law in the recommendation area, which describes the relationship between the amount of training data, the size of model parameters, and an offline metric for query-document retrieval. Based on this scaling law, the optimal amount of data and model parameter allocation can be solved under a given total training resource. However, obtaining a multivariate scaling law requires a large number of experiments, and the offline metrics might not be a good indicator for online metrics, these make it somewhat difficult to apply in real-world industrial applications.",
            "score": 0.5775431911607665,
            "section_title": "Related Work 2.1 Neural Scaling Laws",
            "char_start_offset": 6315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1435,
                    "end": 1439,
                    "matchedPaperCorpusId": "244477730"
                },
                {
                    "start": 1597,
                    "end": 1614,
                    "matchedPaperCorpusId": "268723791"
                },
                {
                    "start": 1615,
                    "end": 1618,
                    "matchedPaperCorpusId": "268723791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "271270413",
            "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
            "text": "The Approach 1 provides a broader solution by predicting the allocation of computational resources across non-vocabulary parameters, vocabulary parameters, and training data based on experimental data points. This method's strength lies in its holistic view, allowing for a balanced resource distribution that potentially enhances model efficiency and performance. However, this approach is constrained by the granularity and range of the experimental data points available, which can introduce errors in the fitting process. The requirement for substantial computational resources to perform these fittings may also limit its accessibility and scalability. Despite these challenges, when experimental data is ample and computational resources are sufficient, the Approach 1 can significantly refine the precision of resource allocation decisions in the development of large-scale language models. \n\nApproach 2 By calculating the derivative of FLOPs with respect to the vocabulary size and solving for zero, this approach fundamentally relies on the precision of the FLOPs equation and our tokenscharacters relationship function. Further, this method does not allow us to independently determine the optimal allocation of non-vocabulary parameters and training data size. Therefore, it necessitates information about the relationships between these attributes and the FLOPs budget from the experimentally fitted scaling laws, making this approach less useful in practice. Despite these limitations, the derivative-based approach offers notable advantages, including closely matched predictions with the scaling laws derived from actual experimental data in the Approach 2. Furthermore, its reliance on numerical solutions rather than exhaustive deep learning experiments makes it rapid and broadly applicable across various tokenizers, highlighting its utility in preliminary model configuration stages where quick estimates are key. \n\nApproach 3 Similar with the Approach 1, the proposed Approach 3 requires multiple experimental runs across different non-vocabulary parameters, vocabulary sizes and number of training data. Therefore, the approach is constrained by the granularity and range of the experimental data points available to some extent. However, the proposed Approach 3 is flexible that it considers the fact that the non-vocabulary parameters and the number of training data are not always following the compute-optimal scaling laws [26], i.e., equal scaling, in real-world applications.",
            "score": 0.5765236783683947,
            "section_title": "B Limitation and future work B.1 Limitations of our proposed approaches Approach 1",
            "char_start_offset": 45538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1933
                },
                {
                    "start": 1936,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2251
                },
                {
                    "start": 2252,
                    "end": 2503
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.469970703125
        },
        {
            "corpus_id": "277787795",
            "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments",
            "text": "Prediction Much work studies scaling behavior in language models. Initially this focused on predicting LM loss from scale as determined by parameter count and tokens trained (Kaplan et al., 2020;Hoffmann et al., 2022). Special consideration is also given to the case of data constrained scaling (Muennighoff et al., 2023;Goyal et al., 2024). Unlike predicting loss, predicting downstream performance from scale is generally harder (Schaeffer et al., 2024). However, recent work has demonstrated it can be done based on a two step prediction that chains together predictions from scale to loss and loss to downstream performance (Gadre et al., 2024;Bhagia et al., 2024;Dubey et al., 2024), sometimes using training loss (Du et al., 2024) or transferring losses from different data recipes (Brandfonbrener et al., 2024;Ruan et al., 2024). The one line of work targeting pretraining data considers the special case of deciding mixing proportions of several data sources optimized through scaling laws (Kang et al., 2024;Ye et al., 2024). Most relevant to our work, Choshen et al. (2024) consider practical methods for better scaling prediction error such as how much compute to use or whether to include intermediate checkpoints. Orthogonally to these findings, we propose a way to assess the accuracy of decisions made with such predictions. \n\nSuites over Data Differences DATADECIDE follows in the footsteps of the Pythia Suite (Biderman et al., 2023) which was the first to offer a controlled comparison of 2 data recipes, using compute scales up to 2 \u00d7 10 22 FLOPs. Subsequent suites have offered 6 data recipes at 9 \u00d7 10 20 scale (Magnusson et al., 2024) and 6 data recipes over a range of scales up to 10 21 (Brandfonbrener et al., 2024). Our DATADECIDE offers a range of 14 scales up to 7 \u00d7 10 20 FLOPs, while including an order of magnitude more fine-grained data differences. Meanwhile, DCLM also makes extensive use of ranking single scale experiments to drive improvement in data recipes (Li et al., 2024).",
            "score": 0.5762673612874634,
            "section_title": "Related Work",
            "char_start_offset": 24088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 321,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 431,
                    "end": 455,
                    "matchedPaperCorpusId": "270357828"
                },
                {
                    "start": 719,
                    "end": 736,
                    "matchedPaperCorpusId": "268681827"
                },
                {
                    "start": 817,
                    "end": 835,
                    "matchedPaperCorpusId": "269899695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1981201171875
        },
        {
            "corpus_id": "267320913",
            "title": "TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese",
            "text": "While empirical evidence seems to point to the fact that existing scaling laws (Hoffmann et al., 2022) may not provide accurate predictions in situations where smaller models are trained for more extended periods (Touvron et al., 2023b;Zhang et al., 2024), in this study, we choose to use the Hoffmann et al. (2022) scaling laws, like done by Dey et al. (2023), to estimate the size of models. Even though extrapolating such boundaries might benefit smaller models, we did not have the budget (or tokens) to sustain longer runs. \n\nAccording to Hoffmann et al. (2022), we can model language modeling loss, L, as a function of model size N (the number of parameters) and training dataset size D (the number of tokens): \n\nWhere A = 406.4, B = 410.7, E = 1.69, \u03b1 = 0.32, and \u03b2 = 0.28 are parameters estimated by the authors after fitting a regression model to a dataset of 400 language model training runs. In their paper, Hoffmann et al. (2022) present estimations for dataset size for many model sizes. With 70B parameters, Chinchilla requires 1.4T tokens according to these laws, which equates to roughly 20 tokens per parameter. Based on this average, we estimated an optimal dataset size for two models: 3.5 and 9.5 billion tokens for 160 and 460 million parameter models, respectively. We considered these to be fair sizes for this project, given that we would be able to train them without requiring much computing while, at the same time, the token count was still within a manageable range, i.e., something we could gather by relying on open-source datasets.3",
            "score": 0.5761073912935595,
            "section_title": "Sizing Up Models and Datasets",
            "char_start_offset": 11710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1564
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2705078125
        },
        {
            "corpus_id": "272525111",
            "title": "Optimization Hyper-parameter Laws for Large Language Models",
            "text": "Large Language Models have driven significant AI advancements, yet their training is resource-intensive and highly sensitive to hyper-parameter selection. While scaling laws provide valuable guidance on model size and data requirements, they fall short in choosing dynamic hyper-parameters, such as learning-rate (LR) schedules, that evolve during training. To bridge this gap, we present Optimization Hyper-parameter Laws (Opt-Laws), a framework that effectively captures the relationship between hyper-parameters and training outcomes, enabling the pre-selection of potential optimal schedules. Grounded in stochastic differential equations, Opt-Laws introduce novel mathematical interpretability and offer a robust theoretical foundation for some popular LR schedules. Our extensive validation across diverse model sizes and data scales demonstrates Opt-Laws' ability to accurately predict training loss and identify optimal LR schedule candidates in pre-training, continual training, and fine-tuning scenarios. This approach significantly reduces computational costs while enhancing overall model performance.",
            "score": 0.5760878549905285,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64208984375
        },
        {
            "corpus_id": "273228196",
            "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
            "text": "This paper delves into the analysis of scaling laws for Dense and MoE Models within the context of LLMs. We extend foundational research on hyperparameters, such as compute budget, batch size, and learning rate (Kaplan et al., 2020;Hoffmann et al., 2022;McCandlish et al., 2018;Li et al., 2024;Chen et al., 2024b), to explore their transferability and applicability across these architectures. Our experiments involve models up to 7 billion parameters and datasets exceeding 100 billion tokens, aiming to uncover universal scaling behaviors potentially applicable to both model types. \n\nOur results verify the hypothesis that certain scaling laws, particularly those related to loss and hyperparameters, may indeed be universal, bridging architectural gaps between Dense and MoE Models. This universality suggests a simplification in hyperparameter tuning across different scales and architectures, which could significantly streamline the training processes for various LLMs. Furthermore, we provide detailed analyses of the dif-arXiv:2410.05661v1 [cs.LG] 8 Oct 2024 ferential impacts of coefficient changes between MoE and Dense Models, offering both empirical and theoretical insights into the superior data efficiency of MoE Models. Concretely, MoE Models can achieve comparable performance with fewer training tokens than Dense Models, alleviating data constraints in LLM training. Our findings can be summarized as follows: \n\n\u2022 Consistent Scaling Law Framework: Both MoE and Dense Models demonstrate a consistent and transferable scaling law framework, encompassing loss scaling as well as optimal batch size and learning rate scaling. This alignment implies that the established practices and insights for optimizing Dense Models can be readily applied to MoE Models, potentially streamlining the process of identifying optimal hyperparameters and reducing experimental complexity.",
            "score": 0.5760819861847268,
            "section_title": "Introduction",
            "char_start_offset": 1719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 254,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 294,
                    "end": 313,
                    "matchedPaperCorpusId": "268597007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "275993733",
            "title": "Scaling Inference-Efficient Language Models",
            "text": "Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to 3.5x difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training a total of 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by 1.8x while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff.",
            "score": 0.5758763397795322,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "273403589",
            "title": "A Comparative Study on Reasoning Patterns of OpenAI's o1 Model",
            "text": "With the emergence of Transformers (Vaswani, 2017) and the scaling laws (Henighan et al., 2020), the researchers try to scale up the parameters of the generative language model. As a result, Ope-nAI's GPT series models (Radford, 2018;Radford et al., 2019;Brown, 2020;Achiam et al., 2023) have achieved remarkable success in the NLP field. Inspired by the scaling law, the rapid development of open-source models has also been achieved through scaling up the size of parameters and collecting huge data for pre-training, such as Qwen (Bai et al., 2023;Yang et al., 2024), Yi (AI et al., 2024), Llama (Touvron et al., 2023;Dubey et al., 2024), and Deepseek (Bi et al., 2024). Apart from these, current researchers are meeting the demands of training LLMs by collecting higher-quality instruction data and pre-training data. Moreover, improving the quality of the collected data has also gained significant attention in developing LLMs. However, the approach of enhancing model performance by increasing model parameters and collecting more data is facing a bottleneck (Snell et al., 2024). Preprint 2.2 TEST TIME COMPUTE METHODS Snell et al. (2024) propose that scaling LLMs Test-time Compute optimally can be more effective than scaling model parameters. Besides, there are some methods designed for adapting Test-time Compute to LLMs' reasoning. OpenAI's o1 model5 is designed to spend more time reasoning before they respond for the sake of obtaining better performance. Wang et al. (2023) conduct a hierarchical hypothesis search to enable inductive reasoning capabilities. Besides, A number of related works have been proposed to augment LLMs with tools with Test-time Compute, which can greatly improve their performance on downstream tasks (Gao et al., 2023;Qin et al., 2023;Qu et al., 2024).",
            "score": 0.5756876036750878,
            "section_title": "LARGE LANGUAGE MODELS",
            "char_start_offset": 4119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 50,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17333984375
        },
        {
            "corpus_id": "257219404",
            "title": "LLaMA: Open and Efficient Foundation Language Models",
            "text": "Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties first appeared when scaling models to a sufficient size (Kaplan et al., 2020), resulting in a line of work that focuses on further scaling these models (Chowdhery et al., 2022;Rae et al., 2021). These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. \n\nThe objective of the scaling laws from Hoffmann et al. ( 2022) is to determine how to best scale the dataset and model sizes for a particular training compute budget. However, this objective disregards the inference budget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens. \n\nThe focus of this work is to train a series of language models that achieve the best possible performance at various inference budgets, by training on more tokens than what is typically used. The resulting models, called LLaMA, ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs. For instance, LLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10\u00d7 smaller. We believe that this model will help democratize the access and study of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter model is also competitive with the best large language models such as Chinchilla or PaLM-540B.",
            "score": 0.5756719670909389,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2138
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56591796875
        },
        {
            "corpus_id": "246608156",
            "title": "Data Scaling Laws in NMT: The Effect of Noise and Architecture",
            "text": "In contrast, decoder-heavy models have shown great promise in applications such as conversational AI (Xu et al., 2020). Prior work (Ghorbani et al., 2021) has demonstrated that, in the context of parameter scaling, encoder-heavy models exhibit significantly different behaviors compared to decoder-heavy models. \n\nTo examine how model shape interacts with sample efficiency, we examine the data scaling characteristics of 5 different models with various degrees of encoder-decoder asymmetry: {2L6L, 6L2L, 6L6L, 28L6L, 6L28L}. Our results indicate that the same scaling parameters \u03b1, p are sufficient to capture the data scaling behavior for all of these models; the only parameter that has to change from model to model is C which is due to the difference in the model capacities. See Figure 1A for the scaling law fit and Appendix A.1 for individual fits of Eq. ( 1) for each model. As such, our results indicate that data efficiency characteristics of encoder-decoder NMT models is largely independent of the model shape. \n\nJoint Data & Parameter Scaling Law: In Eq. ( 1), as D \u2192 \u221e, L(D) \u2192 \u03b1C p which corresponds to the achievable test loss when the learning problem is limited only by the model capacity. Earlier work has studied the behavior of \u03b1C p in the context of parameter scaling laws (Ghorbani et al., 2021;Gordon et al., 2021). Here, we leverage these results to derive a joint data & parameter scaling law. To be specific, instead of empirically fitting the constant C, we replace it with the quantity implied by the model scaling law computed by Ghorbani et al.: \n\nHere, N e and N d correspond to the number of parameters in the encoder & decoder respectively. The only fitted parameters in this equation are \u03b1 and p; the parameters (\u03b2, p e , p d , L \u221e ) appearing in (2) are directly borrowed from Ghorbani et al. (2021).",
            "score": 0.5754712977323811,
            "section_title": "Data Scaling Laws",
            "char_start_offset": 11207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 154,
                    "matchedPaperCorpusId": "237532682"
                },
                {
                    "start": 1295,
                    "end": 1318,
                    "matchedPaperCorpusId": "237532682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1812744140625
        },
        {
            "corpus_id": "276259426",
            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
            "text": "There has been a proliferation of research in recent years pointing to the pivotal role of scale, and how its benefits could be predicted empirically (Hestness et al., 2017;Kaplan et al., 2020;Alabdulmohsin et al., 2022;Bansal et al., 2022;Zhai et al., 2022). Generally, the performance of deep neural networks f (x) (such as its error rate or log-perplexity) often follows a power law f (x) \u223c \u03b2x \u2212c + \u03b5 as one varies a dimension x, such as the data size or model parameters. These \"scaling laws,\" as they are known today, have been used, among others, to determine the training data size needed for a specified level of accuracy (Cho et al., 2015;Beleites et al., 2013;Figueroa et al., 2012) and to optimize the model architecture (Kaplan et al., 2020;Hoffmann et al., 2022;Alabdulmohsin et al., 2024b), with some theoretical justification (Bahri et al., 2021;Hutter, 2021;Sharma and Kaplan, 2022). \n\nBesides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2024), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such In RINS, the model f : X \u2192 Y is split into two parts: the first block f A : X \u2192 X is applied iteratively to its own output r times before passing the output to the second block. RIGHT: Illustrative examples of models with different signatures and degrees.",
            "score": 0.5754028049415385,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 220,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 240,
                    "end": 258,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "35138335"
                },
                {
                    "start": 874,
                    "end": 898,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 1248,
                    "end": 1266,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1362,
                    "end": 1379,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "278033756",
            "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
            "text": "Most scaling laws research focuses on training compute, which scales with model size and parameter count. Recent work has expanded to study model sparsity (MoE or activation sparsity), quantisation precision, and inference budgets. Our work contributes to this last category by controlling attention sparsity (via compression ratio), which directly impacts inference compute and memory requirements, alongside model size and sequence length. \n\nExperimental approach Scaling law data can be gathered either through controlled experiments (Kaplan et al., 2020;Hoffmann et al., 2022) or by analysing existing models across different families (Choshen et al., 2024;Ruan et al., 2024). The former approach is costly but precise, while the latter leverages public data but may be affected by undisclosed differences in training procedures. We conduct controlled experiments with the Qwen 2.5 family across a range of sparse attention algorithms, sparsity levels (compression ratios), sequence lengths, and tasks. \n\nFitting methodology Common approaches to fitting scaling laws include: \n\n\u2022 Observing performance trends to determine the appropriate functional form (e.g., power law, exponential) (Luo et al., 2025). We adopt a log-linear form, assuming a power law relationship for numerical factors. \u2022 Fitting parameters using optimisation algorithms (typically L-BFGS) with robust loss functions like Huber loss (Abnar et al., 2025). We use L-BFGS with Huber loss. \u2022 Managing outliers through careful loss function parameterisation (Frantar et al., 2023) or selective data filtering (Li et al., 2025a). Our use of Huber loss helps mitigate outlier influence. \u2022 Validating on held-out examples, particularly those at the extremes of the parameter range (Yue et al., 2025). We validate by holding out the maximum values along each axis (model size, sequence length, compression ratio). \n\nOur approach aligns with these established practices, as detailed in the main paper and Appendix C.",
            "score": 0.5751312459599038,
            "section_title": "Controlled factors",
            "char_start_offset": 52550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 580,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1578,
                    "end": 1596,
                    "matchedPaperCorpusId": "276618147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "263672005",
            "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
            "text": "Predicting task performance before training is an aspirational objective for the development of predictable AI systems, and a multitude of studies approach this aim from various perspectives. \n\nLoss Scaling Law. Scaling phenomena have been observed across a broad spectrum of deep learning architectures. The power-law scaling behavior of loss in RNN-based models is investigated in Hestness et al. (2017). Kaplan et al. (2020) delineate the loss scaling trends for Transformer-based language models and explores the scaling behavior of optimal hyper-parameters. They formally established the following scaling law \n\nwhere N is the number of non-embedding parameters of LLM, c, \u03b1 are positive coefficients, and L 0 is the irreducible loss representing the randomness in data. This formulation has catalyzed the proliferation of LLMs. Subsequently, scaling laws are established for various domains and scenarios, including multi-modality (Henighan et al., 2020;Zhai et al., 2022), computation constraint scenario (Hoffmann et al., 2022), data engineering (Muennighoff et al., 2023;Sorscher et al., 2022), and reinforcement learning (Gao et al., 2023). Yao & Wang (2023) extend the scaling law into loss prediction by introducing hyper-parameter scaling methods. The relationship of our work with these existing literature is twofold. First, these works concentrate on training and validation loss metrics, which do not reliably predict task performance. Second, our research builds on these scaling laws and extends the mathematical form of Eq.( 1) to the scaling law of task performance. \n\nScaling Behavior of Task Performance. Despite the predictable decrement in LLM loss, task performance improvements are twisted during scaling. While some tasks, predominantly those relying on memorization of knowledge, have shown progressive improvement, numerous tasks exhibit breakthrough behavior as model size increases (Srivastava et al., 2022;Wei et al., 2022a).",
            "score": 0.575029135177663,
            "section_title": "RELATED WORK",
            "char_start_offset": 6852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 960,
                    "end": 978,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1080,
                    "end": 1102,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1131,
                    "end": 1149,
                    "matchedPaperCorpusId": "252992904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388916015625
        },
        {
            "corpus_id": "276249339",
            "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling",
            "text": "Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.",
            "score": 0.5743330694366404,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474365234375
        },
        {
            "corpus_id": "270620190",
            "title": "Reconciling Kaplan and Chinchilla Scaling Laws",
            "text": "Neural scaling laws. Kaplan et al. (2020) & Hoffmann et al. (2022) investigated empirically modeling the relationships between the number of parameters (N ), training tokens (D), training compute (C) and loss (L) in transformers for language modeling. The main functional form of relationship considered was a power law, y = ax b , which is widely used throughout the sciences to describe a relationship between two quantities (x & y) that holds over many orders of magnitude, e.g. (Kello et al., 2010).",
            "score": 0.5735688345356277,
            "section_title": "Set Up",
            "char_start_offset": 767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 503
                }
            ],
            "ref_mentions": [
                {
                    "start": 482,
                    "end": 502,
                    "matchedPaperCorpusId": "131508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31787109375
        },
        {
            "corpus_id": "278714770",
            "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
            "text": "Test-time scaling (TTS) has emerged as a pivotal strategy in enhancing the performance of LLMs by allocating additional computational resources during inference. This approach shifts the traditional emphasis from extensive pretraining to optimizing inference-time computation, enabling models to tackle complex tasks more effectively. Following Muennighoff et al. [2025] and Zhang et al. [2025], we classify test-time scaling methods into: (1) Parallel Scaling [Wang et al., 2023, Brown et al., 2024, Snell et al., 2024, Liu et al., 2025], where parallel computes multiple reasoning chains independently, (2) Sequential Scaling [Madaan et al., 2023, Chen et al., 2024, Muennighoff et al., 2025], where computes a longer reasoning chain and generates the chain sequentially, and (3) Hybrid Scaling [Yao et al., 2023, Gandhi et al., 2024, Wang et al., 2025], where combines the parallel scaling and sequential scaling methods. In this paper, we mainly focus on parallel test-time scaling, which can be adopt on large-scale LLMs efficiently. \n\nAs conclued by Zhang et al. [2025], parallel scaling improves test-time performance by generating multiple reasoning chains in parallel, and then aggregating them together to the final answer. Early evidence that sampling multiple reasoning chains and voting improves robustness came from Self-Consistency (SC) [Wang et al., 2023], inspiring subsequent studies on how many chains to sample for a fixed compute envelope [Snell et al., 2024]. Li et al. [2025] suggest that the chance of finding the correct answer improves while increasing the number of generated responses, which is empirically summarized by a log-linear scaling law [Brown et al., 2024]. Despite the effectiveness of these approaches, the majority of existing parallel test-time scaling methods rely on discrete token-by-token generation, which imposes inherent constraints and limits their expressiveness.",
            "score": 0.5724663210515659,
            "section_title": "Test-Time Scaling",
            "char_start_offset": 5749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 461,
                    "end": 479,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 628,
                    "end": 648,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 648,
                    "end": 667,
                    "matchedPaperCorpusId": "258059885"
                },
                {
                    "start": 797,
                    "end": 814,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1352,
                    "end": 1371,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "260887765",
            "title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
            "text": "Scaling has played a central role in the success of LLMsstarting with the 'scaling laws' paper for causal language models (Kaplan et al., 2020), which found that their performance improves with scaling the model size, data size, and also the amount of compute used for training. This analysis was subsequently expanded to other benchmarks, modalities and downstream tasks (Ghorbani et al., 2021;Alabdulmohsin et al., 2022;Hernandez et al., 2021;Hoffmann et al., 2022). It is often mentioned as a key factor 6 in LLM performance; for instance, Huang et al. (2023b, p.1) state: \"scaling has enabled Large Language Models (LLMs) to achieve state-of-the-art performance on a range of Natural Language Processing (NLP) tasks\". The focus on scaling is in line with the \"bitter lesson\" of Sutton (2019), which states that we should stop working on methods based on the human knowledge of the target problem, and embrace \"search and learning\", because \"the only thing that matters in the long run is the leveraging of computation\". Indeed, the scaling hypothesis seems to be supported by the fact that LLMs have been growing in size for several years: e.g. BERT-base in 2018 had 340M parameters (Devlin et al., 2019), and in 2022 PaLM had 540B parameters in 2022 (Chowdhery et al., 2022). And there is evidence that, even with the same architecture and training data, larger models tend to perform better, even with adversarial evaluation (Bhargava et al., 2021;Ray Choudhury et al., 2022;Wang et al., 2022b). \n\nHowever, it is important to keep in mind that many of the best-known LLMs scaled both the number of parameters and their training data concurrently (besides any differences in architecture and training set-up)7 . While this seems to support the scaling laws hypothesis as formulated by Kaplan et al. ( 2020)-we do not know which of these components is most responsible for the improvement, and most LLMs are not directly comparable by more than one of these criteria.",
            "score": 0.572227566824995,
            "section_title": "Claim: (LLM) Scale Is All You Need",
            "char_start_offset": 19579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 395,
                    "end": 422,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 1187,
                    "end": 1208,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1431,
                    "end": 1454,
                    "matchedPaperCorpusId": "238259403"
                },
                {
                    "start": 1454,
                    "end": 1481,
                    "matchedPaperCorpusId": "252283929"
                },
                {
                    "start": 1481,
                    "end": 1500,
                    "matchedPaperCorpusId": "253098274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2197265625
        },
        {
            "corpus_id": "276249712",
            "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling",
            "text": "Since ChatGPT, large language models (LLMs) have been a rapidly evolving domain that tries to solve problems be-yond traditional language tasks like summarization or question answering (Chen et al., 2023;Yao et al., 2023;Chen et al., 2024b;d). Significantly, the newly released Ope-nAI O1 has demonstrated its strong capability in complex problem-solving through its detailed reasoning steps before outputting the final answer (Jaech et al., 2024). Since then, many researchers have studied how to replicate success from an open-source perspective and how to train models that are even better at efficiently solving problems that still remain unsolvable by the current LLMs (Huang et al., 2024;Zeng et al., 2024;DeepSeek-AI et al., 2025). \n\nThe remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities. Moreover, this paradigm not only enhances inference-time efficiency but also facilitates the generation of high-quality synthetic data, which can be leveraged to train the next generation of models and evaluate their performance (Guan et al., 2025). \n\nA key insight in this line of research is that high-quality data capable of eliciting self-reflection in LLMs is crucial for enhancing their reasoning capabilities. Notably, o1-like models demonstrate improved performance when longer reasoning chains are utilized (Huang et al., 2024;DeepSeek-AI et al., 2025).",
            "score": 0.5721188390177964,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 221,
                    "matchedPaperCorpusId": "252762395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "paperId": "f25cc8cfb985d1bad1f1a070d74dd6171d2d028c",
            "corpusId": 271269939,
            "title": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 60,
            "citationCount": 27,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287946530",
                    "name": "Rulin Shao"
                },
                {
                    "authorId": "2188998958",
                    "name": "Jacqueline He"
                },
                {
                    "authorId": "2290402940",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2288469507",
                    "name": "Tim Dettmers"
                },
                {
                    "authorId": "48872685",
                    "name": "Sewon Min"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2303396379",
                    "name": "Pang Wei Koh"
                }
            ],
            "abstract": "Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.",
            "corpus_id": "271269939",
            "text": "Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8154296875
        },
        {
            "paperId": "8292083dd8f6ae898ea0ee54a6b97997d1a51c9d",
            "corpusId": 271719990,
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 32,
            "citationCount": 698,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314917835",
                    "name": "C. Snell"
                },
                {
                    "authorId": "2315069693",
                    "name": "Jaehoon Lee"
                },
                {
                    "authorId": "2269023101",
                    "name": "Kelvin Xu"
                },
                {
                    "authorId": "2275526115",
                    "name": "Aviral Kumar"
                }
            ],
            "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a\"compute-optimal\"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
            "corpus_id": "271719990",
            "text": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a\"compute-optimal\"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7880859375
        },
        {
            "paperId": "ff277b0c298c5cf890af067f03b9e9243c9087a5",
            "corpusId": 279465274,
            "title": "Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute",
            "venue": "",
            "year": 2025,
            "referenceCount": 33,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.15882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Sheng Liu"
                },
                {
                    "authorId": null,
                    "name": "Tianlang Chen"
                },
                {
                    "authorId": null,
                    "name": "Pan Lu"
                },
                {
                    "authorId": null,
                    "name": "Haotian Ye"
                },
                {
                    "authorId": "2296095658",
                    "name": "Yizheng Chen"
                },
                {
                    "authorId": "2266392022",
                    "name": "Lei Xing"
                },
                {
                    "authorId": null,
                    "name": "James Zou"
                }
            ],
            "abstract": "Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.",
            "corpus_id": "279465274",
            "text": "Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.53076171875
        },
        {
            "paperId": "01d6446eaf0066afe4ffc95cfdcf0d26eec7b0ee",
            "corpusId": 277452334,
            "title": "Scaling Auditory Cognition via Test-Time Compute in Audio Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 37,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316051686",
                    "name": "Ting Dang"
                },
                {
                    "authorId": "2353199976",
                    "name": "Yan Gao"
                },
                {
                    "authorId": "2316274006",
                    "name": "Hong Jia"
                }
            ],
            "abstract": "Large language models (LLMs) have shown exceptional versatility in natural language processing, prompting recent efforts to extend their multimodal capabilities to speech processing through the development of audio large language models (Audio LLMs). While Audio LLMs excel in tasks such as speech recognition and synthesis, it remains unclear how they perform when faced with the auditory cognitive challenges posed by real-world environments, such as audio comprehension and listening recall, particularly in the presence of background noise or overlapping speech. Unlike text-based LLMs, which have access to vast amounts of text data for pre-training, retraining Audio LLMs with diverse auditory cognitive scenes is difficult due to the limited datasets that simulate real-world auditory cognitive scenarios and the challenge of acquiring auditory cognitive labels for training. While test-time compute (TTC) methods have been shown to enhance the capabilities of text-based LLMs during inference, a key challenge lies in designing these TTC methods to improve the auditory capabilities of Audio LLMs. This study aims to address these two research gaps by: i) exploring the auditory cognitive capabilities of Audio LLMs, and ii) enhancing their capabilities using TTC approaches. We have investigated five different Audio LLMs for auditory cognition using a \\textit{self-collected} database and have proposed five TTC approaches to enhance auditory cognitive capabilities during inference. Our findings reveal that Audio LLMs performance decreases in more challenging auditory cognitive tasks. The proposed TTC approaches significantly enhance cognitive auditory capabilities, advancing the development of more adaptable and resilient Audio LLMs for practical applications such as assistive listening devices, voice-based AI assistants, and communication technologies.",
            "corpus_id": "277452334",
            "text": "Large language models (LLMs) have shown exceptional versatility in natural language processing, prompting recent efforts to extend their multimodal capabilities to speech processing through the development of audio large language models (Audio LLMs). While Audio LLMs excel in tasks such as speech recognition and synthesis, it remains unclear how they perform when faced with the auditory cognitive challenges posed by real-world environments, such as audio comprehension and listening recall, particularly in the presence of background noise or overlapping speech. Unlike text-based LLMs, which have access to vast amounts of text data for pre-training, retraining Audio LLMs with diverse auditory cognitive scenes is difficult due to the limited datasets that simulate real-world auditory cognitive scenarios and the challenge of acquiring auditory cognitive labels for training. While test-time compute (TTC) methods have been shown to enhance the capabilities of text-based LLMs during inference, a key challenge lies in designing these TTC methods to improve the auditory capabilities of Audio LLMs. This study aims to address these two research gaps by: i) exploring the auditory cognitive capabilities of Audio LLMs, and ii) enhancing their capabilities using TTC approaches. We have investigated five different Audio LLMs for auditory cognition using a \\textit{self-collected} database and have proposed five TTC approaches to enhance auditory cognitive capabilities during inference. Our findings reveal that Audio LLMs performance decreases in more challenging auditory cognitive tasks. The proposed TTC approaches significantly enhance cognitive auditory capabilities, advancing the development of more adaptable and resilient Audio LLMs for practical applications such as assistive listening devices, voice-based AI assistants, and communication technologies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.294189453125
        },
        {
            "paperId": "ea08910d390cb1e5a60c8f569f6c0891b3df9f59",
            "corpusId": 276161207,
            "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 68,
            "citationCount": 9,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.04128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277696001",
                    "name": "Zhen Ye"
                },
                {
                    "authorId": "2327901613",
                    "name": "Xinfa Zhu"
                },
                {
                    "authorId": "2327173493",
                    "name": "Chi-min Chan"
                },
                {
                    "authorId": "2329291133",
                    "name": "Xinsheng Wang"
                },
                {
                    "authorId": "2277651617",
                    "name": "Xu Tan"
                },
                {
                    "authorId": "2319224656",
                    "name": "Jiahe Lei"
                },
                {
                    "authorId": "2344122751",
                    "name": "Yi Peng"
                },
                {
                    "authorId": "2297845182",
                    "name": "Haohe Liu"
                },
                {
                    "authorId": "2344203872",
                    "name": "Yizhu Jin"
                },
                {
                    "authorId": "2306950520",
                    "name": "Zheqi Dai"
                },
                {
                    "authorId": "2109380683",
                    "name": "Hongzhan Lin"
                },
                {
                    "authorId": "2297922077",
                    "name": "Jianyi Chen"
                },
                {
                    "authorId": "2328017864",
                    "name": "Xingjian Du"
                },
                {
                    "authorId": "46426991",
                    "name": "Liumeng Xue"
                },
                {
                    "authorId": "2144863508",
                    "name": "Yunlin Chen"
                },
                {
                    "authorId": "2257093405",
                    "name": "Zhifei Li"
                },
                {
                    "authorId": "2335081804",
                    "name": "Lei Xie"
                },
                {
                    "authorId": "2307011441",
                    "name": "Qiuqiang Kong"
                },
                {
                    "authorId": "2118270918",
                    "name": "Yi-Ting Guo"
                },
                {
                    "authorId": "2329227132",
                    "name": "Wei Xue"
                }
            ],
            "abstract": "Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.",
            "corpus_id": "276161207",
            "text": "Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6376953125
        },
        {
            "paperId": "f9982586bb0f2c40d3d92e18c0a0b0723305e5a2",
            "corpusId": 277110444,
            "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 31,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2411.19477?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2411.19477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2272979112",
                    "name": "Yanxi Chen"
                },
                {
                    "authorId": "2211993531",
                    "name": "Xuchen Pan"
                },
                {
                    "authorId": "2237607166",
                    "name": "Yaliang Li"
                },
                {
                    "authorId": "2266389996",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "2237499232",
                    "name": "Jingren Zhou"
                }
            ],
            "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N \u00d7 ( K + 1) highly parallelizable LLM calls are needed for solving an input problem. For an input problem, assuming that a generated candidate solution is correct with probability p gen > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p comp > 0 . 5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K : P ( final output is incorrect ) \u2264 (1 \u2212 p gen ) N + \u2308 log 2 N \u2309 e \u2212 2 K ( p comp \u2212 0 . 5) 2 . Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.",
            "corpus_id": "277110444",
            "text": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N \u00d7 ( K + 1) highly parallelizable LLM calls are needed for solving an input problem. For an input problem, assuming that a generated candidate solution is correct with probability p gen > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p comp > 0 . 5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K : P ( final output is incorrect ) \u2264 (1 \u2212 p gen ) N + \u2308 log 2 N \u2309 e \u2212 2 K ( p comp \u2212 0 . 5) 2 . Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.93408203125
        },
        {
            "paperId": "862479f7bd78a69c52a0691766848caa8eb4660f",
            "corpusId": 269740949,
            "title": "Linearizing Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 42,
            "citationCount": 24,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.06640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "72847120",
                    "name": "Jean-Pierre Mercat"
                },
                {
                    "authorId": "2291068185",
                    "name": "Igor Vasiljevic"
                },
                {
                    "authorId": "150299584",
                    "name": "Sedrick Scott Keh"
                },
                {
                    "authorId": "2284685268",
                    "name": "Kushal Arora"
                },
                {
                    "authorId": "2298523",
                    "name": "Achal Dave"
                },
                {
                    "authorId": "2143070716",
                    "name": "Adrien Gaidon"
                },
                {
                    "authorId": "2283843631",
                    "name": "Thomas Kollar"
                }
            ],
            "abstract": "Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.",
            "corpus_id": "269740949",
            "text": "Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.27734375
        },
        {
            "paperId": "f9ca7ee3daa5c35da0fe6adfcd00d6300dee8231",
            "corpusId": 277940798,
            "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 47,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18941, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312897913",
                    "name": "Hongru Cai"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2273661632",
                    "name": "Ruifeng Yuan"
                },
                {
                    "authorId": "2298418808",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2284702952",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "2284641622",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ],
            "abstract": "Generative retrieval reformulates retrieval as an autoregressive generation task, where large language models (LLMs) generate target documents directly from a query. As a novel paradigm, the mechanisms that underpin its performance and scalability remain largely unexplored. We systematically investigate training and inference scaling laws in generative retrieval, exploring how model size, training data scale, and inference-time compute jointly influence performance. We propose a novel evaluation metric inspired by contrastive entropy and generation loss, providing a continuous performance signal that enables robust comparisons across diverse generative retrieval methods. Our experiments show that n-gram-based methods align strongly with training and inference scaling laws. We find that increasing model size, training data scale, and inference-time compute all contribute to improved performance, highlighting the complementary roles of these factors in enhancing generative retrieval. Across these settings, LLaMA models consistently outperform T5 models, suggesting a particular advantage for larger decoder-only models in generative retrieval. Our findings underscore that model sizes, data availability, and inference computation interact to unlock the full potential of generative retrieval, offering new insights for designing and optimizing future systems.",
            "corpus_id": "277940798",
            "text": "Generative retrieval reformulates retrieval as an autoregressive generation task, where large language models (LLMs) generate target documents directly from a query. As a novel paradigm, the mechanisms that underpin its performance and scalability remain largely unexplored. We systematically investigate training and inference scaling laws in generative retrieval, exploring how model size, training data scale, and inference-time compute jointly influence performance. We propose a novel evaluation metric inspired by contrastive entropy and generation loss, providing a continuous performance signal that enables robust comparisons across diverse generative retrieval methods. Our experiments show that n-gram-based methods align strongly with training and inference scaling laws. We find that increasing model size, training data scale, and inference-time compute all contribute to improved performance, highlighting the complementary roles of these factors in enhancing generative retrieval. Across these settings, LLaMA models consistently outperform T5 models, suggesting a particular advantage for larger decoder-only models in generative retrieval. Our findings underscore that model sizes, data availability, and inference computation interact to unlock the full potential of generative retrieval, offering new insights for designing and optimizing future systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8740234375
        },
        {
            "paperId": "a70f75ccedb7b6eb769b959ec9d58a59d44d1cf5",
            "corpusId": 277857467,
            "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 18,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257764593",
                    "name": "Kevin Lin"
                },
                {
                    "authorId": "2355866239",
                    "name": "Charlie Snell"
                },
                {
                    "authorId": "2356041105",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2257345100",
                    "name": "Charles Packer"
                },
                {
                    "authorId": "68974952",
                    "name": "Sarah Wooders"
                },
                {
                    "authorId": "2055174324",
                    "name": "Ion Stoica"
                },
                {
                    "authorId": "2258073138",
                    "name": "Joseph Gonzalez"
                }
            ],
            "abstract": "Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to\"think\"offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.",
            "corpus_id": "277857467",
            "text": "Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to\"think\"offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.80029296875
        },
        {
            "paperId": "7cb7ba6e0cacb40fa4cdf2d8e51504bffac7b32d",
            "corpusId": 279243347,
            "title": "Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets",
            "venue": "",
            "year": 2025,
            "referenceCount": 48,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.04598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2174178585",
                    "name": "Marianna Nezhurina"
                },
                {
                    "authorId": "2308470091",
                    "name": "Tomer Porian"
                },
                {
                    "authorId": "2366012192",
                    "name": "Giovanni Pucceti"
                },
                {
                    "authorId": "2181874847",
                    "name": "Tommie Kerssies"
                },
                {
                    "authorId": "2125377840",
                    "name": "R. Beaumont"
                },
                {
                    "authorId": "2304553123",
                    "name": "Mehdi Cherti"
                },
                {
                    "authorId": "2191688",
                    "name": "J. Jitsev"
                }
            ],
            "abstract": "In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and performance at larger scales. We show here how scaling law derivation can also be used for model and dataset comparison, allowing to decide which procedure is to be preferred for pre-training. For the first time, full scaling laws based on dense measurements across a wide span of model and samples seen scales are derived for two important language-vision learning procedures, CLIP and MaMMUT, that use either contrastive only or contrastive and captioning text generative loss. Ensuring sufficient prediction accuracy for held out points, we use derived scaling laws to compare both models, obtaining evidence for MaMMUT's stronger improvement with scale and better sample efficiency than standard CLIP. To strengthen validity of the comparison, we show scaling laws for various downstream tasks, classification, retrieval, and segmentation, and for different open datasets, DataComp, DFN and Re-LAION, observing consistently the same trends. We show that comparison can also be performed when deriving scaling laws with a constant learning rate schedule, reducing compute cost. Accurate derivation of scaling laws provides thus means to perform model and dataset comparison across scale spans, avoiding misleading conclusions based on measurements from single reference scales only, paving the road for systematic comparison and improvement of open foundation models and datasets for their creation. We release all the pre-trained models with their intermediate checkpoints, including openMaMMUT-L/14, which achieves $80.3\\%$ zero-shot ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/scaling-laws-for-comparison.",
            "corpus_id": "279243347",
            "text": "In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and performance at larger scales. We show here how scaling law derivation can also be used for model and dataset comparison, allowing to decide which procedure is to be preferred for pre-training. For the first time, full scaling laws based on dense measurements across a wide span of model and samples seen scales are derived for two important language-vision learning procedures, CLIP and MaMMUT, that use either contrastive only or contrastive and captioning text generative loss. Ensuring sufficient prediction accuracy for held out points, we use derived scaling laws to compare both models, obtaining evidence for MaMMUT's stronger improvement with scale and better sample efficiency than standard CLIP. To strengthen validity of the comparison, we show scaling laws for various downstream tasks, classification, retrieval, and segmentation, and for different open datasets, DataComp, DFN and Re-LAION, observing consistently the same trends. We show that comparison can also be performed when deriving scaling laws with a constant learning rate schedule, reducing compute cost. Accurate derivation of scaling laws provides thus means to perform model and dataset comparison across scale spans, avoiding misleading conclusions based on measurements from single reference scales only, paving the road for systematic comparison and improvement of open foundation models and datasets for their creation. We release all the pre-trained models with their intermediate checkpoints, including openMaMMUT-L/14, which achieves $80.3\\%$ zero-shot ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/scaling-laws-for-comparison.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.41552734375
        },
        {
            "paperId": "9c8f548b85af8d3c861c9d97b6172c4ee44673d6",
            "corpusId": 275570556,
            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 69,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279750963",
                    "name": "Nanye Ma"
                },
                {
                    "authorId": "2058178039",
                    "name": "Shangyuan Tong"
                },
                {
                    "authorId": "2340654494",
                    "name": "Haolin Jia"
                },
                {
                    "authorId": "2307548497",
                    "name": "Hexiang Hu"
                },
                {
                    "authorId": "2269866136",
                    "name": "Yu-Chuan Su"
                },
                {
                    "authorId": "2326256475",
                    "name": "Mingda Zhang"
                },
                {
                    "authorId": "2340726220",
                    "name": "Xuan Yang"
                },
                {
                    "authorId": "2324838111",
                    "name": "Yandong Li"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                },
                {
                    "authorId": "2325917399",
                    "name": "Xuhui Jia"
                },
                {
                    "authorId": "2324769373",
                    "name": "Saining Xie"
                }
            ],
            "abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "corpus_id": "275570556",
            "text": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8232421875
        },
        {
            "paperId": "1535c04a6c7f55338e0d8d5f17325176ba5b7b24",
            "corpusId": 269982332,
            "title": "Scaling-laws for Large Time-series Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 53,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13867, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266240992",
                    "name": "Thomas D. P. Edwards"
                },
                {
                    "authorId": "2302794109",
                    "name": "James Alvey"
                },
                {
                    "authorId": "81230967",
                    "name": "J. Alsing"
                },
                {
                    "authorId": "2303623210",
                    "name": "Nam H. Nguyen"
                },
                {
                    "authorId": "6315490",
                    "name": "B. Wandelt"
                }
            ],
            "abstract": "Scaling laws for large language models (LLMs) have provided useful guidance in training ever larger models for predictable performance gains. Time series forecasting shares a similar sequential structure to language, and is amenable to large-scale transformer architectures. Here we show that foundational decoder-only time series transformer models exhibit analogous scaling-behavior to LLMs, with architectural details (aspect ratio and number of heads) having a minimal effect over broad ranges. We assemble a large corpus of heterogenous time series data on which to train, and establish for the first time power-law scaling with parameter count, dataset size, and training compute, spanning five orders of magnitude.",
            "corpus_id": "269982332",
            "text": "Scaling laws for large language models (LLMs) have provided useful guidance in training ever larger models for predictable performance gains. Time series forecasting shares a similar sequential structure to language, and is amenable to large-scale transformer architectures. Here we show that foundational decoder-only time series transformer models exhibit analogous scaling-behavior to LLMs, with architectural details (aspect ratio and number of heads) having a minimal effect over broad ranges. We assemble a large corpus of heterogenous time series data on which to train, and establish for the first time power-law scaling with parameter count, dataset size, and training compute, spanning five orders of magnitude.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.44970703125
        },
        {
            "paperId": "03fa0740512a47758940415a6b3c1a635d9aca98",
            "corpusId": 276235717,
            "title": "Training Language Models to Reason Efficiently",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 39,
            "citationCount": 72,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.04463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2138481119",
                    "name": "Daman Arora"
                },
                {
                    "authorId": "2288537572",
                    "name": "Andrea Zanette"
                }
            ],
            "abstract": "Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models. In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.",
            "corpus_id": "276235717",
            "text": "Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models. In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.798828125
        },
        {
            "paperId": "f1c022afe27700efa8da88f2bb5409f919e55e69",
            "corpusId": 257632953,
            "title": "Language Models: The Most Important Compute Challenge of Our Time (Keynote)",
            "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
            "year": 2023,
            "referenceCount": 0,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3582016.3587051?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3582016.3587051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "27439851",
                    "name": "B. Catanzaro"
                }
            ],
            "abstract": "ChatGPT recently became one of the fastest growing new applications in history, thanks to its intriguing text generation capabilities that are able to answer questions, write poetry, and even problem solve. Large Language Models are now being integrated in fundamental ways into products around the tech industry. The possibilities are extraordinary, but much research remains to make these systems reliable and trustworthy, as well as integrate them into applications seamlessly. Additionally, the computational challenges behind large language modeling are also quite important. Systems for training and deploying these models must be highly scalable and run at extreme efficiency, because the amount of work necessary to converge a model can be extraordinarily large. The cost of deploying these models is a barrier to their deployment and must be lowered significantly. In this talk, I\u2019ll discuss the work we have been doing at NVIDIA to optimize systems for Large Language Model training and inference, and highlight some of the challenges that remain for future work.",
            "corpus_id": "257632953",
            "text": "ChatGPT recently became one of the fastest growing new applications in history, thanks to its intriguing text generation capabilities that are able to answer questions, write poetry, and even problem solve. Large Language Models are now being integrated in fundamental ways into products around the tech industry. The possibilities are extraordinary, but much research remains to make these systems reliable and trustworthy, as well as integrate them into applications seamlessly. Additionally, the computational challenges behind large language modeling are also quite important. Systems for training and deploying these models must be highly scalable and run at extreme efficiency, because the amount of work necessary to converge a model can be extraordinarily large. The cost of deploying these models is a barrier to their deployment and must be lowered significantly. In this talk, I\u2019ll discuss the work we have been doing at NVIDIA to optimize systems for Large Language Model training and inference, and highlight some of the challenges that remain for future work.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.4462890625
        }
    ],
    "quotes": {
        "cost": 0.16342800000000002,
        "quotes": [
            {
                "idx": 0,
                "key": "[264172438 | Wang et al. | 2023 | Citations: 117]",
                "snippets": "Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models.\n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models.\n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[266693796 | Sardana et al. | 2023 | Citations: 88]",
                "snippets": "However, the Chinchilla scaling laws only account for the computational costs of training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1629,
                        "end": 1719,
                        "sentence_offsets": [
                            {
                                "start": 1629,
                                "end": 1719
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, the Chinchilla scaling laws only account for the computational costs of training."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[268379614 | Gadre et al. | 2024 | Citations: 48]",
                "snippets": "We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 678,
                        "end": 958,
                        "sentence_offsets": [
                            {
                                "start": 678,
                                "end": 958
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[268875826 | Yun et al. | 2024 | Citations: 8]",
                "snippets": "We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[269449894 | Xiong et al. | 2024 | Citations: 10]",
                "snippets": "Despite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Scaling Laws for Language Models",
                        "pdf_hash": "",
                        "start": 1076,
                        "end": 1571,
                        "sentence_offsets": [
                            {
                                "start": 1076,
                                "end": 1217
                            },
                            {
                                "start": 1218,
                                "end": 1368
                            },
                            {
                                "start": 1369,
                                "end": 1571
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[269899695 | Ruan et al. | 2024 | Citations: 62]",
                "snippets": "Unfortunately, there is no explicit discussion of inference-time compute cost scaling laws in the provided paper snippets. The paper focuses on observational scaling laws related to model capabilities and their relationship to training compute, but does not address formal quantitative scaling laws for inference costs. None",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Unfortunately, there is no explicit discussion of inference-time compute cost scaling laws in the provided paper snippets. The paper focuses on observational scaling laws related to model capabilities and their relationship to training compute, but does not address formal quantitative scaling laws for inference costs. None",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[270703266 | Welleck et al. | 2024 | Citations: 77]",
                "snippets": "However, there is also another lesser-mentioned scaling phenomenon, where adopting more sophisticated methods or scaling compute at inference time (Jones, 2021)",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 251,
                        "end": 411,
                        "sentence_offsets": [
                            {
                                "start": 251,
                                "end": 411
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, there is also another lesser-mentioned scaling phenomenon, where adopting more sophisticated methods or scaling compute at inference time (Jones, 2021)"
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[271571035 | Brown et al. | 2024 | Citations: 330]",
                "snippets": "Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 549,
                        "end": 753,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[271601023 | Wu et al. | 2024 | Citations: 130]",
                "snippets": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies.\n\nWe performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies.\n\nWe performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[272593336 | Sun et al. | 2024 | Citations: 0]",
                "snippets": "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance...Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]...This unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 306,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 164
                            },
                            {
                                "start": 165,
                                "end": 307
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 591,
                        "end": 1183,
                        "sentence_offsets": [
                            {
                                "start": 466,
                                "end": 594
                            },
                            {
                                "start": 595,
                                "end": 733
                            },
                            {
                                "start": 734,
                                "end": 809
                            },
                            {
                                "start": 810,
                                "end": 1006
                            },
                            {
                                "start": 1007,
                                "end": 1142
                            },
                            {
                                "start": 1145,
                                "end": 1327
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1778,
                        "end": 1984,
                        "sentence_offsets": [
                            {
                                "start": 1778,
                                "end": 1983
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[272827424 | Saad-Falcon et al. | 2024 | Citations: 20]",
                "snippets": "A number of recent works show the effectiveness of scaling compute at test time [6,7,24,47]. In particular, Large Language Monkeys [6] characterizes inference-time scaling laws, showing a log-linear relationship between coverage-the fraction of problems solved by at least one attempt-and the number of samples drawn from the model across a broad range of reasoning tasks and LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Scaling Laws of Language Models",
                        "pdf_hash": "",
                        "start": 1173,
                        "end": 1554,
                        "sentence_offsets": [
                            {
                                "start": 1173,
                                "end": 1265
                            },
                            {
                                "start": 1266,
                                "end": 1554
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A number of recent works show the effectiveness of scaling compute at test time [6,7,24,47]. In particular, Large Language Monkeys [6] characterizes inference-time scaling laws, showing a log-linear relationship between coverage-the fraction of problems solved by at least one attempt-and the number of samples drawn from the model across a broad range of reasoning tasks and LLMs."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[273507997 | Levi | 2024 | Citations: 13]",
                "snippets": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts.\n\nRecent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials.\n\nWe then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts.\n\nRecent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials.\n\nWe then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[273850391 | Nori et al. | 2024 | Citations: 13]",
                "snippets": "Scaling laws [KMH + 20] for large language models (LLMs) have proven to be valuable for predicting how model capabilities grow with increasing data, compute, and model size. However, inference-time scaling-which focuses on the value of investing in additional computation at model run-time-is a promising new area of study.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Directions with LLM Run-Time Strategies",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 323,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 173
                            },
                            {
                                "start": 174,
                                "end": 323
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Scaling laws [KMH + 20] for large language models (LLMs) have proven to be valuable for predicting how model capabilities grow with increasing data, compute, and model size. However, inference-time scaling-which focuses on the value of investing in additional computation at model run-time-is a promising new area of study."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[273877632 | Kumar et al. | 2024 | Citations: 29]",
                "snippets": "For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 429,
                        "end": 623,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[275336968 | Lu | 2025 | Citations: 1]",
                "snippets": "Building on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 747,
                        "end": 1173,
                        "sentence_offsets": [
                            {
                                "start": 747,
                                "end": 851
                            },
                            {
                                "start": 852,
                                "end": 1081
                            },
                            {
                                "start": 1082,
                                "end": 1173
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Building on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[275570556 | Ma et al. | 2025 | Citations: 69]",
                "snippets": "Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[275789021 | Jin et al. | 2025 | Citations: 1]",
                "snippets": "Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[275993956 | Xie et al. | 2025 | Citations: 28]",
                "snippets": "Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34].\n\nRecent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34].\n\nRecent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[276249712 | Chen et al. | 2025 | Citations: 1]",
                "snippets": "The remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 741,
                        "end": 1617,
                        "sentence_offsets": [
                            {
                                "start": 741,
                                "end": 976
                            },
                            {
                                "start": 977,
                                "end": 1117
                            },
                            {
                                "start": 1118,
                                "end": 1276
                            },
                            {
                                "start": 1277,
                                "end": 1456
                            },
                            {
                                "start": 1457,
                                "end": 1617
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[276421468 | Sengupta et al. | 2025 | Citations: 0]",
                "snippets": "Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D Additional research questions and guidelines",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 444,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 259
                            },
                            {
                                "start": 260,
                                "end": 444
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[276421795 | Mayilvahanan et al. | 2025 | Citations: 3]",
                "snippets": "Scaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget (Kaplan et al., 2020;Hoffmann et al., 2022;Grattafiori et al., 2024). Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs). FLOPs account for both parameter count and the number of training tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 437,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 200
                            },
                            {
                                "start": 201,
                                "end": 363
                            },
                            {
                                "start": 364,
                                "end": 437
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Scaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget (Kaplan et al., 2020;Hoffmann et al., 2022;Grattafiori et al., 2024). Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs). FLOPs account for both parameter count and the number of training tokens."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[276580891 | Schaeffer et al. | 2025 | Citations: 4]",
                "snippets": "Despite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Discussion and Future Directions",
                        "pdf_hash": "",
                        "start": 1075,
                        "end": 2308,
                        "sentence_offsets": [
                            {
                                "start": 1075,
                                "end": 1256
                            },
                            {
                                "start": 1257,
                                "end": 1580
                            },
                            {
                                "start": 1583,
                                "end": 1716
                            },
                            {
                                "start": 1717,
                                "end": 1920
                            },
                            {
                                "start": 1923,
                                "end": 2199
                            },
                            {
                                "start": 2200,
                                "end": 2308
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[276813519 | Aggarwal et al. | 2025 | Citations: 101]",
                "snippets": "Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning chains or longer ones (Wu et al., 2024;Snell et al., 2024;OpenAI et al., 2024a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 304,
                        "end": 521,
                        "sentence_offsets": [
                            {
                                "start": 304,
                                "end": 521
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning chains or longer ones (Wu et al., 2024;Snell et al., 2024;OpenAI et al., 2024a)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[277271576 | Shen et al. | 2025 | Citations: 0]",
                "snippets": "In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters, and it is proposed that optimizing inference strategies can significantly improve inference accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Scaling law on LLM",
                        "pdf_hash": "",
                        "start": 252,
                        "end": 606,
                        "sentence_offsets": [
                            {
                                "start": 145,
                                "end": 306
                            },
                            {
                                "start": 307,
                                "end": 414
                            },
                            {
                                "start": 415,
                                "end": 662
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters, and it is proposed that optimizing inference strategies can significantly improve inference accuracy."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[277467695 | Singhi et al. | 2025 | Citations: 6]",
                "snippets": "Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 1265,
                        "end": 1470,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[278636433 | Chen et al. | 2025 | Citations: 2]",
                "snippets": "Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2024)(Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2023) is a promising direction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[266693796 | Sardana et al. | 2023 | Citations: 88]": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
                    "[271601023 | Wu et al. | 2024 | Citations: 130]": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs."
                },
                "metadata": [
                    {
                        "section_title": "Discussion and Future Work",
                        "pdf_hash": "",
                        "start": 490,
                        "end": 966,
                        "sentence_offsets": [
                            {
                                "start": 490,
                                "end": 692
                            },
                            {
                                "start": 693,
                                "end": 966
                            }
                        ],
                        "ref_mentions": [
                            "271601023",
                            "278498044",
                            "266693796"
                        ],
                        "quote": "Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2024)(Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2023) is a promising direction."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[278714770 | Xu et al. | 2025 | Citations: 1]",
                "snippets": "Li et al. [2025] suggest that the chance of finding the correct answer improves while increasing the number of generated responses, which is empirically summarized by a log-linear scaling law [Brown et al., 2024].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Test-Time Scaling",
                        "pdf_hash": "",
                        "start": 1482,
                        "end": 1695,
                        "sentence_offsets": [
                            {
                                "start": 1482,
                                "end": 1695
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Li et al. [2025] suggest that the chance of finding the correct answer improves while increasing the number of generated responses, which is empirically summarized by a log-linear scaling law [Brown et al., 2024]."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.020937,
        "cot": "For this query about scaling laws for inference-time compute cost in language models, I need to organize the content into logical dimensions that directly answer the question. Let me plan these dimensions:\n\n1. **Introduction to Scaling Laws**: This should provide background on what scaling laws are in the context of language models, focusing primarily on the established training compute scaling laws before transitioning to inference-time scaling. This should be a synthesis format to provide a coherent explanation of the foundation concepts.\n\n2. **Formal Inference-Time Scaling Laws**: This dimension will directly address the first part of the query by presenting the formal, quantitative scaling laws that have been established for inference-time compute costs. This should be a synthesis format to coherently explain the mathematical formulations and relationships found in research.\n\n3. **Comparison with Training Scaling Laws**: This dimension will address the second part of the query by comparing inference-time scaling laws with the well-established training compute scaling laws. This should be a synthesis format to effectively compare and contrast these different scaling laws.\n\n4. **Applications and Implications of Inference Scaling Laws**: This dimension will discuss how inference scaling laws are being applied in practice and their implications for model design and deployment. This should be a synthesis format to explain the broader impact and significance of these findings.\n\nNow let me assign each quote to the appropriate dimension:",
        "plan": {
            "Introduction to Scaling Laws (synthesis)": [
                0,
                2,
                4,
                9,
                12,
                17,
                20
            ],
            "Formal Inference-Time Scaling Laws (synthesis)": [
                7,
                8,
                10,
                11,
                15,
                17,
                19,
                22,
                23,
                24,
                26
            ],
            "Comparison with Training Scaling Laws (synthesis)": [
                0,
                1,
                3,
                6,
                9,
                13,
                14,
                16,
                18,
                21,
                25
            ],
            "Applications and Implications of Inference Scaling Laws (synthesis)": [
                3,
                5,
                8,
                9,
                14,
                16,
                18,
                19,
                23,
                25
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Scaling Laws",
                "tldr": "Scaling laws describe predictable relationships between model performance and factors like model size, data volume, and compute resources. These mathematical formulations have guided LLM development by revealing power-law relationships that help optimize resource allocation for training. (7 sources)",
                "text": "\nScaling laws have emerged as fundamental principles governing the performance of large language models (LLMs), establishing mathematical relationships between model capabilities and the resources used to develop them. These laws typically express model performance as a power-law function of computational resources, model parameters, and training data volume <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276421795\" paperTitle=\"(Mayilvahanan et al., 2025)\" isShortName></Paper>. The predictable nature of these relationships has become a cornerstone for strategic resource allocation in LLM development.\n\nThe seminal work in this field revealed that neural language models scale in a predictable manner with the Transformer architecture, demonstrating that loss follows a power-law relationship with the amount of computation used for training <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This predictability enables researchers to determine optimal allocation of computational resources and forecast the performance of larger models based on smaller ones.\n\nTraditional scaling laws focus on several key parameters: model size (number of parameters N), dataset size (number of tokens D), and the total compute budget measured in floating-point operations (FLOPs) <Paper corpusId=\"276421795\" paperTitle=\"(Mayilvahanan et al., 2025)\" isShortName></Paper>. Researchers have identified relationships such as the token multiplier (M = N/D), which represents the ratio of training tokens to parameters and helps quantify the degree of model overtraining <Paper corpusId=\"268379614\" paperTitle=\"(Gadre et al., 2024)\" isShortName></Paper>.\n\nThese scaling patterns have demonstrated remarkable consistency across multiple orders of magnitude and have been observed in both language and vision domains <Paper corpusId=\"275993956\" paperTitle=\"(Xie et al., 2025)\" isShortName></Paper>. However, it's important to note that the idealized assumptions of infinite training resources used in theoretical scaling laws may not accurately reflect real-world constraints, potentially limiting their applicability <Paper corpusId=\"269449894\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>.\n\nRecent developments have challenged some established scaling principles. The \"Chinchilla-optimal\" scaling law, which recommended specific ratios of model size to training data, has been questioned by models like Llama 3 and 3.1, which have been trained on significantly more tokens than recommended yet still demonstrate excellent performance <Paper corpusId=\"272593336\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. This has led to proposals for a unified scaling law suggesting that model performance is primarily driven by total compute, regardless of how it's distributed between model size and dataset size.\n\nWhile training scaling laws have been extensively studied, inference-time scaling\u2014which focuses on computational investments at model run-time rather than during training\u2014represents a promising new frontier <Paper corpusId=\"273850391\" paperTitle=\"(Nori et al., 2024)\" isShortName></Paper>. Understanding how model performance scales with inference-time compute has significant implications for deployment efficiency and accessibility of language models.",
                "citations": [
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models.\n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once."
                        ],
                        "paper": {
                            "corpus_id": 264172438,
                            "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2127328758",
                                    "name": "Hongyu Wang"
                                },
                                {
                                    "authorId": "2118866998",
                                    "name": "Shuming Ma"
                                },
                                {
                                    "authorId": "145307652",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "3110003",
                                    "name": "Shaohan Huang"
                                },
                                {
                                    "authorId": "2216204941",
                                    "name": "Huaijie Wang"
                                },
                                {
                                    "authorId": "2259609416",
                                    "name": "Lingxiao Ma"
                                },
                                {
                                    "authorId": "2321493763",
                                    "name": "Fan Yang"
                                },
                                {
                                    "authorId": "2282983050",
                                    "name": "Ruiping Wang"
                                },
                                {
                                    "authorId": "2336830631",
                                    "name": "Yi Wu"
                                },
                                {
                                    "authorId": "2253471545",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 117
                        },
                        "score": 0.86083984375
                    },
                    {
                        "id": "(Mayilvahanan et al., 2025)",
                        "snippets": [
                            "Scaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget (Kaplan et al., 2020;Hoffmann et al., 2022;Grattafiori et al., 2024). Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs). FLOPs account for both parameter count and the number of training tokens."
                        ],
                        "paper": {
                            "corpus_id": 276421795,
                            "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2258718382",
                                    "name": "Prasanna Mayilvahanan"
                                },
                                {
                                    "authorId": "2179052180",
                                    "name": "Thadd\u00e4us Wiedemer"
                                },
                                {
                                    "authorId": "2345922971",
                                    "name": "Sayak Mallick"
                                },
                                {
                                    "authorId": "2261937896",
                                    "name": "Matthias Bethge"
                                },
                                {
                                    "authorId": "40634590",
                                    "name": "Wieland Brendel"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.76416015625
                    },
                    {
                        "id": "(Gadre et al., 2024)",
                        "snippets": [
                            "We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training."
                        ],
                        "paper": {
                            "corpus_id": 268379614,
                            "title": "Language models scale reliably with over-training and on downstream tasks",
                            "authors": [
                                {
                                    "authorId": "1387466862",
                                    "name": "S. Gadre"
                                },
                                {
                                    "authorId": "1438310376",
                                    "name": "G. Smyrnis"
                                },
                                {
                                    "authorId": "34961417",
                                    "name": "Vaishaal Shankar"
                                },
                                {
                                    "authorId": "40895369",
                                    "name": "Suchin Gururangan"
                                },
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "2287946530",
                                    "name": "Rulin Shao"
                                },
                                {
                                    "authorId": "72847120",
                                    "name": "Jean-Pierre Mercat"
                                },
                                {
                                    "authorId": "46372713",
                                    "name": "Alex Fang"
                                },
                                {
                                    "authorId": "2287821949",
                                    "name": "Jeffrey Li"
                                },
                                {
                                    "authorId": "150299584",
                                    "name": "Sedrick Scott Keh"
                                },
                                {
                                    "authorId": "2291068532",
                                    "name": "Rui Xin"
                                },
                                {
                                    "authorId": "2174178585",
                                    "name": "Marianna Nezhurina"
                                },
                                {
                                    "authorId": "2291068185",
                                    "name": "Igor Vasiljevic"
                                },
                                {
                                    "authorId": "2191688",
                                    "name": "J. Jitsev"
                                },
                                {
                                    "authorId": "2257301126",
                                    "name": "Alexandros G. Dimakis"
                                },
                                {
                                    "authorId": "1387994137",
                                    "name": "Gabriel Ilharco"
                                },
                                {
                                    "authorId": "2265621012",
                                    "name": "Shuran Song"
                                },
                                {
                                    "authorId": "2283843631",
                                    "name": "Thomas Kollar"
                                },
                                {
                                    "authorId": "2444742",
                                    "name": "Y. Carmon"
                                },
                                {
                                    "authorId": "2298523",
                                    "name": "Achal Dave"
                                },
                                {
                                    "authorId": "145639495",
                                    "name": "Reinhard Heckel"
                                },
                                {
                                    "authorId": "2037383772",
                                    "name": "Niklas Muennighoff"
                                },
                                {
                                    "authorId": "2253541812",
                                    "name": "Ludwig Schmidt"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 48
                        },
                        "score": 0.8525390625
                    },
                    {
                        "id": "(Xie et al., 2025)",
                        "snippets": [
                            "Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34].\n\nRecent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship."
                        ],
                        "paper": {
                            "corpus_id": 275993956,
                            "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
                            "authors": [
                                {
                                    "authorId": "2320149516",
                                    "name": "Enze Xie"
                                },
                                {
                                    "authorId": "2212250873",
                                    "name": "Junsong Chen"
                                },
                                {
                                    "authorId": "2109814772",
                                    "name": "Yuyang Zhao"
                                },
                                {
                                    "authorId": "2193887687",
                                    "name": "Jincheng Yu"
                                },
                                {
                                    "authorId": "20515689",
                                    "name": "Ligeng Zhu"
                                },
                                {
                                    "authorId": "49417466",
                                    "name": "Yujun Lin"
                                },
                                {
                                    "authorId": "2286139423",
                                    "name": "Zhekai Zhang"
                                },
                                {
                                    "authorId": "2288740166",
                                    "name": "Muyang Li"
                                },
                                {
                                    "authorId": "2325895662",
                                    "name": "Junyu Chen"
                                },
                                {
                                    "authorId": "2114069742",
                                    "name": "Han Cai"
                                },
                                {
                                    "authorId": "2342993953",
                                    "name": "Bingchen Liu"
                                },
                                {
                                    "authorId": "2344575401",
                                    "name": "Daquan Zhou"
                                },
                                {
                                    "authorId": "2283171540",
                                    "name": "Song Han"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 28
                        },
                        "score": 0.837890625
                    },
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "Despite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training."
                        ],
                        "paper": {
                            "corpus_id": 269449894,
                            "title": "Temporal Scaling Law for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2249971338",
                                    "name": "Yizhe Xiong"
                                },
                                {
                                    "authorId": "2298904872",
                                    "name": "Xiansheng Chen"
                                },
                                {
                                    "authorId": "2299108794",
                                    "name": "Xin Ye"
                                },
                                {
                                    "authorId": "2298921971",
                                    "name": "Hui Chen"
                                },
                                {
                                    "authorId": "1818920",
                                    "name": "Zijia Lin"
                                },
                                {
                                    "authorId": "2298903058",
                                    "name": "Haoran Lian"
                                },
                                {
                                    "authorId": "2293626051",
                                    "name": "Jianwei Niu"
                                },
                                {
                                    "authorId": "2242661989",
                                    "name": "Guiguang Ding"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.79296875
                    },
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance",
                            "Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]",
                            "This unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant."
                        ],
                        "paper": {
                            "corpus_id": 272593336,
                            "title": "Scaling Law Hypothesis for Multimodal Model",
                            "authors": [
                                {
                                    "authorId": "2320938961",
                                    "name": "Qingyun Sun"
                                },
                                {
                                    "authorId": "2320723029",
                                    "name": "Zhen Guo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.802734375
                    },
                    {
                        "id": "(Nori et al., 2024)",
                        "snippets": [
                            "Scaling laws [KMH + 20] for large language models (LLMs) have proven to be valuable for predicting how model capabilities grow with increasing data, compute, and model size. However, inference-time scaling-which focuses on the value of investing in additional computation at model run-time-is a promising new area of study."
                        ],
                        "paper": {
                            "corpus_id": 273850391,
                            "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
                            "authors": [
                                {
                                    "authorId": "2268494857",
                                    "name": "Harsha Nori"
                                },
                                {
                                    "authorId": "2637252",
                                    "name": "N. Usuyama"
                                },
                                {
                                    "authorId": "2214807281",
                                    "name": "Nicholas King"
                                },
                                {
                                    "authorId": "2047820455",
                                    "name": "S. McKinney"
                                },
                                {
                                    "authorId": "2329372335",
                                    "name": "Xavier Fernandes"
                                },
                                {
                                    "authorId": "72655349",
                                    "name": "Sheng Zhang"
                                },
                                {
                                    "authorId": "2268493614",
                                    "name": "Eric Horvitz"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.85986328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Formal Inference-Time Scaling Laws",
                "tldr": "Inference-time scaling laws reveal predictable relationships between model performance and computational resources used during inference, often following log-linear patterns. These laws demonstrate that performance can be enhanced by increasing inference compute through multiple attempts or sophisticated inference strategies, without requiring additional training. (11 sources)",
                "text": "\nWhile training scaling laws have been extensively studied, research into inference-time (or test-time) scaling laws has emerged as a significant new direction in language model research. These laws describe how model performance scales with computational resources allocated during inference rather than training <Paper corpusId=\"275570556\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. A foundational discovery in this area is that the relationship between coverage (the fraction of problems solved by at least one attempt) and the number of samples often follows a log-linear pattern, suggesting the existence of formal inference-time scaling laws <Paper corpusId=\"271571035\" paperTitle=\"(Brown et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272827424\" paperTitle=\"(Saad-Falcon et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275993956\" paperTitle=\"(Xie et al., 2025)\" isShortName></Paper>.\n\nResearchers have formalized these observations into quantitative expressions. Wu et al. conducted regression analysis on inference FLOPs (C) and model sizes (N) to establish a relationship between computational budget and optimal model size, expressed as: log\u2081\u2080(C) = 1.19 log\u2081\u2080(N) + 2.03 <Paper corpusId=\"271601023\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This equation allows practitioners to estimate the optimal inference model size for a specific compute budget, providing a practical guideline for deployment decisions.\n\nA key insight from inference scaling laws is that performance can be amplified through multiple inference attempts without further training the model <Paper corpusId=\"273507997\" paperTitle=\"(Levi, 2024)\" isShortName></Paper>. This has been demonstrated across challenging tasks such as coding and formal proofs, where verification of correct answers is possible. The \"inference loss\" exhibits a power-law decay as the number of trials increases, establishing a direct connection between performance improvements and prompting costs <Paper corpusId=\"273507997\" paperTitle=\"(Levi, 2024)\" isShortName></Paper>.\n\nThe empirical evidence consistently shows that increasing inference compute can enhance performance without significantly increasing model parameters <Paper corpusId=\"277271576\" paperTitle=\"(Shen et al., 2025)\" isShortName></Paper>. This enhancement can be achieved through various strategies, such as generating more reasoning chains or longer ones <Paper corpusId=\"276813519\" paperTitle=\"(Aggarwal et al., 2025)\" isShortName></Paper>. Li et al. demonstrated that the chance of finding the correct answer improves with an increasing number of generated responses, empirically summarized by a log-linear scaling law <Paper corpusId=\"278714770\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\nRecent work has further refined these concepts for specific paradigms. For instance, Singhi et al. derived inference scaling laws for the Generation with Rejection Mechanism (GenRM) paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications <Paper corpusId=\"277467695\" paperTitle=\"(Singhi et al., 2025)\" isShortName></Paper>. This suggests that sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more cost-effective than using larger models with simple inference methods <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>.\n\nThe discovery and formalization of inference-time scaling laws represent a significant advancement in understanding how to optimize language model deployment. These laws provide a framework for making informed decisions about allocating computational resources during inference, potentially enabling more efficient and effective use of language models across various applications.",
                "citations": [
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference."
                        ],
                        "paper": {
                            "corpus_id": 275570556,
                            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
                            "authors": [
                                {
                                    "authorId": "2279750963",
                                    "name": "Nanye Ma"
                                },
                                {
                                    "authorId": "2058178039",
                                    "name": "Shangyuan Tong"
                                },
                                {
                                    "authorId": "2340654494",
                                    "name": "Haolin Jia"
                                },
                                {
                                    "authorId": "2307548497",
                                    "name": "Hexiang Hu"
                                },
                                {
                                    "authorId": "2269866136",
                                    "name": "Yu-Chuan Su"
                                },
                                {
                                    "authorId": "2326256475",
                                    "name": "Mingda Zhang"
                                },
                                {
                                    "authorId": "2340726220",
                                    "name": "Xuan Yang"
                                },
                                {
                                    "authorId": "2324838111",
                                    "name": "Yandong Li"
                                },
                                {
                                    "authorId": "35132120",
                                    "name": "T. Jaakkola"
                                },
                                {
                                    "authorId": "2325917399",
                                    "name": "Xuhui Jia"
                                },
                                {
                                    "authorId": "2324769373",
                                    "name": "Saining Xie"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 69
                        },
                        "score": 0.8232421875
                    },
                    {
                        "id": "(Brown et al., 2024)",
                        "snippets": [
                            "Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws."
                        ],
                        "paper": {
                            "corpus_id": 271571035,
                            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
                            "authors": [
                                {
                                    "authorId": "2283198901",
                                    "name": "Bradley Brown"
                                },
                                {
                                    "authorId": "50875781",
                                    "name": "Jordan Juravsky"
                                },
                                {
                                    "authorId": "2283134957",
                                    "name": "Ryan Ehrlich"
                                },
                                {
                                    "authorId": "2313919316",
                                    "name": "Ronald Clark"
                                },
                                {
                                    "authorId": "2151097303",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "2313917068",
                                    "name": "Christopher R'e"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 330
                        },
                        "score": 0.88818359375
                    },
                    {
                        "id": "(Saad-Falcon et al., 2024)",
                        "snippets": [
                            "A number of recent works show the effectiveness of scaling compute at test time [6,7,24,47]. In particular, Large Language Monkeys [6] characterizes inference-time scaling laws, showing a log-linear relationship between coverage-the fraction of problems solved by at least one attempt-and the number of samples drawn from the model across a broad range of reasoning tasks and LLMs."
                        ],
                        "paper": {
                            "corpus_id": 272827424,
                            "title": "Archon: An Architecture Search Framework for Inference-Time Techniques",
                            "authors": [
                                {
                                    "authorId": "2283840040",
                                    "name": "Jon Saad-Falcon"
                                },
                                {
                                    "authorId": "2298071127",
                                    "name": "Adrian Gamarra Lafuente"
                                },
                                {
                                    "authorId": "2322445633",
                                    "name": "Shlok Natarajan"
                                },
                                {
                                    "authorId": "2322449133",
                                    "name": "Nahum Maru"
                                },
                                {
                                    "authorId": "2322445297",
                                    "name": "Hristo Todorov"
                                },
                                {
                                    "authorId": "2179104354",
                                    "name": "E. Guha"
                                },
                                {
                                    "authorId": "2336877105",
                                    "name": "E. K. Buchanan"
                                },
                                {
                                    "authorId": "2322453298",
                                    "name": "Mayee Chen"
                                },
                                {
                                    "authorId": "2820009",
                                    "name": "Neel Guha"
                                },
                                {
                                    "authorId": "2287938277",
                                    "name": "Christopher R\u00e9"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.80419921875
                    },
                    {
                        "id": "(Xie et al., 2025)",
                        "snippets": [
                            "Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34].\n\nRecent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship."
                        ],
                        "paper": {
                            "corpus_id": 275993956,
                            "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
                            "authors": [
                                {
                                    "authorId": "2320149516",
                                    "name": "Enze Xie"
                                },
                                {
                                    "authorId": "2212250873",
                                    "name": "Junsong Chen"
                                },
                                {
                                    "authorId": "2109814772",
                                    "name": "Yuyang Zhao"
                                },
                                {
                                    "authorId": "2193887687",
                                    "name": "Jincheng Yu"
                                },
                                {
                                    "authorId": "20515689",
                                    "name": "Ligeng Zhu"
                                },
                                {
                                    "authorId": "49417466",
                                    "name": "Yujun Lin"
                                },
                                {
                                    "authorId": "2286139423",
                                    "name": "Zhekai Zhang"
                                },
                                {
                                    "authorId": "2288740166",
                                    "name": "Muyang Li"
                                },
                                {
                                    "authorId": "2325895662",
                                    "name": "Junyu Chen"
                                },
                                {
                                    "authorId": "2114069742",
                                    "name": "Han Cai"
                                },
                                {
                                    "authorId": "2342993953",
                                    "name": "Bingchen Liu"
                                },
                                {
                                    "authorId": "2344575401",
                                    "name": "Daquan Zhou"
                                },
                                {
                                    "authorId": "2283171540",
                                    "name": "Song Han"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 28
                        },
                        "score": 0.837890625
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies.\n\nWe performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget."
                        ],
                        "paper": {
                            "corpus_id": 271601023,
                            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
                            "authors": [
                                {
                                    "authorId": "2314347409",
                                    "name": "Yangzhen Wu"
                                },
                                {
                                    "authorId": "48064856",
                                    "name": "Zhiqing Sun"
                                },
                                {
                                    "authorId": "2257057803",
                                    "name": "Shanda Li"
                                },
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                },
                                {
                                    "authorId": "2257099254",
                                    "name": "Yiming Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 130
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Sengupta et al., 2025)",
                        "snippets": [
                            "Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03."
                        ],
                        "paper": {
                            "corpus_id": 276421468,
                            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                            "authors": [
                                {
                                    "authorId": "34920835",
                                    "name": "Ayan Sengupta"
                                },
                                {
                                    "authorId": "2345922770",
                                    "name": "Yash Goel"
                                },
                                {
                                    "authorId": "2249914540",
                                    "name": "Tanmoy Chakraborty"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.78857421875
                    },
                    {
                        "id": "(Levi, 2024)",
                        "snippets": [
                            "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts.\n\nRecent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials.\n\nWe then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs."
                        ],
                        "paper": {
                            "corpus_id": 273507997,
                            "title": "A Simple Model of Inference Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2261494026",
                                    "name": "Noam Levi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.81494140625
                    },
                    {
                        "id": "(Shen et al., 2025)",
                        "snippets": [
                            "In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters, and it is proposed that optimizing inference strategies can significantly improve inference accuracy."
                        ],
                        "paper": {
                            "corpus_id": 277271576,
                            "title": "Long Is More Important Than Difficult for Training Reasoning Models",
                            "authors": [
                                {
                                    "authorId": "2283092321",
                                    "name": "Si Shen"
                                },
                                {
                                    "authorId": "2353326337",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "2214963769",
                                    "name": "Zhixiao Zhao"
                                },
                                {
                                    "authorId": "2351851812",
                                    "name": "Chang Liu"
                                },
                                {
                                    "authorId": "2351808837",
                                    "name": "Tiansheng Zheng"
                                },
                                {
                                    "authorId": "2283097884",
                                    "name": "Danhao Zhu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Aggarwal et al., 2025)",
                        "snippets": [
                            "Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning chains or longer ones (Wu et al., 2024;Snell et al., 2024;OpenAI et al., 2024a)."
                        ],
                        "paper": {
                            "corpus_id": 276813519,
                            "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning",
                            "authors": [
                                {
                                    "authorId": "2114841965",
                                    "name": "Pranjal Aggarwal"
                                },
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 101
                        },
                        "score": 0.7724609375
                    },
                    {
                        "id": "(Xu et al., 2025)",
                        "snippets": [
                            "Li et al. [2025] suggest that the chance of finding the correct answer improves while increasing the number of generated responses, which is empirically summarized by a log-linear scaling law [Brown et al., 2024]."
                        ],
                        "paper": {
                            "corpus_id": 278714770,
                            "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
                            "authors": [
                                {
                                    "authorId": "26339093",
                                    "name": "Yige Xu"
                                },
                                {
                                    "authorId": "2325014301",
                                    "name": "Xu Guo"
                                },
                                {
                                    "authorId": "49512311",
                                    "name": "Zhiwei Zeng"
                                },
                                {
                                    "authorId": "2238949687",
                                    "name": "Chunyan Miao"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.806640625
                    },
                    {
                        "id": "(Singhi et al., 2025)",
                        "snippets": [
                            "Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications."
                        ],
                        "paper": {
                            "corpus_id": 277467695,
                            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
                            "authors": [
                                {
                                    "authorId": "1970199394",
                                    "name": "Nishad Singhi"
                                },
                                {
                                    "authorId": "2317010356",
                                    "name": "Hritik Bansal"
                                },
                                {
                                    "authorId": "2090537547",
                                    "name": "Arian Hosseini"
                                },
                                {
                                    "authorId": "2263888488",
                                    "name": "Aditya Grover"
                                },
                                {
                                    "authorId": "2256646491",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "2329738651",
                                    "name": "Marcus Rohrbach"
                                },
                                {
                                    "authorId": "2329738066",
                                    "name": "Anna Rohrbach"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.9208984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparison with Training Scaling Laws",
                "tldr": "While training scaling laws focus on model size and dataset volume, inference scaling laws emphasize computational efficiency during model deployment. Research indicates that optimizing for inference can lead to different model configurations than those suggested by training-focused laws, particularly favoring smaller models trained on larger datasets. (12 sources)",
                "text": "\nTraining and inference scaling laws represent two complementary frameworks for understanding language model performance, each with distinct implications for model design and resource allocation. The foundational training scaling laws, such as those established by Kaplan et al., revealed power-law relationships between model performance, parameter count, and dataset size <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. However, these laws primarily focus on training computation rather than inference costs, which become increasingly significant as models are deployed at scale <Paper corpusId=\"266693796\" paperTitle=\"(Sardana et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>.\n\nA key distinction is that while training costs are incurred once, inference costs scale with each use of the model, making them potentially more impactful for widely deployed systems <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This consideration has led researchers to propose \"Inference-Optimal Scaling Laws\" that predict performance in relation to energy consumption during deployment rather than training <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. Such approaches recognize that model configurations optimized for training efficiency may not be optimal for inference scenarios.\n\nRecent research challenges some established training scaling principles, particularly the Chinchilla-optimal allocation of compute resources. Models like Llama 3 and 3.1, which were trained on significantly more tokens than recommended by Chinchilla's formula, have demonstrated excellent performance, suggesting that the relationship between model size and dataset size may be more flexible than previously thought <Paper corpusId=\"272593336\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. This has led to proposals for a unified scaling law where total compute, rather than its specific allocation between model size and dataset size, primarily drives performance <Paper corpusId=\"272593336\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>.\n\nThe trade-offs between training and inference optimization become particularly apparent in specialized architectures. For Mixture-of-Experts (MoE) models, simply increasing the number of experts improves training efficiency without increasing training costs, but this approach creates significant burdens during inference <Paper corpusId=\"268875826\" paperTitle=\"(Yun et al., 2024)\" isShortName></Paper>. This discrepancy highlights the need for scaling laws that incorporate inference efficiency alongside validation loss <Paper corpusId=\"268875826\" paperTitle=\"(Yun et al., 2024)\" isShortName></Paper>.\n\nInterestingly, the relationship between training and inference optimization can sometimes be counterintuitive. Kumar et al. found that for quantized models, degradation from post-training quantization increases as models are trained on more data, potentially making additional pretraining data actually harmful for inference performance <Paper corpusId=\"273877632\" paperTitle=\"(Kumar et al., 2024)\" isShortName></Paper>. This suggests that optimization for inference may require different training strategies than those prescribed by traditional scaling laws.\n\nAn emerging solution to balancing these competing objectives involves training smaller models on substantially larger datasets, which can optimize efficiency across both training and deployment phases <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>. Similarly, sparse pre-training approaches can achieve comparable model quality to dense pre-training while providing significant computational savings during inference through reduced model size <Paper corpusId=\"275789021\" paperTitle=\"(Jin et al., 2025)\" isShortName></Paper>.\n\nThe field is now witnessing an evolution from traditional pretraining scaling laws toward inference-time optimization <Paper corpusId=\"276249712\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>. This shift recognizes that while pretraining data availability is inherently constrained, performance can still be enhanced by optimizing inference strategies and computational allocation <Paper corpusId=\"276249712\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>.\n\nRecent theoretical work also offers insights into why neural scaling laws follow power-law relationships. Schaeffer et al. propose that pretraining cross-entropy loss may be composed of multiple functions decaying at different rates, with the slowest-decaying term eventually dominating as compute scales up <Paper corpusId=\"276580891\" paperTitle=\"(Schaeffer et al., 2025)\" isShortName></Paper>. This perspective may explain why power-law relationships become apparent only at sufficient scale, potentially connecting the mathematical foundations of both training and inference scaling laws.\n\nLooking forward, determining optimal allocation of parameters and computation under various inference constraints (such as memory, latency, and batch size) represents a promising direction for extending inference-optimal scaling laws <Paper corpusId=\"278636433\" paperTitle=\"(Chen et al._1, 2025)\" isShortName></Paper> <Paper corpusId=\"271601023\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. These approaches acknowledge that sophisticated inference methods with smaller models may offer more cost-effective solutions than larger models with simpler inference techniques <Paper corpusId=\"270703266\" paperTitle=\"(Welleck et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models.\n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once."
                        ],
                        "paper": {
                            "corpus_id": 264172438,
                            "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2127328758",
                                    "name": "Hongyu Wang"
                                },
                                {
                                    "authorId": "2118866998",
                                    "name": "Shuming Ma"
                                },
                                {
                                    "authorId": "145307652",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "3110003",
                                    "name": "Shaohan Huang"
                                },
                                {
                                    "authorId": "2216204941",
                                    "name": "Huaijie Wang"
                                },
                                {
                                    "authorId": "2259609416",
                                    "name": "Lingxiao Ma"
                                },
                                {
                                    "authorId": "2321493763",
                                    "name": "Fan Yang"
                                },
                                {
                                    "authorId": "2282983050",
                                    "name": "Ruiping Wang"
                                },
                                {
                                    "authorId": "2336830631",
                                    "name": "Yi Wu"
                                },
                                {
                                    "authorId": "2253471545",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 117
                        },
                        "score": 0.86083984375
                    },
                    {
                        "id": "(Sardana et al., 2023)",
                        "snippets": [
                            "However, the Chinchilla scaling laws only account for the computational costs of training."
                        ],
                        "paper": {
                            "corpus_id": 266693796,
                            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2277217297",
                                    "name": "Nikhil Sardana"
                                },
                                {
                                    "authorId": "2040790531",
                                    "name": "Sasha Doubov"
                                },
                                {
                                    "authorId": "2277215716",
                                    "name": "Jonathan Frankle"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 88
                        },
                        "score": 0.818359375
                    },
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance",
                            "Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]",
                            "This unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant."
                        ],
                        "paper": {
                            "corpus_id": 272593336,
                            "title": "Scaling Law Hypothesis for Multimodal Model",
                            "authors": [
                                {
                                    "authorId": "2320938961",
                                    "name": "Qingyun Sun"
                                },
                                {
                                    "authorId": "2320723029",
                                    "name": "Zhen Guo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.802734375
                    },
                    {
                        "id": "(Yun et al., 2024)",
                        "snippets": [
                            "We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss."
                        ],
                        "paper": {
                            "corpus_id": 268875826,
                            "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2294718538",
                                    "name": "Longfei Yun"
                                },
                                {
                                    "authorId": "2152482391",
                                    "name": "Yonghao Zhuang"
                                },
                                {
                                    "authorId": "2294813888",
                                    "name": "Yao Fu"
                                },
                                {
                                    "authorId": "2243336934",
                                    "name": "Eric P. Xing"
                                },
                                {
                                    "authorId": "2294828709",
                                    "name": "Hao Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.796875
                    },
                    {
                        "id": "(Kumar et al., 2024)",
                        "snippets": [
                            "For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful."
                        ],
                        "paper": {
                            "corpus_id": 273877632,
                            "title": "Scaling Laws for Precision",
                            "authors": [
                                {
                                    "authorId": "2256990733",
                                    "name": "Tanishq Kumar"
                                },
                                {
                                    "authorId": "2172356226",
                                    "name": "Zachary Ankner"
                                },
                                {
                                    "authorId": "2259914006",
                                    "name": "B. Spector"
                                },
                                {
                                    "authorId": "77327149",
                                    "name": "Blake Bordelon"
                                },
                                {
                                    "authorId": "2037383772",
                                    "name": "Niklas Muennighoff"
                                },
                                {
                                    "authorId": "1690452",
                                    "name": "Mansheej Paul"
                                },
                                {
                                    "authorId": "2577481",
                                    "name": "Cengiz Pehlevan"
                                },
                                {
                                    "authorId": "2313917068",
                                    "name": "Christopher R'e"
                                },
                                {
                                    "authorId": "2334471571",
                                    "name": "Aditi Raghunathan"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 29
                        },
                        "score": 0.89892578125
                    },
                    {
                        "id": "(Lu, 2025)",
                        "snippets": [
                            "Building on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."
                        ],
                        "paper": {
                            "corpus_id": 275336968,
                            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2338865687",
                                    "name": "Chien-Ping Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.89794921875
                    },
                    {
                        "id": "(Jin et al., 2025)",
                        "snippets": [
                            "Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference."
                        ],
                        "paper": {
                            "corpus_id": 275789021,
                            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2341534973",
                                    "name": "Tian Jin"
                                },
                                {
                                    "authorId": "34657588",
                                    "name": "Ahmed Imtiaz Humayun"
                                },
                                {
                                    "authorId": "3399348",
                                    "name": "Utku Evci"
                                },
                                {
                                    "authorId": "1929462",
                                    "name": "Suvinay Subramanian"
                                },
                                {
                                    "authorId": "2296991252",
                                    "name": "Amir Yazdanbakhsh"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                },
                                {
                                    "authorId": "2533850",
                                    "name": "G. Dziugaite"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1
                        },
                        "score": 0.83154296875
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "The remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities."
                        ],
                        "paper": {
                            "corpus_id": 276249712,
                            "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling",
                            "authors": [
                                {
                                    "authorId": "2278582297",
                                    "name": "Weizhe Chen"
                                },
                                {
                                    "authorId": "2256845651",
                                    "name": "Sven Koenig"
                                },
                                {
                                    "authorId": "1796375",
                                    "name": "B. Dilkina"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.77197265625
                    },
                    {
                        "id": "(Schaeffer et al., 2025)",
                        "snippets": [
                            "Despite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws."
                        ],
                        "paper": {
                            "corpus_id": 276580891,
                            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
                            "authors": [
                                {
                                    "authorId": "1749176844",
                                    "name": "Rylan Schaeffer"
                                },
                                {
                                    "authorId": "2327048379",
                                    "name": "Joshua Kazdan"
                                },
                                {
                                    "authorId": "2294572631",
                                    "name": "John Hughes"
                                },
                                {
                                    "authorId": "50875781",
                                    "name": "Jordan Juravsky"
                                },
                                {
                                    "authorId": "2333593482",
                                    "name": "Sara Price"
                                },
                                {
                                    "authorId": "2287830769",
                                    "name": "Aengus Lynch"
                                },
                                {
                                    "authorId": "2334069920",
                                    "name": "Erik Jones"
                                },
                                {
                                    "authorId": "2311693657",
                                    "name": "Robert Kirk"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                },
                                {
                                    "authorId": "143812875",
                                    "name": "Oluwasanmi Koyejo"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.80908203125
                    },
                    {
                        "id": "(Chen et al._1, 2025)",
                        "snippets": [
                            "Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2024)(Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2023) is a promising direction."
                        ],
                        "paper": {
                            "corpus_id": 278636433,
                            "title": "Parallel Scaling Law for Language Models",
                            "authors": [
                                {
                                    "authorId": "2125101083",
                                    "name": "Mouxiang Chen"
                                },
                                {
                                    "authorId": "2321578848",
                                    "name": "Binyuan Hui"
                                },
                                {
                                    "authorId": "2248072386",
                                    "name": "Zeyu Cui"
                                },
                                {
                                    "authorId": "2328943044",
                                    "name": "Jiaxin Yang"
                                },
                                {
                                    "authorId": "2248487202",
                                    "name": "Dayiheng Liu"
                                },
                                {
                                    "authorId": "2362357192",
                                    "name": "Jianling Sun"
                                },
                                {
                                    "authorId": "2326803484",
                                    "name": "Junyang Lin"
                                },
                                {
                                    "authorId": "2361700209",
                                    "name": "Zhongxin Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.88671875
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies.\n\nWe performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget."
                        ],
                        "paper": {
                            "corpus_id": 271601023,
                            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
                            "authors": [
                                {
                                    "authorId": "2314347409",
                                    "name": "Yangzhen Wu"
                                },
                                {
                                    "authorId": "48064856",
                                    "name": "Zhiqing Sun"
                                },
                                {
                                    "authorId": "2257057803",
                                    "name": "Shanda Li"
                                },
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                },
                                {
                                    "authorId": "2257099254",
                                    "name": "Yiming Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 130
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Welleck et al., 2024)",
                        "snippets": [
                            "However, there is also another lesser-mentioned scaling phenomenon, where adopting more sophisticated methods or scaling compute at inference time (Jones, 2021)"
                        ],
                        "paper": {
                            "corpus_id": 270703266,
                            "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                },
                                {
                                    "authorId": "2138301112",
                                    "name": "Amanda Bertsch"
                                },
                                {
                                    "authorId": "1580418311",
                                    "name": "Matthew Finlayson"
                                },
                                {
                                    "authorId": "2184031883",
                                    "name": "Hailey Schoelkopf"
                                },
                                {
                                    "authorId": "2253395527",
                                    "name": "Alex Xie"
                                },
                                {
                                    "authorId": "2265547593",
                                    "name": "Graham Neubig"
                                },
                                {
                                    "authorId": "2308102420",
                                    "name": "Ilia Kulikov"
                                },
                                {
                                    "authorId": "2265540561",
                                    "name": "Zaid Harchaoui"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 77
                        },
                        "score": 0.86376953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Implications of Inference Scaling Laws",
                "tldr": "Inference scaling laws are driving practical changes in model design, encouraging smaller models trained on larger datasets for deployment efficiency. These laws are reshaping resource allocation strategies across the AI industry and opening new research directions in synthetic data generation and advanced inference algorithms. (10 sources)",
                "text": "\nInference scaling laws have significant practical applications that are reshaping how language models are designed, deployed, and optimized. A key implication is the shift toward prioritizing smaller models trained on substantially larger datasets to optimize efficiency during deployment, especially in resource-constrained environments <Paper corpusId=\"272593336\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. This approach balances efficiency across both training and deployment phases, acknowledging that inference costs\u2014which scale with each model use\u2014can ultimately outweigh one-time training costs <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>.\n\nThe practical benefits of this approach are becoming increasingly apparent. Research indicates that sparse pre-training can achieve comparable model quality to dense pre-training while enabling significant computational savings during inference through reduced model size <Paper corpusId=\"275789021\" paperTitle=\"(Jin et al., 2025)\" isShortName></Paper>. These findings are challenging traditional scaling paradigms and leading to more deployment-optimized model architectures.\n\nFor specialized architectures like Mixture-of-Experts (MoE) models, inference scaling laws have particularly important implications. The tendency to increase expert count to improve training efficiency without increasing training costs creates significant challenges during inference, necessitating the incorporation of inference efficiency as an additional metric alongside validation loss <Paper corpusId=\"268875826\" paperTitle=\"(Yun et al., 2024)\" isShortName></Paper>. This represents a broader trend toward more holistic evaluation frameworks that consider both training and inference performance.\n\nThe equation established by Wu et al., log\u2081\u2080(C) = 1.19 log\u2081\u2080(N) + 2.03, provides a practical guideline for estimating optimal inference model size for specific compute budgets <Paper corpusId=\"271601023\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. This relationship allows practitioners to make more informed decisions about model selection based on available computational resources, potentially reducing unnecessary overhead in deployment scenarios.\n\nA particularly promising application of inference scaling laws is the development of sophisticated inference algorithms that can enhance performance without increasing model size. Research suggests that employing advanced test-time computation strategies\u2014such as iterative refinement or tree search\u2014with smaller models may be more cost-effective than using larger models with simple inference methods <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This approach offers a path to improved performance without the substantial resource requirements of larger models.\n\nThe implications of inference scaling laws extend to data generation strategies as well. As the availability of high-quality training data becomes increasingly constrained, researchers are exploring methods to synthesize data using existing models to extend the limits of pretraining scaling laws <Paper corpusId=\"276249712\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>. This creates a virtuous cycle where inference-optimized models can be used to generate data that improves future model performance.\n\nLooking forward, determining how to allocate parameters and parallel computation under various inference constraints\u2014such as memory limitations, latency requirements, and batch size considerations\u2014represents a promising direction for extending inference-optimal scaling laws <Paper corpusId=\"278636433\" paperTitle=\"(Chen et al._1, 2025)\" isShortName></Paper> <Paper corpusId=\"266693796\" paperTitle=\"(Sardana et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271601023\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. These efforts acknowledge that the ideal model architecture depends heavily on the specific deployment context and constraints.\n\nThe evolution from traditional pretraining scaling laws toward inference-time optimization reflects a maturing understanding that performance can be enhanced by optimizing inference strategies without significantly increasing model parameters <Paper corpusId=\"277271576\" paperTitle=\"(Shen et al., 2025)\" isShortName></Paper>. This shift recognizes that while model size has dominated scaling discussions, inference compute offers an additional dimension for performance improvement that has been comparatively underexplored.",
                "citations": [
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance",
                            "Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]",
                            "This unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant."
                        ],
                        "paper": {
                            "corpus_id": 272593336,
                            "title": "Scaling Law Hypothesis for Multimodal Model",
                            "authors": [
                                {
                                    "authorId": "2320938961",
                                    "name": "Qingyun Sun"
                                },
                                {
                                    "authorId": "2320723029",
                                    "name": "Zhen Guo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.802734375
                    },
                    {
                        "id": "(Lu, 2025)",
                        "snippets": [
                            "Building on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."
                        ],
                        "paper": {
                            "corpus_id": 275336968,
                            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2338865687",
                                    "name": "Chien-Ping Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.89794921875
                    },
                    {
                        "id": "(Jin et al., 2025)",
                        "snippets": [
                            "Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference."
                        ],
                        "paper": {
                            "corpus_id": 275789021,
                            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2341534973",
                                    "name": "Tian Jin"
                                },
                                {
                                    "authorId": "34657588",
                                    "name": "Ahmed Imtiaz Humayun"
                                },
                                {
                                    "authorId": "3399348",
                                    "name": "Utku Evci"
                                },
                                {
                                    "authorId": "1929462",
                                    "name": "Suvinay Subramanian"
                                },
                                {
                                    "authorId": "2296991252",
                                    "name": "Amir Yazdanbakhsh"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                },
                                {
                                    "authorId": "2533850",
                                    "name": "G. Dziugaite"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1
                        },
                        "score": 0.83154296875
                    },
                    {
                        "id": "(Yun et al., 2024)",
                        "snippets": [
                            "We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss."
                        ],
                        "paper": {
                            "corpus_id": 268875826,
                            "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2294718538",
                                    "name": "Longfei Yun"
                                },
                                {
                                    "authorId": "2152482391",
                                    "name": "Yonghao Zhuang"
                                },
                                {
                                    "authorId": "2294813888",
                                    "name": "Yao Fu"
                                },
                                {
                                    "authorId": "2243336934",
                                    "name": "Eric P. Xing"
                                },
                                {
                                    "authorId": "2294828709",
                                    "name": "Hao Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.796875
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies.\n\nWe performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget."
                        ],
                        "paper": {
                            "corpus_id": 271601023,
                            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
                            "authors": [
                                {
                                    "authorId": "2314347409",
                                    "name": "Yangzhen Wu"
                                },
                                {
                                    "authorId": "48064856",
                                    "name": "Zhiqing Sun"
                                },
                                {
                                    "authorId": "2257057803",
                                    "name": "Shanda Li"
                                },
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                },
                                {
                                    "authorId": "2257099254",
                                    "name": "Yiming Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 130
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Sengupta et al., 2025)",
                        "snippets": [
                            "Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03."
                        ],
                        "paper": {
                            "corpus_id": 276421468,
                            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                            "authors": [
                                {
                                    "authorId": "34920835",
                                    "name": "Ayan Sengupta"
                                },
                                {
                                    "authorId": "2345922770",
                                    "name": "Yash Goel"
                                },
                                {
                                    "authorId": "2249914540",
                                    "name": "Tanmoy Chakraborty"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.78857421875
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "The remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities."
                        ],
                        "paper": {
                            "corpus_id": 276249712,
                            "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling",
                            "authors": [
                                {
                                    "authorId": "2278582297",
                                    "name": "Weizhe Chen"
                                },
                                {
                                    "authorId": "2256845651",
                                    "name": "Sven Koenig"
                                },
                                {
                                    "authorId": "1796375",
                                    "name": "B. Dilkina"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.77197265625
                    },
                    {
                        "id": "(Chen et al._1, 2025)",
                        "snippets": [
                            "Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2024)(Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2023) is a promising direction."
                        ],
                        "paper": {
                            "corpus_id": 278636433,
                            "title": "Parallel Scaling Law for Language Models",
                            "authors": [
                                {
                                    "authorId": "2125101083",
                                    "name": "Mouxiang Chen"
                                },
                                {
                                    "authorId": "2321578848",
                                    "name": "Binyuan Hui"
                                },
                                {
                                    "authorId": "2248072386",
                                    "name": "Zeyu Cui"
                                },
                                {
                                    "authorId": "2328943044",
                                    "name": "Jiaxin Yang"
                                },
                                {
                                    "authorId": "2248487202",
                                    "name": "Dayiheng Liu"
                                },
                                {
                                    "authorId": "2362357192",
                                    "name": "Jianling Sun"
                                },
                                {
                                    "authorId": "2326803484",
                                    "name": "Junyang Lin"
                                },
                                {
                                    "authorId": "2361700209",
                                    "name": "Zhongxin Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.88671875
                    },
                    {
                        "id": "(Sardana et al., 2023)",
                        "snippets": [
                            "However, the Chinchilla scaling laws only account for the computational costs of training."
                        ],
                        "paper": {
                            "corpus_id": 266693796,
                            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2277217297",
                                    "name": "Nikhil Sardana"
                                },
                                {
                                    "authorId": "2040790531",
                                    "name": "Sasha Doubov"
                                },
                                {
                                    "authorId": "2277215716",
                                    "name": "Jonathan Frankle"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 88
                        },
                        "score": 0.818359375
                    },
                    {
                        "id": "(Shen et al., 2025)",
                        "snippets": [
                            "In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters, and it is proposed that optimizing inference strategies can significantly improve inference accuracy."
                        ],
                        "paper": {
                            "corpus_id": 277271576,
                            "title": "Long Is More Important Than Difficult for Training Reasoning Models",
                            "authors": [
                                {
                                    "authorId": "2283092321",
                                    "name": "Si Shen"
                                },
                                {
                                    "authorId": "2353326337",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "2214963769",
                                    "name": "Zhixiao Zhao"
                                },
                                {
                                    "authorId": "2351851812",
                                    "name": "Chang Liu"
                                },
                                {
                                    "authorId": "2351808837",
                                    "name": "Tiansheng Zheng"
                                },
                                {
                                    "authorId": "2283097884",
                                    "name": "Danhao Zhu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.88623046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.10053000000000001
    }
}