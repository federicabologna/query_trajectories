{
    "query": "What are the most effective techniques for measuring the difficulty of datapoints when constructing a curriculum for training large language models, and how do these techniques influence training outcomes?",
    "user_id": "lib_user",
    "task_id": "1f46b8d0-b652-4a3f-abcc-6fdd851c0a90",
    "timestamp": "2025-06-24T00:34:58.536717",
    "n_retrieval": 256,
    "n_retrieved": 275,
    "n_candidates": 44,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.537042,
    "decomposed_query": {
        "rewritten_query": "Most effective techniques for measuring datapoint difficulty when constructing a curriculum for training large language models, and how these techniques influence training outcomes.",
        "keyword_query": "measuring datapoint difficulty curriculum training large language models influence training outcomes",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009825,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301165169",
                    "name": "Jisu Kim"
                },
                {
                    "authorId": "2301167177",
                    "name": "Juhwan Lee"
                }
            ],
            "abstract": "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.",
            "corpus_id": 269756933,
            "sentences": [
                {
                    "corpus_id": "269756933",
                    "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                    "text": "Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).However, these metrics may not fully reflect the complexity of a dataset.It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics.Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective.By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones.This approach requires organizing data by its level of complexity, for which we have established three principal criteria.By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.",
                    "score": 0.4064586197702927,
                    "section_title": "Related Work",
                    "char_start_offset": 5440,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 164,
                            "end": 237
                        },
                        {
                            "start": 237,
                            "end": 361
                        },
                        {
                            "start": 361,
                            "end": 473
                        },
                        {
                            "start": 473,
                            "end": 625
                        },
                        {
                            "start": 627,
                            "end": 676
                        },
                        {
                            "start": 678,
                            "end": 831
                        },
                        {
                            "start": 831,
                            "end": 953
                        },
                        {
                            "start": 953,
                            "end": 1090
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 140,
                            "end": 163,
                            "matchedPaperCorpusId": "255221201"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                },
                {
                    "corpus_id": "269756933",
                    "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                    "text": "Curriculum learning, introduced by Bengio et al. in 2009(Bengio et al., 2009), is a learning strategy where models are trained on data from simple to complex, mirroring human learning processes.This approach enhances model performance by establishing a foundational understanding before introducing more complex data.It has been widely applied in natural language processing (NLP) and computer vision.(Guo et al., 2018;Antonios Platanios et al., 2019;Tay et al., 2019).\n\nIn NLP, curriculum learning has proven to be effective by organizing the training sequence to begin with simpler texts and progressively transition to more complex ones.The complexity of these texts is assessed based on various criteria, including text length, the rarity of vocabulary, and edit distance.This structured approach has successfully demonstrated the efficacy of curriculum learning in the training of Pretrained Language Models (PLMs) (Chang et al., 2021;Nagatsuka et al., 2023).\n\nRecent advancements in large-scale language and multimodal models have highlighted the effective integration of curriculum learning with cutting-edge techniques, significantly boosting performance across various applications.Notably, projects such as WavLLM (Hu et al., 2024) and AutoWebGLM (Lai et al., 2024) showcase the potential of these integrations, tailoring the learning process to gradually introduce more complex tasks, thereby enhancing model robustness and application specificity.Alongside these innovations, further research has validated the effectiveness of curriculum learning in instruction tuning for language models.This approach not only systematically improves models' abilities to handle progressively challenging tasks but also optimizes their learning trajectory for better performance in specialized tasks.By methodically structuring the training process, these initiatives have set a new standard in the development and refinement of AI models, marking a significant step forward in their evolution.\n\nIn the development of training data for curricu-lum learning, accurately measuring data difficulty is crucial.Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging.Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).",
                    "score": 0.5206892323392261,
                    "section_title": "Related Work",
                    "char_start_offset": 3231,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 194,
                            "end": 317
                        },
                        {
                            "start": 317,
                            "end": 401
                        },
                        {
                            "start": 401,
                            "end": 469
                        },
                        {
                            "start": 471,
                            "end": 640
                        },
                        {
                            "start": 640,
                            "end": 776
                        },
                        {
                            "start": 776,
                            "end": 964
                        },
                        {
                            "start": 966,
                            "end": 1191
                        },
                        {
                            "start": 1191,
                            "end": 1459
                        },
                        {
                            "start": 1459,
                            "end": 1602
                        },
                        {
                            "start": 1602,
                            "end": 1798
                        },
                        {
                            "start": 1798,
                            "end": 1992
                        },
                        {
                            "start": 1994,
                            "end": 2104
                        },
                        {
                            "start": 2104,
                            "end": 2209
                        },
                        {
                            "start": 2209,
                            "end": 2373
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 35,
                            "end": 56,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 56,
                            "end": 77,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 401,
                            "end": 419,
                            "matchedPaperCorpusId": "51920640"
                        },
                        {
                            "start": 940,
                            "end": 963,
                            "matchedPaperCorpusId": "255221201"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning\n# Venue: arXiv.org\n# Authors: Jisu Kim, Juhwan Lee\n## Abstract\nThe rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.\n## Related Work\nCurriculum learning, introduced by Bengio et al. in 2009(Bengio et al., 2009), is a learning strategy where models are trained on data from simple to complex, mirroring human learning processes.This approach enhances model performance by establishing a foundational understanding before introducing more complex data.It has been widely applied in natural language processing (NLP) and computer vision.(Guo et al., 2018;Antonios Platanios et al., 2019;Tay et al., 2019).\n\nIn NLP, curriculum learning has proven to be effective by organizing the training sequence to begin with simpler texts and progressively transition to more complex ones.The complexity of these texts is assessed based on various criteria, including text length, the rarity of vocabulary, and edit distance.This structured approach has successfully demonstrated the efficacy of curriculum learning in the training of Pretrained Language Models (PLMs) (Chang et al., 2021;Nagatsuka et al., 2023).\n\nRecent advancements in large-scale language and multimodal models have highlighted the effective integration of curriculum learning with cutting-edge techniques, significantly boosting performance across various applications.Notably, projects such as WavLLM (Hu et al., 2024) and AutoWebGLM (Lai et al., 2024) showcase the potential of these integrations, tailoring the learning process to gradually introduce more complex tasks, thereby enhancing model robustness and application specificity.Alongside these innovations, further research has validated the effectiveness of curriculum learning in instruction tuning for language models.This approach not only systematically improves models' abilities to handle progressively challenging tasks but also optimizes their learning trajectory for better performance in specialized tasks.By methodically structuring the training process, these initiatives have set a new standard in the development and refinement of AI models, marking a significant step forward in their evolution.\n\nIn the development of training data for curricu-lum learning, accurately measuring data difficulty is crucial.Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging.Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).\n...\nTraditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).However, these metrics may not fully reflect the complexity of a dataset.It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics.Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective.By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones.This approach requires organizing data by its level of complexity, for which we have established three principal criteria.By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.",
            "reference_string": "[269756933 | Kim et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework",
            "venue": "",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05695, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366142077",
                    "name": "Lingyuan Liu"
                },
                {
                    "authorId": "2366090070",
                    "name": "Mengxiang Zhang"
                }
            ],
            "abstract": "Knowledge Distillation (KD) compresses large language models (LLMs) by transferring the teacher model's capabilities to a smaller student model, reducing inference cost and memory usage while maintaining performance. However, existing KD methods for LLMs often fail to prevent significant shifts in the student model's distribution during training, leading to issues such as catastrophic forgetting, mode collapse, and training-inference mismatch. To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of\"progressive overload\"(POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures. By starting with the easiest samples and progressively increasing the difficulty, the approach enhances both the stability and efficiency of learning. Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families. Our findings highlight the effectiveness of sorted training samples in KD for LLMs. More generally, our work demonstrates how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.",
            "corpus_id": 279243528,
            "sentences": [],
            "relevance_judgement": 0.95458984375,
            "relevance_judgment_input_expanded": "# Title: Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework\n# Venue: \n# Authors: Lingyuan Liu, Mengxiang Zhang\n## Abstract\nKnowledge Distillation (KD) compresses large language models (LLMs) by transferring the teacher model's capabilities to a smaller student model, reducing inference cost and memory usage while maintaining performance. However, existing KD methods for LLMs often fail to prevent significant shifts in the student model's distribution during training, leading to issues such as catastrophic forgetting, mode collapse, and training-inference mismatch. To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of\"progressive overload\"(POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures. By starting with the easiest samples and progressively increasing the difficulty, the approach enhances both the stability and efficiency of learning. Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families. Our findings highlight the effectiveness of sorted training samples in KD for LLMs. More generally, our work demonstrates how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.\n",
            "reference_string": "[279243528 | Liu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 21,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2284673632",
                    "name": "Alex Nguyen"
                },
                {
                    "authorId": "2284595153",
                    "name": "Jingbo Shang"
                }
            ],
            "abstract": "Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.",
            "corpus_id": 267740312,
            "sentences": [
                {
                    "corpus_id": "267740312",
                    "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                    "text": "In this paper, we introduce a learning percentagebased metric for assessing the difficulty of samples. We demonstrate that language models ranging from 1B to 13B sizes can self-select high-quality training data by employing this metric. Additionally, we empirically validate the transferability of data hardness across different model sizes, showcasing the efficient curation of high-quality training data by smaller models. Furthermore, we propose an optimized version of the metric that offers increased speed while maintaining equal efficacy. In future, we aim to explore methods for automatically transforming easy training samples into more challenging ones. \n\nOur examination reveals prevalence of noisy samples within the LP(1) and LP app (1) subsets of data. The detection and mitigation of noisy samples are imperative to mitigate their influence on the dataset. We leave this for future work.",
                    "score": 0.3614349632155474,
                    "section_title": "Conclusion",
                    "char_start_offset": 22528,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 663
                        },
                        {
                            "start": 666,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 902
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9521484375
                },
                {
                    "corpus_id": "267740312",
                    "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                    "text": "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).",
                    "score": 0.3832814172581209,
                    "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
                    "char_start_offset": 1531,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 96,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 775
                        },
                        {
                            "start": 778,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1064
                        },
                        {
                            "start": 1067,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1797
                        },
                        {
                            "start": 1798,
                            "end": 2002
                        },
                        {
                            "start": 2003,
                            "end": 2113
                        },
                        {
                            "start": 2114,
                            "end": 2241
                        },
                        {
                            "start": 2242,
                            "end": 2329
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 150,
                            "end": 172,
                            "matchedPaperCorpusId": "249060677"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.943359375
                }
            ],
            "relevance_judgement": 0.9521484375,
            "relevance_judgment_input_expanded": "# Title: Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Dheeraj Mekala, Alex Nguyen, Jingbo Shang\n## Abstract\nInstruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.\n## arXiv:2402.10430v1 [cs.CL] 16 Feb 2024\nIn this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).\n\n## Conclusion\nIn this paper, we introduce a learning percentagebased metric for assessing the difficulty of samples. We demonstrate that language models ranging from 1B to 13B sizes can self-select high-quality training data by employing this metric. Additionally, we empirically validate the transferability of data hardness across different model sizes, showcasing the efficient curation of high-quality training data by smaller models. Furthermore, we propose an optimized version of the metric that offers increased speed while maintaining equal efficacy. In future, we aim to explore methods for automatically transforming easy training samples into more challenging ones. \n\nOur examination reveals prevalence of noisy samples within the LP(1) and LP app (1) subsets of data. The detection and mitigation of noisy samples are imperative to mitigate their influence on the dataset. We leave this for future work.",
            "reference_string": "[267740312 | Mekala et al. | 2024 | Citations: 21]"
        },
        {
            "title": "P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for Optimizing LLM Training",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.05541?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.05541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2224854311",
                    "name": "Yingxuan Yang"
                },
                {
                    "authorId": "2315948077",
                    "name": "Huayi Wang"
                },
                {
                    "authorId": "2111875607",
                    "name": "Muning Wen"
                },
                {
                    "authorId": "2315940339",
                    "name": "Weinan Zhang"
                }
            ],
            "abstract": "In the rapidly evolving field of Large Language Models (LLMs), selecting high-quality data for fine-tuning is essential. This paper focuses on task-specific data pruning and selection to enhance fine-tuning. We introduce an innovative framework, termed P3 , which improves LLM performance through a dynamic, adaptive training strategy. Specifically, P3 comprises the following components: (1) P olicy-driven Difficulty Measurement: we begin by measuring the difficulty of data based on the model\u2019s real-time performance, transitioning from static, predefined metrics to more dynamic and adaptable ones. (2) P ace-adaptive Selection: we employ self-paced learning (SPL) to gradually select increasingly challenging data, thereby progressively enhancing the model\u2019s performance. (3) Diversity P romotion: we integrate Determinantal Point Process (DPP) into the selection process to promote the diversity within and be-tween samples, enriching the learning process. We have validated our method on two well-known LLM datasets, APPS and MATH, designed for logical reasoning scenarios. The results show that our P3 framework significantly improves training outcomes compared to traditional methods. By fundamentally refining data selection and utilization strategies, P3 not only advances theoretical understanding of dynamic training approaches but also provides a versatile framework that can revolutionize model training in natural language processing.",
            "corpus_id": 271855275,
            "sentences": [],
            "relevance_judgement": 0.94580078125,
            "relevance_judgment_input_expanded": "# Title: P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for Optimizing LLM Training\n# Venue: arXiv.org\n# Authors: Yingxuan Yang, Huayi Wang, Muning Wen, Weinan Zhang\n## Abstract\nIn the rapidly evolving field of Large Language Models (LLMs), selecting high-quality data for fine-tuning is essential. This paper focuses on task-specific data pruning and selection to enhance fine-tuning. We introduce an innovative framework, termed P3 , which improves LLM performance through a dynamic, adaptive training strategy. Specifically, P3 comprises the following components: (1) P olicy-driven Difficulty Measurement: we begin by measuring the difficulty of data based on the model\u2019s real-time performance, transitioning from static, predefined metrics to more dynamic and adaptable ones. (2) P ace-adaptive Selection: we employ self-paced learning (SPL) to gradually select increasingly challenging data, thereby progressively enhancing the model\u2019s performance. (3) Diversity P romotion: we integrate Determinantal Point Process (DPP) into the selection process to promote the diversity within and be-tween samples, enriching the learning process. We have validated our method on two well-known LLM datasets, APPS and MATH, designed for logical reasoning scenarios. The results show that our P3 framework significantly improves training outcomes compared to traditional methods. By fundamentally refining data selection and utilization strategies, P3 not only advances theoretical understanding of dynamic training approaches but also provides a versatile framework that can revolutionize model training in natural language processing.\n",
            "reference_string": "[271855275 | Yang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
            "venue": "Interspeech",
            "year": 2022,
            "reference_count": 51,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2208.05782",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.05782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2181120338",
                    "name": "Georgios Karakasidis"
                },
                {
                    "authorId": "2066645527",
                    "name": "Tam'as Gr'osz"
                },
                {
                    "authorId": "1719346",
                    "name": "M. Kurimo"
                }
            ],
            "abstract": "It is common knowledge that the quantity and quality of the training data play a significant role in the creation of a good machine learning model. In this paper, we take it one step further and demonstrate that the way the training examples are arranged is also of crucial importance. Curriculum Learning is built on the observation that organized and structured assimilation of knowledge has the ability to enable faster training and better comprehension. When humans learn to speak, they first try to utter basic phones and then gradually move towards more complex structures such as words and sentences. This methodology is known as Curriculum Learning, and we employ it in the context of Automatic Speech Recognition. We hypothesize that end-to-end models can achieve better performance when provided with an organized training set consisting of examples that exhibit an increasing level of difficulty (i.e. a curriculum). To impose structure on the training set and to define the notion of an easy example, we explored multiple scoring functions that either use feedback from an external neural network or incorporate feedback from the model itself. Empirical results show that with different curriculums we can balance the training times and the network's performance.",
            "corpus_id": 251493126,
            "sentences": [
                {
                    "corpus_id": "251493126",
                    "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
                    "text": "In the context of machine learning, CL aims to rank the training examples based on their difficulty. Each example is assigned a score and then the data set is ordered accordingly (from the easiest to the hardest example) with the goal of assisting the learning process. Empirical results show that this approach, when done properly, can speed up convergence and improve the stability of the training process of neural networks [12]. \n\nWe can think of CL as a way to provide guidance to a training model. A commonly used analogy to the real world, is that of the teacher-student relationship. The teacher has to create the curriculum in a way so that the student will neither get bored of the easy material, nor get overwhelmed by the hard examples. Instead, there should be an increasing level of difficulty throughout the curriculum. Note that this has nothing to do with the teacher-student transfer learning technique that is commonly used in deep learning. \n\nTo create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch [13]. The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model. \n\nTo continue the analogy of CL with human learning, the transfer-learning approach is equivalent to a teacher network trying to help a student, while in the adaptive approach the student is an autodidact trying to adapt to the learning difficulty.",
                    "score": 0.357211827885274,
                    "section_title": "Curriculum Learning",
                    "char_start_offset": 2609,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 432
                        },
                        {
                            "start": 435,
                            "end": 503
                        },
                        {
                            "start": 504,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 748
                        },
                        {
                            "start": 749,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 960
                        },
                        {
                            "start": 963,
                            "end": 1098
                        },
                        {
                            "start": 1099,
                            "end": 1205
                        },
                        {
                            "start": 1208,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1373
                        },
                        {
                            "start": 1374,
                            "end": 1481
                        },
                        {
                            "start": 1484,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1755
                        },
                        {
                            "start": 1756,
                            "end": 1895
                        },
                        {
                            "start": 1898,
                            "end": 2024
                        },
                        {
                            "start": 2025,
                            "end": 2162
                        },
                        {
                            "start": 2163,
                            "end": 2263
                        },
                        {
                            "start": 2266,
                            "end": 2512
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1631,
                            "end": 1635,
                            "matchedPaperCorpusId": "102350936"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93603515625
                }
            ],
            "relevance_judgement": 0.93603515625,
            "relevance_judgment_input_expanded": "# Title: Comparison and Analysis of New Curriculum Criteria for End-to-End ASR\n# Venue: Interspeech\n# Authors: Georgios Karakasidis, Tam'as Gr'osz, M. Kurimo\n## Abstract\nIt is common knowledge that the quantity and quality of the training data play a significant role in the creation of a good machine learning model. In this paper, we take it one step further and demonstrate that the way the training examples are arranged is also of crucial importance. Curriculum Learning is built on the observation that organized and structured assimilation of knowledge has the ability to enable faster training and better comprehension. When humans learn to speak, they first try to utter basic phones and then gradually move towards more complex structures such as words and sentences. This methodology is known as Curriculum Learning, and we employ it in the context of Automatic Speech Recognition. We hypothesize that end-to-end models can achieve better performance when provided with an organized training set consisting of examples that exhibit an increasing level of difficulty (i.e. a curriculum). To impose structure on the training set and to define the notion of an easy example, we explored multiple scoring functions that either use feedback from an external neural network or incorporate feedback from the model itself. Empirical results show that with different curriculums we can balance the training times and the network's performance.\n## Curriculum Learning\nIn the context of machine learning, CL aims to rank the training examples based on their difficulty. Each example is assigned a score and then the data set is ordered accordingly (from the easiest to the hardest example) with the goal of assisting the learning process. Empirical results show that this approach, when done properly, can speed up convergence and improve the stability of the training process of neural networks [12]. \n\nWe can think of CL as a way to provide guidance to a training model. A commonly used analogy to the real world, is that of the teacher-student relationship. The teacher has to create the curriculum in a way so that the student will neither get bored of the easy material, nor get overwhelmed by the hard examples. Instead, there should be an increasing level of difficulty throughout the curriculum. Note that this has nothing to do with the teacher-student transfer learning technique that is commonly used in deep learning. \n\nTo create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch [13]. The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model. \n\nTo continue the analogy of CL with human learning, the transfer-learning approach is equivalent to a teacher network trying to help a student, while in the adaptive approach the student is an autodidact trying to adapt to the learning difficulty.",
            "reference_string": "[251493126 | Karakasidis et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks",
            "venue": "AACL",
            "year": 2020,
            "reference_count": 45,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.aacl-srw.3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2033736232",
                    "name": "Antonio Laverghetta"
                },
                {
                    "authorId": "1716200134",
                    "name": "Jamshidbek Mirzakhalov"
                },
                {
                    "authorId": "2143879",
                    "name": "John Licato"
                }
            ],
            "abstract": "Curriculum learning, a training strategy where training data are ordered based on their difficulty, has been shown to improve performance and reduce training time on various NLP tasks. While much work over the years has developed novel approaches for generating curricula, these strategies are typically only suited for the task they were designed for. This work explores developing a task-agnostic model for problem difficulty and applying it to the Stanford Natural Language Inference (SNLI) dataset. Using the human responses that come with the dev set of SNLI, we train both regression and classification models to predict how many annotators will answer a question correctly and then project the difficulty estimates onto the full SNLI train set to create the curriculum. We argue that our curriculum is effectively capturing difficulty for this task through various analyses of both the model and the predicted difficulty scores.",
            "corpus_id": 227905455,
            "sentences": [
                {
                    "corpus_id": "227905455",
                    "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks",
                    "text": "Recent advances on natural language processing (NLP) benchmarks have been driven by increasingly sophisticated language models, which are pretrained on enormous amounts of data before use. Refinements of this process has led to increasingly powerful language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and more recently T5 (Raffel et al., 2019). Such models are quickly saturating even new tasks that have undergone a rigorous adversarial filtering process (Zellers et al., 2018). However, these downstream performance improvements also require more computational resources and data to train the models, which is not always feasible. Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;Wang et al., 2020;Zhou et al., 2020;Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored. Prior approaches either use a hand-crafted measure of difficulty that works well for a particular task or design an architecture that automatically creates the curriculum during training. In either case, the curriculum is formed using some information-theoretic measure of difficulty (semantic distance, feedback from a separate network, etc.), and it is difficult to interpret why they work well for some tasks and not others. In a practical sense, it is seldom clear how to apply a previously investigated curricula directly to another task. \n\nIn this paper, we explore how to address these shortcomings by creating what we call a taskagnostic model of difficulty, which we argue can, in principle, be applied to any supervised learning task. We use this model to investigate what makes a good difficulty measure for curricula beyond how it affects downstream performance to better explain why one curriculum should be preferred over another.",
                    "score": 0.47972418954279183,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 976
                        },
                        {
                            "start": 979,
                            "end": 1247
                        },
                        {
                            "start": 1248,
                            "end": 1435
                        },
                        {
                            "start": 1436,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1791
                        },
                        {
                            "start": 1794,
                            "end": 1992
                        },
                        {
                            "start": 1993,
                            "end": 2192
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 280,
                            "end": 301,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 486,
                            "end": 508,
                            "matchedPaperCorpusId": "52019251"
                        },
                        {
                            "start": 683,
                            "end": 696,
                            "matchedPaperCorpusId": "2105042"
                        },
                        {
                            "start": 923,
                            "end": 941,
                            "matchedPaperCorpusId": "220045412"
                        },
                        {
                            "start": 941,
                            "end": 959,
                            "matchedPaperCorpusId": "220047761"
                        },
                        {
                            "start": 959,
                            "end": 975,
                            "matchedPaperCorpusId": "220045816"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93310546875
                }
            ],
            "relevance_judgement": 0.93310546875,
            "relevance_judgment_input_expanded": "# Title: Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks\n# Venue: AACL\n# Authors: Antonio Laverghetta, Jamshidbek Mirzakhalov, John Licato\n## Abstract\nCurriculum learning, a training strategy where training data are ordered based on their difficulty, has been shown to improve performance and reduce training time on various NLP tasks. While much work over the years has developed novel approaches for generating curricula, these strategies are typically only suited for the task they were designed for. This work explores developing a task-agnostic model for problem difficulty and applying it to the Stanford Natural Language Inference (SNLI) dataset. Using the human responses that come with the dev set of SNLI, we train both regression and classification models to predict how many annotators will answer a question correctly and then project the difficulty estimates onto the full SNLI train set to create the curriculum. We argue that our curriculum is effectively capturing difficulty for this task through various analyses of both the model and the predicted difficulty scores.\n## Introduction\nRecent advances on natural language processing (NLP) benchmarks have been driven by increasingly sophisticated language models, which are pretrained on enormous amounts of data before use. Refinements of this process has led to increasingly powerful language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and more recently T5 (Raffel et al., 2019). Such models are quickly saturating even new tasks that have undergone a rigorous adversarial filtering process (Zellers et al., 2018). However, these downstream performance improvements also require more computational resources and data to train the models, which is not always feasible. Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;Wang et al., 2020;Zhou et al., 2020;Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored. Prior approaches either use a hand-crafted measure of difficulty that works well for a particular task or design an architecture that automatically creates the curriculum during training. In either case, the curriculum is formed using some information-theoretic measure of difficulty (semantic distance, feedback from a separate network, etc.), and it is difficult to interpret why they work well for some tasks and not others. In a practical sense, it is seldom clear how to apply a previously investigated curricula directly to another task. \n\nIn this paper, we explore how to address these shortcomings by creating what we call a taskagnostic model of difficulty, which we argue can, in principle, be applied to any supervised learning task. We use this model to investigate what makes a good difficulty measure for curricula beyond how it affects downstream performance to better explain why one curriculum should be preferred over another.",
            "reference_string": "[227905455 | Laverghetta et al. | 2020 | Citations: 2]"
        },
        {
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 52,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.20121, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1659451954",
                    "name": "Mohamed Elgaar"
                },
                {
                    "authorId": "143656058",
                    "name": "Hadi Amiri"
                }
            ],
            "abstract": "We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.",
            "corpus_id": 264819795,
            "sentences": [
                {
                    "corpus_id": "264819795",
                    "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
                    "text": "Second, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2017;Arpit et al., 2017) in neural models. We will address both issues as part of this research. \n\nThe contributions of this paper are: \n\n\u2022 incorporating human-verified linguistic complexity information to establish an effective scoring function for assessing the difficulty of text data with respect to NLP models, \u2022 deriving linguistic curricula for NLP models based on linguistic complexity of data and model behavior during training, and \u2022 identifying the core sets of linguistic complexity indices that most contribute to learning NLP tasks by models. \n\nWe evaluate our approach on several NLP tasks that require significant linguistic knowledge and reasoning to be addressed. Experimental results show that our approach can uncover latent linguistic knowledge that is most important for addressing NLP tasks. In addition, our approach obtains consistent performance gain over competing models. Source code and data is available at https://github.com/CLU-UML/Ling-CL. We present a framework for multiview curriculum learning using linguistic complexity indices. Our framework estimates the importance of various linguistic complexity indices, aggregates the resulting importance scores to determine the difficulty of samples for learning NLP tasks, and develops novel curricula for training models using complexity indices. The list of all indices used is in Appendix A.",
                    "score": 0.45283379289724596,
                    "section_title": "Introduction",
                    "char_start_offset": 3511,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 303
                        },
                        {
                            "start": 306,
                            "end": 342
                        },
                        {
                            "start": 345,
                            "end": 763
                        },
                        {
                            "start": 766,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1535
                        },
                        {
                            "start": 1536,
                            "end": 1582
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 192,
                            "end": 212,
                            "matchedPaperCorpusId": "6212000"
                        },
                        {
                            "start": 212,
                            "end": 231,
                            "matchedPaperCorpusId": "11455421"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93115234375
                },
                {
                    "corpus_id": "264819795",
                    "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
                    "text": "Effective curricula improve learning in humans (Tabibian et al., 2019;Nishimura, 2018) and machines (Bengio et al., 2009;Kumar et al., 2010;Zhou et al., 2020;Castells et al., 2020). Curriculum learning has been found effective in many NLP tasks (Settles and Meeder, 2016;Amiri et al., 2017;Platanios et al., 2019;Zhang et al., 2019;Amiri, 2019;Xu et al., 2020;Lalor and Yu, 2020;Jafarpour et al., 2021;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). A multiview curriculum is a curriculum able to integrate multiple difficulty scores simultaneously and leverage their collective value (Vakil and Amiri, 2023). \n\nWe assume there exists a subset of linguistic complexity indices that are most influential to learning an NLP task by a particular model. To identify these indices for each model and NLP task, we derive a weight factor \u03c1 i \u2208 [\u22121, 1] for each linguistic index that quantifies how well the index estimates the true difficulty of data samples to the model, determined by model instantaneous loss against validation data. By learning these weight factors, we obtain precise estimations that shed light on the core linguistic complexity indices that each model needs at different stages of its training to learn an NLP task. In addition, these estimates can be readily used for linguistic curriculum development, e.g., by training models with linguistically easy samples (with respect to the model) and gradually introducing linguistically challenging samples. \n\nTo achieve the above goals, we should address two gaps in the existing literature: First, existing curricula are often limited to a single criterion of difficulty and are not applicable to multiview settings. This is while difficulty is a condition that can be realized from multiple perspectives, can vary across a continuum for different models, and can dynamically change as the model improves. Second, existing approaches quantify the difficulty of data based on instantaneous training loss.",
                    "score": 0.3475544306000783,
                    "section_title": "Introduction",
                    "char_start_offset": 1615,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 637
                        },
                        {
                            "start": 640,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 1057
                        },
                        {
                            "start": 1058,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1495
                        },
                        {
                            "start": 1498,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 1993
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 47,
                            "end": 70,
                            "matchedPaperCorpusId": "58947495"
                        },
                        {
                            "start": 70,
                            "end": 86,
                            "matchedPaperCorpusId": "52157421"
                        },
                        {
                            "start": 100,
                            "end": 121,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 140,
                            "end": 158,
                            "matchedPaperCorpusId": "227275560"
                        },
                        {
                            "start": 158,
                            "end": 180,
                            "matchedPaperCorpusId": "227275456"
                        },
                        {
                            "start": 271,
                            "end": 290,
                            "matchedPaperCorpusId": "1916665"
                        },
                        {
                            "start": 290,
                            "end": 313,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 313,
                            "end": 332,
                            "matchedPaperCorpusId": "155089817"
                        },
                        {
                            "start": 332,
                            "end": 344,
                            "matchedPaperCorpusId": "174799918"
                        },
                        {
                            "start": 344,
                            "end": 360,
                            "matchedPaperCorpusId": "220045816"
                        },
                        {
                            "start": 360,
                            "end": 379,
                            "matchedPaperCorpusId": "226226711"
                        },
                        {
                            "start": 379,
                            "end": 402,
                            "matchedPaperCorpusId": "236486249"
                        },
                        {
                            "start": 402,
                            "end": 424,
                            "matchedPaperCorpusId": "238856825"
                        },
                        {
                            "start": 424,
                            "end": 450,
                            "matchedPaperCorpusId": "247518847"
                        },
                        {
                            "start": 450,
                            "end": 476,
                            "matchedPaperCorpusId": "250391006"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78857421875
                }
            ],
            "relevance_judgement": 0.93115234375,
            "relevance_judgment_input_expanded": "# Title: Ling-CL: Understanding NLP Models through Linguistic Curricula\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mohamed Elgaar, Hadi Amiri\n## Abstract\nWe employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.\n## Introduction\nEffective curricula improve learning in humans (Tabibian et al., 2019;Nishimura, 2018) and machines (Bengio et al., 2009;Kumar et al., 2010;Zhou et al., 2020;Castells et al., 2020). Curriculum learning has been found effective in many NLP tasks (Settles and Meeder, 2016;Amiri et al., 2017;Platanios et al., 2019;Zhang et al., 2019;Amiri, 2019;Xu et al., 2020;Lalor and Yu, 2020;Jafarpour et al., 2021;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). A multiview curriculum is a curriculum able to integrate multiple difficulty scores simultaneously and leverage their collective value (Vakil and Amiri, 2023). \n\nWe assume there exists a subset of linguistic complexity indices that are most influential to learning an NLP task by a particular model. To identify these indices for each model and NLP task, we derive a weight factor \u03c1 i \u2208 [\u22121, 1] for each linguistic index that quantifies how well the index estimates the true difficulty of data samples to the model, determined by model instantaneous loss against validation data. By learning these weight factors, we obtain precise estimations that shed light on the core linguistic complexity indices that each model needs at different stages of its training to learn an NLP task. In addition, these estimates can be readily used for linguistic curriculum development, e.g., by training models with linguistically easy samples (with respect to the model) and gradually introducing linguistically challenging samples. \n\nTo achieve the above goals, we should address two gaps in the existing literature: First, existing curricula are often limited to a single criterion of difficulty and are not applicable to multiview settings. This is while difficulty is a condition that can be realized from multiple perspectives, can vary across a continuum for different models, and can dynamically change as the model improves. Second, existing approaches quantify the difficulty of data based on instantaneous training loss.\n...\nSecond, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2017;Arpit et al., 2017) in neural models. We will address both issues as part of this research. \n\nThe contributions of this paper are: \n\n\u2022 incorporating human-verified linguistic complexity information to establish an effective scoring function for assessing the difficulty of text data with respect to NLP models, \u2022 deriving linguistic curricula for NLP models based on linguistic complexity of data and model behavior during training, and \u2022 identifying the core sets of linguistic complexity indices that most contribute to learning NLP tasks by models. \n\nWe evaluate our approach on several NLP tasks that require significant linguistic knowledge and reasoning to be addressed. Experimental results show that our approach can uncover latent linguistic knowledge that is most important for addressing NLP tasks. In addition, our approach obtains consistent performance gain over competing models. Source code and data is available at https://github.com/CLU-UML/Ling-CL. We present a framework for multiview curriculum learning using linguistic complexity indices. Our framework estimates the importance of various linguistic complexity indices, aggregates the resulting importance scores to determine the difficulty of samples for learning NLP tasks, and develops novel curricula for training models using complexity indices. The list of all indices used is in Appendix A.",
            "reference_string": "[264819795 | Elgaar et al. | 2023 | Citations: 2]"
        },
        {
            "title": "LeRaC: Learning Rate Curriculum",
            "venue": "International Journal of Computer Vision",
            "year": 2022,
            "reference_count": 78,
            "citation_count": 9,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.09180",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.09180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2154573729",
                    "name": "Florinel-Alin Croitoru"
                },
                {
                    "authorId": "103931166",
                    "name": "Nicolae-C\u0103t\u0103lin Ristea"
                },
                {
                    "authorId": "1817759",
                    "name": "Radu Tudor Ionescu"
                },
                {
                    "authorId": "1703601",
                    "name": "N. Sebe"
                }
            ],
            "abstract": "Most curriculum learning methods require an approach to sort the data samples by difficulty, which is often cumbersome to perform. In this work, we propose a novel curriculum learning approach termed Learning Rate Curriculum (LeRaC), which leverages the use of a different learning rate for each layer of a neural network to create a data-agnostic curriculum during the initial training epochs. More specifically, LeRaC assigns higher learning rates to neural layers closer to the input, gradually decreasing the learning rates as the layers are placed farther away from the input. The learning rates increase at various paces during the first training iterations, until they all reach the same value. From this point on, the neural model is trained as usual. This creates a model-level curriculum learning strategy that does not require sorting the examples by difficulty and is compatible with any neural network, generating higher performance levels regardless of the architecture. We conduct comprehensive experiments on 12 data sets from the computer vision (CIFAR-10, CIFAR-100, Tiny ImageNet, ImageNet-1K, Food-101, UTKFace, PASCAL VOC), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D) domains, considering various convolutional (ResNet-18, Wide-ResNet-50, DenseNet-121, YOLOv5), recurrent (LSTM) and transformer (CvT, BERT, SepTr) architectures. We compare our approach with the conventional training regime, as well as with Curriculum by Smoothing (CBS), a state-of-the-art data-agnostic curriculum learning approach. Unlike CBS, our performance improvements over the standard training regime are consistent across all data sets and models. Furthermore, we significantly surpass CBS in terms of training time (there is no additional cost over the standard training regime for LeRaC). Our code is freely available at: https://github.com/CroitoruAlin/LeRaC.",
            "corpus_id": 248887633,
            "sentences": [
                {
                    "corpus_id": "248887633",
                    "title": "LeRaC: Learning Rate Curriculum",
                    "text": "Curriculum learning was initially introduced by Bengio et al. [1] as a training strategy that helps machine learning models to generalize better when the training examples are presented in the ascending order of their difficulty. Extensive surveys on curriculum learning methods, including the most recent advancements on the topic, were conducted by Soviany et al. [2] and Wang et al. [4]. In the former survey, Soviany et al. [2] emphasized that curriculum learning is not only applied at the data level, but also with respect to the other components involved in a machine learning approach, namely at the model level, the task level and the objective (loss) level. Regardless of the component on which curriculum learning is applied, the technique has demonstrated its effectiveness on a broad range of machine learning tasks, from computer vision [1,7,[27][28][29][30][31][32][33][34] to natural language processing [1,[35][36][37][38] and audio processing [39,40]. \n\nThe main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input [41][42][43] or metrics based on domain-specific heuristics. For instance, the text length [36,[44][45][46] and the word frequency [1,38] have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works [32,33]. Other solutions employed difficulty estimators [47] or even the confidence level of the predictions made by the neural network [48,49] to approximate the complexity of the data samples. Other studies [27][28][29] used the error of a previously trained model to estimate the difficulty of each sample. Such solutions have shown their utility in specific application domains. Nonetheless, measuring the difficulty remains problematic when implementing standard (data-level) curriculum learning strategies, at least in some application domains. Therefore, several alternatives have emerged over time, handling the drawback and improving the conventional curriculum learning approach.",
                    "score": 0.47118286798823417,
                    "section_title": "Curriculum Learning",
                    "char_start_offset": 6876,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 969
                        },
                        {
                            "start": 972,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1714
                        },
                        {
                            "start": 1715,
                            "end": 1829
                        },
                        {
                            "start": 1830,
                            "end": 1902
                        },
                        {
                            "start": 1903,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2209
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 366,
                            "end": 369,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 386,
                            "end": 389,
                            "matchedPaperCorpusId": "232362223"
                        },
                        {
                            "start": 428,
                            "end": 431,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 854,
                            "end": 856,
                            "matchedPaperCorpusId": "220301592"
                        },
                        {
                            "start": 856,
                            "end": 860,
                            "matchedPaperCorpusId": "266998701"
                        },
                        {
                            "start": 860,
                            "end": 864,
                            "matchedPaperCorpusId": "254246401"
                        },
                        {
                            "start": 864,
                            "end": 868,
                            "matchedPaperCorpusId": "256808576"
                        },
                        {
                            "start": 868,
                            "end": 872,
                            "matchedPaperCorpusId": "8105909"
                        },
                        {
                            "start": 872,
                            "end": 876,
                            "matchedPaperCorpusId": "51876228"
                        },
                        {
                            "start": 876,
                            "end": 880,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 884,
                            "end": 888,
                            "matchedPaperCorpusId": "5658192"
                        },
                        {
                            "start": 923,
                            "end": 927,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 927,
                            "end": 931,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 931,
                            "end": 935,
                            "matchedPaperCorpusId": "1900277"
                        },
                        {
                            "start": 935,
                            "end": 939,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 961,
                            "end": 965,
                            "matchedPaperCorpusId": "19805513"
                        },
                        {
                            "start": 965,
                            "end": 968,
                            "matchedPaperCorpusId": "11590585"
                        },
                        {
                            "start": 1215,
                            "end": 1219,
                            "matchedPaperCorpusId": "8502955"
                        },
                        {
                            "start": 1219,
                            "end": 1223,
                            "matchedPaperCorpusId": "204539326"
                        },
                        {
                            "start": 1223,
                            "end": 1227,
                            "matchedPaperCorpusId": "221995570"
                        },
                        {
                            "start": 1306,
                            "end": 1310,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 1314,
                            "end": 1318,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 1318,
                            "end": 1322,
                            "matchedPaperCorpusId": "233433844"
                        },
                        {
                            "start": 1349,
                            "end": 1352,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1520,
                            "end": 1524,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 1576,
                            "end": 1580,
                            "matchedPaperCorpusId": "879067"
                        },
                        {
                            "start": 1656,
                            "end": 1660,
                            "matchedPaperCorpusId": "10364203"
                        },
                        {
                            "start": 1660,
                            "end": 1663,
                            "matchedPaperCorpusId": "102350936"
                        },
                        {
                            "start": 1729,
                            "end": 1733,
                            "matchedPaperCorpusId": "266998701"
                        },
                        {
                            "start": 1733,
                            "end": 1737,
                            "matchedPaperCorpusId": "254246401"
                        },
                        {
                            "start": 1737,
                            "end": 1741,
                            "matchedPaperCorpusId": "256808576"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                }
            ],
            "relevance_judgement": 0.92578125,
            "relevance_judgment_input_expanded": "# Title: LeRaC: Learning Rate Curriculum\n# Venue: International Journal of Computer Vision\n# Authors: Florinel-Alin Croitoru, Nicolae-C\u0103t\u0103lin Ristea, Radu Tudor Ionescu, N. Sebe\n## Abstract\nMost curriculum learning methods require an approach to sort the data samples by difficulty, which is often cumbersome to perform. In this work, we propose a novel curriculum learning approach termed Learning Rate Curriculum (LeRaC), which leverages the use of a different learning rate for each layer of a neural network to create a data-agnostic curriculum during the initial training epochs. More specifically, LeRaC assigns higher learning rates to neural layers closer to the input, gradually decreasing the learning rates as the layers are placed farther away from the input. The learning rates increase at various paces during the first training iterations, until they all reach the same value. From this point on, the neural model is trained as usual. This creates a model-level curriculum learning strategy that does not require sorting the examples by difficulty and is compatible with any neural network, generating higher performance levels regardless of the architecture. We conduct comprehensive experiments on 12 data sets from the computer vision (CIFAR-10, CIFAR-100, Tiny ImageNet, ImageNet-1K, Food-101, UTKFace, PASCAL VOC), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D) domains, considering various convolutional (ResNet-18, Wide-ResNet-50, DenseNet-121, YOLOv5), recurrent (LSTM) and transformer (CvT, BERT, SepTr) architectures. We compare our approach with the conventional training regime, as well as with Curriculum by Smoothing (CBS), a state-of-the-art data-agnostic curriculum learning approach. Unlike CBS, our performance improvements over the standard training regime are consistent across all data sets and models. Furthermore, we significantly surpass CBS in terms of training time (there is no additional cost over the standard training regime for LeRaC). Our code is freely available at: https://github.com/CroitoruAlin/LeRaC.\n## Curriculum Learning\nCurriculum learning was initially introduced by Bengio et al. [1] as a training strategy that helps machine learning models to generalize better when the training examples are presented in the ascending order of their difficulty. Extensive surveys on curriculum learning methods, including the most recent advancements on the topic, were conducted by Soviany et al. [2] and Wang et al. [4]. In the former survey, Soviany et al. [2] emphasized that curriculum learning is not only applied at the data level, but also with respect to the other components involved in a machine learning approach, namely at the model level, the task level and the objective (loss) level. Regardless of the component on which curriculum learning is applied, the technique has demonstrated its effectiveness on a broad range of machine learning tasks, from computer vision [1,7,[27][28][29][30][31][32][33][34] to natural language processing [1,[35][36][37][38] and audio processing [39,40]. \n\nThe main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input [41][42][43] or metrics based on domain-specific heuristics. For instance, the text length [36,[44][45][46] and the word frequency [1,38] have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works [32,33]. Other solutions employed difficulty estimators [47] or even the confidence level of the predictions made by the neural network [48,49] to approximate the complexity of the data samples. Other studies [27][28][29] used the error of a previously trained model to estimate the difficulty of each sample. Such solutions have shown their utility in specific application domains. Nonetheless, measuring the difficulty remains problematic when implementing standard (data-level) curriculum learning strategies, at least in some application domains. Therefore, several alternatives have emerged over time, handling the drawback and improving the conventional curriculum learning approach.",
            "reference_string": "[248887633 | Croitoru et al. | 2022 | Citations: 9]"
        },
        {
            "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-emnlp.286.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.286?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "102489044",
                    "name": "Jianing Zhou"
                },
                {
                    "authorId": "41048608",
                    "name": "Ziheng Zeng"
                },
                {
                    "authorId": "2008458",
                    "name": "Hongyu Gong"
                },
                {
                    "authorId": "2263637139",
                    "name": "Suma Bhat"
                }
            ],
            "abstract": "Non-compositional expressions, by virtue of their non-compositionality, are a classic \u2018pain in the neck\u2019 for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we pro-pose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model\u2019s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https: //github.com/zhjjn/CL2Gen.git .",
            "corpus_id": 266166227,
            "sentences": [
                {
                    "corpus_id": "266166227",
                    "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
                    "text": "In this section, we briefly introduce our proposed curriculum learning method for non-compositional expression generation. Curriculum learning for efficiently leveraging available data resources consists of two main parts: a measure of difficulty of training instances, and an arrangement of the training examples using this measure. Accordingly, for non-compositional expression generation, we propose a data arrangement method for dynamically arranging the training examples according to a newly studied difficulty metric. In addition, due to the current large pre-trained language models' insufficiency in processing non-compositional expressions (Dankers et al., 2022), non-compositional expressions that are difficulty for LMs to understand would have a high perplexity score and the representations between non-compositional expressions and their constituent words would be large. Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty. \n\nMoreover, in our experiments, we observe that following the curriculum learning principle of arranging the training examples based on their difficulty levels, the problem of forgetting arises due to the gradual shift of distribution in domain difficulty. Therefore, to alleviate this forgetting problem, we propose a simple yet effective continual learning method. Figure 1 demonstrates the workflow of our proposed curriculum learning framework and its details as follows.",
                    "score": 0.3755236031441558,
                    "section_title": "Framework",
                    "char_start_offset": 9802,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1008
                        },
                        {
                            "start": 1011,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1484
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 650,
                            "end": 672,
                            "matchedPaperCorpusId": "248780588"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jianing Zhou, Ziheng Zeng, Hongyu Gong, Suma Bhat\n## Abstract\nNon-compositional expressions, by virtue of their non-compositionality, are a classic \u2018pain in the neck\u2019 for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we pro-pose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model\u2019s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https: //github.com/zhjjn/CL2Gen.git .\n## Framework\nIn this section, we briefly introduce our proposed curriculum learning method for non-compositional expression generation. Curriculum learning for efficiently leveraging available data resources consists of two main parts: a measure of difficulty of training instances, and an arrangement of the training examples using this measure. Accordingly, for non-compositional expression generation, we propose a data arrangement method for dynamically arranging the training examples according to a newly studied difficulty metric. In addition, due to the current large pre-trained language models' insufficiency in processing non-compositional expressions (Dankers et al., 2022), non-compositional expressions that are difficulty for LMs to understand would have a high perplexity score and the representations between non-compositional expressions and their constituent words would be large. Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty. \n\nMoreover, in our experiments, we observe that following the curriculum learning principle of arranging the training examples based on their difficulty levels, the problem of forgetting arises due to the gradual shift of distribution in domain difficulty. Therefore, to alleviate this forgetting problem, we propose a simple yet effective continual learning method. Figure 1 demonstrates the workflow of our proposed curriculum learning framework and its details as follows.",
            "reference_string": "[266166227 | Zhou et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Assessing and Scoring Difficulty of Hard-to-Solve Data in Summarization Tasks",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3519548",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3519548?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3519548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2159741574",
                    "name": "Jeesu Jung"
                },
                {
                    "authorId": "2065447556",
                    "name": "H. Seo"
                },
                {
                    "authorId": "2210707252",
                    "name": "Hyuk Namgoong"
                },
                {
                    "authorId": "2313048573",
                    "name": "Sangkeun Jung"
                }
            ],
            "abstract": "In recent years, data-driven and machine learning-based natural language processing (NLP) technologies have effectively addressed various challenges. To further enhance the performance of NLP models, it is crucial to understand the types of data that a model can handle well and those it struggles with. This study introduces a method to discern which types of data can be effectively processed by given neural network-based models and which pose difficulties. We define the criteria for hard-to-solve data, construct a pairwise easy-hard dataset, and propose a neural scoring model. This model ascertains the difficulty level of each data instance. To utilize the proposed difficulty level as an application, we employed curriculum learning. The experimental results show that our methodology can effectively distinguish between easy and hard data, and performance improves when applying the curriculum learning approach.",
            "corpus_id": 274886046,
            "sentences": [
                {
                    "corpus_id": "274886046",
                    "title": "Assessing and Scoring Difficulty of Hard-to-Solve Data in Summarization Tasks",
                    "text": "In recent years, data-driven and machine learning-based natural language processing (NLP) technologies have effectively addressed various challenges. To further enhance the performance of NLP models, it is crucial to understand the types of data that a model can handle well and those it struggles with. This study introduces a method to discern which types of data can be effectively processed by given neural network-based models and which pose difficulties. We define the criteria for hard-to-solve data, construct a pairwise easy-hard dataset, and propose a neural scoring model. This model ascertains the difficulty level of each data instance. To utilize the proposed difficulty level as an application, we employed curriculum learning. The experimental results show that our methodology can effectively distinguish between easy and hard data, and performance improves when applying the curriculum learning approach.",
                    "score": 0.3544289123930477,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: Assessing and Scoring Difficulty of Hard-to-Solve Data in Summarization Tasks\n# Venue: IEEE Access\n# Authors: Jeesu Jung, H. Seo, Hyuk Namgoong, Sangkeun Jung\n## Abstract\nIn recent years, data-driven and machine learning-based natural language processing (NLP) technologies have effectively addressed various challenges. To further enhance the performance of NLP models, it is crucial to understand the types of data that a model can handle well and those it struggles with. This study introduces a method to discern which types of data can be effectively processed by given neural network-based models and which pose difficulties. We define the criteria for hard-to-solve data, construct a pairwise easy-hard dataset, and propose a neural scoring model. This model ascertains the difficulty level of each data instance. To utilize the proposed difficulty level as an application, we employed curriculum learning. The experimental results show that our methodology can effectively distinguish between easy and hard data, and performance improves when applying the curriculum learning approach.\n",
            "reference_string": "[274886046 | Jung et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 50,
            "citation_count": 12,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.03380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152846184",
                    "name": "Sanghwan Bae"
                },
                {
                    "authorId": "2354048562",
                    "name": "Jiwoo Hong"
                },
                {
                    "authorId": "2294848361",
                    "name": "M. Lee"
                },
                {
                    "authorId": "2354326988",
                    "name": "Hanbyul Kim"
                },
                {
                    "authorId": "2353956752",
                    "name": "JeongYeon Nam"
                },
                {
                    "authorId": "10469987",
                    "name": "Donghyun Kwak"
                }
            ],
            "abstract": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning ability of Large Language Models (LLMs). However, due to the sparsity of rewards in RORL, effective training is highly dependent on the selection of problems of appropriate difficulty. Although curriculum learning attempts to address this by adjusting difficulty, it often relies on static schedules, and even recent online filtering methods lack theoretical grounding and a systematic understanding of their effectiveness. In this work, we theoretically and empirically show that curating the batch with the problems that the training model achieves intermediate accuracy on the fly can maximize the effectiveness of RORL training, namely balanced online difficulty filtering. We first derive that the lower bound of the KL divergence between the initial and the optimal policy can be expressed with the variance of the sampled accuracy. Building on those insights, we show that balanced filtering can maximize the lower bound, leading to better performance. Experimental results across five challenging math reasoning benchmarks show that balanced online filtering yields an additional 10% in AIME and 4% improvements in average over plain GRPO. Moreover, further analysis shows the gains in sample efficiency and training time efficiency, exceeding the maximum reward of plain GRPO within 60% training time and the volume of the training set.",
            "corpus_id": 277596006,
            "sentences": [
                {
                    "corpus_id": "277596006",
                    "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
                    "text": "Reinforcement Learning (RL) has become a key training paradigm for training large language models (LLMs) specialized in reasoning tasks, exemplified by OpenAI o1 (OpenAI et al., 2024) and DeepSeek-R1 (Guo et al., 2025). These models utilize Reasoning-Oriented Reinforcement Learning (RORL), where verifiable rewards like correctness in mathematical or logical problems serve as the primary supervision signal (Lambert et al., 2024). \n\nAs RORL increasingly targets high-complexity reasoning tasks, designing effective learning dynamics becomes crucial to help models progressively acquire the necessary capabilities. Effective learning has long been studied in the education domain, where theories such as the Zone of Proximal Development (ZPD) (Cole, 1978;Tzannetos et al., 2023) emphasize that learning is most efficient when tasks are neither too easy nor too hard, but instead fall within a learner's optimal challenge zone. This has motivated a variety of strategies in language modeling, from curriculum learning that introduces harder problems progressively (Team et al., 2025), to difficulty-aware data curation that selects or filters examples based on estimated pass rates or diversity (Muennighoff et al., 2025;Ye et al., 2025). Online filtering methods further explore this idea by dynamically adjusting the training data to match the current ability of the model (Cui et al., 2025). However, while previous work demonstrate the empirical effectiveness of such techniques, they often lack a detailed analysis of why or when certain difficulty distributions yield better learning outcomes. With G rollouts for each prompt x, we measure the pass rate p(x) as the average accuracy and filter them by predefined thresholds: e.g., 0.25 \u2264 p(x) \u2264 0.75 in this case. We recursively stack filtered prompts until the train batch size meets the fixed size N. We elaborate on the asynchronous implementation in Appendix A. \n\nIn this work, we conduct extensive experiments and provide theoretical analysis to understand how and why difficulty filtering improves learning in RORL.",
                    "score": 0.37162588208855885,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 432
                        },
                        {
                            "start": 435,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1769
                        },
                        {
                            "start": 1770,
                            "end": 1858
                        },
                        {
                            "start": 1859,
                            "end": 1921
                        },
                        {
                            "start": 1924,
                            "end": 2077
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.921875
                }
            ],
            "relevance_judgement": 0.921875,
            "relevance_judgment_input_expanded": "# Title: Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning\n# Venue: arXiv.org\n# Authors: Sanghwan Bae, Jiwoo Hong, M. Lee, Hanbyul Kim, JeongYeon Nam, Donghyun Kwak\n## Abstract\nReasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning ability of Large Language Models (LLMs). However, due to the sparsity of rewards in RORL, effective training is highly dependent on the selection of problems of appropriate difficulty. Although curriculum learning attempts to address this by adjusting difficulty, it often relies on static schedules, and even recent online filtering methods lack theoretical grounding and a systematic understanding of their effectiveness. In this work, we theoretically and empirically show that curating the batch with the problems that the training model achieves intermediate accuracy on the fly can maximize the effectiveness of RORL training, namely balanced online difficulty filtering. We first derive that the lower bound of the KL divergence between the initial and the optimal policy can be expressed with the variance of the sampled accuracy. Building on those insights, we show that balanced filtering can maximize the lower bound, leading to better performance. Experimental results across five challenging math reasoning benchmarks show that balanced online filtering yields an additional 10% in AIME and 4% improvements in average over plain GRPO. Moreover, further analysis shows the gains in sample efficiency and training time efficiency, exceeding the maximum reward of plain GRPO within 60% training time and the volume of the training set.\n## Introduction\nReinforcement Learning (RL) has become a key training paradigm for training large language models (LLMs) specialized in reasoning tasks, exemplified by OpenAI o1 (OpenAI et al., 2024) and DeepSeek-R1 (Guo et al., 2025). These models utilize Reasoning-Oriented Reinforcement Learning (RORL), where verifiable rewards like correctness in mathematical or logical problems serve as the primary supervision signal (Lambert et al., 2024). \n\nAs RORL increasingly targets high-complexity reasoning tasks, designing effective learning dynamics becomes crucial to help models progressively acquire the necessary capabilities. Effective learning has long been studied in the education domain, where theories such as the Zone of Proximal Development (ZPD) (Cole, 1978;Tzannetos et al., 2023) emphasize that learning is most efficient when tasks are neither too easy nor too hard, but instead fall within a learner's optimal challenge zone. This has motivated a variety of strategies in language modeling, from curriculum learning that introduces harder problems progressively (Team et al., 2025), to difficulty-aware data curation that selects or filters examples based on estimated pass rates or diversity (Muennighoff et al., 2025;Ye et al., 2025). Online filtering methods further explore this idea by dynamically adjusting the training data to match the current ability of the model (Cui et al., 2025). However, while previous work demonstrate the empirical effectiveness of such techniques, they often lack a detailed analysis of why or when certain difficulty distributions yield better learning outcomes. With G rollouts for each prompt x, we measure the pass rate p(x) as the average accuracy and filter them by predefined thresholds: e.g., 0.25 \u2264 p(x) \u2264 0.75 in this case. We recursively stack filtered prompts until the train batch size meets the fixed size N. We elaborate on the asynchronous implementation in Appendix A. \n\nIn this work, we conduct extensive experiments and provide theoretical analysis to understand how and why difficulty filtering improves learning in RORL.",
            "reference_string": "[277596006 | Bae et al. | 2025 | Citations: 12]"
        },
        {
            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 205,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2025.3554586",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3554586?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3554586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326763710",
                    "name": "Dingzong Zhang"
                },
                {
                    "authorId": "2345374431",
                    "name": "Devi Listiyani"
                },
                {
                    "authorId": "2302811064",
                    "name": "Priyanka Singh"
                },
                {
                    "authorId": "2309099465",
                    "name": "Manoranjan Mohanty"
                }
            ],
            "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) enables the transfer of capabilities from proprietary LLMs to open-source models. This survey provides a detailed discussion of the basic principles, algorithms, and implementation methods of knowledge distillation. It explores KD\u2019s impact on LLMs, emphasizing its utility in model compression, performance enhancement, and self-improvement. Through the analysis of practical examples such as DistilBERT, TinyBERT, and MobileBERT, the paper demonstrates how knowledge distillation can markedly enhance the efficiency and applicability of large language models in real-world scenarios. The discussion encompasses the varied applications of KD across multiple domains, including industrial systems, embedded systems, Natural Language Processing (NLP), multi-modal processing, and vertical domains, such as medicine, law, science, finance, and materials science. This survey outlines current KD methodologies and future research directions, highlighting its role in advancing AI technologies and fostering innovation across different sectors.",
            "corpus_id": 277398866,
            "sentences": [
                {
                    "corpus_id": "277398866",
                    "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
                    "text": "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes. \n\nModel compression techniques like KD, pruning, quantization, and adaptive KD each have unique pros and cons. KD transfers knowledge from a larger teacher model to a smaller student model, significantly reducing size while maintaining accuracy [6], especially in natural language processing [16]. However, it relies on large labeled datasets and struggles with very small models [6]. Pruning removes redundant weights or neurons for better compression and lower computational demands, but often sacrifices accuracy and requires fine-tuning [48], [49]. Quantization reduces model precision, saving memory and speeding up inference, though it can degrade accuracy in complex tasks without specialized hardware [50]. Adaptive KD customizes the distillation process based on input difficulty or model alignment, improving performance but increasing training complexity [47], [51]. While KD and its adaptive forms balance performance and size, pruning and quantization excel in extreme compression scenarios, highlighting the complementary nature of these methods.",
                    "score": 0.3474169228328696,
                    "section_title": "1) ADDITIONAL TECHNIQUES AND APPLICATIONS",
                    "char_start_offset": 26928,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 796
                        },
                        {
                            "start": 799,
                            "end": 907
                        },
                        {
                            "start": 908,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1181
                        },
                        {
                            "start": 1182,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1857
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1042,
                            "end": 1045,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1177,
                            "end": 1180,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1344,
                            "end": 1348,
                            "matchedPaperCorpusId": "212628335"
                        },
                        {
                            "start": 1506,
                            "end": 1510,
                            "matchedPaperCorpusId": "67855262"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9208984375
                }
            ],
            "relevance_judgement": 0.9208984375,
            "relevance_judgment_input_expanded": "# Title: Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models\n# Venue: IEEE Access\n# Authors: Dingzong Zhang, Devi Listiyani, Priyanka Singh, Manoranjan Mohanty\n## Abstract\nIn the era of Large Language Models (LLMs), Knowledge Distillation (KD) enables the transfer of capabilities from proprietary LLMs to open-source models. This survey provides a detailed discussion of the basic principles, algorithms, and implementation methods of knowledge distillation. It explores KD\u2019s impact on LLMs, emphasizing its utility in model compression, performance enhancement, and self-improvement. Through the analysis of practical examples such as DistilBERT, TinyBERT, and MobileBERT, the paper demonstrates how knowledge distillation can markedly enhance the efficiency and applicability of large language models in real-world scenarios. The discussion encompasses the varied applications of KD across multiple domains, including industrial systems, embedded systems, Natural Language Processing (NLP), multi-modal processing, and vertical domains, such as medicine, law, science, finance, and materials science. This survey outlines current KD methodologies and future research directions, highlighting its role in advancing AI technologies and fostering innovation across different sectors.\n## 1) ADDITIONAL TECHNIQUES AND APPLICATIONS\nFor instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes. \n\nModel compression techniques like KD, pruning, quantization, and adaptive KD each have unique pros and cons. KD transfers knowledge from a larger teacher model to a smaller student model, significantly reducing size while maintaining accuracy [6], especially in natural language processing [16]. However, it relies on large labeled datasets and struggles with very small models [6]. Pruning removes redundant weights or neurons for better compression and lower computational demands, but often sacrifices accuracy and requires fine-tuning [48], [49]. Quantization reduces model precision, saving memory and speeding up inference, though it can degrade accuracy in complex tasks without specialized hardware [50]. Adaptive KD customizes the distillation process based on input difficulty or model alignment, improving performance but increasing training complexity [47], [51]. While KD and its adaptive forms balance performance and size, pruning and quantization excel in extreme compression scenarios, highlighting the complementary nature of these methods.",
            "reference_string": "[277398866 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Curriculum Learning for Natural Language Understanding",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 45,
            "citation_count": 206,
            "influential_citation_count": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.542.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.acl-main.542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1754285124",
                    "name": "Benfeng Xu"
                },
                {
                    "authorId": "48378753",
                    "name": "L. Zhang"
                },
                {
                    "authorId": "1855978",
                    "name": "Zhendong Mao"
                },
                {
                    "authorId": "143906199",
                    "name": "Quan Wang"
                },
                {
                    "authorId": "143994657",
                    "name": "Hongtao Xie"
                },
                {
                    "authorId": "1699819",
                    "name": "Yongdong Zhang"
                }
            ],
            "abstract": "With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.",
            "corpus_id": 220045816,
            "sentences": [
                {
                    "corpus_id": "220045816",
                    "title": "Curriculum Learning for Natural Language Understanding",
                    "text": "Curriculum Learning (CL) is first proposed by (Bengio et al., 2009) in machine learning area, where the definition of easy examples is established ahead, and an easy-to-difficult curriculum is arranged accordingly for the learning procedure. Recent developments have successfully applied CL in computer vision areas (Jiang et al., 2017;Guo et al., 2018;Hacohen and Weinshall, 2019). It is observed in these works that by excluding the negative impact of difficult or even noisy examples in early training stage, an appropriate CL strategy can guide learning towards a better local minima in parameter space, especially for highly non-convex deep models. We argue that language models like transformer, which is hard to train (Popel and Bojar, 2018), should also benefit from CL in the context of learning NLU tasks, and such idea still remains unexplored. \n\nThe key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks. \n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model. In general, our CL approach is not constrained to any particular task, and does not rely on human prior heuristics about the task or dataset.",
                    "score": 0.35605017868841315,
                    "section_title": "Introduction",
                    "char_start_offset": 2107,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 241
                        },
                        {
                            "start": 242,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 855
                        },
                        {
                            "start": 858,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 1188
                        },
                        {
                            "start": 1189,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1619
                        },
                        {
                            "start": 1622,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1823
                        },
                        {
                            "start": 1824,
                            "end": 1954
                        },
                        {
                            "start": 1955,
                            "end": 2167
                        },
                        {
                            "start": 2168,
                            "end": 2309
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 46,
                            "end": 67,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 336,
                            "end": 353,
                            "matchedPaperCorpusId": "51920640"
                        },
                        {
                            "start": 725,
                            "end": 748,
                            "matchedPaperCorpusId": "4556964"
                        },
                        {
                            "start": 1129,
                            "end": 1147,
                            "matchedPaperCorpusId": "51920640"
                        },
                        {
                            "start": 1202,
                            "end": 1223,
                            "matchedPaperCorpusId": "873046"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: Curriculum Learning for Natural Language Understanding\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Benfeng Xu, L. Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, Yongdong Zhang\n## Abstract\nWith the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.\n## Introduction\nCurriculum Learning (CL) is first proposed by (Bengio et al., 2009) in machine learning area, where the definition of easy examples is established ahead, and an easy-to-difficult curriculum is arranged accordingly for the learning procedure. Recent developments have successfully applied CL in computer vision areas (Jiang et al., 2017;Guo et al., 2018;Hacohen and Weinshall, 2019). It is observed in these works that by excluding the negative impact of difficult or even noisy examples in early training stage, an appropriate CL strategy can guide learning towards a better local minima in parameter space, especially for highly non-convex deep models. We argue that language models like transformer, which is hard to train (Popel and Bojar, 2018), should also benefit from CL in the context of learning NLU tasks, and such idea still remains unexplored. \n\nThe key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks. \n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model. In general, our CL approach is not constrained to any particular task, and does not rely on human prior heuristics about the task or dataset.",
            "reference_string": "[220045816 | Xu et al. | 2020 | Citations: 206]"
        },
        {
            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
            "venue": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.18761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268760204",
                    "name": "Aryaman Chobey"
                },
                {
                    "authorId": "2268760018",
                    "name": "Oliver Smith"
                },
                {
                    "authorId": "2268796061",
                    "name": "Anzi Wang"
                },
                {
                    "authorId": "2268760229",
                    "name": "Grusha Prasad"
                }
            ],
            "abstract": "The use of neural language models to model human behavior has met with mixed success. While some work has found that the surprisal estimates from these models can be used to predict a wide range of human neural and behavioral responses, other work studying more complex syntactic phenomena has found that these surprisal estimates generate incorrect behavioral predictions. This paper explores the extent to which the misalignment between empirical and model-predicted behavior can be minimized by training models on more developmentally plausible data, such as in the BabyLM Challenge. We trained teacher language models on the BabyLM\"strict-small\"dataset and used sentence level surprisal estimates from these teacher models to create a curriculum. We found tentative evidence that our curriculum made it easier for models to acquire linguistic knowledge from the training data: on the subset of tasks in the BabyLM challenge suite evaluating models' grammatical knowledge of English, models first trained on the BabyLM data curriculum and then on a few randomly ordered training epochs performed slightly better than models trained on randomly ordered epochs alone. This improved linguistic knowledge acquisition did not result in better alignment with human reading behavior, however: models trained on the BabyLM dataset (with or without a curriculum) generated predictions that were as misaligned with human behavior as models trained on larger less curated datasets. This suggests that training on developmentally plausible datasets alone is likely insufficient to generate language models capable of accurately predicting human language processing.",
            "corpus_id": 265506572,
            "sentences": [
                {
                    "corpus_id": "265506572",
                    "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
                    "text": "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see Soviany et al. (2022); Wang et al. (2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
                    "score": 0.3837117580874127,
                    "section_title": "Background",
                    "char_start_offset": 3761,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 532
                        },
                        {
                            "start": 535,
                            "end": 824
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 41,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 454,
                            "end": 471,
                            "matchedPaperCorpusId": "220045816"
                        },
                        {
                            "start": 490,
                            "end": 511,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 513,
                            "end": 531,
                            "matchedPaperCorpusId": "232362223"
                        },
                        {
                            "start": 804,
                            "end": 823,
                            "matchedPaperCorpusId": "232362223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91162109375
                }
            ],
            "relevance_judgement": 0.91162109375,
            "relevance_judgment_input_expanded": "# Title: Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?\n# Venue: Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning\n# Authors: Aryaman Chobey, Oliver Smith, Anzi Wang, Grusha Prasad\n## Abstract\nThe use of neural language models to model human behavior has met with mixed success. While some work has found that the surprisal estimates from these models can be used to predict a wide range of human neural and behavioral responses, other work studying more complex syntactic phenomena has found that these surprisal estimates generate incorrect behavioral predictions. This paper explores the extent to which the misalignment between empirical and model-predicted behavior can be minimized by training models on more developmentally plausible data, such as in the BabyLM Challenge. We trained teacher language models on the BabyLM\"strict-small\"dataset and used sentence level surprisal estimates from these teacher models to create a curriculum. We found tentative evidence that our curriculum made it easier for models to acquire linguistic knowledge from the training data: on the subset of tasks in the BabyLM challenge suite evaluating models' grammatical knowledge of English, models first trained on the BabyLM data curriculum and then on a few randomly ordered training epochs performed slightly better than models trained on randomly ordered epochs alone. This improved linguistic knowledge acquisition did not result in better alignment with human reading behavior, however: models trained on the BabyLM dataset (with or without a curriculum) generated predictions that were as misaligned with human behavior as models trained on larger less curated datasets. This suggests that training on developmentally plausible datasets alone is likely insufficient to generate language models capable of accurately predicting human language processing.\n## Background\nCurriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see Soviany et al. (2022); Wang et al. (2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
            "reference_string": "[265506572 | Chobey et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Evolutionary Data Measures: Understanding the Difficulty of Text Classification Tasks",
            "venue": "Conference on Computational Natural Language Learning",
            "year": 2018,
            "reference_count": 64,
            "citation_count": 29,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/K18-1037.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.01910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2061439167",
                    "name": "Edward Collins"
                },
                {
                    "authorId": "80666414",
                    "name": "N. Rozanov"
                },
                {
                    "authorId": "2117925041",
                    "name": "Bingbing Zhang"
                }
            ],
            "abstract": "Classification tasks are usually analysed and improved through new model architectures or hyperparameter optimisation but the underlying properties of datasets are discovered on an ad-hoc basis as errors occur. However, understanding the properties of the data is crucial in perfecting models. In this paper we analyse exactly which characteristics of a dataset best determine how difficult that dataset is for the task of text classification. We then propose an intuitive measure of difficulty for text classification datasets which is simple and fast to calculate. We show that this measure generalises to unseen data by comparing it to stateof-the-art datasets and results. This measure can be used to analyse the precise source of errors in a dataset and allows fast estimation of how difficult a dataset is to learn. We searched for this measure by training 12 classical and neural network based models on 78 real-world datasets, then use a genetic algorithm to discover the best measure of difficulty. Our difficulty-calculating code1 and datasets2 are publicly available.",
            "corpus_id": 53096414,
            "sentences": [
                {
                    "corpus_id": "53096414",
                    "title": "Evolutionary Data Measures: Understanding the Difficulty of Text Classification Tasks",
                    "text": "When their models do not achieve good results, ML practitioners could potentially calculate thousands of statistics to see what aspects of their datasets are stopping their models from learning. Given this, how do practitioners tell which statistics are the most useful to calculate? Which ones will tell them the most? What changes could they make which will produce the biggest increase in model performance? \n\nIn this work, we have presented two measures of text classification dataset difficulty which can be used as analysis tools and performance estimators. We have shown that these measures generalise to unseen datasets. Our recommended measure can be calculated simply by counting the words and labels of a dataset and is formed by adding five different, unweighted statistics together. As the difficulty measure is an unweighted sum, its components can be examined individually to analyse the sources of difficulty in a dataset. \n\nThere are two main benefits to this difficulty measure. Firstly, it will reduce the time that practitioners need to spend analysing their data in order to improve model scores. As we have demonstrated which statistics are most indicative of dataset difficulty, practitioners need only calculate these to discover the sources of difficulty in their data. Secondly, the difficulty measure can be used as a performance estimator. When practitioners approach new tasks they need only calculate these simple statistics in order to estimate how well models are likely to perform. \n\nFurthermore, this work has shown that for text classification the areas of Class Diversity, Balance and Interference are essential to measure in order to understand difficulty. Data Complexity is also important, but to a lesser extent. \n\nFuture work should firstly experiment with nonlinear but interpretable methods of combining statistics into a difficulty measure such as decision trees. Furthermore, it should apply this difficulty measure to other NLP tasks that may require deeper linguistic knowledge than text classification, such as named entity recognition and parsing. Such tasks may require more advanced features than simple word counts as were used in this work.",
                    "score": 0.3579152567309107,
                    "section_title": "Conclusion",
                    "char_start_offset": 28723,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 410
                        },
                        {
                            "start": 413,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 938
                        },
                        {
                            "start": 941,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1294
                        },
                        {
                            "start": 1295,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1514
                        },
                        {
                            "start": 1517,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1752
                        },
                        {
                            "start": 1755,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2096
                        },
                        {
                            "start": 2097,
                            "end": 2193
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: Evolutionary Data Measures: Understanding the Difficulty of Text Classification Tasks\n# Venue: Conference on Computational Natural Language Learning\n# Authors: Edward Collins, N. Rozanov, Bingbing Zhang\n## Abstract\nClassification tasks are usually analysed and improved through new model architectures or hyperparameter optimisation but the underlying properties of datasets are discovered on an ad-hoc basis as errors occur. However, understanding the properties of the data is crucial in perfecting models. In this paper we analyse exactly which characteristics of a dataset best determine how difficult that dataset is for the task of text classification. We then propose an intuitive measure of difficulty for text classification datasets which is simple and fast to calculate. We show that this measure generalises to unseen data by comparing it to stateof-the-art datasets and results. This measure can be used to analyse the precise source of errors in a dataset and allows fast estimation of how difficult a dataset is to learn. We searched for this measure by training 12 classical and neural network based models on 78 real-world datasets, then use a genetic algorithm to discover the best measure of difficulty. Our difficulty-calculating code1 and datasets2 are publicly available.\n## Conclusion\nWhen their models do not achieve good results, ML practitioners could potentially calculate thousands of statistics to see what aspects of their datasets are stopping their models from learning. Given this, how do practitioners tell which statistics are the most useful to calculate? Which ones will tell them the most? What changes could they make which will produce the biggest increase in model performance? \n\nIn this work, we have presented two measures of text classification dataset difficulty which can be used as analysis tools and performance estimators. We have shown that these measures generalise to unseen datasets. Our recommended measure can be calculated simply by counting the words and labels of a dataset and is formed by adding five different, unweighted statistics together. As the difficulty measure is an unweighted sum, its components can be examined individually to analyse the sources of difficulty in a dataset. \n\nThere are two main benefits to this difficulty measure. Firstly, it will reduce the time that practitioners need to spend analysing their data in order to improve model scores. As we have demonstrated which statistics are most indicative of dataset difficulty, practitioners need only calculate these to discover the sources of difficulty in their data. Secondly, the difficulty measure can be used as a performance estimator. When practitioners approach new tasks they need only calculate these simple statistics in order to estimate how well models are likely to perform. \n\nFurthermore, this work has shown that for text classification the areas of Class Diversity, Balance and Interference are essential to measure in order to understand difficulty. Data Complexity is also important, but to a lesser extent. \n\nFuture work should firstly experiment with nonlinear but interpretable methods of combining statistics into a difficulty measure such as decision trees. Furthermore, it should apply this difficulty measure to other NLP tasks that may require deeper linguistic knowledge than text classification, such as named entity recognition and parsing. Such tasks may require more advanced features than simple word counts as were used in this work.",
            "reference_string": "[53096414 | Collins et al. | 2018 | Citations: 29]"
        },
        {
            "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 34,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2172356226",
                    "name": "Zachary Ankner"
                },
                {
                    "authorId": "73066558",
                    "name": "Cody Blakeney"
                },
                {
                    "authorId": "34824875",
                    "name": "Kartik K. Sreenivasan"
                },
                {
                    "authorId": "2304322161",
                    "name": "Max Marion"
                },
                {
                    "authorId": "2028252288",
                    "name": "Matthew L. Leavitt"
                },
                {
                    "authorId": "1690452",
                    "name": "Mansheej Paul"
                }
            ],
            "abstract": "In this work, we investigate whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models. While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, we investigate whether smaller models can be used for perplexity-based pruning and how pruning is affected by the domain composition of the data being pruned. We demonstrate that for multiple dataset compositions, perplexity-based pruning of pretraining data can \\emph{significantly} improve downstream task performance: pruning based on perplexities computed with a 125 million parameter model improves the average performance on downstream tasks of a 3 billion parameter model by up to 2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach commensurate baseline performance. Furthermore, we demonstrate that such perplexity-based data pruning also yields downstream performance gains in the over-trained and data-constrained regimes.",
            "corpus_id": 270199394,
            "sentences": [
                {
                    "corpus_id": "270199394",
                    "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
                    "text": "A large focus of the machine learning community has been improving the performance of large language models (LLMs) while reducing their training costs.In this work, we consider how to improve the quality of an LLM by improving the quality of its pretraining data.Although there are many techniques to improve data quality, such as augmenting training samples with additional information (Li et al., 2024;Korbak et al., 2023), in this work we focus on the predominant method of data pruning: intelligently selecting a high-quality subset of a larger dataset to train on.\n\nData pruning is commonly used for quality filtering of noisy text data.Simple approaches include using symbolic rules (Bane et al., 2022;Raffel et al., 2020) or using simple classifiers to determine high-quality samples (Wenzek et al., 2020).However, in addition to basic quality filtering, more complex data pruning techniques are also applied to datasets to further improve their quality.Xie et al. (2023b) perform importance resampling where importance scores are calculated based on feature similarity to a target text.Tirumala et al. (2023) prune datasets by deduplicating and diversifying data based on a pretrained language model's embeddings of the text samples.Xie et al. (2023a) re-weight domain proportions based on learnability as determined by a smaller proxy model.Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.\n\nIn this work, we thoroughly investigate the impact that data pruning based on sample perplexity (Marion et al., 2023) has on LLM pretraining.In particular, we focus on the interplay between pretraining dataset composition and pruning methodology.We further evaluate perplexity pruning in the overtrained and data-constrained regimes.We also investigate whether evaluating the quality of data interventions based on upstream test set perplexity is a sound methodology for gauging downstream performance.",
                    "score": 0.36167795445881606,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 151,
                            "end": 263
                        },
                        {
                            "start": 263,
                            "end": 569
                        },
                        {
                            "start": 571,
                            "end": 642
                        },
                        {
                            "start": 642,
                            "end": 813
                        },
                        {
                            "start": 813,
                            "end": 961
                        },
                        {
                            "start": 961,
                            "end": 1094
                        },
                        {
                            "start": 1094,
                            "end": 1241
                        },
                        {
                            "start": 1241,
                            "end": 1350
                        },
                        {
                            "start": 1350,
                            "end": 1568
                        },
                        {
                            "start": 1570,
                            "end": 1711
                        },
                        {
                            "start": 1711,
                            "end": 1816
                        },
                        {
                            "start": 1816,
                            "end": 1903
                        },
                        {
                            "start": 1903,
                            "end": 2072
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 387,
                            "end": 404,
                            "matchedPaperCorpusId": "260866107"
                        },
                        {
                            "start": 404,
                            "end": 424,
                            "matchedPaperCorpusId": "257020046"
                        },
                        {
                            "start": 689,
                            "end": 708,
                            "matchedPaperCorpusId": "252186406"
                        },
                        {
                            "start": 708,
                            "end": 728,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 791,
                            "end": 812,
                            "matchedPaperCorpusId": "207870323"
                        },
                        {
                            "start": 961,
                            "end": 979,
                            "matchedPaperCorpusId": "256627727"
                        },
                        {
                            "start": 1094,
                            "end": 1116,
                            "matchedPaperCorpusId": "261076313"
                        },
                        {
                            "start": 1241,
                            "end": 1259,
                            "matchedPaperCorpusId": "258741043"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                }
            ],
            "relevance_judgement": 0.90283203125,
            "relevance_judgment_input_expanded": "# Title: Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models\n# Venue: International Conference on Learning Representations\n# Authors: Zachary Ankner, Cody Blakeney, Kartik K. Sreenivasan, Max Marion, Matthew L. Leavitt, Mansheej Paul\n## Abstract\nIn this work, we investigate whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models. While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, we investigate whether smaller models can be used for perplexity-based pruning and how pruning is affected by the domain composition of the data being pruned. We demonstrate that for multiple dataset compositions, perplexity-based pruning of pretraining data can \\emph{significantly} improve downstream task performance: pruning based on perplexities computed with a 125 million parameter model improves the average performance on downstream tasks of a 3 billion parameter model by up to 2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach commensurate baseline performance. Furthermore, we demonstrate that such perplexity-based data pruning also yields downstream performance gains in the over-trained and data-constrained regimes.\n## Introduction\nA large focus of the machine learning community has been improving the performance of large language models (LLMs) while reducing their training costs.In this work, we consider how to improve the quality of an LLM by improving the quality of its pretraining data.Although there are many techniques to improve data quality, such as augmenting training samples with additional information (Li et al., 2024;Korbak et al., 2023), in this work we focus on the predominant method of data pruning: intelligently selecting a high-quality subset of a larger dataset to train on.\n\nData pruning is commonly used for quality filtering of noisy text data.Simple approaches include using symbolic rules (Bane et al., 2022;Raffel et al., 2020) or using simple classifiers to determine high-quality samples (Wenzek et al., 2020).However, in addition to basic quality filtering, more complex data pruning techniques are also applied to datasets to further improve their quality.Xie et al. (2023b) perform importance resampling where importance scores are calculated based on feature similarity to a target text.Tirumala et al. (2023) prune datasets by deduplicating and diversifying data based on a pretrained language model's embeddings of the text samples.Xie et al. (2023a) re-weight domain proportions based on learnability as determined by a smaller proxy model.Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.\n\nIn this work, we thoroughly investigate the impact that data pruning based on sample perplexity (Marion et al., 2023) has on LLM pretraining.In particular, we focus on the interplay between pretraining dataset composition and pruning methodology.We further evaluate perplexity pruning in the overtrained and data-constrained regimes.We also investigate whether evaluating the quality of data interventions based on upstream test set perplexity is a sound methodology for gauging downstream performance.",
            "reference_string": "[270199394 | Ankner et al. | 2024 | Citations: 34]"
        },
        {
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-long.104.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.07412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1659451954",
                    "name": "Mohamed Elgaar"
                },
                {
                    "authorId": "143656058",
                    "name": "Hadi Amiri"
                }
            ],
            "abstract": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.",
            "corpus_id": 259370648,
            "sentences": [
                {
                    "corpus_id": "259370648",
                    "title": "HuCurl: Human-induced Curriculum Discovery",
                    "text": "We introduce an effective curriculum learning framework that employs prior knowledge about sample difficulty in its training paradigm for curriculum discovery. The proposed framework initially partitions its input data into several groups of increasing difficulty, defines parameterized func-  Table 2. tions to weight sample losses in each difficulty group, moves samples across difficulty groups based on their learning progress, and enables tuning the parameters of the weight function to discover novel curricula. We demonstrate that this framework is capable of representing several categories of curriculum learning approaches. The task of curriculum discovery alleviates the limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. In addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. The task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. To mitigate the computational : Confidence assignment to samples in our datasets by three CL approaches. The x-axis is the epoch number, and y-axis is the average weight assigned to samples of each difficulty group. Blue (solid) is easy, orange (dashed) is medium, and green (dash-dot) is hard. The shaded area is the 95% CI over the datasets with five random seeds each. The curves are monotonic for most parts, and can be approximated by our framework.\n\ncost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset.\n\nThere are several promising areas for future work. These include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. Finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research.",
                    "score": 0.4001090548068646,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 27435,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8955078125
                }
            ],
            "relevance_judgement": 0.8955078125,
            "relevance_judgment_input_expanded": "# Title: HuCurl: Human-induced Curriculum Discovery\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Mohamed Elgaar, Hadi Amiri\n## Abstract\nWe introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.\n## Conclusion and Future Work\nWe introduce an effective curriculum learning framework that employs prior knowledge about sample difficulty in its training paradigm for curriculum discovery. The proposed framework initially partitions its input data into several groups of increasing difficulty, defines parameterized func-  Table 2. tions to weight sample losses in each difficulty group, moves samples across difficulty groups based on their learning progress, and enables tuning the parameters of the weight function to discover novel curricula. We demonstrate that this framework is capable of representing several categories of curriculum learning approaches. The task of curriculum discovery alleviates the limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. In addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. The task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. To mitigate the computational : Confidence assignment to samples in our datasets by three CL approaches. The x-axis is the epoch number, and y-axis is the average weight assigned to samples of each difficulty group. Blue (solid) is easy, orange (dashed) is medium, and green (dash-dot) is hard. The shaded area is the 95% CI over the datasets with five random seeds each. The curves are monotonic for most parts, and can be approximated by our framework.\n\ncost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset.\n\nThere are several promising areas for future work. These include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. Finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research.",
            "reference_string": "[259370648 | Elgaar et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Understanding Dataset Difficulty with V-Usable Information",
            "venue": "International Conference on Machine Learning",
            "year": 2021,
            "reference_count": 67,
            "citation_count": 252,
            "influential_citation_count": 45,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.08420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "10324691",
                    "name": "Kawin Ethayarajh"
                },
                {
                    "authorId": "1699545",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2133324514",
                    "name": "Swabha Swayamdipta"
                }
            ],
            "abstract": "Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as the lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\\mathcal{V}$. We further introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also permit the converse: for a given model $\\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.",
            "corpus_id": 250340652,
            "sentences": [
                {
                    "corpus_id": "250340652",
                    "title": "Understanding Dataset Difficulty with V-Usable Information",
                    "text": "Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as the lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\\mathcal{V}$. We further introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also permit the converse: for a given model $\\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.",
                    "score": 0.3832814172581209,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                },
                {
                    "corpus_id": "250340652",
                    "title": "Understanding Dataset Difficulty with V-Usable Information",
                    "text": "Xu et al. (2019) show that it can be measured using the predictive V-information framework, which generalizes Shannon information to consider computational constraints. \n\nOur work extends the above framework by framing dataset difficulty as the lack of V-usable information. 1 The higher the V-usable information, the easier the dataset is for V. Not only does this framework allow comparisons of models w.r.t. the same dataset, but also of different datasets w.r.t. the same model. Figure 1 illustrates that different datasets provide different amounts of usable information for the same model, even when the task is identical (i.e., natural language inference in the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) datasets). \n\nBuilding on the aggregate estimate of dataset difficulty, we introduce a measure called pointwise V-information (PVI) for estimating the difficulty of each instance w.r.t. a given distribution ( \u00a73). PVI estimates allow us to compare not only individual instances, but also the difficulty of slices of data w.r.t V. On datasets containing more usable information (e.g., SNLI), PVI estimates are highly correlated (Pearson r \u2265 0.75) across different models, seeds, and training time, and with human judgments of difficulty. \n\nComparisons of V-usable information before and after isolating an input attribute shed light on why the dataset is easy or difficult for V ( \u00a74), which has significant implications for interpretability in AI 2 (Miller, 2019). Specifically, we use V-usable information to identify some limitations in benchmarks that are widely used in NLP to test for a model's understanding of different language phenomena: \n\n\u2022 Word ordering has a limited impact on the difficulty of a popular natural language entailment benchmark, SNLI (Bowman et al., 2015), even though entailment describes a causal relationship. \n\n\u2022 Some of the most difficult instances in SNLI and a popular grammaticality detection benchmark, CoLA (Warstadt et al., 2018), are mislabelled.",
                    "score": 0.3614349632155474,
                    "section_title": "Introduction",
                    "char_start_offset": 1846,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 171,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 743
                        },
                        {
                            "start": 746,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1268
                        },
                        {
                            "start": 1271,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1678
                        },
                        {
                            "start": 1681,
                            "end": 1871
                        },
                        {
                            "start": 1874,
                            "end": 2017
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 674,
                            "end": 695,
                            "matchedPaperCorpusId": "14604520"
                        },
                        {
                            "start": 1481,
                            "end": 1495,
                            "matchedPaperCorpusId": "36024272"
                        },
                        {
                            "start": 1793,
                            "end": 1814,
                            "matchedPaperCorpusId": "14604520"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82861328125
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: Understanding Dataset Difficulty with V-Usable Information\n# Venue: International Conference on Machine Learning\n# Authors: Kawin Ethayarajh, Yejin Choi, Swabha Swayamdipta\n## Abstract\nEstimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as the lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\\mathcal{V}$. We further introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also permit the converse: for a given model $\\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.\n## Introduction\nXu et al. (2019) show that it can be measured using the predictive V-information framework, which generalizes Shannon information to consider computational constraints. \n\nOur work extends the above framework by framing dataset difficulty as the lack of V-usable information. 1 The higher the V-usable information, the easier the dataset is for V. Not only does this framework allow comparisons of models w.r.t. the same dataset, but also of different datasets w.r.t. the same model. Figure 1 illustrates that different datasets provide different amounts of usable information for the same model, even when the task is identical (i.e., natural language inference in the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) datasets). \n\nBuilding on the aggregate estimate of dataset difficulty, we introduce a measure called pointwise V-information (PVI) for estimating the difficulty of each instance w.r.t. a given distribution ( \u00a73). PVI estimates allow us to compare not only individual instances, but also the difficulty of slices of data w.r.t V. On datasets containing more usable information (e.g., SNLI), PVI estimates are highly correlated (Pearson r \u2265 0.75) across different models, seeds, and training time, and with human judgments of difficulty. \n\nComparisons of V-usable information before and after isolating an input attribute shed light on why the dataset is easy or difficult for V ( \u00a74), which has significant implications for interpretability in AI 2 (Miller, 2019). Specifically, we use V-usable information to identify some limitations in benchmarks that are widely used in NLP to test for a model's understanding of different language phenomena: \n\n\u2022 Word ordering has a limited impact on the difficulty of a popular natural language entailment benchmark, SNLI (Bowman et al., 2015), even though entailment describes a causal relationship. \n\n\u2022 Some of the most difficult instances in SNLI and a popular grammaticality detection benchmark, CoLA (Warstadt et al., 2018), are mislabelled.",
            "reference_string": "[250340652 | Ethayarajh et al. | 2021 | Citations: 252]"
        },
        {
            "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 44,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2211.11297",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.11297, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2056108122",
                    "name": "Qi Jia"
                },
                {
                    "authorId": "5826956",
                    "name": "Yizhu Liu"
                },
                {
                    "authorId": "2112389755",
                    "name": "Haifeng Tang"
                },
                {
                    "authorId": "1796651",
                    "name": "Kenny Q. Zhu"
                }
            ],
            "abstract": "Curriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by the \u201ceasy-to-hard\u201d intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words, i.e., do sequence completion, and gradually extends to generate the whole output sequence. Comprehensive experiments show that it generalizes well to different tasks and achieves significant improvements over strong baselines.",
            "corpus_id": 253734400,
            "sentences": [
                {
                    "corpus_id": "253734400",
                    "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
                    "text": "Curriculum learning (CL) proposed by Bengio et al. (2009) provides performance improvements on a number of machine learning tasks. It mimics the learning process of humans by training models with samples in a more meaningful order, i.e., from the easy ones to the hard ones. Therefore, ranking training samples by difficulty lies in the core of CL, which is also the key challenge when it's applied to natural language generation (NLG) tasks.\n\nPrevious work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018;Kocmi and Bojar, 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020;Kumar et al., 2019;Shen and Feng, 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks. For * The corresponding author. example, summarization focuses more on generating concise outputs while style transfer emphasizes style changes. So the former should pay attention to the ratio between the lengths of the output and the input (the more compressed the more difficult), while the latter should focus on differences in style between the input and output (the more different the more difficult). Designing a comprehensive or universal scoring function is difficult or even impossible under this definition of CL.\n\nIn this paper, we propose an alternative to sample-wise CL, which we call in-sample CL (ICL). ICL re-orders the learning sequence within the sample. One particular ICL re-ordering strategy which we find effective is to predict the last few tokens given a long prefix first from the original output, and then gradually increase the number of tokens at the end while shortening the prefix, to create an easy-to-hard training order. Such a curriculum learning strategy focuses more on the difficulty of language generation itself, leading to a better generalization ability among tasks.\n\nActually, we are not the first to propose the idea of ICL. Liang et al. (2021) introduced the notion of \"token-wise curriculum learning(TCL)\". Illustrations",
                    "score": 0.3518616325930283,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 57,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 641,
                            "end": 659,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 659,
                            "end": 681,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 784,
                            "end": 803,
                            "matchedPaperCorpusId": "220047761"
                        },
                        {
                            "start": 822,
                            "end": 842,
                            "matchedPaperCorpusId": "218470266"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.892578125
                }
            ],
            "relevance_judgement": 0.892578125,
            "relevance_judgment_input_expanded": "# Title: In-sample Curriculum Learning by Sequence Completion for Natural Language Generation\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Qi Jia, Yizhu Liu, Haifeng Tang, Kenny Q. Zhu\n## Abstract\nCurriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by the \u201ceasy-to-hard\u201d intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words, i.e., do sequence completion, and gradually extends to generate the whole output sequence. Comprehensive experiments show that it generalizes well to different tasks and achieves significant improvements over strong baselines.\n## Introduction\nCurriculum learning (CL) proposed by Bengio et al. (2009) provides performance improvements on a number of machine learning tasks. It mimics the learning process of humans by training models with samples in a more meaningful order, i.e., from the easy ones to the hard ones. Therefore, ranking training samples by difficulty lies in the core of CL, which is also the key challenge when it's applied to natural language generation (NLG) tasks.\n\nPrevious work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018;Kocmi and Bojar, 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020;Kumar et al., 2019;Shen and Feng, 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks. For * The corresponding author. example, summarization focuses more on generating concise outputs while style transfer emphasizes style changes. So the former should pay attention to the ratio between the lengths of the output and the input (the more compressed the more difficult), while the latter should focus on differences in style between the input and output (the more different the more difficult). Designing a comprehensive or universal scoring function is difficult or even impossible under this definition of CL.\n\nIn this paper, we propose an alternative to sample-wise CL, which we call in-sample CL (ICL). ICL re-orders the learning sequence within the sample. One particular ICL re-ordering strategy which we find effective is to predict the last few tokens given a long prefix first from the original output, and then gradually increase the number of tokens at the end while shortening the prefix, to create an easy-to-hard training order. Such a curriculum learning strategy focuses more on the difficulty of language generation itself, leading to a better generalization ability among tasks.\n\nActually, we are not the first to propose the idea of ICL. Liang et al. (2021) introduced the notion of \"token-wise curriculum learning(TCL)\". Illustrations",
            "reference_string": "[253734400 | Jia et al. | 2022 | Citations: 4]"
        },
        {
            "title": "Symmetric Self-Paced Learning for Domain Generalization",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/29639/31084",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29639?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2110600983",
                    "name": "Di Zhao"
                },
                {
                    "authorId": "34930533",
                    "name": "Yun Sing Koh"
                },
                {
                    "authorId": "2276344048",
                    "name": "Gillian Dobbie"
                },
                {
                    "authorId": "2293565669",
                    "name": "Hongsheng Hu"
                },
                {
                    "authorId": "2293453267",
                    "name": "Philippe Fournier-Viger"
                }
            ],
            "abstract": "Deep learning methods often suffer performance degradation due to domain shift, where discrepancies exist between training and testing data distributions.\nDomain generalization mitigates this problem by leveraging information from multiple source domains to enhance model generalization capabilities for unseen domains.\nHowever, existing domain generalization methods typically present examples to the model in a random manner, overlooking the potential benefits of structured data presentation.\nTo bridge this gap, we propose a novel learning strategy, Symmetric Self-Paced Learning (SSPL), for domain generalization.\nSSPL consists of a Symmetric Self-Paced training scheduler and a Gradient-based Difficulty Measure (GDM).\nSpecifically, the proposed training scheduler initially focuses on easy examples, gradually shifting emphasis to harder examples as training progresses.\nGDM dynamically evaluates example difficulty through the gradient magnitude with respect to the example itself.\nExperiments across five popular benchmark datasets demonstrate the effectiveness of the proposed learning strategy.",
            "corpus_id": 268696658,
            "sentences": [
                {
                    "corpus_id": "268696658",
                    "title": "Symmetric Self-Paced Learning for Domain Generalization",
                    "text": "Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses. \n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty. The GDM is computed with Equation 8, where \u03be x denotes the difficulty of the example x. ColorJitter. An Empirical Risk Minimization (ERM) baseline is also included, which merges data from all source domains without utilizing domain generalization techniques. Evaluation Metrics. We adopt the leave-one-out-test evaluation strategy as the evaluation metric following the prior works (Li et al. 2017b;Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2022). Specifically, we select one domain as the test domain at a time and use the remaining domains as the source domains for training. We report the accuracy for each separate domain. Performance measures are reported as top-1 classification accuracy (%) averaged over ten runs, along with their corresponding 95% confidence intervals. \n\nNetwork Structure. The network structure is chosen by following the previous work (Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2020). In the Digits dataset, images are resized to 32 \u00d7 32 and converted to RGB by replicating channels. The backbone of the neural network is constructed by 3 \u00d7 3 Conv layers (64 kernels), each followed by a",
                    "score": 0.3865058553038846,
                    "section_title": "Gradient-based Difficulty Measure",
                    "char_start_offset": 9632,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 448
                        },
                        {
                            "start": 451,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 853
                        },
                        {
                            "start": 854,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 1925
                        },
                        {
                            "start": 1926,
                            "end": 2077
                        },
                        {
                            "start": 2080,
                            "end": 2098
                        },
                        {
                            "start": 2099,
                            "end": 2217
                        },
                        {
                            "start": 2218,
                            "end": 2316
                        },
                        {
                            "start": 2317,
                            "end": 2420
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1675,
                            "end": 1692,
                            "matchedPaperCorpusId": "6037691"
                        },
                        {
                            "start": 1692,
                            "end": 1713,
                            "matchedPaperCorpusId": "81978372"
                        },
                        {
                            "start": 1713,
                            "end": 1728,
                            "matchedPaperCorpusId": "59553457"
                        },
                        {
                            "start": 2162,
                            "end": 2184,
                            "matchedPaperCorpusId": "81978372"
                        },
                        {
                            "start": 2184,
                            "end": 2199,
                            "matchedPaperCorpusId": "59553457"
                        },
                        {
                            "start": 2199,
                            "end": 2215,
                            "matchedPaperCorpusId": "212718063"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89013671875
                }
            ],
            "relevance_judgement": 0.89013671875,
            "relevance_judgment_input_expanded": "# Title: Symmetric Self-Paced Learning for Domain Generalization\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Di Zhao, Yun Sing Koh, Gillian Dobbie, Hongsheng Hu, Philippe Fournier-Viger\n## Abstract\nDeep learning methods often suffer performance degradation due to domain shift, where discrepancies exist between training and testing data distributions.\nDomain generalization mitigates this problem by leveraging information from multiple source domains to enhance model generalization capabilities for unseen domains.\nHowever, existing domain generalization methods typically present examples to the model in a random manner, overlooking the potential benefits of structured data presentation.\nTo bridge this gap, we propose a novel learning strategy, Symmetric Self-Paced Learning (SSPL), for domain generalization.\nSSPL consists of a Symmetric Self-Paced training scheduler and a Gradient-based Difficulty Measure (GDM).\nSpecifically, the proposed training scheduler initially focuses on easy examples, gradually shifting emphasis to harder examples as training progresses.\nGDM dynamically evaluates example difficulty through the gradient magnitude with respect to the example itself.\nExperiments across five popular benchmark datasets demonstrate the effectiveness of the proposed learning strategy.\n## Gradient-based Difficulty Measure\nCurrent methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses. \n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty. The GDM is computed with Equation 8, where \u03be x denotes the difficulty of the example x. ColorJitter. An Empirical Risk Minimization (ERM) baseline is also included, which merges data from all source domains without utilizing domain generalization techniques. Evaluation Metrics. We adopt the leave-one-out-test evaluation strategy as the evaluation metric following the prior works (Li et al. 2017b;Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2022). Specifically, we select one domain as the test domain at a time and use the remaining domains as the source domains for training. We report the accuracy for each separate domain. Performance measures are reported as top-1 classification accuracy (%) averaged over ten runs, along with their corresponding 95% confidence intervals. \n\nNetwork Structure. The network structure is chosen by following the previous work (Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2020). In the Digits dataset, images are resized to 32 \u00d7 32 and converted to RGB by replicating channels. The backbone of the neural network is constructed by 3 \u00d7 3 Conv layers (64 kernels), each followed by a",
            "reference_string": "[268696658 | Zhao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 17,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.16302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268493467",
                    "name": "Anusha Sabbineni"
                },
                {
                    "authorId": "2268492466",
                    "name": "Nikhil Anand"
                },
                {
                    "authorId": "2268489661",
                    "name": "Maria Minakova"
                }
            ],
            "abstract": "While data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their\"usefulness\"or\"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection.",
            "corpus_id": 265466496,
            "sentences": [
                {
                    "corpus_id": "265466496",
                    "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection",
                    "text": "While data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their\"usefulness\"or\"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection.",
                    "score": 0.3827494020605189,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87158203125
                }
            ],
            "relevance_judgement": 0.87158203125,
            "relevance_judgment_input_expanded": "# Title: Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection\n# Venue: arXiv.org\n# Authors: Anusha Sabbineni, Nikhil Anand, Maria Minakova\n## Abstract\nWhile data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their\"usefulness\"or\"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection.\n",
            "reference_string": "[265466496 | Sabbineni et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Curriculum Optimization for Low-Resource Speech Recognition",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2022,
            "reference_count": 23,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2202.08883",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.08883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2134705213",
                    "name": "Anastasia Kuznetsova"
                },
                {
                    "authorId": "47311290",
                    "name": "Anurag Kumar"
                },
                {
                    "authorId": "2155269740",
                    "name": "Jennifer Drexler Fox"
                },
                {
                    "authorId": "3262036",
                    "name": "Francis M. Tyers"
                }
            ],
            "abstract": "Modern end-to-end speech recognition models show astonishing results in transcribing audio signals into written text. However, conventional data feeding pipelines may be sub-optimal for low-resource speech recognition, which still remains a challenging task. We propose an automated curriculum learning approach to optimize the sequence of training examples based on both the progress of the model while training and prior knowledge about the difficulty of the training examples. We introduce a new difficulty measure called compression ratio that can be used as a scoring function for raw audio in various noise conditions. The proposed method improves speech recognition Word Error Rate performance by up to 33% relative over the baseline system.",
            "corpus_id": 246996522,
            "sentences": [
                {
                    "corpus_id": "246996522",
                    "title": "Curriculum Optimization for Low-Resource Speech Recognition",
                    "text": "[9] studied the dynamics of deep neural networks trained with dynamic instance hardness and showed that the model revisits harder samples more often due to higher variance in gradient values while easier examples tend to stay in the minima as soon as the minima are reached. However, [8] finds that in different empirical settings both learning harder and easy tasks first can benefit the model. \n\nCurriculum learning has been successfully used in natural language processing tasks such as language modelling [5,10] neural machine translation (NMT) [11,12,13], keyword spotting [14], and speech recognition [15,16]. Most commonly two complexity strategies are employed: model competencebased [11,12,9] and data-driven. Complexity measures for data-driven learning include sentence/utterance length [16], language model score, n-gram size [5,10], word frequency ranking, and sentence norm [13]. Speech processing systems can rely on speech to noise ratio (SNR) as a measure of difficulty by gradually blending more and more noise into clean speech signals [15,14] ditionally, we demonstrate that both an external teacher curriculum and learner's progress are important for low-resource ASR. The teacher curriculum acts as a reliable prior while the student can construct its own curriculum in an automated manner based on progress gains. We experiment with compression ratio and text-based difficulty measures to show that the signal-based prior leads to a more optimal solution. \n\nThe remainder of the paper as organized as follows: Section 2 describes the complexity measures and methods; Sections 3 and 4 discuss the experimental setting and the results; Section 5 summarizes our contributions.",
                    "score": 0.38704866882156796,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2165,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 395
                        },
                        {
                            "start": 398,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 893
                        },
                        {
                            "start": 894,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1478
                        },
                        {
                            "start": 1481,
                            "end": 1696
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 284,
                            "end": 287,
                            "matchedPaperCorpusId": "102350936"
                        },
                        {
                            "start": 509,
                            "end": 512,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 512,
                            "end": 515,
                            "matchedPaperCorpusId": "11137059"
                        },
                        {
                            "start": 549,
                            "end": 553,
                            "matchedPaperCorpusId": "20639213"
                        },
                        {
                            "start": 553,
                            "end": 556,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 578,
                            "end": 582,
                            "matchedPaperCorpusId": "231979234"
                        },
                        {
                            "start": 607,
                            "end": 611,
                            "matchedPaperCorpusId": "14928979"
                        },
                        {
                            "start": 611,
                            "end": 614,
                            "matchedPaperCorpusId": "33957080"
                        },
                        {
                            "start": 692,
                            "end": 696,
                            "matchedPaperCorpusId": "20639213"
                        },
                        {
                            "start": 696,
                            "end": 699,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 699,
                            "end": 701,
                            "matchedPaperCorpusId": "227275560"
                        },
                        {
                            "start": 798,
                            "end": 802,
                            "matchedPaperCorpusId": "33957080"
                        },
                        {
                            "start": 838,
                            "end": 841,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 841,
                            "end": 844,
                            "matchedPaperCorpusId": "11137059"
                        },
                        {
                            "start": 1055,
                            "end": 1059,
                            "matchedPaperCorpusId": "14928979"
                        },
                        {
                            "start": 1059,
                            "end": 1062,
                            "matchedPaperCorpusId": "231979234"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.869140625
                }
            ],
            "relevance_judgement": 0.869140625,
            "relevance_judgment_input_expanded": "# Title: Curriculum Optimization for Low-Resource Speech Recognition\n# Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing\n# Authors: Anastasia Kuznetsova, Anurag Kumar, Jennifer Drexler Fox, Francis M. Tyers\n## Abstract\nModern end-to-end speech recognition models show astonishing results in transcribing audio signals into written text. However, conventional data feeding pipelines may be sub-optimal for low-resource speech recognition, which still remains a challenging task. We propose an automated curriculum learning approach to optimize the sequence of training examples based on both the progress of the model while training and prior knowledge about the difficulty of the training examples. We introduce a new difficulty measure called compression ratio that can be used as a scoring function for raw audio in various noise conditions. The proposed method improves speech recognition Word Error Rate performance by up to 33% relative over the baseline system.\n## INTRODUCTION\n[9] studied the dynamics of deep neural networks trained with dynamic instance hardness and showed that the model revisits harder samples more often due to higher variance in gradient values while easier examples tend to stay in the minima as soon as the minima are reached. However, [8] finds that in different empirical settings both learning harder and easy tasks first can benefit the model. \n\nCurriculum learning has been successfully used in natural language processing tasks such as language modelling [5,10] neural machine translation (NMT) [11,12,13], keyword spotting [14], and speech recognition [15,16]. Most commonly two complexity strategies are employed: model competencebased [11,12,9] and data-driven. Complexity measures for data-driven learning include sentence/utterance length [16], language model score, n-gram size [5,10], word frequency ranking, and sentence norm [13]. Speech processing systems can rely on speech to noise ratio (SNR) as a measure of difficulty by gradually blending more and more noise into clean speech signals [15,14] ditionally, we demonstrate that both an external teacher curriculum and learner's progress are important for low-resource ASR. The teacher curriculum acts as a reliable prior while the student can construct its own curriculum in an automated manner based on progress gains. We experiment with compression ratio and text-based difficulty measures to show that the signal-based prior leads to a more optimal solution. \n\nThe remainder of the paper as organized as follows: Section 2 describes the complexity measures and methods; Sections 3 and 4 discuss the experimental setting and the results; Section 5 summarizes our contributions.",
            "reference_string": "[246996522 | Kuznetsova et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 51,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347167772",
                    "name": "Chengyin Xu"
                },
                {
                    "authorId": "2347098852",
                    "name": "Kaiyuan Chen"
                },
                {
                    "authorId": "2347490159",
                    "name": "Xiao Li"
                },
                {
                    "authorId": "2344089410",
                    "name": "Ke Shen"
                },
                {
                    "authorId": "2344501830",
                    "name": "Chenggang Li"
                }
            ],
            "abstract": "The escalating scale and cost of Large Language Models (LLMs) training necessitate accurate pre-training prediction of downstream task performance for efficient resource allocation. This is challenged by: 1) the emergence phenomenon, where metrics become meaningful only after extensive training, hindering prediction by smaller models; and 2) uneven task difficulty and inconsistent performance scaling patterns, leading to high metric variability. Current prediction methods lack accuracy and reliability. We propose a Clustering-On-Difficulty (COD) framework for downstream performance prediction. The COD framework clusters tasks by their difficulty scaling features, thereby establishing a more stable and predictable support subset through the exclusion of tasks exhibiting non-emergent behavior or irregular scaling. We adopt a performance scaling law to predict cluster-wise performance with theoretical support. Predictable subset performance acts as an intermediate predictor for the full evaluation set. We further derive a mapping function to accurately extrapolate the performance of the subset to the full set. Applied to an LLM with 70B parameters, COD achieved a 1.36% average prediction error across eight key LLM benchmarks, offering actionable insights for resource allocation and training monitoring of LLMs pretraining.",
            "corpus_id": 276574659,
            "sentences": [
                {
                    "corpus_id": "276574659",
                    "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
                    "text": "The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the\"emergence phenomenon\", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.",
                    "score": 0.361267713854594,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8662109375
                }
            ],
            "relevance_judgement": 0.8662109375,
            "relevance_judgment_input_expanded": "# Title: Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective\n# Venue: arXiv.org\n# Authors: Chengyin Xu, Kaiyuan Chen, Xiao Li, Ke Shen, Chenggang Li\n## Abstract\nThe escalating scale and cost of Large Language Models (LLMs) training necessitate accurate pre-training prediction of downstream task performance for efficient resource allocation. This is challenged by: 1) the emergence phenomenon, where metrics become meaningful only after extensive training, hindering prediction by smaller models; and 2) uneven task difficulty and inconsistent performance scaling patterns, leading to high metric variability. Current prediction methods lack accuracy and reliability. We propose a Clustering-On-Difficulty (COD) framework for downstream performance prediction. The COD framework clusters tasks by their difficulty scaling features, thereby establishing a more stable and predictable support subset through the exclusion of tasks exhibiting non-emergent behavior or irregular scaling. We adopt a performance scaling law to predict cluster-wise performance with theoretical support. Predictable subset performance acts as an intermediate predictor for the full evaluation set. We further derive a mapping function to accurately extrapolate the performance of the subset to the full set. Applied to an LLM with 70B parameters, COD achieved a 1.36% average prediction error across eight key LLM benchmarks, offering actionable insights for resource allocation and training monitoring of LLMs pretraining.\n",
            "reference_string": "[276574659 | Xu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 86,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.12499",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.12499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48810605",
                    "name": "Fenia Christopoulou"
                },
                {
                    "authorId": "2346538",
                    "name": "Gerasimos Lampouras"
                },
                {
                    "authorId": "2676143",
                    "name": "Ignacio Iacobacci"
                }
            ],
            "abstract": "Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.",
            "corpus_id": 247694098,
            "sentences": [
                {
                    "corpus_id": "247694098",
                    "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
                    "text": "Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.",
                    "score": 0.3937872470658935,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.86328125,
            "relevance_judgment_input_expanded": "# Title: Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Fenia Christopoulou, Gerasimos Lampouras, Ignacio Iacobacci\n## Abstract\nCurriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.\n",
            "reference_string": "[247694098 | Christopoulou et al. | 2022 | Citations: 4]"
        },
        {
            "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 61,
            "citation_count": 211,
            "influential_citation_count": 33,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.12032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2150655891",
                    "name": "Ming Li"
                },
                {
                    "authorId": "2144289768",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "2111336489",
                    "name": "Zhitao Li"
                },
                {
                    "authorId": "1391200710",
                    "name": "Jiuhai Chen"
                },
                {
                    "authorId": "2108451006",
                    "name": "Lichang Chen"
                },
                {
                    "authorId": "145292435",
                    "name": "Ning Cheng"
                },
                {
                    "authorId": "66063851",
                    "name": "Jianzong Wang"
                },
                {
                    "authorId": "2213956781",
                    "name": "Tianyi Zhou"
                },
                {
                    "authorId": "91353860",
                    "name": "Jing Xiao"
                }
            ],
            "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model\u2019s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available.",
            "corpus_id": 261076515,
            "sentences": [
                {
                    "corpus_id": "261076515",
                    "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
                    "text": "In our study, we introduce a novel approach for autonomously identifying the most impactful training samples, which we refer to as \"cherry data\", from extensive open-source datasets. These data samples are particularly effective in enhancing LLM instruction tuning. Central to our hypothesis is the idea that LLMs, through initial training with a small amount of instruction data, can inherently learn to discern and follow instructions, allowing them to estimate the difficulty of instruction data. \n\nOur method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\". This methodology, which emphasizes data quality over quantity, differs markedly from existing techniques that rely on external models for data curation. \n\nExtensive experimental results validate the efficacy of our method. By applying our methodology to the Alpaca and WizardLM instruction tuning datasets, our model outperforms the official Alpaca model with only approximately 5% data selected and outperforms the reimplemented WizardLM model with approximately 10% data selected. The key contributions of this paper: \n\n\u2022 We propose a self-guided approach enabling models to autonomously select the \"cherry data\" from vast open-source datasets. This innovation minimizes manual curation and optimizes the use of existing data resources, reducing costs and streamlining training.",
                    "score": 0.3800005381345688,
                    "section_title": "Introduction",
                    "char_start_offset": 1752,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 499
                        },
                        {
                            "start": 502,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1127
                        },
                        {
                            "start": 1128,
                            "end": 1341
                        },
                        {
                            "start": 1342,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1687
                        },
                        {
                            "start": 1690,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 2017
                        },
                        {
                            "start": 2018,
                            "end": 2054
                        },
                        {
                            "start": 2057,
                            "end": 2181
                        },
                        {
                            "start": 2182,
                            "end": 2315
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8603515625
                }
            ],
            "relevance_judgement": 0.8603515625,
            "relevance_judgment_input_expanded": "# Title: From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, Jing Xiao\n## Abstract\nIn the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model\u2019s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available.\n## Introduction\nIn our study, we introduce a novel approach for autonomously identifying the most impactful training samples, which we refer to as \"cherry data\", from extensive open-source datasets. These data samples are particularly effective in enhancing LLM instruction tuning. Central to our hypothesis is the idea that LLMs, through initial training with a small amount of instruction data, can inherently learn to discern and follow instructions, allowing them to estimate the difficulty of instruction data. \n\nOur method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\". This methodology, which emphasizes data quality over quantity, differs markedly from existing techniques that rely on external models for data curation. \n\nExtensive experimental results validate the efficacy of our method. By applying our methodology to the Alpaca and WizardLM instruction tuning datasets, our model outperforms the official Alpaca model with only approximately 5% data selected and outperforms the reimplemented WizardLM model with approximately 10% data selected. The key contributions of this paper: \n\n\u2022 We propose a self-guided approach enabling models to autonomously select the \"cherry data\" from vast open-source datasets. This innovation minimizes manual curation and optimizes the use of existing data resources, reducing costs and streamlining training.",
            "reference_string": "[261076515 | Li et al. | 2023 | Citations: 211]"
        },
        {
            "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models",
            "venue": "The Web Conference",
            "year": 2024,
            "reference_count": 20,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3641257",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3589335.3641257?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3589335.3641257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256599610",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2261888564",
                    "name": "Yuwei Zhou"
                },
                {
                    "authorId": "2191043236",
                    "name": "Hong Chen"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ],
            "abstract": "This tutorial focuses on curriculum learning (CL), an important topic in machine learning, which gains an increasing amount of attention in the research community. CL is a learning paradigm that enables machines to learn from easy data to hard data, imitating the meaningful procedure of human learning with curricula. As an easy-to-use plug-in, CL has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision, natural language processing, data mining, reinforcement learning, etc. Therefore, it is essential introducing CL to more scholars and researchers in the machine learning community. However, there have been no tutorials on CL so far, motivating the organization of our tutorial on CL at WWW 2024. To give a comprehensive tutorial on CL, we plan to organize it from the following aspects: (1) theories, (2) approaches, (3) applications, (4) tools and (5) future directions. First, we introduce the motivations, theories and insights behind CL. Second, we advocate novel, high-quality approaches, as well as innovative solutions to the challenging problems in CL. Then we present the applications of CL in various scenarios, followed by some relevant tools. In the end, we discuss open questions and the future direction in the era of large language models. We believe this topic is at the core of the scope of WWW and is attractive to the audience interested in machine learning from both academia and industry.",
            "corpus_id": 269762685,
            "sentences": [
                {
                    "corpus_id": "269762685",
                    "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models",
                    "text": "A general framework for curriculum design consists of two core components: Difficulty Measurer and Training Scheduler, which decide two things respectively: 1) What kind of training data is supposed to be easier than other data?2) When should we present harder data for training, and how much more?Thus, we can divide existing CL methods into two types: when both the Difficulty Measurer and Training Scheduler are designed by human prior knowledge with no data-driven algorithms involved, we call the CL method predefined CL.If any (or both) of the two components are learned by data-driven models or algorithms, then we denote the CL method as automatic CL.\n\nIn the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning.All representative approaches of both categories will be reviewed and discussed in this tutorial.",
                    "score": 0.3554091959757717,
                    "section_title": "Approaches",
                    "char_start_offset": 5464,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 228,
                            "end": 298
                        },
                        {
                            "start": 298,
                            "end": 526
                        },
                        {
                            "start": 526,
                            "end": 659
                        },
                        {
                            "start": 661,
                            "end": 717
                        },
                        {
                            "start": 717,
                            "end": 822
                        },
                        {
                            "start": 822,
                            "end": 970
                        },
                        {
                            "start": 970,
                            "end": 1060
                        },
                        {
                            "start": 1060,
                            "end": 1155
                        },
                        {
                            "start": 1155,
                            "end": 1269
                        },
                        {
                            "start": 1269,
                            "end": 1422
                        },
                        {
                            "start": 1422,
                            "end": 1519
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85546875
                }
            ],
            "relevance_judgement": 0.85546875,
            "relevance_judgment_input_expanded": "# Title: Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models\n# Venue: The Web Conference\n# Authors: Xin Wang, Yuwei Zhou, Hong Chen, Wenwu Zhu\n## Abstract\nThis tutorial focuses on curriculum learning (CL), an important topic in machine learning, which gains an increasing amount of attention in the research community. CL is a learning paradigm that enables machines to learn from easy data to hard data, imitating the meaningful procedure of human learning with curricula. As an easy-to-use plug-in, CL has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision, natural language processing, data mining, reinforcement learning, etc. Therefore, it is essential introducing CL to more scholars and researchers in the machine learning community. However, there have been no tutorials on CL so far, motivating the organization of our tutorial on CL at WWW 2024. To give a comprehensive tutorial on CL, we plan to organize it from the following aspects: (1) theories, (2) approaches, (3) applications, (4) tools and (5) future directions. First, we introduce the motivations, theories and insights behind CL. Second, we advocate novel, high-quality approaches, as well as innovative solutions to the challenging problems in CL. Then we present the applications of CL in various scenarios, followed by some relevant tools. In the end, we discuss open questions and the future direction in the era of large language models. We believe this topic is at the core of the scope of WWW and is attractive to the audience interested in machine learning from both academia and industry.\n## Approaches\nA general framework for curriculum design consists of two core components: Difficulty Measurer and Training Scheduler, which decide two things respectively: 1) What kind of training data is supposed to be easier than other data?2) When should we present harder data for training, and how much more?Thus, we can divide existing CL methods into two types: when both the Difficulty Measurer and Training Scheduler are designed by human prior knowledge with no data-driven algorithms involved, we call the CL method predefined CL.If any (or both) of the two components are learned by data-driven models or algorithms, then we denote the CL method as automatic CL.\n\nIn the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning.All representative approaches of both categories will be reviewed and discussed in this tutorial.",
            "reference_string": "[269762685 | Wang et al. | 2024 | Citations: 4]"
        },
        {
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 31,
            "citation_count": 112,
            "influential_citation_count": 14,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.00739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49469742",
                    "name": "Xuan Zhang"
                },
                {
                    "authorId": "48387892",
                    "name": "Manish Kumar"
                },
                {
                    "authorId": "3115181",
                    "name": "Huda Khayrallah"
                },
                {
                    "authorId": "38730896",
                    "name": "Kenton Murray"
                },
                {
                    "authorId": "3456371",
                    "name": "Jeremy Gwinnup"
                },
                {
                    "authorId": "3219152",
                    "name": "Marianna J. Martindale"
                },
                {
                    "authorId": "145324163",
                    "name": "Paul McNamee"
                },
                {
                    "authorId": "1800354",
                    "name": "Kevin Duh"
                },
                {
                    "authorId": "2954727",
                    "name": "Marine Carpuat"
                }
            ],
            "abstract": "Machine translation systems based on deep neural networks are expensive to train. Curriculum learning aims to address this issue by choosing the order in which samples are presented during training to help train better models faster. We adopt a probabilistic view of curriculum learning, which lets us flexibly evaluate the impact of curricula design, and perform an extensive exploration on a German-English translation task. Results show that it is possible to improve convergence time at no loss in translation quality. However, results are highly sensitive to the choice of sample difficulty criteria, curriculum schedule and other hyperparameters.",
            "corpus_id": 53295888,
            "sentences": [
                {
                    "corpus_id": "53295888",
                    "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
                    "text": "We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018). \n\nWe conduct an extensive empirical exploration of curriculum learning on a German-English translation task, implementing all training strategies in the Sockeye NMT toolkit. 1 . Our experiments confirm that curriculum learning can improve convergence speed without loss of translation quality, and show that viewing curriculum learning more flexibly than strictly training on easy samples first has some benefits. We also demonstrate that curriculum learning is highly sensitive to hyperpa-rameters, and no clear single best strategy emerges from the experiments. \n\nIn this sense, our conclusions are both positive and negative: We have confirmed that curriculum learning can be an effective method for training expensive models like those in NMT, but careful design of the specific curriculum hyperparameters is important in practice. \n\n2 Related Work Bengio et al. (2009) coined the term of curriculum learning to refer to techniques that guide the training of learning systems \"by choosing which examples to present and in which order to present them in the learning system\", and hypothesize that training on easier samples first is beneficial. While organizing training samples based on difficulty has been demonstrated in NLP outside of neural models -e.g., Spitkovsky et al. (2010) bootstrap unsupervised dependency parsers by learning from incrementally longer sentences -curriculum learning has gained popularity to address the difficult optimization problem of training deep neural models (Bengio, 2012). Bengio et al. (2009) improve neural language model training using a curriculum based on increasing vocabulary size. More recently, Tsvetkov et al. (2016) improve word embedding training using Bayesian optimization to order paragraphs in the training corpus based on a range of distributional and linguistic features (diversity, simplicity, prototypicality).",
                    "score": 0.3746168734416545,
                    "section_title": "Introduction",
                    "char_start_offset": 1954,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 265
                        },
                        {
                            "start": 268,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 829
                        },
                        {
                            "start": 832,
                            "end": 1101
                        },
                        {
                            "start": 1104,
                            "end": 1413
                        },
                        {
                            "start": 1414,
                            "end": 1779
                        },
                        {
                            "start": 1780,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 2137
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 200,
                            "end": 220,
                            "matchedPaperCorpusId": "550225"
                        },
                        {
                            "start": 220,
                            "end": 246,
                            "matchedPaperCorpusId": "7921428"
                        },
                        {
                            "start": 246,
                            "end": 264,
                            "matchedPaperCorpusId": "20639213"
                        },
                        {
                            "start": 1119,
                            "end": 1139,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 1529,
                            "end": 1553,
                            "matchedPaperCorpusId": "1363892"
                        },
                        {
                            "start": 1780,
                            "end": 1800,
                            "matchedPaperCorpusId": "873046"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85400390625
                },
                {
                    "corpus_id": "53295888",
                    "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
                    "text": "Curriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009;Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia). \n\nIn this paper, we conduct an empirical exploration of curriculum learning for Neural Machine Translation (NMT). NMT is a good test case for curriculum learning as training is prohibitively slow in the large data conditions required to reach good performance (Koehn and Knowles, 2017). However, designing a curriculum for NMT training is a complex problem. First, it is not clear how to quantify sample difficulty for this task. Second, NMT systems already rely on established data organization methods to deal with the scale and varying length of training samples (Khomenko et al., 2016;Doetsch et al., 2017;Sennrich et al., 2017;Hieber et al., 2017), and it is not clear how a curriculum should interact with these existing design decisions. Kocmi and Bojar (2017) showed that constructing and ordering mini-batches based on sample length or word frequency helps when training for one epoch. It remains to be seen how curricula impact training until convergence. \n\nTo address these issues, we adopt a probabilistic view of curriculum learning that lets us explore a wide range of curricula flexibly. Our approach does not order samples in a deterministic fashion. Instead, each sample has a probability of being selected for training, and this probability changes depending on the difficulty of the sample and on the curriculum's schedule. We explore difficulty criteria based on NMT model scores as well as linguistic properties. We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018).",
                    "score": 0.38419889265292506,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 504
                        },
                        {
                            "start": 507,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1470
                        },
                        {
                            "start": 1473,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1671
                        },
                        {
                            "start": 1672,
                            "end": 1847
                        },
                        {
                            "start": 1848,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2204
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 41,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 409,
                            "end": 430,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 765,
                            "end": 790,
                            "matchedPaperCorpusId": "11080756"
                        },
                        {
                            "start": 1071,
                            "end": 1094,
                            "matchedPaperCorpusId": "14333788"
                        },
                        {
                            "start": 1115,
                            "end": 1137,
                            "matchedPaperCorpusId": "905565"
                        },
                        {
                            "start": 1250,
                            "end": 1272,
                            "matchedPaperCorpusId": "26468344"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80126953125
                }
            ],
            "relevance_judgement": 0.85400390625,
            "relevance_judgment_input_expanded": "# Title: An Empirical Exploration of Curriculum Learning for Neural Machine Translation\n# Venue: arXiv.org\n# Authors: Xuan Zhang, Manish Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J. Martindale, Paul McNamee, Kevin Duh, Marine Carpuat\n## Abstract\nMachine translation systems based on deep neural networks are expensive to train. Curriculum learning aims to address this issue by choosing the order in which samples are presented during training to help train better models faster. We adopt a probabilistic view of curriculum learning, which lets us flexibly evaluate the impact of curricula design, and perform an extensive exploration on a German-English translation task. Results show that it is possible to improve convergence time at no loss in translation quality. However, results are highly sensitive to the choice of sample difficulty criteria, curriculum schedule and other hyperparameters.\n## Introduction\nCurriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009;Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia). \n\nIn this paper, we conduct an empirical exploration of curriculum learning for Neural Machine Translation (NMT). NMT is a good test case for curriculum learning as training is prohibitively slow in the large data conditions required to reach good performance (Koehn and Knowles, 2017). However, designing a curriculum for NMT training is a complex problem. First, it is not clear how to quantify sample difficulty for this task. Second, NMT systems already rely on established data organization methods to deal with the scale and varying length of training samples (Khomenko et al., 2016;Doetsch et al., 2017;Sennrich et al., 2017;Hieber et al., 2017), and it is not clear how a curriculum should interact with these existing design decisions. Kocmi and Bojar (2017) showed that constructing and ordering mini-batches based on sample length or word frequency helps when training for one epoch. It remains to be seen how curricula impact training until convergence. \n\nTo address these issues, we adopt a probabilistic view of curriculum learning that lets us explore a wide range of curricula flexibly. Our approach does not order samples in a deterministic fashion. Instead, each sample has a probability of being selected for training, and this probability changes depending on the difficulty of the sample and on the curriculum's schedule. We explore difficulty criteria based on NMT model scores as well as linguistic properties. We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018).\n...\nWe consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018). \n\nWe conduct an extensive empirical exploration of curriculum learning on a German-English translation task, implementing all training strategies in the Sockeye NMT toolkit. 1 . Our experiments confirm that curriculum learning can improve convergence speed without loss of translation quality, and show that viewing curriculum learning more flexibly than strictly training on easy samples first has some benefits. We also demonstrate that curriculum learning is highly sensitive to hyperpa-rameters, and no clear single best strategy emerges from the experiments. \n\nIn this sense, our conclusions are both positive and negative: We have confirmed that curriculum learning can be an effective method for training expensive models like those in NMT, but careful design of the specific curriculum hyperparameters is important in practice. \n\n2 Related Work Bengio et al. (2009) coined the term of curriculum learning to refer to techniques that guide the training of learning systems \"by choosing which examples to present and in which order to present them in the learning system\", and hypothesize that training on easier samples first is beneficial. While organizing training samples based on difficulty has been demonstrated in NLP outside of neural models -e.g., Spitkovsky et al. (2010) bootstrap unsupervised dependency parsers by learning from incrementally longer sentences -curriculum learning has gained popularity to address the difficult optimization problem of training deep neural models (Bengio, 2012). Bengio et al. (2009) improve neural language model training using a curriculum based on increasing vocabulary size. More recently, Tsvetkov et al. (2016) improve word embedding training using Bayesian optimization to order paragraphs in the training corpus based on a range of distributional and linguistic features (diversity, simplicity, prototypicality).",
            "reference_string": "[53295888 | Zhang et al. | 2018 | Citations: 112]"
        },
        {
            "title": "Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts",
            "venue": "Workshop on Innovative Use of NLP for Building Educational Applications",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.09482, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9225039",
                    "name": "Donya Rooein"
                },
                {
                    "authorId": "2043232919",
                    "name": "Paul R\u00f6ttger"
                },
                {
                    "authorId": "1583917976",
                    "name": "Anastassia Shaitarova"
                },
                {
                    "authorId": "2267334203",
                    "name": "Dirk Hovy"
                }
            ],
            "abstract": "Using large language models (LLMs) for educational applications like dialogue-based teaching is a hot topic. Effective teaching, however, requires teachers to adapt the difficulty of content and explanations to the education level of their students. Even the best LLMs today struggle to do this well. If we want to improve LLMs on this adaptation task, we need to be able to measure adaptation success reliably. However, current Static metrics for text difficulty, like the Flesch-Kincaid Reading Ease score, are known to be crude and brittle. We, therefore, introduce and evaluate a new set of Prompt-based metrics for text difficulty. Based on a user study, we create Prompt-based metrics as inputs for LLMs. They leverage LLM\u2019s general language understanding capabilities to capture more abstract and complex features than Static metrics. Regression experiments show that adding our Prompt-based metrics significantly improves text difficulty classification over Static metrics alone. Our results demonstrate the promise of using LLMs to evaluate text adaptation to different education levels.",
            "corpus_id": 269773266,
            "sentences": [
                {
                    "corpus_id": "269773266",
                    "title": "Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts",
                    "text": "Using large language models (LLMs) for educational applications like dialogue-based teaching is a hot topic. Effective teaching, however, requires teachers to adapt the difficulty of content and explanations to the education level of their students. Even the best LLMs today struggle to do this well. If we want to improve LLMs on this adaptation task, we need to be able to measure adaptation success reliably. However, current Static metrics for text difficulty, like the Flesch-Kincaid Reading Ease score, are known to be crude and brittle. We, therefore, introduce and evaluate a new set of Prompt-based metrics for text difficulty. Based on a user study, we create Prompt-based metrics as inputs for LLMs. They leverage LLM\u2019s general language understanding capabilities to capture more abstract and complex features than Static metrics. Regression experiments show that adding our Prompt-based metrics significantly improves text difficulty classification over Static metrics alone. Our results demonstrate the promise of using LLMs to evaluate text adaptation to different education levels.",
                    "score": 0.3721991141151649,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85205078125
                }
            ],
            "relevance_judgement": 0.85205078125,
            "relevance_judgment_input_expanded": "# Title: Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts\n# Venue: Workshop on Innovative Use of NLP for Building Educational Applications\n# Authors: Donya Rooein, Paul R\u00f6ttger, Anastassia Shaitarova, Dirk Hovy\n## Abstract\nUsing large language models (LLMs) for educational applications like dialogue-based teaching is a hot topic. Effective teaching, however, requires teachers to adapt the difficulty of content and explanations to the education level of their students. Even the best LLMs today struggle to do this well. If we want to improve LLMs on this adaptation task, we need to be able to measure adaptation success reliably. However, current Static metrics for text difficulty, like the Flesch-Kincaid Reading Ease score, are known to be crude and brittle. We, therefore, introduce and evaluate a new set of Prompt-based metrics for text difficulty. Based on a user study, we create Prompt-based metrics as inputs for LLMs. They leverage LLM\u2019s general language understanding capabilities to capture more abstract and complex features than Static metrics. Regression experiments show that adding our Prompt-based metrics significantly improves text difficulty classification over Static metrics alone. Our results demonstrate the promise of using LLMs to evaluate text adaptation to different education levels.\n",
            "reference_string": "[269773266 | Rooein et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding",
            "venue": "International Conference on Software Engineering",
            "year": 2021,
            "reference_count": 61,
            "citation_count": 80,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2112.02268",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.02268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1455165108",
                    "name": "Deze Wang"
                },
                {
                    "authorId": "10414205",
                    "name": "Zhouyang Jia"
                },
                {
                    "authorId": "50342128",
                    "name": "Shanshan Li"
                },
                {
                    "authorId": null,
                    "name": "Yue Yu"
                },
                {
                    "authorId": "2153657836",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2114050396",
                    "name": "Wei Dong"
                },
                {
                    "authorId": "144078016",
                    "name": "Xiangke Liao"
                }
            ],
            "abstract": "With the great success of pre-trained models, the pretrain-then-fine tune paradigm has been widely adopted on downstream tasks for source code understanding. However, compared to costly training a large-scale model from scratch, how to effectively adapt pre-trained models to a new task has not been fully explored. In this paper, we propose an approach to bridge pre-trained models and code-related tasks. We exploit semantic-preserving transformation to enrich downstream data diversity, and help pre-trained models learn semantic features invariant to these semantically equivalent transformations. Further, we introduce curriculum learning to or-ganize the transformed data in an easy-to-hard manner to fine-tune existing pre-trained models. We apply our approach to a range of pre-trained models, and they significantly outperform the state-of-the-art models on tasks for source code understanding, such as algorithm classification, code clone detection, and code search. Our experiments even show that without heavy pre-training on code data, natural language pre-trained model RoBERTa fine-tuned with our lightweight approach could outperform or rival existing code pre-trained models fine-tuned on the above tasks, such as CodeBERT and GraphCodeBERT. This finding suggests that there is still much room for improvement in code pre-trained models.",
            "corpus_id": 244908620,
            "sentences": [
                {
                    "corpus_id": "244908620",
                    "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding",
                    "text": "The key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3. \n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37,53], and they should be more difficult to learn as verified in Section 2.3. \n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process. \n\nIn particular, it should be noted that in the process of learning the augmented data we do not strictly follow the order of 1 \u2212    programs to  \u2212    programs, since we find that some programs have far more transformed program variants than others and multiple transformations could cause the data to be unbalanced. Therefore, we sample an equal number of augmented samples from the transformed program variants of each sample in the original training set for learning, and the data statistics are shown in Table 2. This method is easy to implement on general models, and we illustrate its effects in the following experiments. \n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy. \n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks. \n\nOnce the scoring function is determined, we still need to define the pace at which we transition from easy samples to harder samples.",
                    "score": 0.38235100636636543,
                    "section_title": "Curriculum Strategy",
                    "char_start_offset": 16540,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 82
                        },
                        {
                            "start": 83,
                            "end": 191
                        },
                        {
                            "start": 194,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 590
                        },
                        {
                            "start": 593,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 880
                        },
                        {
                            "start": 883,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1509
                        },
                        {
                            "start": 1512,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1687
                        },
                        {
                            "start": 1690,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1967
                        },
                        {
                            "start": 1968,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2194
                        },
                        {
                            "start": 2197,
                            "end": 2330
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 515,
                            "end": 518,
                            "matchedPaperCorpusId": "204743994"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8408203125
                }
            ],
            "relevance_judgement": 0.8408203125,
            "relevance_judgment_input_expanded": "# Title: Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding\n# Venue: International Conference on Software Engineering\n# Authors: Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, Xiangke Liao\n## Abstract\nWith the great success of pre-trained models, the pretrain-then-fine tune paradigm has been widely adopted on downstream tasks for source code understanding. However, compared to costly training a large-scale model from scratch, how to effectively adapt pre-trained models to a new task has not been fully explored. In this paper, we propose an approach to bridge pre-trained models and code-related tasks. We exploit semantic-preserving transformation to enrich downstream data diversity, and help pre-trained models learn semantic features invariant to these semantically equivalent transformations. Further, we introduce curriculum learning to or-ganize the transformed data in an easy-to-hard manner to fine-tune existing pre-trained models. We apply our approach to a range of pre-trained models, and they significantly outperform the state-of-the-art models on tasks for source code understanding, such as algorithm classification, code clone detection, and code search. Our experiments even show that without heavy pre-training on code data, natural language pre-trained model RoBERTa fine-tuned with our lightweight approach could outperform or rival existing code pre-trained models fine-tuned on the above tasks, such as CodeBERT and GraphCodeBERT. This finding suggests that there is still much room for improvement in code pre-trained models.\n## Curriculum Strategy\nThe key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3. \n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37,53], and they should be more difficult to learn as verified in Section 2.3. \n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process. \n\nIn particular, it should be noted that in the process of learning the augmented data we do not strictly follow the order of 1 \u2212    programs to  \u2212    programs, since we find that some programs have far more transformed program variants than others and multiple transformations could cause the data to be unbalanced. Therefore, we sample an equal number of augmented samples from the transformed program variants of each sample in the original training set for learning, and the data statistics are shown in Table 2. This method is easy to implement on general models, and we illustrate its effects in the following experiments. \n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy. \n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks. \n\nOnce the scoring function is determined, we still need to define the pace at which we transition from easy samples to harder samples.",
            "reference_string": "[244908620 | Wang et al. | 2021 | Citations: 80]"
        },
        {
            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.06424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116659852",
                    "name": "Joonho Lee"
                },
                {
                    "authorId": "2301016207",
                    "name": "Jae Oh Woo"
                },
                {
                    "authorId": "2301014929",
                    "name": "Juree Seok"
                },
                {
                    "authorId": "2301015562",
                    "name": "Parisa Hassanzadeh"
                },
                {
                    "authorId": "2301015224",
                    "name": "Wooseok Jang"
                },
                {
                    "authorId": "2301016428",
                    "name": "JuYoun Son"
                },
                {
                    "authorId": "91748824",
                    "name": "Sima Didari"
                },
                {
                    "authorId": "2301014892",
                    "name": "Baruch Gutow"
                },
                {
                    "authorId": "2065513368",
                    "name": "Heng Hao"
                },
                {
                    "authorId": "2301015932",
                    "name": "Hankyu Moon"
                },
                {
                    "authorId": "2301166595",
                    "name": "Wenjun Hu"
                },
                {
                    "authorId": "2301413463",
                    "name": "Yeong-Dae Kwon"
                },
                {
                    "authorId": "2301133161",
                    "name": "Taehee Lee"
                },
                {
                    "authorId": "2301015935",
                    "name": "Seungjai Min"
                }
            ],
            "abstract": "Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.",
            "corpus_id": 269741199,
            "sentences": [
                {
                    "corpus_id": "269741199",
                    "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
                    "text": "We propose introducing uncertainty to both the query selection and the RL fine-tuning steps to address these challenges. Moreover, to mitigate the hallucination problem, trustworthy language models should refrain from answering questions when the answer is unknown, which can occur due to ambiguity in the question or the questioner's intent. Cole et al., 2023 proposed that using sampling-based confidence scores to quantify repetition in model outputs is crucial for addressing hallucination issues and helps improve accuracy, especially for ambiguous questions. \n\nCurriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2022). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018;Zhang et al., 2021;Campos, 2021;Weber et al., 2023). \n\nStudies regarding the deployment of CL at the pre-training stage of LMs are focused on reducing the pre-training computational cost and the instability of the auto-regressive training emerging when increasing the models' size, batch size, sequence length, and learning rate. Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). Ranaldi et al., 2023 proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks. \n\nWang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs.",
                    "score": 0.3614550781548812,
                    "section_title": "I. Related Work",
                    "char_start_offset": 80668,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 564
                        },
                        {
                            "start": 567,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 1140
                        },
                        {
                            "start": 1143,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1944
                        },
                        {
                            "start": 1945,
                            "end": 2243
                        },
                        {
                            "start": 2246,
                            "end": 2362
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 343,
                            "end": 360,
                            "matchedPaperCorpusId": "258866001"
                        },
                        {
                            "start": 801,
                            "end": 823,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 1070,
                            "end": 1088,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1088,
                            "end": 1107,
                            "matchedPaperCorpusId": "243766208"
                        },
                        {
                            "start": 1945,
                            "end": 1965,
                            "matchedPaperCorpusId": "265068175"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83544921875
                }
            ],
            "relevance_judgement": 0.83544921875,
            "relevance_judgment_input_expanded": "# Title: Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation\n# Venue: International Conference on Machine Learning\n# Authors: Joonho Lee, Jae Oh Woo, Juree Seok, Parisa Hassanzadeh, Wooseok Jang, JuYoun Son, Sima Didari, Baruch Gutow, Heng Hao, Hankyu Moon, Wenjun Hu, Yeong-Dae Kwon, Taehee Lee, Seungjai Min\n## Abstract\nAssessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.\n## I. Related Work\nWe propose introducing uncertainty to both the query selection and the RL fine-tuning steps to address these challenges. Moreover, to mitigate the hallucination problem, trustworthy language models should refrain from answering questions when the answer is unknown, which can occur due to ambiguity in the question or the questioner's intent. Cole et al., 2023 proposed that using sampling-based confidence scores to quantify repetition in model outputs is crucial for addressing hallucination issues and helps improve accuracy, especially for ambiguous questions. \n\nCurriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2022). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018;Zhang et al., 2021;Campos, 2021;Weber et al., 2023). \n\nStudies regarding the deployment of CL at the pre-training stage of LMs are focused on reducing the pre-training computational cost and the instability of the auto-regressive training emerging when increasing the models' size, batch size, sequence length, and learning rate. Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). Ranaldi et al., 2023 proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks. \n\nWang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs.",
            "reference_string": "[269741199 | Lee et al. | 2024 | Citations: 2]"
        },
        {
            "title": "CBM: Curriculum by Masking",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310341007",
                    "name": "Andrei Jarca"
                },
                {
                    "authorId": "2154573729",
                    "name": "Florinel-Alin Croitoru"
                },
                {
                    "authorId": "2249763264",
                    "name": "R. Ionescu"
                }
            ],
            "abstract": "We propose Curriculum by Masking (CBM), a novel state-of-the-art curriculum learning strategy that effectively creates an easy-to-hard training schedule via patch (token) masking, offering significant accuracy improvements over the conventional training regime and previous curriculum learning (CL) methods. CBM leverages gradient magnitudes to prioritize the masking of salient image regions via a novel masking algorithm and a novel masking block. Our approach enables controlling sample difficulty via the patch masking ratio, generating an effective easy-to-hard curriculum by gradually introducing harder samples as training progresses. CBM operates with two easily configurable parameters, i.e. the number of patches and the curriculum schedule, making it a versatile curriculum learning approach for object recognition and detection. We conduct experiments with various neural architectures, ranging from convolutional networks to vision transformers, on five benchmark data sets (CIFAR-10, CIFAR-100, ImageNet, Food-101 and PASCAL VOC), to compare CBM with conventional as well as curriculum-based training regimes. Our results reveal the superiority of our strategy compared with the state-of-the-art curriculum learning regimes. We also observe improvements in transfer learning contexts, where CBM surpasses previous work by considerable margins in terms of accuracy. We release our code for free non-commercial use at https://github.com/CroitoruAlin/CBM.",
            "corpus_id": 271051051,
            "sentences": [
                {
                    "corpus_id": "271051051",
                    "title": "CBM: Curriculum by Masking",
                    "text": "Curriculum learning is a training technique introduced by Bengio et al. [3], which provides the training examples in a meaningful order, from easy to hard, to neural networks.The objective is to enhance the performance of neural models, while also improving the convergence speed of the training process.Since its introduction, curriculum learning has proven its effectiveness in various domains, such as computer vision [3,7,10,19,25,43,44,45], natural language processing [3,10,30,36,39,47], and signal processing [1,10,40].The method has been very successful and has undergone extensive development, as illustrated in some recent surveys [46,52].These developments range from strategies for measuring data difficulty [3,23,30,38,43,45,48,54,60] to methods focusing on other aspects of the training process [5,10,6,28,44].A wellknown method to apply curriculum learning is by defining a metric that evaluates the complexity of the data, and subsequently arranging the training examples from the simplest to the most challenging ones based on the respective metric.Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images [43,45].In natural language processing, word frequency [3,36] and sequence length [8,30,48,60] are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design [26,38,54].\n\nThe aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input [26], which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model.",
                    "score": 0.46489840021319273,
                    "section_title": "Related Work",
                    "char_start_offset": 4940,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 175,
                            "end": 304
                        },
                        {
                            "start": 304,
                            "end": 526
                        },
                        {
                            "start": 526,
                            "end": 649
                        },
                        {
                            "start": 649,
                            "end": 824
                        },
                        {
                            "start": 824,
                            "end": 1066
                        },
                        {
                            "start": 1066,
                            "end": 1166
                        },
                        {
                            "start": 1166,
                            "end": 1286
                        },
                        {
                            "start": 1286,
                            "end": 1418
                        },
                        {
                            "start": 1418,
                            "end": 1517
                        },
                        {
                            "start": 1519,
                            "end": 1588
                        },
                        {
                            "start": 1588,
                            "end": 1723
                        },
                        {
                            "start": 1723,
                            "end": 1845
                        },
                        {
                            "start": 1845,
                            "end": 1953
                        },
                        {
                            "start": 1953,
                            "end": 2126
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 72,
                            "end": 75,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 421,
                            "end": 424,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 424,
                            "end": 426,
                            "matchedPaperCorpusId": "5658192"
                        },
                        {
                            "start": 429,
                            "end": 432,
                            "matchedPaperCorpusId": "8105909"
                        },
                        {
                            "start": 432,
                            "end": 435,
                            "matchedPaperCorpusId": "51876228"
                        },
                        {
                            "start": 435,
                            "end": 438,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 438,
                            "end": 441,
                            "matchedPaperCorpusId": "220301592"
                        },
                        {
                            "start": 441,
                            "end": 444,
                            "matchedPaperCorpusId": "208138033"
                        },
                        {
                            "start": 474,
                            "end": 477,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 480,
                            "end": 483,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 483,
                            "end": 486,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 486,
                            "end": 489,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 489,
                            "end": 492,
                            "matchedPaperCorpusId": "1900277"
                        },
                        {
                            "start": 516,
                            "end": 519,
                            "matchedPaperCorpusId": "11590585"
                        },
                        {
                            "start": 522,
                            "end": 525,
                            "matchedPaperCorpusId": "19805513"
                        },
                        {
                            "start": 641,
                            "end": 645,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 645,
                            "end": 648,
                            "matchedPaperCorpusId": "232362223"
                        },
                        {
                            "start": 720,
                            "end": 723,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 723,
                            "end": 726,
                            "matchedPaperCorpusId": "879067"
                        },
                        {
                            "start": 726,
                            "end": 729,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 729,
                            "end": 732,
                            "matchedPaperCorpusId": "8502955"
                        },
                        {
                            "start": 732,
                            "end": 735,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 735,
                            "end": 738,
                            "matchedPaperCorpusId": "208138033"
                        },
                        {
                            "start": 738,
                            "end": 741,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 741,
                            "end": 744,
                            "matchedPaperCorpusId": "221995570"
                        },
                        {
                            "start": 744,
                            "end": 747,
                            "matchedPaperCorpusId": "233433844"
                        },
                        {
                            "start": 809,
                            "end": 812,
                            "matchedPaperCorpusId": "231986287"
                        },
                        {
                            "start": 815,
                            "end": 817,
                            "matchedPaperCorpusId": "190000064"
                        },
                        {
                            "start": 817,
                            "end": 820,
                            "matchedPaperCorpusId": "3568073"
                        },
                        {
                            "start": 820,
                            "end": 823,
                            "matchedPaperCorpusId": "220301592"
                        },
                        {
                            "start": 1278,
                            "end": 1282,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 1282,
                            "end": 1285,
                            "matchedPaperCorpusId": "208138033"
                        },
                        {
                            "start": 1333,
                            "end": 1336,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 1336,
                            "end": 1339,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1363,
                            "end": 1366,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 1366,
                            "end": 1369,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 1369,
                            "end": 1372,
                            "matchedPaperCorpusId": "233433844"
                        },
                        {
                            "start": 1506,
                            "end": 1510,
                            "matchedPaperCorpusId": "204539326"
                        },
                        {
                            "start": 1510,
                            "end": 1513,
                            "matchedPaperCorpusId": "8502955"
                        },
                        {
                            "start": 1513,
                            "end": 1516,
                            "matchedPaperCorpusId": "221995570"
                        },
                        {
                            "start": 1683,
                            "end": 1687,
                            "matchedPaperCorpusId": "204539326"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83203125
                }
            ],
            "relevance_judgement": 0.83203125,
            "relevance_judgment_input_expanded": "# Title: CBM: Curriculum by Masking\n# Venue: European Conference on Artificial Intelligence\n# Authors: Andrei Jarca, Florinel-Alin Croitoru, R. Ionescu\n## Abstract\nWe propose Curriculum by Masking (CBM), a novel state-of-the-art curriculum learning strategy that effectively creates an easy-to-hard training schedule via patch (token) masking, offering significant accuracy improvements over the conventional training regime and previous curriculum learning (CL) methods. CBM leverages gradient magnitudes to prioritize the masking of salient image regions via a novel masking algorithm and a novel masking block. Our approach enables controlling sample difficulty via the patch masking ratio, generating an effective easy-to-hard curriculum by gradually introducing harder samples as training progresses. CBM operates with two easily configurable parameters, i.e. the number of patches and the curriculum schedule, making it a versatile curriculum learning approach for object recognition and detection. We conduct experiments with various neural architectures, ranging from convolutional networks to vision transformers, on five benchmark data sets (CIFAR-10, CIFAR-100, ImageNet, Food-101 and PASCAL VOC), to compare CBM with conventional as well as curriculum-based training regimes. Our results reveal the superiority of our strategy compared with the state-of-the-art curriculum learning regimes. We also observe improvements in transfer learning contexts, where CBM surpasses previous work by considerable margins in terms of accuracy. We release our code for free non-commercial use at https://github.com/CroitoruAlin/CBM.\n## Related Work\nCurriculum learning is a training technique introduced by Bengio et al. [3], which provides the training examples in a meaningful order, from easy to hard, to neural networks.The objective is to enhance the performance of neural models, while also improving the convergence speed of the training process.Since its introduction, curriculum learning has proven its effectiveness in various domains, such as computer vision [3,7,10,19,25,43,44,45], natural language processing [3,10,30,36,39,47], and signal processing [1,10,40].The method has been very successful and has undergone extensive development, as illustrated in some recent surveys [46,52].These developments range from strategies for measuring data difficulty [3,23,30,38,43,45,48,54,60] to methods focusing on other aspects of the training process [5,10,6,28,44].A wellknown method to apply curriculum learning is by defining a metric that evaluates the complexity of the data, and subsequently arranging the training examples from the simplest to the most challenging ones based on the respective metric.Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images [43,45].In natural language processing, word frequency [3,36] and sequence length [8,30,48,60] are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design [26,38,54].\n\nThe aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input [26], which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model.",
            "reference_string": "[271051051 | Jarca et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Self-Guided Curriculum Learning for Neural Machine Translation",
            "venue": "International Workshop on Spoken Language Translation",
            "year": 2021,
            "reference_count": 32,
            "citation_count": 17,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.iwslt-1.25.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.04475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144764522",
                    "name": "Lei Zhou"
                },
                {
                    "authorId": "46573238",
                    "name": "Liang Ding"
                },
                {
                    "authorId": "1800354",
                    "name": "Kevin Duh"
                },
                {
                    "authorId": "2293543",
                    "name": "Ryohei Sasano"
                },
                {
                    "authorId": "2874038",
                    "name": "Koichi Takeda"
                }
            ],
            "abstract": "In supervised learning, a well-trained model should be able to recover ground truth accurately, i.e. the predicted labels are expected to resemble the ground truth labels as much as possible. Inspired by this, we formulate a difficulty criterion based on the recovery degrees of training examples. Motivated by the intuition that after skimming through the training corpus, the neural machine translation (NMT) model \u201cknows\u201d how to schedule a suitable curriculum according to learning difficulty, we propose a self-guided curriculum learning strategy that encourages the NMT model to learn from easy to hard on the basis of recovery degrees. Specifically, we adopt sentence-level BLEU score as the proxy of recovery degree. Experimental results on translation benchmarks including WMT14 English-German and WMT17 Chinese-English demonstrate that our proposed method considerably improves the recovery degree, thus consistently improving the translation performance.",
            "corpus_id": 234338422,
            "sentences": [
                {
                    "corpus_id": "234338422",
                    "title": "Self-Guided Curriculum Learning for Neural Machine Translation",
                    "text": "Inspired by the learning behavior of human, Curriculum Learning (CL) for neural network training starts from a basic idea of starting small, namely better to start from easier aspects of a task and then progress towards aspects with increasing level of difficulty (Elman, 1993). Bengio et al. (2009) achieves significant performance boost on tasks by forcing models to learn training examples following an order from \"easy\" to \"difficult\". They further explain CL method with two important constituents, how to rank training examples by learning difficulty, and how to schedule the presentation of training examples based on that rank. * Partial of this work was done when the first author was visiting at CLSP, JHU.\u0177  Figure 1: The NMT model is well-trained on parallel corpus D, {(x 1 , y 1 ), (x 2 , y 2 )} \u2208 D. Taking x 1 and x 2 as the input, the recovery degrees of y 1 are significantly better than that of y 2 . Note that the distance between y i and\u0177 i represents the recovery degrees, which indicate by dashed arrows.\n\nIn the scenario of neural machine translation (NMT), empirical studies have shown that CL strategy contributes to convergence speed and model performance (Zhang et al., 2018;Platanios et al., 2019;Zhang et al., 2019;Liu et al., 2020;Zhan et al., 2021;Ruiter et al., 2020). In these approaches, the learning difficulty of each training example is measured by different difficulty criteria. Early approaches depend on prior knowledge from various sources, including manually crafted features like sentence length and word rarity (Kocmi and Bojar, 2017). The drawback lies in the fact that human understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples, to approximate the perspective of an NMT model. For example, Platanios et al. (2019) turns discrete numerical difficulty scores into relative probabilities and then construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derives",
                    "score": 0.3577488156236219,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 264,
                            "end": 277,
                            "matchedPaperCorpusId": "2105042"
                        },
                        {
                            "start": 1203,
                            "end": 1226,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 1226,
                            "end": 1245,
                            "matchedPaperCorpusId": "155089817"
                        },
                        {
                            "start": 1245,
                            "end": 1262,
                            "matchedPaperCorpusId": "219260306"
                        },
                        {
                            "start": 1262,
                            "end": 1280,
                            "matchedPaperCorpusId": "232104951"
                        },
                        {
                            "start": 1280,
                            "end": 1300,
                            "matchedPaperCorpusId": "222176797"
                        },
                        {
                            "start": 1556,
                            "end": 1579,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 1853,
                            "end": 1876,
                            "matchedPaperCorpusId": "85498775"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83203125
                }
            ],
            "relevance_judgement": 0.83203125,
            "relevance_judgment_input_expanded": "# Title: Self-Guided Curriculum Learning for Neural Machine Translation\n# Venue: International Workshop on Spoken Language Translation\n# Authors: Lei Zhou, Liang Ding, Kevin Duh, Ryohei Sasano, Koichi Takeda\n## Abstract\nIn supervised learning, a well-trained model should be able to recover ground truth accurately, i.e. the predicted labels are expected to resemble the ground truth labels as much as possible. Inspired by this, we formulate a difficulty criterion based on the recovery degrees of training examples. Motivated by the intuition that after skimming through the training corpus, the neural machine translation (NMT) model \u201cknows\u201d how to schedule a suitable curriculum according to learning difficulty, we propose a self-guided curriculum learning strategy that encourages the NMT model to learn from easy to hard on the basis of recovery degrees. Specifically, we adopt sentence-level BLEU score as the proxy of recovery degree. Experimental results on translation benchmarks including WMT14 English-German and WMT17 Chinese-English demonstrate that our proposed method considerably improves the recovery degree, thus consistently improving the translation performance.\n## Introduction\nInspired by the learning behavior of human, Curriculum Learning (CL) for neural network training starts from a basic idea of starting small, namely better to start from easier aspects of a task and then progress towards aspects with increasing level of difficulty (Elman, 1993). Bengio et al. (2009) achieves significant performance boost on tasks by forcing models to learn training examples following an order from \"easy\" to \"difficult\". They further explain CL method with two important constituents, how to rank training examples by learning difficulty, and how to schedule the presentation of training examples based on that rank. * Partial of this work was done when the first author was visiting at CLSP, JHU.\u0177  Figure 1: The NMT model is well-trained on parallel corpus D, {(x 1 , y 1 ), (x 2 , y 2 )} \u2208 D. Taking x 1 and x 2 as the input, the recovery degrees of y 1 are significantly better than that of y 2 . Note that the distance between y i and\u0177 i represents the recovery degrees, which indicate by dashed arrows.\n\nIn the scenario of neural machine translation (NMT), empirical studies have shown that CL strategy contributes to convergence speed and model performance (Zhang et al., 2018;Platanios et al., 2019;Zhang et al., 2019;Liu et al., 2020;Zhan et al., 2021;Ruiter et al., 2020). In these approaches, the learning difficulty of each training example is measured by different difficulty criteria. Early approaches depend on prior knowledge from various sources, including manually crafted features like sentence length and word rarity (Kocmi and Bojar, 2017). The drawback lies in the fact that human understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples, to approximate the perspective of an NMT model. For example, Platanios et al. (2019) turns discrete numerical difficulty scores into relative probabilities and then construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derives",
            "reference_string": "[234338422 | Zhou et al. | 2021 | Citations: 17]"
        },
        {
            "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 38,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.07617",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.07617, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165377996",
                    "name": "Mingyu Lee"
                },
                {
                    "authorId": "8289290",
                    "name": "Jun-Hyung Park"
                },
                {
                    "authorId": "2145466424",
                    "name": "Junho Kim"
                },
                {
                    "authorId": "14694269",
                    "name": "Kang-Min Kim"
                },
                {
                    "authorId": "66593705",
                    "name": "SangKeun Lee"
                }
            ],
            "abstract": "Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model. CCM has two key differences from existing curriculum learning approaches to effectively reflect the nature of MLM. First, we introduce a novel curriculum that evaluates the MLM difficulty of each token based on a carefully-designed linguistic difficulty criterion. Second, we construct a curriculum that masks easy words and phrases first and gradually masks related ones to the previously masked ones based on a knowledge graph. Experimental results show that CCM significantly improves pre-training efficiency. Specifically, the model trained with CCM shows comparative performance with the original BERT on the General Language Understanding Evaluation benchmark at half of the training cost.",
            "corpus_id": 254685579,
            "sentences": [
                {
                    "corpus_id": "254685579",
                    "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
                    "text": "CoLA Recent NLP studies have shown that curriculum learning (CL), presenting examples in an easyto-difficult order rather than presenting them randomly, can accelerate the model convergence and improve the generalization performance (Zhang et al., 2018;Tay et al., 2019;Zhan et al., 2021). There mainly exist two criteria for assessing the difficulty of examples, 1) model-based criteria  and 2) linguisticinspired criteria (Sachan and Xing, 2016;Tay et al., 2019;Nagatsuka et al., 2021;Campos, 2021). Model-based criteria measure the difficulty of each example using task-specific models. However, these criteria are unsuitable for reducing the computation cost of pre-training, given that they require calculating the loss of every example in a large pretraining corpus using language models. In contrast, linguistic-inspired criteria can efficiently assess the difficulty of examples based on prior knowledge and rules. Therefore, we adopt CL with linguistic difficulty criteria into MLM to improve the efficiency of pre-training.\n\nHowever, we argue that existing linguisticsinspired criteria, such as length, rarity, and masking ratio of a sequence, do not effectively reflect the nature of MLM, as verified empirically in Table 1.\n\nThe difficulty associated with MLM is significantly affected by the choice of tokens to be masked in the given sequence, rather than by the given sequence itself. For example, given \"The man is a Stanford <mask> student\", we can easily predict that the masked token would be University, whereas given \"The man is a <mask> University student\", it would be relatively difficult to predict the original token due to the insufficient clues in the context. Then, how can we measure the MLM difficulty? MLM can be viewed as predicting masked tokens based on other contextual tokens related to masked tokens. Therefore, if a word is related to many other words and phrases, it is likely that it has several clues in the context that make MLM easier.\n\nIn this paper, we propose a novel concept-based curriculum masking (CCM) for improving pretraining efficiency by considering the nature of MLM. We consider words and phrases that are related to several other concepts as easy ones and define them as the initial concepts to be masked first. To identify them,",
                    "score": 0.35921931896715387,
                    "section_title": "Methods",
                    "char_start_offset": 784,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 253,
                            "end": 270,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 270,
                            "end": 288,
                            "matchedPaperCorpusId": "232104951"
                        },
                        {
                            "start": 424,
                            "end": 447,
                            "matchedPaperCorpusId": "16503693"
                        },
                        {
                            "start": 447,
                            "end": 464,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 464,
                            "end": 487,
                            "matchedPaperCorpusId": "244048238"
                        },
                        {
                            "start": 487,
                            "end": 500,
                            "matchedPaperCorpusId": "236912520"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83056640625
                },
                {
                    "corpus_id": "254685579",
                    "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
                    "text": "Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model. CCM has two key differences from existing curriculum learning approaches to effectively reflect the nature of MLM. First, we introduce a novel curriculum that evaluates the MLM difficulty of each token based on a carefully-designed linguistic difficulty criterion. Second, we construct a curriculum that masks easy words and phrases first and gradually masks related ones to the previously masked ones based on a knowledge graph. Experimental results show that CCM significantly improves pre-training efficiency. Specifically, the model trained with CCM shows comparative performance with the original BERT on the General Language Understanding Evaluation benchmark at half of the training cost.",
                    "score": 0.36557481361097766,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.810546875
                }
            ],
            "relevance_judgement": 0.83056640625,
            "relevance_judgment_input_expanded": "# Title: Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mingyu Lee, Jun-Hyung Park, Junho Kim, Kang-Min Kim, SangKeun Lee\n## Abstract\nSelf-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model. CCM has two key differences from existing curriculum learning approaches to effectively reflect the nature of MLM. First, we introduce a novel curriculum that evaluates the MLM difficulty of each token based on a carefully-designed linguistic difficulty criterion. Second, we construct a curriculum that masks easy words and phrases first and gradually masks related ones to the previously masked ones based on a knowledge graph. Experimental results show that CCM significantly improves pre-training efficiency. Specifically, the model trained with CCM shows comparative performance with the original BERT on the General Language Understanding Evaluation benchmark at half of the training cost.\n## Methods\nCoLA Recent NLP studies have shown that curriculum learning (CL), presenting examples in an easyto-difficult order rather than presenting them randomly, can accelerate the model convergence and improve the generalization performance (Zhang et al., 2018;Tay et al., 2019;Zhan et al., 2021). There mainly exist two criteria for assessing the difficulty of examples, 1) model-based criteria  and 2) linguisticinspired criteria (Sachan and Xing, 2016;Tay et al., 2019;Nagatsuka et al., 2021;Campos, 2021). Model-based criteria measure the difficulty of each example using task-specific models. However, these criteria are unsuitable for reducing the computation cost of pre-training, given that they require calculating the loss of every example in a large pretraining corpus using language models. In contrast, linguistic-inspired criteria can efficiently assess the difficulty of examples based on prior knowledge and rules. Therefore, we adopt CL with linguistic difficulty criteria into MLM to improve the efficiency of pre-training.\n\nHowever, we argue that existing linguisticsinspired criteria, such as length, rarity, and masking ratio of a sequence, do not effectively reflect the nature of MLM, as verified empirically in Table 1.\n\nThe difficulty associated with MLM is significantly affected by the choice of tokens to be masked in the given sequence, rather than by the given sequence itself. For example, given \"The man is a Stanford <mask> student\", we can easily predict that the masked token would be University, whereas given \"The man is a <mask> University student\", it would be relatively difficult to predict the original token due to the insufficient clues in the context. Then, how can we measure the MLM difficulty? MLM can be viewed as predicting masked tokens based on other contextual tokens related to masked tokens. Therefore, if a word is related to many other words and phrases, it is likely that it has several clues in the context that make MLM easier.\n\nIn this paper, we propose a novel concept-based curriculum masking (CCM) for improving pretraining efficiency by considering the nature of MLM. We consider words and phrases that are related to several other concepts as easy ones and define them as the initial concepts to be masked first. To identify them,",
            "reference_string": "[254685579 | Lee et al. | 2022 | Citations: 12]"
        },
        {
            "title": "Self-Supervised Curriculum Learning for Spelling Error Correction",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 50,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-main.281.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.emnlp-main.281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2141788101",
                    "name": "Zifa Gan"
                },
                {
                    "authorId": "49507285",
                    "name": "Hongfei Xu"
                },
                {
                    "authorId": "144539290",
                    "name": "Hongying Zan"
                }
            ],
            "abstract": "Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model\u2019s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for: 1) scoring the difficulty of training data, and 2) evaluating the competence of the model. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38% F1).",
            "corpus_id": 243865335,
            "sentences": [
                {
                    "corpus_id": "243865335",
                    "title": "Self-Supervised Curriculum Learning for Spelling Error Correction",
                    "text": "Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model\u2019s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for: 1) scoring the difficulty of training data, and 2) evaluating the competence of the model. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38% F1).",
                    "score": 0.3609391653047017,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82958984375
                }
            ],
            "relevance_judgement": 0.82958984375,
            "relevance_judgment_input_expanded": "# Title: Self-Supervised Curriculum Learning for Spelling Error Correction\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Zifa Gan, Hongfei Xu, Hongying Zan\n## Abstract\nSpelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model\u2019s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for: 1) scoring the difficulty of training data, and 2) evaluating the competence of the model. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38% F1).\n",
            "reference_string": "[243865335 | Gan et al. | 2021 | Citations: 13]"
        },
        {
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/26707/26479",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v37i12.26707?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v37i12.26707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8647899",
                    "name": "Zijie Zeng"
                },
                {
                    "authorId": "65953975",
                    "name": "D. Ga\u0161evi\u0107"
                },
                {
                    "authorId": "49901492",
                    "name": "Guangliang Chen"
                }
            ],
            "abstract": "Automatic Text Scoring (ATS) is a widely-investigated task in education. Existing approaches often stressed the structure design of an ATS model and neglected the training process of the model. Considering the difficult nature of this task, we argued that the performance of an ATS model could be potentially boosted by carefully selecting data of varying complexities in the training process. Therefore, we aimed to investigate the effectiveness of curriculum learning (CL) in scoring educational text. Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms.",
            "corpus_id": 259593617,
            "sentences": [
                {
                    "corpus_id": "259593617",
                    "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
                    "text": "Inspired by Wang, Chen, and Zhu (2021), which indicated that CL can be highly effective in enhancing a supervised prediction model when dealing with a difficult task (e.g., the automatic scoring of student-authored responses), we aimed to investigate how CL can be tapped to improve the performance of ATS in education. Formally, our study was guided by the following Research Question: RQ To what extent can curriculum learning strategies boost the performance of ATS methods used in education? \n\nTo answer the above question, we centered our work on the design of the two key components of a CL strategy (Wang, Chen, and Zhu 2021; Liu et al. 2018): (i) difficulty measurer, which determines the relative difficulty level of a training data sample; and (ii) training scheduler, which determines the data subset that should be input to a model in a specific training epoch based on the evaluation from the difficulty measurer. Inspired by previous works on proposing effective CL strategies in the broader NLP research (e.g., Spelling Error Correction (Gan, Xu, and Zan 2021) and Natural Answer Generation (Liu et al. 2018)) as well as the works on automatically characterizing textual data in education, we devised two types of CL strategies in this work, i.e., pre-defined and automatic, which are grouped according to whether any or both of the two key components described above are pre-defined by human experts or automatically learned in a data-driven fashion. It should be noted that these naming terminologies are in line with those summarized by Wang, Chen, and Zhu (2021). Specifically, we studied a total of four pre-defined CL strategies, in which the difficulty level of a piece of written text can be measured via calculating its length, readability, the number of grammatical errors or unique words it contains, and the training scheduler is defined as the linear continuous schedulers (Wang, Chen, and Zhu 2021). As documented in relevant CL studies in computer vision and NLP, in addition to presenting the training data in an easy-to-hard fashion, sometimes a model can achieve better prediction performance by reverting the training order to hard-to-easy (denoted as anti-curriculum).",
                    "score": 0.43073657767330814,
                    "section_title": "Introduction",
                    "char_start_offset": 3568,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 495
                        },
                        {
                            "start": 498,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1466
                        },
                        {
                            "start": 1467,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1928
                        },
                        {
                            "start": 1929,
                            "end": 2203
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 633,
                            "end": 648,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1052,
                            "end": 1075,
                            "matchedPaperCorpusId": "243865335"
                        },
                        {
                            "start": 1106,
                            "end": 1122,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1555,
                            "end": 1581,
                            "matchedPaperCorpusId": "58981386"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.826171875
                }
            ],
            "relevance_judgement": 0.826171875,
            "relevance_judgment_input_expanded": "# Title: On the Effectiveness of Curriculum Learning in Educational Text Scoring\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Zijie Zeng, D. Ga\u0161evi\u0107, Guangliang Chen\n## Abstract\nAutomatic Text Scoring (ATS) is a widely-investigated task in education. Existing approaches often stressed the structure design of an ATS model and neglected the training process of the model. Considering the difficult nature of this task, we argued that the performance of an ATS model could be potentially boosted by carefully selecting data of varying complexities in the training process. Therefore, we aimed to investigate the effectiveness of curriculum learning (CL) in scoring educational text. Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms.\n## Introduction\nInspired by Wang, Chen, and Zhu (2021), which indicated that CL can be highly effective in enhancing a supervised prediction model when dealing with a difficult task (e.g., the automatic scoring of student-authored responses), we aimed to investigate how CL can be tapped to improve the performance of ATS in education. Formally, our study was guided by the following Research Question: RQ To what extent can curriculum learning strategies boost the performance of ATS methods used in education? \n\nTo answer the above question, we centered our work on the design of the two key components of a CL strategy (Wang, Chen, and Zhu 2021; Liu et al. 2018): (i) difficulty measurer, which determines the relative difficulty level of a training data sample; and (ii) training scheduler, which determines the data subset that should be input to a model in a specific training epoch based on the evaluation from the difficulty measurer. Inspired by previous works on proposing effective CL strategies in the broader NLP research (e.g., Spelling Error Correction (Gan, Xu, and Zan 2021) and Natural Answer Generation (Liu et al. 2018)) as well as the works on automatically characterizing textual data in education, we devised two types of CL strategies in this work, i.e., pre-defined and automatic, which are grouped according to whether any or both of the two key components described above are pre-defined by human experts or automatically learned in a data-driven fashion. It should be noted that these naming terminologies are in line with those summarized by Wang, Chen, and Zhu (2021). Specifically, we studied a total of four pre-defined CL strategies, in which the difficulty level of a piece of written text can be measured via calculating its length, readability, the number of grammatical errors or unique words it contains, and the training scheduler is defined as the linear continuous schedulers (Wang, Chen, and Zhu 2021). As documented in relevant CL studies in computer vision and NLP, in addition to presenting the training data in an easy-to-hard fashion, sometimes a model can achieve better prediction performance by reverting the training order to hard-to-easy (denoted as anti-curriculum).",
            "reference_string": "[259593617 | Zeng et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 26,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.11919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356617563",
                    "name": "Qianjin Yu"
                },
                {
                    "authorId": "2355802308",
                    "name": "Keyu Wu"
                },
                {
                    "authorId": "2355823766",
                    "name": "Zihan Chen"
                },
                {
                    "authorId": "2355786652",
                    "name": "Chushu Zhang"
                },
                {
                    "authorId": "2355643664",
                    "name": "Manlin Mei"
                },
                {
                    "authorId": "2356799678",
                    "name": "Lingjun Huang"
                },
                {
                    "authorId": "2355645282",
                    "name": "Fang Tan"
                },
                {
                    "authorId": "2355785017",
                    "name": "Yongsheng Du"
                },
                {
                    "authorId": "2356331597",
                    "name": "Kunlin Liu"
                },
                {
                    "authorId": "2355782385",
                    "name": "Yurui Zhu"
                }
            ],
            "abstract": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks.",
            "corpus_id": 277824032,
            "sentences": [
                {
                    "corpus_id": "277824032",
                    "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
                    "text": "Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;Lee and Song, 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability.",
                    "score": 0.457876626190469,
                    "section_title": "LLM-Adaptive Difficulty Grading",
                    "char_start_offset": 5169,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 890
                        },
                        {
                            "start": 893,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1263
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 551,
                            "end": 570,
                            "matchedPaperCorpusId": "272706931"
                        },
                        {
                            "start": 650,
                            "end": 668,
                            "matchedPaperCorpusId": "275757809"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8232421875
                }
            ],
            "relevance_judgement": 0.8232421875,
            "relevance_judgment_input_expanded": "# Title: Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading\n# Venue: arXiv.org\n# Authors: Qianjin Yu, Keyu Wu, Zihan Chen, Chushu Zhang, Manlin Mei, Lingjun Huang, Fang Tan, Yongsheng Du, Kunlin Liu, Yurui Zhu\n## Abstract\nRecently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks.\n## LLM-Adaptive Difficulty Grading\nTraditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;Lee and Song, 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability.",
            "reference_string": "[277824032 | Yu et al. | 2025 | Citations: 3]"
        },
        {
            "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 62,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.03073",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.03073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2067056655",
                    "name": "Neeraj Varshney"
                },
                {
                    "authorId": "1817207",
                    "name": "Swaroop Mishra"
                },
                {
                    "authorId": "2064619864",
                    "name": "Chitta Baral"
                }
            ],
            "abstract": "Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students\u2019 potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5% instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2% higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.",
            "corpus_id": 247292373,
            "sentences": [
                {
                    "corpus_id": "247292373",
                    "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
                    "text": "and calculate the average predictive correctness for each instance. Finally, we compute the difficulty score by subtracting this averaged correctness value from 1. This ensures that an instance that is answered correctly with high confidence under many training configurations gets assigned a low difficulty score as it corresponds to an easy instance. In contrast, an instance that is often answered incorrectly gets assigned a high difficulty score. Algorithm 1 summarizes this approach.\n\nWe use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1).\n\nWe note that difficulty computation is not our primary contribution. Prior work (Swayamdipta et al., 2020;Xu et al., 2020) has explored different ways to achieve this. However, our approach uses 120 predictions from models trained with different configurations for its computation and hence is more reliable. Equipped with difficulty scores of evaluation instances, we now demonstrate five applications of ILDAE in the following sections.",
                    "score": 0.375857175152596,
                    "section_title": "Method",
                    "char_start_offset": 6076,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1097,
                            "end": 1113,
                            "matchedPaperCorpusId": "220045816"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8203125
                }
            ],
            "relevance_judgement": 0.8203125,
            "relevance_judgment_input_expanded": "# Title: ILDAE: Instance-Level Difficulty Analysis of Evaluation Data\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Neeraj Varshney, Swaroop Mishra, Chitta Baral\n## Abstract\nKnowledge of difficulty level of questions helps a teacher in several ways, such as estimating students\u2019 potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5% instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2% higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.\n## Method\nand calculate the average predictive correctness for each instance. Finally, we compute the difficulty score by subtracting this averaged correctness value from 1. This ensures that an instance that is answered correctly with high confidence under many training configurations gets assigned a low difficulty score as it corresponds to an easy instance. In contrast, an instance that is often answered incorrectly gets assigned a high difficulty score. Algorithm 1 summarizes this approach.\n\nWe use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1).\n\nWe note that difficulty computation is not our primary contribution. Prior work (Swayamdipta et al., 2020;Xu et al., 2020) has explored different ways to achieve this. However, our approach uses 120 predictions from models trained with different configurations for its computation and hence is more reliable. Equipped with difficulty scores of evaluation instances, we now demonstrate five applications of ILDAE in the following sections.",
            "reference_string": "[247292373 | Varshney et al. | 2022 | Citations: 19]"
        },
        {
            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
            "venue": "ACM Transactions on Knowledge Discovery from Data",
            "year": 2022,
            "reference_count": 84,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.07427",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.07427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152350103",
                    "name": "Weiyao Zhu"
                },
                {
                    "authorId": "2061463125",
                    "name": "Ou Wu"
                },
                {
                    "authorId": "2165378509",
                    "name": "Fengguang Su"
                },
                {
                    "authorId": "2165450358",
                    "name": "Yingjun Deng"
                }
            ],
            "abstract": "\u2018\u2018Easy/hard sample\u201d is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.",
            "corpus_id": 248810776,
            "sentences": [
                {
                    "corpus_id": "248810776",
                    "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
                    "text": "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task. Although learning difficulty has no formal and consensus definition, it has been widely discussed and utilized in previous machine learning literature, including noise-aware, curriculum, and metric learning. \n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty. \n\nDue to lack of a theoretical basis, different learning difficulty measures are based on different heuristic cues or empirical observations, resulting that each measure usually only suits specific application scenarios. A clearer understanding of the essence of a sample's learning difficulty can at least facilitate explaining difficulty-based weighting methods and designing more effective learning difficulty measures. However, we are still far from concluding that we have a comprehensive understanding of learning difficulty: \n\n(1) There is no formal definition of the learning difficulty of a sample. Different studies exhibit different understandings of learning difficulty.",
                    "score": 0.4278847912168745,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 568
                        },
                        {
                            "start": 571,
                            "end": 657
                        },
                        {
                            "start": 658,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1761
                        },
                        {
                            "start": 1764,
                            "end": 1982
                        },
                        {
                            "start": 1983,
                            "end": 2184
                        },
                        {
                            "start": 2185,
                            "end": 2293
                        },
                        {
                            "start": 2296,
                            "end": 2369
                        },
                        {
                            "start": 2370,
                            "end": 2444
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 202,
                            "end": 206,
                            "matchedPaperCorpusId": "6644398"
                        },
                        {
                            "start": 206,
                            "end": 209,
                            "matchedPaperCorpusId": "47252984"
                        },
                        {
                            "start": 212,
                            "end": 215,
                            "matchedPaperCorpusId": "231807280"
                        },
                        {
                            "start": 848,
                            "end": 852,
                            "matchedPaperCorpusId": "52902973"
                        },
                        {
                            "start": 852,
                            "end": 855,
                            "matchedPaperCorpusId": "231807280"
                        },
                        {
                            "start": 1162,
                            "end": 1166,
                            "matchedPaperCorpusId": "47252984"
                        },
                        {
                            "start": 1352,
                            "end": 1356,
                            "matchedPaperCorpusId": "207995300"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8095703125
                }
            ],
            "relevance_judgement": 0.8095703125,
            "relevance_judgment_input_expanded": "# Title: Exploring the Learning Difficulty of Data: Theory and Measure\n# Venue: ACM Transactions on Knowledge Discovery from Data\n# Authors: Weiyao Zhu, Ou Wu, Fengguang Su, Yingjun Deng\n## Abstract\n\u2018\u2018Easy/hard sample\u201d is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.\n## Introduction\nThe partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task. Although learning difficulty has no formal and consensus definition, it has been widely discussed and utilized in previous machine learning literature, including noise-aware, curriculum, and metric learning. \n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty. \n\nDue to lack of a theoretical basis, different learning difficulty measures are based on different heuristic cues or empirical observations, resulting that each measure usually only suits specific application scenarios. A clearer understanding of the essence of a sample's learning difficulty can at least facilitate explaining difficulty-based weighting methods and designing more effective learning difficulty measures. However, we are still far from concluding that we have a comprehensive understanding of learning difficulty: \n\n(1) There is no formal definition of the learning difficulty of a sample. Different studies exhibit different understandings of learning difficulty.",
            "reference_string": "[248810776 | Zhu et al. | 2022 | Citations: 6]"
        },
        {
            "title": "SAVA: Scalable Learning-Agnostic Data Valuation",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304474212",
                    "name": "Samuel Kessler"
                },
                {
                    "authorId": "2304718669",
                    "name": "Tam Le"
                },
                {
                    "authorId": "2304715378",
                    "name": "Vu Nguyen"
                }
            ],
            "abstract": "Selecting data for training machine learning models is crucial since large, web-scraped, real datasets contain noisy artifacts that affect the quality and relevance of individual data points. These noisy artifacts will impact model performance. We formulate this problem as a data valuation task, assigning a value to data points in the training set according to how similar or dissimilar they are to a clean and curated validation set. Recently, LAVA demonstrated the use of optimal transport (OT) between a large noisy training dataset and a clean validation set, to value training data efficiently, without the dependency on model performance. However, the LAVA algorithm requires the entire dataset as an input, this limits its application to larger datasets. Inspired by the scalability of stochastic (gradient) approaches which carry out computations on batches of data points instead of the entire dataset, we analogously propose SAVA, a scalable variant of LAVA with its computation on batches of data points. Intuitively, SAVA follows the same scheme as LAVA which leverages the hierarchically defined OT for data valuation. However, while LAVA processes the whole dataset, SAVA divides the dataset into batches of data points, and carries out the OT problem computation on those batches. Moreover, our theoretical derivations on the trade-off of using entropic regularization for OT problems include refinements of prior work. We perform extensive experiments, to demonstrate that SAVA can scale to large datasets with millions of data points and does not trade off data valuation performance.",
            "corpus_id": 270215134,
            "sentences": [
                {
                    "corpus_id": "270215134",
                    "title": "SAVA: Scalable Learning-Agnostic Data Valuation",
                    "text": "One can also prune data by how similar embeddings are to a cluster center or prototype (Sorscher et al., 2022) and by assessing diversity within each cluster (Abbas et al.; Tirumala et al., 2023). It has also been shown that pruning data according to how easy they are to be forgotten over the course of training -as a measure of difficulty -results in training on less data while maintaining performance (Toneva et al., 2018). These data selection methods although related, do not directly measure the importance of each training datapoint with respect to a clean validation set like LAVA (Just et al., 2023) and SAVA. Meta-learning is also used to learn datapoint importance weights by evaluating with a clean validation set (Ren et al., 2018). Similarly to LAVA and SAVA the distributional distance between a clean validation set and a large noisy dataset can be assessed using n-grams in NLP for selecting data to train large language models (Xie et al., 2023).",
                    "score": 0.3922802224520986,
                    "section_title": "G.2.3 SUPERVISED PROTOTYPES",
                    "char_start_offset": 42060,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 965
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 87,
                            "end": 110,
                            "matchedPaperCorpusId": "250113273"
                        },
                        {
                            "start": 173,
                            "end": 195,
                            "matchedPaperCorpusId": "261076313"
                        },
                        {
                            "start": 590,
                            "end": 609,
                            "matchedPaperCorpusId": "258426444"
                        },
                        {
                            "start": 727,
                            "end": 745,
                            "matchedPaperCorpusId": "4321928"
                        },
                        {
                            "start": 946,
                            "end": 964,
                            "matchedPaperCorpusId": "256627727"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80615234375
                }
            ],
            "relevance_judgement": 0.80615234375,
            "relevance_judgment_input_expanded": "# Title: SAVA: Scalable Learning-Agnostic Data Valuation\n# Venue: International Conference on Learning Representations\n# Authors: Samuel Kessler, Tam Le, Vu Nguyen\n## Abstract\nSelecting data for training machine learning models is crucial since large, web-scraped, real datasets contain noisy artifacts that affect the quality and relevance of individual data points. These noisy artifacts will impact model performance. We formulate this problem as a data valuation task, assigning a value to data points in the training set according to how similar or dissimilar they are to a clean and curated validation set. Recently, LAVA demonstrated the use of optimal transport (OT) between a large noisy training dataset and a clean validation set, to value training data efficiently, without the dependency on model performance. However, the LAVA algorithm requires the entire dataset as an input, this limits its application to larger datasets. Inspired by the scalability of stochastic (gradient) approaches which carry out computations on batches of data points instead of the entire dataset, we analogously propose SAVA, a scalable variant of LAVA with its computation on batches of data points. Intuitively, SAVA follows the same scheme as LAVA which leverages the hierarchically defined OT for data valuation. However, while LAVA processes the whole dataset, SAVA divides the dataset into batches of data points, and carries out the OT problem computation on those batches. Moreover, our theoretical derivations on the trade-off of using entropic regularization for OT problems include refinements of prior work. We perform extensive experiments, to demonstrate that SAVA can scale to large datasets with millions of data points and does not trade off data valuation performance.\n## G.2.3 SUPERVISED PROTOTYPES\nOne can also prune data by how similar embeddings are to a cluster center or prototype (Sorscher et al., 2022) and by assessing diversity within each cluster (Abbas et al.; Tirumala et al., 2023). It has also been shown that pruning data according to how easy they are to be forgotten over the course of training -as a measure of difficulty -results in training on less data while maintaining performance (Toneva et al., 2018). These data selection methods although related, do not directly measure the importance of each training datapoint with respect to a clean validation set like LAVA (Just et al., 2023) and SAVA. Meta-learning is also used to learn datapoint importance weights by evaluating with a clean validation set (Ren et al., 2018). Similarly to LAVA and SAVA the distributional distance between a clean validation set and a large noisy dataset can be assessed using n-grams in NLP for selecting data to train large language models (Xie et al., 2023).",
            "reference_string": "[270215134 | Kessler et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 30,
            "citation_count": 64,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.434.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.07552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144026731",
                    "name": "Jason Wei"
                },
                {
                    "authorId": "2150608874",
                    "name": "Chengyu Huang"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                },
                {
                    "authorId": "2153510147",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "1845277073",
                    "name": "Shiqi Xu"
                }
            ],
            "abstract": "Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation\u2014a technique particularly suitable for training with limited data\u2014for this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.",
            "corpus_id": 232233485,
            "sentences": [
                {
                    "corpus_id": "232233485",
                    "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
                    "text": "Effective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect  Table 2: Gradual curriculum augmentation with three schedules. Curriculum: temperature \u03c4 increases. Control: \u03c4 is randomly selected every fifty updates. Anti: decreasing \u03c4 . Results are shown for ten seeds. a greater improvement from curriculum learning. Figure 4 compares standard and two-stage curriculum augmentation for various temperatures, with results averaged over all four datasets. At low temperature, augmented examples remained pretty similar to original examples, and so the range of difficulty in examples was small and therefore curriculum learning showed little improvement. At higher temperatures, however, augmented examples became quite different from original examples, and so the range of difficulty in examples was much larger and therefore curriculum data augmentation improved over standard augmentation more. Whereas Wei and Zou (2019) recommend \u03c4 \u2208 {0.05, 0.1}, our curriculum framework liberates us to use much larger \u03c4 and maintain relatively robust improvements even at \u03c4 \u2208 {0.4, 0.5} when standard augmentation is no longer useful.",
                    "score": 0.3690609843505156,
                    "section_title": "Ablation: Augmentation Temperature",
                    "char_start_offset": 12689,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8037109375
                }
            ],
            "relevance_judgement": 0.8037109375,
            "relevance_judgment_input_expanded": "# Title: Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Jason Wei, Chengyu Huang, Soroush Vosoughi, Yu Cheng, Shiqi Xu\n## Abstract\nFew-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation\u2014a technique particularly suitable for training with limited data\u2014for this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.\n## Ablation: Augmentation Temperature\nEffective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect  Table 2: Gradual curriculum augmentation with three schedules. Curriculum: temperature \u03c4 increases. Control: \u03c4 is randomly selected every fifty updates. Anti: decreasing \u03c4 . Results are shown for ten seeds. a greater improvement from curriculum learning. Figure 4 compares standard and two-stage curriculum augmentation for various temperatures, with results averaged over all four datasets. At low temperature, augmented examples remained pretty similar to original examples, and so the range of difficulty in examples was small and therefore curriculum learning showed little improvement. At higher temperatures, however, augmented examples became quite different from original examples, and so the range of difficulty in examples was much larger and therefore curriculum data augmentation improved over standard augmentation more. Whereas Wei and Zou (2019) recommend \u03c4 \u2208 {0.05, 0.1}, our curriculum framework liberates us to use much larger \u03c4 and maintain relatively robust improvements even at \u03c4 \u2208 {0.4, 0.5} when standard augmentation is no longer useful.",
            "reference_string": "[232233485 | Wei et al. | 2021 | Citations: 64]"
        },
        {
            "title": "Let the Model Decide its Curriculum for Multitask Learning",
            "venue": "Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing",
            "year": 2022,
            "reference_count": 38,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.09898",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.09898, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2067056655",
                    "name": "Neeraj Varshney"
                },
                {
                    "authorId": "1817207",
                    "name": "Swaroop Mishra"
                },
                {
                    "authorId": "2064619864",
                    "name": "Chitta Baral"
                }
            ],
            "abstract": "t",
            "corpus_id": 248965007,
            "sentences": [
                {
                    "corpus_id": "248965007",
                    "title": "Let the Model Decide its Curriculum for Multitask Learning",
                    "text": "In addition to arranging the training instances into a learning curriculum, computing difficulty scores using model-based techniques has shown its benefits in several other areas, such as improving selective prediction ability (Varshney et al., 2022b), under-standing training dynamics (Swayamdipta et al., 2020), and efficient evaluations (Varshney et al., 2022a). However, these techniques present a few challenges: \n\n1. Computation: They involve calculating the difficulty scores of training instances which requires additional computation. However, this computation is only required during training and not required during inference. Hence, it does not add any computational overhead at inference time when deployed in an application. 2. Noisy Instances: Training instances that are wrongly annotated/noisy will most certainly get assigned a very high difficulty score and hence will learned at the end during training. This is unlikely to hamper learning when the number of noisy instances is small. However, it may negatively impact the model's learning when the training dataset has a non-trivial number of noisy instances. We plan to investigate this in our future work.",
                    "score": 0.4147960497555019,
                    "section_title": "H Limitations of Computing Difficulty Scores using Model-based Techniques",
                    "char_start_offset": 12171,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 417
                        },
                        {
                            "start": 420,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1178
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 227,
                            "end": 251,
                            "matchedPaperCorpusId": "247778884"
                        },
                        {
                            "start": 286,
                            "end": 312,
                            "matchedPaperCorpusId": "221856637"
                        },
                        {
                            "start": 340,
                            "end": 364,
                            "matchedPaperCorpusId": "247292373"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79638671875
                }
            ],
            "relevance_judgement": 0.79638671875,
            "relevance_judgment_input_expanded": "# Title: Let the Model Decide its Curriculum for Multitask Learning\n# Venue: Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing\n# Authors: Neeraj Varshney, Swaroop Mishra, Chitta Baral\n## Abstract\nt\n## H Limitations of Computing Difficulty Scores using Model-based Techniques\nIn addition to arranging the training instances into a learning curriculum, computing difficulty scores using model-based techniques has shown its benefits in several other areas, such as improving selective prediction ability (Varshney et al., 2022b), under-standing training dynamics (Swayamdipta et al., 2020), and efficient evaluations (Varshney et al., 2022a). However, these techniques present a few challenges: \n\n1. Computation: They involve calculating the difficulty scores of training instances which requires additional computation. However, this computation is only required during training and not required during inference. Hence, it does not add any computational overhead at inference time when deployed in an application. 2. Noisy Instances: Training instances that are wrongly annotated/noisy will most certainly get assigned a very high difficulty score and hence will learned at the end during training. This is unlikely to hamper learning when the number of noisy instances is small. However, it may negatively impact the model's learning when the training dataset has a non-trivial number of noisy instances. We plan to investigate this in our future work.",
            "reference_string": "[248965007 | Varshney et al. | 2022 | Citations: 8]"
        },
        {
            "title": "SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.15134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3401413",
                    "name": "Habib Hajimolahoseini"
                },
                {
                    "authorId": "152432835",
                    "name": "Omar Mohamed Awad"
                },
                {
                    "authorId": "2238208401",
                    "name": "Walid Ahmed"
                },
                {
                    "authorId": "2244623536",
                    "name": "Austin Wen"
                },
                {
                    "authorId": "2265581307",
                    "name": "Saina Asani"
                },
                {
                    "authorId": "2265582207",
                    "name": "Mohammad Hassanpour"
                },
                {
                    "authorId": "2265581967",
                    "name": "Farnoosh Javadi"
                },
                {
                    "authorId": "2268405981",
                    "name": "Mehdi Ahmadi"
                },
                {
                    "authorId": "71106411",
                    "name": "Foozhan Ataiefard"
                },
                {
                    "authorId": "2265599969",
                    "name": "Kangling Liu"
                },
                {
                    "authorId": "2238404323",
                    "name": "Yang Liu"
                }
            ],
            "abstract": "In this paper, we present SwiftLearn, a data-efficient approach to accelerate training of deep learning models using a subset of data samples selected during the warm-up stages of training. This subset is selected based on an importance criteria measured over the entire dataset during warm-up stages, aiming to preserve the model performance with fewer examples during the rest of training. The importance measure we propose could be updated during training every once in a while, to make sure that all of the data samples have a chance to return to the training loop if they show a higher importance. The model architecture is unchanged but since the number of data samples controls the number of forward and backward passes during training, we can reduce the training time by reducing the number of training samples used in each epoch of training. Experimental results on a variety of CV and NLP models during both pretraining and finetuning show that the model performance could be preserved while achieving a significant speed-up during training. More specifically, BERT finetuning on GLUE benchmark shows that almost 90% of the data can be dropped achieving an end-to-end average speedup of 3.36x while keeping the average accuracy drop less than 0.92%.",
            "corpus_id": 265456857,
            "sentences": [
                {
                    "corpus_id": "265456857",
                    "title": "SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling",
                    "text": "Alternatively, a scaled-down model can be trained to serve as a proxy for selecting important data samples based on predefined metrics (Coleman et al., 2020). Other metrics include the number of times a model misclassifies a sample and the high variance of gradients, indicating example difficulty (Dadalto et al., 2023). Prediction depth, the layer at which a k-NN classifier can successfully classify an example, can also serve as a measure of computational difficulty (Ma and Karaman, 2018). \n\nCurriculum learning methods revolve around identifying the optimal order of data samples to accelerate model convergence. Unlike dataset pruning, these methods do not remove data samples but emphasize the importance of the order in which data samples are presented during training. Curriculum learning methods define metrics to quantify the importance of each data sample, and the order of learning is determined based on these scores. These metrics can be crafted manually or generated automatically (Zhang and Pfister, 2021;de Mathelin et al., 2022). While handcrafted metrics may be task-specific, they often outperform automated metrics. Nevertheless, both types of metrics remain valuable for various tasks. It is worth noting that most dataset reduction techniques have primarily been explored in image classification tasks, with limited exploration in text-based tasks. \n\nIn this paper, we propose SwiftLearn, a technique that could be considered as a combination of dataset pruning and curiculum learning. The proposed algorithm is described in the following sections.",
                    "score": 0.35365419522738994,
                    "section_title": "Introduction",
                    "char_start_offset": 3928,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 494
                        },
                        {
                            "start": 497,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1373
                        },
                        {
                            "start": 1376,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1573
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7900390625
                }
            ],
            "relevance_judgement": 0.7900390625,
            "relevance_judgment_input_expanded": "# Title: SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling\n# Venue: arXiv.org\n# Authors: Habib Hajimolahoseini, Omar Mohamed Awad, Walid Ahmed, Austin Wen, Saina Asani, Mohammad Hassanpour, Farnoosh Javadi, Mehdi Ahmadi, Foozhan Ataiefard, Kangling Liu, Yang Liu\n## Abstract\nIn this paper, we present SwiftLearn, a data-efficient approach to accelerate training of deep learning models using a subset of data samples selected during the warm-up stages of training. This subset is selected based on an importance criteria measured over the entire dataset during warm-up stages, aiming to preserve the model performance with fewer examples during the rest of training. The importance measure we propose could be updated during training every once in a while, to make sure that all of the data samples have a chance to return to the training loop if they show a higher importance. The model architecture is unchanged but since the number of data samples controls the number of forward and backward passes during training, we can reduce the training time by reducing the number of training samples used in each epoch of training. Experimental results on a variety of CV and NLP models during both pretraining and finetuning show that the model performance could be preserved while achieving a significant speed-up during training. More specifically, BERT finetuning on GLUE benchmark shows that almost 90% of the data can be dropped achieving an end-to-end average speedup of 3.36x while keeping the average accuracy drop less than 0.92%.\n## Introduction\nAlternatively, a scaled-down model can be trained to serve as a proxy for selecting important data samples based on predefined metrics (Coleman et al., 2020). Other metrics include the number of times a model misclassifies a sample and the high variance of gradients, indicating example difficulty (Dadalto et al., 2023). Prediction depth, the layer at which a k-NN classifier can successfully classify an example, can also serve as a measure of computational difficulty (Ma and Karaman, 2018). \n\nCurriculum learning methods revolve around identifying the optimal order of data samples to accelerate model convergence. Unlike dataset pruning, these methods do not remove data samples but emphasize the importance of the order in which data samples are presented during training. Curriculum learning methods define metrics to quantify the importance of each data sample, and the order of learning is determined based on these scores. These metrics can be crafted manually or generated automatically (Zhang and Pfister, 2021;de Mathelin et al., 2022). While handcrafted metrics may be task-specific, they often outperform automated metrics. Nevertheless, both types of metrics remain valuable for various tasks. It is worth noting that most dataset reduction techniques have primarily been explored in image classification tasks, with limited exploration in text-based tasks. \n\nIn this paper, we propose SwiftLearn, a technique that could be considered as a combination of dataset pruning and curiculum learning. The proposed algorithm is described in the following sections.",
            "reference_string": "[265456857 | Hajimolahoseini et al. | 2023 | Citations: 2]"
        },
        {
            "title": "A Comprehensive Survey on Curriculum Learning",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 118,
            "citation_count": 22,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.13166, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "51310474",
                    "name": "Yudong Chen"
                },
                {
                    "authorId": "145583986",
                    "name": "Wenwu Zhu"
                }
            ],
            "abstract": "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in tool, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing, etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer + Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. Finally, we present brief discussions on the relationships between CL and other methods, and point out potential future research directions deserving further investigations.",
            "corpus_id": 225066701,
            "sentences": [
                {
                    "corpus_id": "225066701",
                    "title": "A Comprehensive Survey on Curriculum Learning",
                    "text": "Recall the definition of CL of \"training from easier data to harder data\". In essence, to design a such curriculum, we need to decide two things: 1) What kind of training data is supposed to be easier than other data? 2) When should we present more harder data for training, and how much more? The issue 1) can be abstracted to a Difficulty Measurer, which decides the relative \"easiness\" of each data example. In predefined CL methods, this measurement of example difficulty is predefined by human experts, while automatic CL methods let the machine to measure the difficulty. On the other hand, the issue 2) can be abstracted to a Training Scheduler, which decides the sequence of data subsets throughout the training process based on the judgment of the Difficulty Measurer. \n\nTherefore, a general framework for curriculum design consists of these two core components: Difficulty Measurer + Training Scheduler, which is illustrated in  [29], the authors conclude the two core components as scoring function and pacing function, which share the same spirit with Difficulty Measurer and Training Scheduler, respectively, while the latter names are chosen to be more abstract and clearer. \n\nLet us take the experiment in Fig 1 as an instantiation example for this CL framework. In that experiment, Difficulty Measurer is the annotation by a human expert, who decides that some fruit images in the dataset are easier than other images, according to their recognizability and complexity. In addition, Training Scheduler can be, for example, a linear scheduler (see Sec. IV-B2 for details) that starts with 40% of easiest examples in each class, and increase this proportion by 5% each epoch until 100%. In this way, an effective curriculum is successfully designed by filling the general CL framework with appropriate Difficulty Measurer and Training Scheduler according to the specific task of image classification. \n\nAccording to this general framework, we could also clarify the scopes of predefined CL and automatic CL in the next two sections. Specifically, when both the Difficulty Measurer and Training Scheduler are totally designed by human prior knowledge with no data-driven models or algorithms involved, we call the method predefined CL.",
                    "score": 0.43779405230245966,
                    "section_title": "A. The General Framework of Difficulty Measurer + Training Scheduler",
                    "char_start_offset": 21317,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 777
                        },
                        {
                            "start": 780,
                            "end": 1188
                        },
                        {
                            "start": 1191,
                            "end": 1277
                        },
                        {
                            "start": 1278,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1914
                        },
                        {
                            "start": 1917,
                            "end": 2046
                        },
                        {
                            "start": 2047,
                            "end": 2248
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7890625
                }
            ],
            "relevance_judgement": 0.7890625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey on Curriculum Learning\n# Venue: arXiv.org\n# Authors: Xin Wang, Yudong Chen, Wenwu Zhu\n## Abstract\nCurriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in tool, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing, etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer + Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. Finally, we present brief discussions on the relationships between CL and other methods, and point out potential future research directions deserving further investigations.\n## A. The General Framework of Difficulty Measurer + Training Scheduler\nRecall the definition of CL of \"training from easier data to harder data\". In essence, to design a such curriculum, we need to decide two things: 1) What kind of training data is supposed to be easier than other data? 2) When should we present more harder data for training, and how much more? The issue 1) can be abstracted to a Difficulty Measurer, which decides the relative \"easiness\" of each data example. In predefined CL methods, this measurement of example difficulty is predefined by human experts, while automatic CL methods let the machine to measure the difficulty. On the other hand, the issue 2) can be abstracted to a Training Scheduler, which decides the sequence of data subsets throughout the training process based on the judgment of the Difficulty Measurer. \n\nTherefore, a general framework for curriculum design consists of these two core components: Difficulty Measurer + Training Scheduler, which is illustrated in  [29], the authors conclude the two core components as scoring function and pacing function, which share the same spirit with Difficulty Measurer and Training Scheduler, respectively, while the latter names are chosen to be more abstract and clearer. \n\nLet us take the experiment in Fig 1 as an instantiation example for this CL framework. In that experiment, Difficulty Measurer is the annotation by a human expert, who decides that some fruit images in the dataset are easier than other images, according to their recognizability and complexity. In addition, Training Scheduler can be, for example, a linear scheduler (see Sec. IV-B2 for details) that starts with 40% of easiest examples in each class, and increase this proportion by 5% each epoch until 100%. In this way, an effective curriculum is successfully designed by filling the general CL framework with appropriate Difficulty Measurer and Training Scheduler according to the specific task of image classification. \n\nAccording to this general framework, we could also clarify the scopes of predefined CL and automatic CL in the next two sections. Specifically, when both the Difficulty Measurer and Training Scheduler are totally designed by human prior knowledge with no data-driven models or algorithms involved, we call the method predefined CL.",
            "reference_string": "[225066701 | Wang et al. | 2020 | Citations: 22]"
        },
        {
            "title": "Data Selection Curriculum for Abstractive Text Summarization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 17,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-emnlp.537.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.537?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48904791",
                    "name": "Shichao Sun"
                },
                {
                    "authorId": "2273661632",
                    "name": "Ruifeng Yuan"
                },
                {
                    "authorId": "2237429230",
                    "name": "Jianfei He"
                },
                {
                    "authorId": "2314396",
                    "name": "Ziqiang Cao"
                },
                {
                    "authorId": "2237591981",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2237752880",
                    "name": "Xiaohua Jia"
                }
            ],
            "abstract": ",",
            "corpus_id": 266177072,
            "sentences": [
                {
                    "corpus_id": "266177072",
                    "title": "Data Selection Curriculum for Abstractive Text Summarization",
                    "text": "Abstractive Text Summarization (ATS) aims to generate concise summaries while preserving essential content. Recent studies (Liu et al., 2023b;Zhang et al., 2023;Liu et al., 2023a) have shown that Large Language Models (LLMs), including GPT-3.5 (Ouyang et al., 2022), can produce summaries more favored by human annotators compared to reference summaries from well-established datasets, such as CNN/DailyMail (Hermann et al., 2015). Additionally, Liu et al. (2023a) experimentally assert that contrastive learning based methods applied to smaller summarization models, like BART (Lewis et al., 2020), can deliver performance on par with LLMs. This finding renews the importance of training smaller models via contrastive learning, as it offers the advantage of reducing computational costs. However, ATS models are commonly trained using large-scale data that is randomly shuffled. \n\nThis study aims to explore the potential for optimizing ATS models through the strategic utilization of data selection and curriculum learning (Bengio et al., 2009) in conjunction with contrastive learning techniques. It is important to note that not all data is equally valuable, and the presence of redundant or even detrimental examples can impede the performance of ATS systems (Mohiuddin et al., 2022). Furthermore, the ordering in which data is presented during model training, as emphasized by curriculum learning principles, can have a significant impact on both the efficiency and effectiveness of the learning process. Consequently, there is a fundamental necessity to develop a scoring system that can accurately assess the learning difficulty of individual samples and furtherly can be used for data selection and curriculum learning. \n\nIn this study, we introduce a novel scoring system called the Data Selection Curriculum (DSC) score, which serves as a measure of learning difficulty. The DSC score considers two essential factors: the difficulty of enhancing the ATS model through a specific instance and the expected performance of the model on that particular instance. Instances on which the current model already exhibits promising performance and stands to benefit significantly from slight adjustments are assigned as a lower learning difficulty. This is because these instances have the potential to contribute more to the final performance.",
                    "score": 0.3506161605087486,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 789
                        },
                        {
                            "start": 790,
                            "end": 880
                        },
                        {
                            "start": 883,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1729
                        },
                        {
                            "start": 1732,
                            "end": 1882
                        },
                        {
                            "start": 1883,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2251
                        },
                        {
                            "start": 2252,
                            "end": 2347
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 123,
                            "end": 142,
                            "matchedPaperCorpusId": "254685611"
                        },
                        {
                            "start": 244,
                            "end": 265,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 1026,
                            "end": 1047,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 1265,
                            "end": 1289,
                            "matchedPaperCorpusId": "247762191"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7890625
                }
            ],
            "relevance_judgement": 0.7890625,
            "relevance_judgment_input_expanded": "# Title: Data Selection Curriculum for Abstractive Text Summarization\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Shichao Sun, Ruifeng Yuan, Jianfei He, Ziqiang Cao, Wenjie Li, Xiaohua Jia\n## Abstract\n,\n## Introduction\nAbstractive Text Summarization (ATS) aims to generate concise summaries while preserving essential content. Recent studies (Liu et al., 2023b;Zhang et al., 2023;Liu et al., 2023a) have shown that Large Language Models (LLMs), including GPT-3.5 (Ouyang et al., 2022), can produce summaries more favored by human annotators compared to reference summaries from well-established datasets, such as CNN/DailyMail (Hermann et al., 2015). Additionally, Liu et al. (2023a) experimentally assert that contrastive learning based methods applied to smaller summarization models, like BART (Lewis et al., 2020), can deliver performance on par with LLMs. This finding renews the importance of training smaller models via contrastive learning, as it offers the advantage of reducing computational costs. However, ATS models are commonly trained using large-scale data that is randomly shuffled. \n\nThis study aims to explore the potential for optimizing ATS models through the strategic utilization of data selection and curriculum learning (Bengio et al., 2009) in conjunction with contrastive learning techniques. It is important to note that not all data is equally valuable, and the presence of redundant or even detrimental examples can impede the performance of ATS systems (Mohiuddin et al., 2022). Furthermore, the ordering in which data is presented during model training, as emphasized by curriculum learning principles, can have a significant impact on both the efficiency and effectiveness of the learning process. Consequently, there is a fundamental necessity to develop a scoring system that can accurately assess the learning difficulty of individual samples and furtherly can be used for data selection and curriculum learning. \n\nIn this study, we introduce a novel scoring system called the Data Selection Curriculum (DSC) score, which serves as a measure of learning difficulty. The DSC score considers two essential factors: the difficulty of enhancing the ATS model through a specific instance and the expected performance of the model on that particular instance. Instances on which the current model already exhibits promising performance and stands to benefit significantly from slight adjustments are assigned as a lower learning difficulty. This is because these instances have the potential to contribute more to the final performance.",
            "reference_string": "[266177072 | Sun et al. | 2023 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "269756933",
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "text": "Curriculum learning, introduced by Bengio et al. in 2009(Bengio et al., 2009), is a learning strategy where models are trained on data from simple to complex, mirroring human learning processes.This approach enhances model performance by establishing a foundational understanding before introducing more complex data.It has been widely applied in natural language processing (NLP) and computer vision.(Guo et al., 2018;Antonios Platanios et al., 2019;Tay et al., 2019).\n\nIn NLP, curriculum learning has proven to be effective by organizing the training sequence to begin with simpler texts and progressively transition to more complex ones.The complexity of these texts is assessed based on various criteria, including text length, the rarity of vocabulary, and edit distance.This structured approach has successfully demonstrated the efficacy of curriculum learning in the training of Pretrained Language Models (PLMs) (Chang et al., 2021;Nagatsuka et al., 2023).\n\nRecent advancements in large-scale language and multimodal models have highlighted the effective integration of curriculum learning with cutting-edge techniques, significantly boosting performance across various applications.Notably, projects such as WavLLM (Hu et al., 2024) and AutoWebGLM (Lai et al., 2024) showcase the potential of these integrations, tailoring the learning process to gradually introduce more complex tasks, thereby enhancing model robustness and application specificity.Alongside these innovations, further research has validated the effectiveness of curriculum learning in instruction tuning for language models.This approach not only systematically improves models' abilities to handle progressively challenging tasks but also optimizes their learning trajectory for better performance in specialized tasks.By methodically structuring the training process, these initiatives have set a new standard in the development and refinement of AI models, marking a significant step forward in their evolution.\n\nIn the development of training data for curricu-lum learning, accurately measuring data difficulty is crucial.Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging.Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).",
            "score": 0.5206892323392261,
            "section_title": "Related Work",
            "char_start_offset": 3231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 469
                },
                {
                    "start": 471,
                    "end": 640
                },
                {
                    "start": 640,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 964
                },
                {
                    "start": 966,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1798
                },
                {
                    "start": 1798,
                    "end": 1992
                },
                {
                    "start": 1994,
                    "end": 2104
                },
                {
                    "start": 2104,
                    "end": 2209
                },
                {
                    "start": 2209,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 56,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 56,
                    "end": 77,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 401,
                    "end": 419,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 940,
                    "end": 963,
                    "matchedPaperCorpusId": "255221201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "266977266",
            "title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks",
            "text": "How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied. Our code is available at: https://github.com/allenai/easy-to-hard-generalization",
            "score": 0.4858720585459405,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.708984375
        },
        {
            "corpus_id": "227905455",
            "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks",
            "text": "Recent advances on natural language processing (NLP) benchmarks have been driven by increasingly sophisticated language models, which are pretrained on enormous amounts of data before use. Refinements of this process has led to increasingly powerful language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and more recently T5 (Raffel et al., 2019). Such models are quickly saturating even new tasks that have undergone a rigorous adversarial filtering process (Zellers et al., 2018). However, these downstream performance improvements also require more computational resources and data to train the models, which is not always feasible. Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;Wang et al., 2020;Zhou et al., 2020;Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored. Prior approaches either use a hand-crafted measure of difficulty that works well for a particular task or design an architecture that automatically creates the curriculum during training. In either case, the curriculum is formed using some information-theoretic measure of difficulty (semantic distance, feedback from a separate network, etc.), and it is difficult to interpret why they work well for some tasks and not others. In a practical sense, it is seldom clear how to apply a previously investigated curricula directly to another task. \n\nIn this paper, we explore how to address these shortcomings by creating what we call a taskagnostic model of difficulty, which we argue can, in principle, be applied to any supervised learning task. We use this model to investigate what makes a good difficulty measure for curricula beyond how it affects downstream performance to better explain why one curriculum should be preferred over another.",
            "score": 0.47972418954279183,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 301,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 486,
                    "end": 508,
                    "matchedPaperCorpusId": "52019251"
                },
                {
                    "start": 683,
                    "end": 696,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 923,
                    "end": 941,
                    "matchedPaperCorpusId": "220045412"
                },
                {
                    "start": 941,
                    "end": 959,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 959,
                    "end": 975,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "248887633",
            "title": "LeRaC: Learning Rate Curriculum",
            "text": "Curriculum learning was initially introduced by Bengio et al. [1] as a training strategy that helps machine learning models to generalize better when the training examples are presented in the ascending order of their difficulty. Extensive surveys on curriculum learning methods, including the most recent advancements on the topic, were conducted by Soviany et al. [2] and Wang et al. [4]. In the former survey, Soviany et al. [2] emphasized that curriculum learning is not only applied at the data level, but also with respect to the other components involved in a machine learning approach, namely at the model level, the task level and the objective (loss) level. Regardless of the component on which curriculum learning is applied, the technique has demonstrated its effectiveness on a broad range of machine learning tasks, from computer vision [1,7,[27][28][29][30][31][32][33][34] to natural language processing [1,[35][36][37][38] and audio processing [39,40]. \n\nThe main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input [41][42][43] or metrics based on domain-specific heuristics. For instance, the text length [36,[44][45][46] and the word frequency [1,38] have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works [32,33]. Other solutions employed difficulty estimators [47] or even the confidence level of the predictions made by the neural network [48,49] to approximate the complexity of the data samples. Other studies [27][28][29] used the error of a previously trained model to estimate the difficulty of each sample. Such solutions have shown their utility in specific application domains. Nonetheless, measuring the difficulty remains problematic when implementing standard (data-level) curriculum learning strategies, at least in some application domains. Therefore, several alternatives have emerged over time, handling the drawback and improving the conventional curriculum learning approach.",
            "score": 0.47118286798823417,
            "section_title": "Curriculum Learning",
            "char_start_offset": 6876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2209
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 428,
                    "end": 431,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 854,
                    "end": 856,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "266998701"
                },
                {
                    "start": 860,
                    "end": 864,
                    "matchedPaperCorpusId": "254246401"
                },
                {
                    "start": 864,
                    "end": 868,
                    "matchedPaperCorpusId": "256808576"
                },
                {
                    "start": 868,
                    "end": 872,
                    "matchedPaperCorpusId": "8105909"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 923,
                    "end": 927,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "1900277"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "19805513"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "11590585"
                },
                {
                    "start": 1215,
                    "end": 1219,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 1219,
                    "end": 1223,
                    "matchedPaperCorpusId": "204539326"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "221995570"
                },
                {
                    "start": 1306,
                    "end": 1310,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1318,
                    "end": 1322,
                    "matchedPaperCorpusId": "233433844"
                },
                {
                    "start": 1349,
                    "end": 1352,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "10364203"
                },
                {
                    "start": 1660,
                    "end": 1663,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "matchedPaperCorpusId": "266998701"
                },
                {
                    "start": 1733,
                    "end": 1737,
                    "matchedPaperCorpusId": "254246401"
                },
                {
                    "start": 1737,
                    "end": 1741,
                    "matchedPaperCorpusId": "256808576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "271051051",
            "title": "CBM: Curriculum by Masking",
            "text": "Curriculum learning is a training technique introduced by Bengio et al. [3], which provides the training examples in a meaningful order, from easy to hard, to neural networks.The objective is to enhance the performance of neural models, while also improving the convergence speed of the training process.Since its introduction, curriculum learning has proven its effectiveness in various domains, such as computer vision [3,7,10,19,25,43,44,45], natural language processing [3,10,30,36,39,47], and signal processing [1,10,40].The method has been very successful and has undergone extensive development, as illustrated in some recent surveys [46,52].These developments range from strategies for measuring data difficulty [3,23,30,38,43,45,48,54,60] to methods focusing on other aspects of the training process [5,10,6,28,44].A wellknown method to apply curriculum learning is by defining a metric that evaluates the complexity of the data, and subsequently arranging the training examples from the simplest to the most challenging ones based on the respective metric.Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images [43,45].In natural language processing, word frequency [3,36] and sequence length [8,30,48,60] are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design [26,38,54].\n\nThe aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input [26], which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model.",
            "score": 0.46489840021319273,
            "section_title": "Related Work",
            "char_start_offset": 4940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1166
                },
                {
                    "start": 1166,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1517
                },
                {
                    "start": 1519,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1845
                },
                {
                    "start": 1845,
                    "end": 1953
                },
                {
                    "start": 1953,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 75,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 424,
                    "end": 426,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 429,
                    "end": 432,
                    "matchedPaperCorpusId": "8105909"
                },
                {
                    "start": 432,
                    "end": 435,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 438,
                    "end": 441,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 474,
                    "end": 477,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "1900277"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "11590585"
                },
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "19805513"
                },
                {
                    "start": 641,
                    "end": 645,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 645,
                    "end": 648,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 723,
                    "end": 726,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 735,
                    "end": 738,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "221995570"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "233433844"
                },
                {
                    "start": 809,
                    "end": 812,
                    "matchedPaperCorpusId": "231986287"
                },
                {
                    "start": 815,
                    "end": 817,
                    "matchedPaperCorpusId": "190000064"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "3568073"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1336,
                    "end": 1339,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1363,
                    "end": 1366,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1366,
                    "end": 1369,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1369,
                    "end": 1372,
                    "matchedPaperCorpusId": "233433844"
                },
                {
                    "start": 1506,
                    "end": 1510,
                    "matchedPaperCorpusId": "204539326"
                },
                {
                    "start": 1510,
                    "end": 1513,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 1513,
                    "end": 1516,
                    "matchedPaperCorpusId": "221995570"
                },
                {
                    "start": 1683,
                    "end": 1687,
                    "matchedPaperCorpusId": "204539326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "254366532",
            "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
            "text": "These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al. 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022). However, implementing different CL strategies for different user tasks can require a lot of code-refactoring, which is time-consuming and error-prone. In addition, existing implementations have less consideration on scalability, which makes it difficult to analyze and index large-scale training data based on different difficulty metrics. \u2022 Existing data routing techniques such as token drop/bypass/pruning were mostly designed for inference and inapplicable to training. TokenBypass (Hou et al. 2022), to our knowledge the only data routing technique for foundation model pretraining, skips the compute of part of the input tokens at some middle layers during BERT pretraining, reducing pretraining cost while maintaining model quality. However, it requires several special implementations that may only work for the tested BERT pretraining case, such as the importance score-based token dropping decisions and the whitelist for special tokens. This could limit the possibility and benefit of applying it to other cases. In the NLP area, curriculum learning has been applied on small-scale one-stage tasks and downstream finetuning tasks, such as neural machine translation (NMT) (Kocmi and Bojar 2017;Bojar et al. 2017;Zhang et al. 2018;Platanios et al. 2019;Zhang et al. 2019) and natural language understanding (NLU) (Sachan andXing 2016, 2018;Tay et al. 2019;Xu et al. 2020). There are also a few works that explore curriculum learning for language model pretraining (Press, Smith, and Lewis 2020; Zhang et al. 2021;Campos 2021;Li, Zhang, and He 2022). However, one common limitation among existing works is that there does not exist a scalable and customizable curriculum learning library, making it difficult to analyze large-scale data and explore custom difficulty metrics/pacing functions.",
            "score": 0.4631668933867917,
            "section_title": "Introduction",
            "char_start_offset": 1863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 265,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 849,
                    "end": 866,
                    "matchedPaperCorpusId": "247628024"
                },
                {
                    "start": 1568,
                    "end": 1586,
                    "matchedPaperCorpusId": "9460719"
                },
                {
                    "start": 1604,
                    "end": 1626,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1626,
                    "end": 1643,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 1686,
                    "end": 1697,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 1697,
                    "end": 1713,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 1713,
                    "end": 1729,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1729,
                    "end": 1744,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1868,
                    "end": 1886,
                    "matchedPaperCorpusId": "233433844"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "251622541",
            "title": "PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation",
            "text": "Data augmentation techniques create artificial data mixed with the original data for improved performance. Traditional data augmentation techniques in the language community include word-level perturbation such as synonym replacement, random insertion, random swap, and random deletion (Wei and Zou, 2019). Sentence-level techniques such as Round-trip Translation (Sennrich et al., 2016b) exploits the use of machine translation models to * The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). translate the input sentence to another language before translating back to the source language which can be essentially treated as a form of paraphrasing.\n\nCurriculum learning presents training instances in a meaningful order with increasing difficulties to neural models for a boost in performance. Traditional curriculum learning (Bengio et al., 2009;Liu et al., 2018Liu et al., , 2020Platanios et al., 2019;Xu et al., 2020a,b;Su et al., 2021) categorizes the original training instances into different levels of difficulties to be gradually presented to the model where a core component called difficulty measure, which is usually defined as a numerical number where a bigger number indicates a more difficult sample.\n\nCombining the merits of the above two mentioned techniques, Curriculum Data Augmentation (CDA) creates synthetic data with increasing levels of difficulties to be presented to our neural models. Existing CDA defines the ratio of the words perturbation as the difficulty measure for curriculums and a gradual course which increases the difficulty of curriculums when the training loss plateaus (Wei et al., 2021), which then ends when the most challenging curriculum ends. Although existing CDA is effective, yet there are several disadvantages. First, it employs word-level perturbation. This superficial operation keeps the augmentation to have a similar sentence structure as the original one. Next, it employs random insertion, random swap, and random deletion for augmentation. Although this can be durable as for text classification (Wei et al., 2021), this is not suitable for generation tasks, particularly when many words are perturbed, which can even easily break the sentence grammar. Third, it uses a gradual course",
            "score": 0.4591617304724098,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 957,
                    "end": 978,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4111328125
        },
        {
            "corpus_id": "277824032",
            "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
            "text": "Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;Lee and Song, 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability.",
            "score": 0.457876626190469,
            "section_title": "LLM-Adaptive Difficulty Grading",
            "char_start_offset": 5169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1263
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 570,
                    "matchedPaperCorpusId": "272706931"
                },
                {
                    "start": 650,
                    "end": 668,
                    "matchedPaperCorpusId": "275757809"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "Second, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2017;Arpit et al., 2017) in neural models. We will address both issues as part of this research. \n\nThe contributions of this paper are: \n\n\u2022 incorporating human-verified linguistic complexity information to establish an effective scoring function for assessing the difficulty of text data with respect to NLP models, \u2022 deriving linguistic curricula for NLP models based on linguistic complexity of data and model behavior during training, and \u2022 identifying the core sets of linguistic complexity indices that most contribute to learning NLP tasks by models. \n\nWe evaluate our approach on several NLP tasks that require significant linguistic knowledge and reasoning to be addressed. Experimental results show that our approach can uncover latent linguistic knowledge that is most important for addressing NLP tasks. In addition, our approach obtains consistent performance gain over competing models. Source code and data is available at https://github.com/CLU-UML/Ling-CL. We present a framework for multiview curriculum learning using linguistic complexity indices. Our framework estimates the importance of various linguistic complexity indices, aggregates the resulting importance scores to determine the difficulty of samples for learning NLP tasks, and develops novel curricula for training models using complexity indices. The list of all indices used is in Appendix A.",
            "score": 0.45283379289724596,
            "section_title": "Introduction",
            "char_start_offset": 3511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1582
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 212,
                    "matchedPaperCorpusId": "6212000"
                },
                {
                    "start": 212,
                    "end": 231,
                    "matchedPaperCorpusId": "11455421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "225066701",
            "title": "A Comprehensive Survey on Curriculum Learning",
            "text": "Recall the definition of CL of \"training from easier data to harder data\". In essence, to design a such curriculum, we need to decide two things: 1) What kind of training data is supposed to be easier than other data? 2) When should we present more harder data for training, and how much more? The issue 1) can be abstracted to a Difficulty Measurer, which decides the relative \"easiness\" of each data example. In predefined CL methods, this measurement of example difficulty is predefined by human experts, while automatic CL methods let the machine to measure the difficulty. On the other hand, the issue 2) can be abstracted to a Training Scheduler, which decides the sequence of data subsets throughout the training process based on the judgment of the Difficulty Measurer. \n\nTherefore, a general framework for curriculum design consists of these two core components: Difficulty Measurer + Training Scheduler, which is illustrated in  [29], the authors conclude the two core components as scoring function and pacing function, which share the same spirit with Difficulty Measurer and Training Scheduler, respectively, while the latter names are chosen to be more abstract and clearer. \n\nLet us take the experiment in Fig 1 as an instantiation example for this CL framework. In that experiment, Difficulty Measurer is the annotation by a human expert, who decides that some fruit images in the dataset are easier than other images, according to their recognizability and complexity. In addition, Training Scheduler can be, for example, a linear scheduler (see Sec. IV-B2 for details) that starts with 40% of easiest examples in each class, and increase this proportion by 5% each epoch until 100%. In this way, an effective curriculum is successfully designed by filling the general CL framework with appropriate Difficulty Measurer and Training Scheduler according to the specific task of image classification. \n\nAccording to this general framework, we could also clarify the scopes of predefined CL and automatic CL in the next two sections. Specifically, when both the Difficulty Measurer and Training Scheduler are totally designed by human prior knowledge with no data-driven models or algorithms involved, we call the method predefined CL.",
            "score": 0.43779405230245966,
            "section_title": "A. The General Framework of Difficulty Measurer + Training Scheduler",
            "char_start_offset": 21317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 1188
                },
                {
                    "start": 1191,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1914
                },
                {
                    "start": 1917,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "250391006",
            "title": "On Curriculum Learning for Commonsense Reasoning",
            "text": "The initial data order imposed by the teacher model is updated at regular intervals during training by taking the learner model's current state into account (Kong et al., 2021). Importantly, we propose to reverse the ranking to a difficulty-to-easy curriculum in ACL, in order to reinforce feedback from the hardto-learn data points, which has been shown to be beneficial for generalization (Swayamdipta et al., 2020). In order to measure difficulty, we explore three different data-sample informativeness scoring methods i.e. Question Answering Probability (QAP) (Zhang and Bansal, 2019), Energy-based Out-of-Distribution Score (Liu et al., 2020) and Cartography-based Variability (Swayamdipta et al., 2020). Our work is most related to Xu et al. (2020) which splits the training data into N meta-datasets, trains N models for computing the curriculum and follows a heuristically designed training regimen. In contrast, we train a single model for computing the curriculum and use Bayesian optimization (Snoek et al., 2012) to find the best pacing of the curriculum for the target dataset, which is more effective than Xu et al. (2020) as we show in Sec. 5.4, besides being computationally efficient. \n\nWe analyze these methods on five commonsense reasoning datasets dealing with various tasks such as reasoning about social interactions (SocialIQA; Sap et al. (2019)), reading comprehension (Cos-mosQA; Huang et al. ( 2019)), natural language inference (HellaSwag; Zellers et al. (2019)), pronoun resolution (WinoGrande; Sakaguchi et al. (2020)) and adversarial commonsense (CODAH; Chen et al. (2019)). We explore curriculum learning in full-model finetuning as parameter-efficient tuning and show significant improvements using curriculum learning on each of these datasets. We also demonstrate that curriculum learning prevents the learner model from over-fitting on the training set, which leads to improved generalization to in-domain and out-of-domain data.",
            "score": 0.43669890280479046,
            "section_title": "Introduction",
            "char_start_offset": 2157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 417,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 564,
                    "end": 588,
                    "matchedPaperCorpusId": "202572810"
                },
                {
                    "start": 629,
                    "end": 647,
                    "matchedPaperCorpusId": "222208700"
                },
                {
                    "start": 682,
                    "end": 708,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 738,
                    "end": 754,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1004,
                    "end": 1024,
                    "matchedPaperCorpusId": "632197"
                },
                {
                    "start": 1120,
                    "end": 1136,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1351,
                    "end": 1368,
                    "matchedPaperCorpusId": "128296356"
                },
                {
                    "start": 1467,
                    "end": 1488,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 1523,
                    "end": 1546,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1584,
                    "end": 1602,
                    "matchedPaperCorpusId": "104292422"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "Pretrained language models are the foundation for achieving impressive results on many natural language processing tasks today (Peters et al., 2018;Devlin et al., 2019;Yang et al., 2019) but they are also prohibitively expensive to train, requiring enormous computing resources to be effective. The exploding demand of computations, together with the resulting massive energy cost (Strubell et al., 2019), pose serious obstacles to the development of new pretrained models, thus, leading to a number of recent research efforts aimed towards addressing this problem by proposing approaches for improving model efficiency  or sample efficiency (Clark et al., 2020), to name just a few.\n\nThe primary method researchers have explored to address this problem is to develop smaller language models Jiao et al., 2020). In this paper we study a complementary approach to simplify pretraining of language models based on corpus ordering via curriculum learning (Bengio et al., 2009). The motivation is that curriculum learning has been shown to help with faster convergence (Guo et al., 2018;Hacohen and Weinshall, 2019) which, in combination with simpler language models, will give us a sustainable platform for future research into language models.\n\nAlthough curriculum learning strategies have been successfully employed in many areas of machine learning, on a wide range of tasks Soviany et al., 2021), little is understood about the role of corpus ordering in the context of pretraining language models with the exception of some notable early works on language modeling (Bengio et al., 2009;Graves et al., 2017). In this paper, we continue this line of investigation by asking the question whether curriculum based pretraining of transformer language models provides any benefits when compared with traditional vanilla training.\n\nIn order to create a curriculum from an unlabeled text corpus for such self-supervised form of learning, one needs to define a measure of complexity. We explore if metrics of text readability difficulty designed for humans can serve as beneficial metric in creating a curriculum for machine learning models. The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on.",
            "score": 0.43624447239284614,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 148,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 148,
                    "end": 168,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 381,
                    "end": 404,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 952,
                    "end": 972,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1065,
                    "end": 1083,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 1083,
                    "end": 1111,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1567,
                    "end": 1588,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1588,
                    "end": 1608,
                    "matchedPaperCorpusId": "11137059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "256631050",
            "title": "Data Cartography for Low-Resource Neural Machine Translation",
            "text": "Metrics We compare the notion of sample difficulty from Data Maps against heuristic data characterizations from the literature. At each phase and for the overall Data Maps, we divide the data by regions and compute Pearson's correlation coefficient between the distance of a training example from the origin (using the variability and confidence values as a coordinate pair) and various measures of difficulty. There is no clear way to   define what makes a MT training sample difficult, however hypotheses about properties of easy vs. difficult samples emerge from prior work on data cleaning and curriculum learning. We include dual cross entropy (Junczys-Dowmunt, 2018) as a measurement of data noise, and sentence length, token frequency and word norm embeddings as sentence difficulty criteria from the curriculum learning literature (Zhang et al., 2019;Platanios et al., 2019;Liu et al., 2020). Sentence length and token frequency are computed at the word level; although we saw similar trends at the subword level. \n\nFindings We find that most metrics considered have a weak correlation with the distance from the origin according to Data Maps (Appendix Figures 4,5). Only dual cross-entropy achieves a moderate correlation, primarily in the hard regions, which is not surprising given that it is based on model scores just like Data Maps. We use a rule of thumb for cutoffs at 0.3 and 0.5 for weak and moderate correlation, as outlined by Hinkle et al. (2003). This might explain why no clear data difficulty metrics has emerged from the curriculum learning literature, where data ordering strategies that improve translation quality do not always align with meaningful intuitions about sample difficulty (Zhang et al., 2019), and raise the question of whether the difficulty captured by Data Maps is purely related to model uncertainty rather than intrinsic data uncertainty.",
            "score": 0.4353487993929209,
            "section_title": "Correlation with Data Difficulty Heuristics",
            "char_start_offset": 16360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 839,
                    "end": 859,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 882,
                    "end": 899,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 1447,
                    "end": 1467,
                    "matchedPaperCorpusId": "121247505"
                },
                {
                    "start": 1713,
                    "end": 1733,
                    "matchedPaperCorpusId": "155089817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6083984375
        },
        {
            "corpus_id": "259593617",
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "text": "Inspired by Wang, Chen, and Zhu (2021), which indicated that CL can be highly effective in enhancing a supervised prediction model when dealing with a difficult task (e.g., the automatic scoring of student-authored responses), we aimed to investigate how CL can be tapped to improve the performance of ATS in education. Formally, our study was guided by the following Research Question: RQ To what extent can curriculum learning strategies boost the performance of ATS methods used in education? \n\nTo answer the above question, we centered our work on the design of the two key components of a CL strategy (Wang, Chen, and Zhu 2021; Liu et al. 2018): (i) difficulty measurer, which determines the relative difficulty level of a training data sample; and (ii) training scheduler, which determines the data subset that should be input to a model in a specific training epoch based on the evaluation from the difficulty measurer. Inspired by previous works on proposing effective CL strategies in the broader NLP research (e.g., Spelling Error Correction (Gan, Xu, and Zan 2021) and Natural Answer Generation (Liu et al. 2018)) as well as the works on automatically characterizing textual data in education, we devised two types of CL strategies in this work, i.e., pre-defined and automatic, which are grouped according to whether any or both of the two key components described above are pre-defined by human experts or automatically learned in a data-driven fashion. It should be noted that these naming terminologies are in line with those summarized by Wang, Chen, and Zhu (2021). Specifically, we studied a total of four pre-defined CL strategies, in which the difficulty level of a piece of written text can be measured via calculating its length, readability, the number of grammatical errors or unique words it contains, and the training scheduler is defined as the linear continuous schedulers (Wang, Chen, and Zhu 2021). As documented in relevant CL studies in computer vision and NLP, in addition to presenting the training data in an easy-to-hard fashion, sometimes a model can achieve better prediction performance by reverting the training order to hard-to-easy (denoted as anti-curriculum).",
            "score": 0.43073657767330814,
            "section_title": "Introduction",
            "char_start_offset": 3568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 633,
                    "end": 648,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1052,
                    "end": 1075,
                    "matchedPaperCorpusId": "243865335"
                },
                {
                    "start": 1106,
                    "end": 1122,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1555,
                    "end": 1581,
                    "matchedPaperCorpusId": "58981386"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.826171875
        },
        {
            "corpus_id": "215842252",
            "title": "An Empirical Study of Smoothing Techniques for Language Modeling",
            "text": "The novel methods new-avg-count and new-one-count perform well uniformly across training data sizes, and are superior for trigram models.Notice that while performance is relatively consistent across corpora, it varies widely with respect to training set size and n-gram order.\n\nThe method interp-del-int performs significantly worse than interp-held-out, though they differ only in the data used to train the \u03bb's.However, we delete one word at a time in interp-del-int; we   In Figure 7, we show how the values of the parameters \u03b4 and c min affect the performance of methods katz and new-avg-count, respectively, over several training data sizes.Notice that poor parameter setting can lead to very significant losses in performance, and that optimal parameter settings depend on training set size.\n\nTo give an informal estimate of the difficulty of implementation of each method, in To our knowledge, this is the first empirical comparison of smoothing techniques in language modeling of such scope: no other study has used multiple training data sizes, corpora, or has performed parameter optimization.We show that in order to completely characterize the relative performance of two techniques, it is necessary to consider multiple training set sizes and to try both bigram and trigram models.Multiple runs should be performed whenever possible to discover whether any calculated differences are statistically significant.Furthermore, we show that sub-optimal parameter selection can also significantly affect relative performance.We find that the two most widely used techniques, Katz smoothing and Jelinek-Mercer smoothing, perform consistently well across training set sizes for both bigram and trigram models, with Katz smoothing performing better on trigram models produced from large training sets and on bigram models in general.These results question the generality of the previous reference result concerning Katz smoothing: Katz (1987) reported that his method slightly outperforms an unspecified version of Jelinek-Mercer smoothing on a single training set of 750,000 words.Furthermore, we show that Church-Gale smoothing, which previously had not been compared with common smoothing techniques, outperforms all existing methods on bigram models produced from large training sets.",
            "score": 0.4295477182417691,
            "section_title": "Results",
            "char_start_offset": 16831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 137,
                    "end": 276
                },
                {
                    "start": 278,
                    "end": 413
                },
                {
                    "start": 413,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 797
                },
                {
                    "start": 799,
                    "end": 1103
                },
                {
                    "start": 1103,
                    "end": 1294
                },
                {
                    "start": 1294,
                    "end": 1423
                },
                {
                    "start": 1423,
                    "end": 1532
                },
                {
                    "start": 1532,
                    "end": 1837
                },
                {
                    "start": 1837,
                    "end": 2086
                },
                {
                    "start": 2086,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 1935,
                    "end": 1946,
                    "matchedPaperCorpusId": "6555412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08758544921875
        },
        {
            "corpus_id": "248810776",
            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
            "text": "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task. Although learning difficulty has no formal and consensus definition, it has been widely discussed and utilized in previous machine learning literature, including noise-aware, curriculum, and metric learning. \n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty. \n\nDue to lack of a theoretical basis, different learning difficulty measures are based on different heuristic cues or empirical observations, resulting that each measure usually only suits specific application scenarios. A clearer understanding of the essence of a sample's learning difficulty can at least facilitate explaining difficulty-based weighting methods and designing more effective learning difficulty measures. However, we are still far from concluding that we have a comprehensive understanding of learning difficulty: \n\n(1) There is no formal definition of the learning difficulty of a sample. Different studies exhibit different understandings of learning difficulty.",
            "score": 0.4278847912168745,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2293
                },
                {
                    "start": 2296,
                    "end": 2369
                },
                {
                    "start": 2370,
                    "end": 2444
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "6644398"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "231807280"
                },
                {
                    "start": 848,
                    "end": 852,
                    "matchedPaperCorpusId": "52902973"
                },
                {
                    "start": 852,
                    "end": 855,
                    "matchedPaperCorpusId": "231807280"
                },
                {
                    "start": 1162,
                    "end": 1166,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "207995300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "266166181",
            "title": "Byte-ranked Curriculum Learning for BabyLM Strict-small Shared Task 2023",
            "text": "Large language models have been highly successful across a wide variety of tasks in Natural Language Processing. Due to the rapidly increasing model size and training data size, however, the cost to train new models is prohibitively expensive for many researchers. The BabyLM Challenge is a shared task designed to highlight methods for training language models at a smaller scale. These methods may lead to improvements in scaling up training more efficiently, training language models in low-resource settings, and drawing upon the way human children acquire language. \n\nIn this work, the strict-small track allowed our models to use a given dataset containing around ten million words from data sources that a child may encounter when learning language. No tools which used outside data for pretraining were allowed, reducing the ability to use many existing pipelines. This restriction is realistic for many lowresource scenarios in which these tools are lacking. \n\nThis work explores ordering training data by bytes per line for a curriculum learning approach. This measure of difficulty is inspired by the use of byte-based byte-pair-encoding tokenization and is easy to apply without needing any domain knowledge of the dataset. The results show that curriculum learning with this setup obtains improved results on benchmark evaluations when training for a set number of epochs. In settings in which additional tools, data, or computational resources are available, this curriculum setup is easy to apply and further evaluation in those settings is a potential area for future work. \n\nThis work used the Augie High-Performance Computing cluster, funded by award NSF 2018933, at Villanova University.",
            "score": 0.42358726343911346,
            "section_title": "Conclusion",
            "char_start_offset": 24019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "264439064",
            "title": "Irreducible Curriculum for Language Model Pretraining",
            "text": "Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on validation perplexity across all 7 domains compared to random uniform baseline and the anti-curriculum strategy. Our method also reduces the sharpness of the network and illustrates a better 5-shot accuracy on MMLU benchmarks.",
            "score": 0.4215131654193981,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71533203125
        },
        {
            "corpus_id": "226226711",
            "title": "Dynamic Data Selection for Curriculum Learning via Ability Estimation",
            "text": "In addition to test-set performance, we analyzed the efficiency of the curriculum learning training methods. For each experiment, we calculated the average number of training epochs required to reach the point of early stopping (based on held out dev set accuracy). For BERT base , fully supervised training is almost always the most efficient (Table 5). This should not be surprising, as the model is already pre-trained, and fine-tuning only requires a small number of passes over the task data.\n\nFor training the LSTM model, efficiency results are more mixed (Table 6). In most cases the fullysupervised training is again most efficient, however DDaCLAE does not incur significant efficiency costs. For QQP and RTE, DDaCLAE is the most efficient training paradigm. For MNLI, QNLI, and SST-2, DDaCLAE efficiency is within the 95% CI of the baseline results. Recall that for the LSTM model DDaCLAE is the most effective in terms of test set accuracy as well, so we can say that the improved test set performance does not come at the cost of training time efficiency. Figure 3 shows percentage plots of estimated difficulty for two of the GLUE classification tasks, QNLI and MRPC. As the plots show, the distribution in difficulty varies between the tasks. For MRPC, there are more difficult examples, percentage-wise, than in the QNLI data set. This reflects the current state of the GLUE leaderboard, where the top-performing model accuracies are 97.8% and 92.6% on QNLI and MRPC, respectively. 6 This is also reflected in our results, where model performance is higher for QNLI than MRPC (Table 2). Knowing the distribution of difficulty in a data set is useful information for model development and evaluation strategies. In the case of curriculum learning we leverage this learned difficulty to train our models.  Comparing training with DDaCLAE to training a fully-supervised baseline, the average impact on training time ranges from an additional few minutes for smaller data sets (e.g., MRPC) to an additional few hours for the larger data sets (e.g., MNLI). This impact grows with the data set size because when estimating ability, all of the training",
            "score": 0.41686657502673496,
            "section_title": "Training Efficiency (H3)",
            "char_start_offset": 21526,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "17138835",
            "title": "Exploring Large-Data Issues in the Curriculum: A Case Study with MapReduce",
            "text": "Over the past couple of decades, the field of computational linguistics, and more broadly, human language technologies, has seen the emergence and later dominance of empirical techniques and datadriven research. Concomitant with this trend is the requirement of systems and algorithms to handle large quantities of data. Banko and Brill (2001) were among the first to demonstrate the importance of dataset size as a significant factor governing prediction accuracy in a supervised machine learning task. In fact, they argue that size of training set is perhaps more important than the choice of machine learning algorithm itself. Similarly, experiments in question answering have shown the ef-fectiveness of simple pattern-matching techniques when applied to large quantities of data (Brill et al., 2001). More recently, this line of argumentation has been echoed in experiments with large-scale language models. Brants et al. (2007) show that for statistical machine translation, a simple smoothing method (dubbed Stupid Backoff) approaches the quality of Kneser-Ney Smoothing as the amount of training data increases, and with the simple method one can process significantly more data. \n\nGiven these observations, it is important to integrate discussions of large-data issues into any course on human language technology. Most existing courses focus on smaller-sized problems and datasets that can be processed on students' personal computers, making them ill-prepared to cope with the vast quantities of data in operational environments. Even when larger datasets are leveraged in the classroom, they are mostly used as static resources. Thus, students experience a disconnect as they transition from a learning environment to one where they work on real-world problems. \n\nNevertheless, there are at least two major challenges associated with explicit treatment of largedata issues in an HLT curriculum: \n\n\u2022 The first concerns resources: it is unclear where one might acquire the hardware to support educational activities, especially if such activities are in direct competition with research. \n\n\u2022 The second involves complexities inherently associated with parallel and distributed processing, currently the only practical solution to large-data problems. For any course, it is diffi-cult to retain focus on HLT-relevant problems, since the exploration of large-data issues necessitates (time-consuming) forays into parallel and distributed computing.",
            "score": 0.4160265860183876,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1906
                },
                {
                    "start": 1909,
                    "end": 2097
                },
                {
                    "start": 2100,
                    "end": 2260
                },
                {
                    "start": 2261,
                    "end": 2456
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 343,
                    "matchedPaperCorpusId": "6645623"
                },
                {
                    "start": 913,
                    "end": 933,
                    "matchedPaperCorpusId": "633992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1773681640625
        },
        {
            "corpus_id": "273162494",
            "title": "Scaling Parameter-Constrained Language Models with Quality Data",
            "text": "Recent advancements in language model (LM) development have been significantly influenced by the exploration of scaling laws, which articulate the relationship between training loss, dataset size, and the number of model parameters (Hestness et al., 2017;Kaplan et al., 2020;Aghajanyan et al., 2023). These scaling laws have been instrumental in predicting the computational resources necessary for training increasingly large models and have provided a framework for understanding how model performance scales with data and parameters (Hoffmann et al., 2022;Kaplan et al., 2020). However, these laws primarily focus on the quantity of data and model size, often underestimating the critical role of data quality in model generalization. \n\nIn this work, we challenge the prevailing focus 1 on merely increasing data volume and model size by emphasizing the importance of data quality, particularly in scenarios constrained by the number of model parameters. We argue that for sub-billion parameter models, the quality of data-or what we term as effective training tokens -plays a more decisive role in model performance than previously recognized. This perspective shifts the paradigm from a quantity-centric view to a quality-centric approach in the development of language models. Further, we provide qualitative measures of standard data refinement techniques including data sampling (Penedo et al., 2023;Wang et al., 2024;Albalak et al., 2024) and text synthesis (Liu et al., 2024), applied to a pretraining corpus such as Re-finedWeb (Penedo et al., 2023). This helps to formulate the relationship between the diversity and syntheticity of pretraining data in order to compute the number of effective training tokens, which evaluate the impact of data quality in terms of model size and the token number. Further, we conduct extensive experiments across eight different benchmarks to evaluate the impact of data refinement techniques which allow us to significantly outperform models trained on randomly selected data samples, across a spectrum of model sizes ranging from 25 million to 1.5 billion parameters. \n\nBy integrating the notion of effective token size into the scaling law formulation, we extend the existing scaling law formulation to better capture the nuances of data quality.",
            "score": 0.41549521235762266,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 299,
                    "matchedPaperCorpusId": "255570036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343505859375
        },
        {
            "corpus_id": "237635337",
            "title": "Towards A Measure Of General Machine Intelligence",
            "text": "In this section, we compute the values of the g-index and its components for some well-known large language models. We use a small dataset of text prompts and their associated flow-based programs to finetune transformer-based models before measuring their g-index scores. We construct a small dataset of real-world tasks from 16 task domains to train the models. The task domains are described in Appendix A. A sample of the dataset is available at https://github.com/mayahq/g-indexbenchmark. We finetune four transformer models: GPT2-345M, GPT2-774M, and GPT2-1.5B from [51], and GPT-Neo-2.7B from [62,55]. We use the HuggingFace implementations [59] of the transformer models in the experiments. \n\nWith the current set of experiments, we aim to measure skill-acquisition efficiency via the g-index with tasks of low generalization difficulty. The average domain distance \u2126 between the training set and test sets across all experiments is 0.09. The training samples range from 640 to 10240 across all the experiments. In every experiment, the training samples were distributed equally across all 16 task domains. After training, the models are tested with 5 unseen samples per task to obtain their average performance \u03b8. In every experiment, the number of training epochs was held constant at 30. When synthesizing the programs, we hold the temperature of the models at a constant 0.7, and allow only one attempt at synthesis. We expect more attempts for a given task specification will yield better performance [63]. We examine the following relationships: \n\n1. average performance \u03b8 vs program size (Figure 10): The programs generated for each task are of different sizes. The size of the program (number of characters in the program text) affects how easily it can be generated. For instance, the number of tokens transformer models can generate is bounded by their context window. We expect model performance to falter as the size of the program to generate increases. \n\n2. skill levels vs program size (Figure 11): The skill of an intelligence system is its ability to consistently generate correct programs to solve tasks in a given domain.",
            "score": 0.41520557188558943,
            "section_title": "Experiments",
            "char_start_offset": 40777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1973
                },
                {
                    "start": 1976,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271484375
        },
        {
            "corpus_id": "248965007",
            "title": "Let the Model Decide its Curriculum for Multitask Learning",
            "text": "In addition to arranging the training instances into a learning curriculum, computing difficulty scores using model-based techniques has shown its benefits in several other areas, such as improving selective prediction ability (Varshney et al., 2022b), under-standing training dynamics (Swayamdipta et al., 2020), and efficient evaluations (Varshney et al., 2022a). However, these techniques present a few challenges: \n\n1. Computation: They involve calculating the difficulty scores of training instances which requires additional computation. However, this computation is only required during training and not required during inference. Hence, it does not add any computational overhead at inference time when deployed in an application. 2. Noisy Instances: Training instances that are wrongly annotated/noisy will most certainly get assigned a very high difficulty score and hence will learned at the end during training. This is unlikely to hamper learning when the number of noisy instances is small. However, it may negatively impact the model's learning when the training dataset has a non-trivial number of noisy instances. We plan to investigate this in our future work.",
            "score": 0.4147960497555019,
            "section_title": "H Limitations of Computing Difficulty Scores using Model-based Techniques",
            "char_start_offset": 12171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1178
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 251,
                    "matchedPaperCorpusId": "247778884"
                },
                {
                    "start": 286,
                    "end": 312,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 340,
                    "end": 364,
                    "matchedPaperCorpusId": "247292373"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "254366532",
            "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
            "text": "Recently, large-scale deep learning models are empowering us to achieve more in many ways, such as code generation (GitHub 2021) and text-to-image generation (Ramesh et al. 2022;Rombach et al. 2022). To keep improving the service quality, deep learning model architecture evolves rapidly, and the model size is also growing at a tremendous speed. The increasing model size leads to unprecedented training cost (especially for foundation model pretraining), which recently grows to 2 months on thousands of GPUs/T-PUs (Smith et al. 2022;Chowdhery et al. 2022). On the other hand, a less-emphasized perspective is that data scale is actually increasing at a similar speed as model scale, and Figure 1: Model scale (number of parameters) and data scale (number of consumed training tokens ) of representative language models in the last 5 years (Devlin et al. 2019;Shoeybi et al. 2019;Brown et al. 2020;Scao et al. 2022;Chowdhery et al. 2022). \n\nthe training cost is proportional to both of them. As plotted in Fig. 1, for several representative language models in the last 5 years both the model and data scales increase at a similar speed. Recent works including Chinchilla (Hoffmann et al. 2022) and PaLM 2 (Google 2023) emphasize the need of increasing data scale at an even faster speed. This demonstrates the importance of improving data efficiency: achieve same model quality with less data and reduced training cost, or achieve better model quality with the same amount of data and similar training cost. \n\nThere are two popular research directions among existing data efficiency techniques: Data sampling techniques aim to improve the convergence speed by sampling the most suitable next data batch from the whole data pool; Data routing techniques aim to reduce the computation by routing each data to only a subset of the model components. These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al. 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022).",
            "score": 0.41329363313437717,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 178,
                    "end": 198,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 842,
                    "end": 862,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 882,
                    "end": 900,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 900,
                    "end": 917,
                    "matchedPaperCorpusId": "16503693"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "On first approximation, the difficulty of training a model to perform well on a task appears to be primarily determined by the task's intrinsic complexity. However, the field of natural language processing presents a compelling counterexample. Despite the vast, inherent complexity of natural language, significant developments have been made in automating classic NLP tasks, such as translation and classification, through the development and application of large language models. We think this progress can largely be explained by the vast amount of natural language data freely available on the internet, which has enabled the training of these models on an unprecedented scale. The success in automating NLP tasks suggests that the availability of abundant, relevant training data can help overcome the challenges posed by intrinsically complex tasks. \n\nIn domains where data is less abundant, such as general-purpose robotics, we speculate that the difficulty of achieving high-performance in these domains may be significantly influenced by the transfer gap from a cheap, abundant pre-training distribution to the downstream task of interest. This is because, although collecting finetuning data can be expensive, it may be feasible to leverage transfer learning from much cheaper pre-training distributions with additional model scaling. Indeed, roboticists have identified the hardness of effective sim2real transfer as a key difficulty in making progress in robotics (see, for example, Weng 2019). \n\nGiven that our framework enables the direct measurement of the transfer gap, we believe it can be a valuable tool for estimating the difficulty of achieving high performance on tasks in various domains. By quantifying the transfer gap between cheap, abundant pre-training data (such as internet text data) and the target task data, researchers can gain insights into the potential challenges and feasibility of achieving high performance on specific tasks using machine learning. This information can help guide decisions on resource allocation, such as whether to focus on collecting more task-specific data or investing in larger-scale pre-training. Ultimately, understanding the transfer gap may be key to unlocking progress in domains where data scarcity has been a significant barrier to performance.",
            "score": 0.409693050453808,
            "section_title": "The transfer gap can set the difficulty of achieving high performance in novel domains",
            "char_start_offset": 25356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2314
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.525390625
        },
        {
            "corpus_id": "265051074",
            "title": "Data Factors for Better Compositional Generalization",
            "text": "We show how models behave differently with different distributions of example difficulties. \n\nSimpler examples make generalization easier on SCAN*. The results are shown in Fig. 5. All the models are tested on the same testing set similar to the original SCAN Jump testing set. For both settings, we can see a steadily decreasing trend when the examples become harder. When the maximum length is reduced from 500 to 62, the performance is increased from 16.65% to 47.31%.9 With only 2 unique primitives per example, the performance is also increased to 49.14%. These results demonstrate that easier examples can make the correct composition easier to learn. \n\nMix of simple and hard examples needed on real language datasets. We next examine the impact of example difficulty on more complex larger-scale natural language datasets. Due to the flexible and diverse nature of natural language in real datasets, models now not only need to understand the correct composition but also need to capture other language uses through potentially non-compositional ways. Therefore, the trends in natural language datasets can be different from the previous observation. For this study, we conduct experiments on ATIS (Price, 1990;Dahl et al., 1994), SM-CalFlow (Andreas et al., 2020) and the compositional version SMCalFlow-CS (Yin et al., 2021). For all three datasets, we train models on multiple subsets with different difficulties but all con- tain 25% of the training examples. Since on these datasets, we no longer have access to the ground truth example complexity, so we present results with prototype-based difficulty in Figure 4. We observe similar but more complicated trends on larger-scale natural language datasets. First, we observe that difficult examples become important on these datasets. On all three datasets, using only the easiest examples leads to the worst results. We suspect that only using the simplest examples does not provide enough coverage for all the diverse linguistic phenomena in the dataset. However, the best performance is also not achieved with the most difficult examples, but at the medium level. Additionally, we notice that even in largerscale natural language datasets, simpler examples are still important for compositional generalization.",
            "score": 0.4095521819373453,
            "section_title": "Experiments",
            "char_start_offset": 17954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 1206,
                    "end": 1219,
                    "matchedPaperCorpusId": "3047811"
                },
                {
                    "start": 1219,
                    "end": 1237,
                    "matchedPaperCorpusId": "8180378"
                },
                {
                    "start": 1316,
                    "end": 1334,
                    "matchedPaperCorpusId": "235097473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6162109375
        },
        {
            "corpus_id": "236912520",
            "title": "Curriculum learning for language modeling",
            "text": "Seeking to represent natural language, researchers have found language models (LM) with Sesame Street-inspired names [1] [2] [3] to be incredibly effective methods of producing language representations (LR). These LM's have leverage transfer learning by training on a large text corpus to learn a good representation of language which can then be used in a down steam task like Question Answering or Entity Resolution. While these LMs have shown to be excellent methods to enable language understanding, the ability to train these models is becoming increasingly computationally expensive [4]. Since model performance is closely tied to the size of training data, model size, and compute used to train [5] the bulk of existing research has focused on scaling these aspects without much focus on increasing efficiency of training. Seeking to explore what methods could be used to make LM training more efficient we study the effect of curriculum learning by training ELMo with a wide variety of curricula. Curriculum learning (CL) is a training methodology which applies structure to a models training data. CL has been studied broadly in natural language processing and has been very successful in domains like Neural Machine Translation (NMT) where CL based models are able to train faster and produce better results [6] [7] [8] than unstructured, stochastic sampling. Focusing on LMs, Xu et al. [9] showed that CL can be used in LM finetuning as a way to improve task performance. Despite an abundance of work exploring CL and LMs to the best of our knowledge we are the first to examine the effect of curriculum learning in LM pre-training and transfer performance. To evaluate the effect of CL on LMs we train ELMo with a variety of curricula on the wikitext-2 and wikitext-103 [10] without modification of training time or model hyperparameters. We evaluate model performance on the pre-training task and on the GLUE Benchmark [11] building on the work of Competence Based Curriculum Learning [12] by modifying training sampler within the LM to produce a dataset with gradually increasing difficulty 2 . The contributions of our work are: \n\n\u2022 Exploration of the effects of curriculum learning for language modeling finding no clear improvement to models that use curriculum methods for training.",
            "score": 0.40804421716307404,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2143
                },
                {
                    "start": 2146,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 589,
                    "end": 592,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 1326,
                    "end": 1329,
                    "matchedPaperCorpusId": "215746703"
                },
                {
                    "start": 1397,
                    "end": 1400,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1932,
                    "end": 1936,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.615234375
        },
        {
            "corpus_id": "269756933",
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "text": "Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).However, these metrics may not fully reflect the complexity of a dataset.It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics.Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective.By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones.This approach requires organizing data by its level of complexity, for which we have established three principal criteria.By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.",
            "score": 0.4064586197702927,
            "section_title": "Related Work",
            "char_start_offset": 5440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 625
                },
                {
                    "start": 627,
                    "end": 676
                },
                {
                    "start": 678,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1090
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 163,
                    "matchedPaperCorpusId": "255221201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "273950013",
            "title": "Fox-1: Open Small Language Model for Cloud and Edge",
            "text": "These costs also restrict most researchers from participating, and the high deployment costs further hinder the widespread application of these advanced AI technologies. \n\nRecently, there has been a growing interest in training Small Language Models (SLMs) that achieve performance comparable to models four times their size. Examples of these models include the Phi series Gunasekar et al. [2023a], Li et al. [2023], Abdin et al. [2024], TinyLlama Zhang et al. [2024], OpenELM Mehta et al. [2024], Gemma Team et al. [2024], MiniCPM Hu et al. [2024] and Qwen Bai et al. [2023]. Research on these models explores various aspects of training effective SLMs, such as data paradigms, model architecture, and tokenizers. However, it remains unclear how to further enhance the training curriculum and data organization at each stage. \n\nIn this report, we introduce Fox-1, a series of Small Language Models (SLMs) that primarily explore research issues related to training curricula. We present a high-performance SLM with extensive investigation into training techniques. We release all our model weights publicly under the Apache 2.0 license, making them accessible on the TensorOpera AI Platform and Hugging Face3 .",
            "score": 0.40430144918892086,
            "section_title": "Introduction",
            "char_start_offset": 1761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1211
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.079345703125
        },
        {
            "corpus_id": "221660842",
            "title": "Curriculum Learning with Diversity for Supervised Computer Vision Tasks",
            "text": "Curriculum learning. Bengio et al. [2] introduced the idea of curriculum learning (CL) to train artificial intelligence, proving that the standard learning paradigm used in human educational systems could also be applied to automatic models. CL represents a class of easy-tohard approaches, which have successfully been employed in a wide range of machine learning applications, from natural language processing [8,16,19,21,31], to computer vision [6,7,9,15,18,27,35], or audio processing [1,22]. \n\nOne of the main limitations of CL is that it assumes the existence of a predefined metric which can rank the samples from easy to hard. These metrics are usually task-dependent with various solutions being proposed for each. For example, in text processing, the length of the sentence can be used to estimate the difficulty of the input (shorter sentences are easier) [21,30], while the number and the size of objects in a certain sample can provide enough insights about difficulty in image processing tasks (images with few large objects are easier) [27,29]. In our paper, we employ the image difficulty estimator of Ionescu et al. [12] which was trained considering the time required by human annotators to identify the presence of certain classes in images. \n\nTo alleviate the challenge of finding a predefined difficulty metric, Kumar et al. [17] introduce self-paced learning (SPL), a set of approaches in which the model ranks the samples from easy to hard during training, based on its current progress. For example, the inputs with the smaller loss at a certain time during training are easier than the samples with higher loss. Many papers apply SPL successfully [26,32,33], and some methods combine prior knowledge with live training information, creating self-paced with curriculum techniques [14,36]. Even so, SPL still has some limitations, requiring a methodology on how to select the samples and how much to emphasize easier examples. Our approach is on the borderline between CL and SPL, but we consider it to be pure curriculum, although we use training information to advantage less visited classes. During training, we only count the labels of the training samples, which is a priori information, and not the learning progress.",
            "score": 0.40250817981748205,
            "section_title": "Related Work",
            "char_start_offset": 5718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2246
                }
            ],
            "ref_mentions": [
                {
                    "start": 415,
                    "end": 418,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "397556"
                },
                {
                    "start": 448,
                    "end": 451,
                    "matchedPaperCorpusId": "10364203"
                },
                {
                    "start": 451,
                    "end": 453,
                    "matchedPaperCorpusId": "8105909"
                },
                {
                    "start": 453,
                    "end": 455,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "11423732"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "11590585"
                },
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "19805513"
                },
                {
                    "start": 867,
                    "end": 871,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 871,
                    "end": 874,
                    "matchedPaperCorpusId": "1900277"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 1133,
                    "end": 1137,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 1346,
                    "end": 1350,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1672,
                    "end": 1676,
                    "matchedPaperCorpusId": "3405508"
                },
                {
                    "start": 1676,
                    "end": 1679,
                    "matchedPaperCorpusId": "10118550"
                },
                {
                    "start": 1679,
                    "end": 1682,
                    "matchedPaperCorpusId": "813192"
                },
                {
                    "start": 1804,
                    "end": 1808,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 1808,
                    "end": 1811,
                    "matchedPaperCorpusId": "52114770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6259765625
        },
        {
            "corpus_id": "254017982",
            "title": "Detecting Entities in the Astrophysics Literature: A Comparison of Word-based and Span-based Entity Recognition Methods",
            "text": "We compare word-based and span-based entity recognition models using both RoBERTa-base and RoBERTa-large models. Results in Table 1 show that span-based model outperforms word-based model by 0.011 F 1 when RoBERTa-base is used, while 0.015 F 1 when RoBERTa-large is used. From Table 1, we also observe modest benefit of using RoBERTa-large over RoBERTa-base (0.019 with word-based and 0.023 with span-based). \n\nTask-adaptive pre-training does not guarantee better performance Some studies have shown that pre-trained language models are more effective when pre-training data is similar to downstream task data (Dai et al., 2019). Task-adaptive pretraining (Howard and Ruder, 2018;Gururangan et al., 2020)-continue pre-training on the unlabeled training set for a given task-is a cheap adaptation technique that aims to reduce the disparities between models pre-trained on generic data and domain-specific task data. Step 0 means the vanilla RoBERTa-large is used. \n\nWe continue pre-training RoBERTa-large on the DEAL training set using masked language modeling. The total number of optimization steps is 3,000 (\u2248 100 epochs), and we save checkpoints every 600 steps. During the task-adaptive pre-training stage, we observe both the training and development losses keep decreasing, however, the resulting task-adaptive pre-trained checkpoints seem to be very unstable and do not guarantee improved effectiveness (Figure 2). Note that Gururangan et al. (2020) reported improved effectiveness via taskadaptive pre-training RoBERTa-base, whereas we use RoBERTa-large. We conjecture the observed instability may be attributed to the optimization difficulties discussed by Mosbach et al. (2021), when continue training large size models on small data. \n\nErrors due to over-segmentation One problem we observe is that many domain-specific terminologies are split into multiple sub-tokens and then taken as input to the encoder.",
            "score": 0.4022309088984814,
            "section_title": "Results and Discussion",
            "char_start_offset": 6807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 610,
                    "end": 628,
                    "matchedPaperCorpusId": "90262493"
                },
                {
                    "start": 656,
                    "end": 680,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 680,
                    "end": 704,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1433,
                    "end": 1457,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1667,
                    "end": 1688,
                    "matchedPaperCorpusId": "219558836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "276742147",
            "title": "Your Model is Overconfident, and Other Lies We Tell Ourselves",
            "text": "We can also leverage the different pools of models to assess how their factors of variation might impact data complexity metrics. In particular, our heterogeneous group of 1B models was defined with respect to different PLMs and training subsets, and therefore we can measure whether pretraining conditions are more impactful than supervised finetuning data. In practice, we can measure how likely it is that two predictions for a specific datapoint will match, given that they were made by classifiers trained from the same model or by classifiers trained on the same training subset of SNLI. This can be measured using common language effect sizes derived from a Mann-Whitney U test. Doing suggests a statistically significant effect from both splits and models (p < 10 \u221244 ) with a very small effect size (f \u2248 51.2%) on SNLI. On MNLI, we find a somewhat stronger effect (f = 53.10%, p < \u03f5) when considering classifiers derived from the same PLM; as for the training data, it appears to yield the opposite effect, though with a much higher pvalue (f = 49.77%, p < 10 \u22123 ). For DynaSent, recall we have no sub-splits to experiment with; however we do find a positive effect when considering classifiers derived from the same PLMs (f = 53.63%, p < \u03f5). \n\nOur homogeneous <1B pool also allows us to look into whether responses are more likely to differ for two models with a larger difference in number of parameters. To test this, we can measure the likelihood of the parameter count difference being larger when the predictions differ using U tests. Doing so, we can observe a common language effect size of f = 45.96% on SNLI, f = 45.63% on MNLI, and f = 45.69% on DynaSent. We can likewise observe a similar effect when focusing on our heterogeneous pool: we find a common language effect size of f = 48.90% for SNLI, f = 45.63% for MNLI, and f = 43.72% on DynaSent. In other words, predictions that match tend to come from models with more similar parameter counts.",
            "score": 0.4014826279354535,
            "section_title": "B.3 Factors shaping model dissensus",
            "char_start_offset": 34369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06488037109375
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "We introduce an effective curriculum learning framework that employs prior knowledge about sample difficulty in its training paradigm for curriculum discovery. The proposed framework initially partitions its input data into several groups of increasing difficulty, defines parameterized func-  Table 2. tions to weight sample losses in each difficulty group, moves samples across difficulty groups based on their learning progress, and enables tuning the parameters of the weight function to discover novel curricula. We demonstrate that this framework is capable of representing several categories of curriculum learning approaches. The task of curriculum discovery alleviates the limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. In addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. The task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. To mitigate the computational : Confidence assignment to samples in our datasets by three CL approaches. The x-axis is the epoch number, and y-axis is the average weight assigned to samples of each difficulty group. Blue (solid) is easy, orange (dashed) is medium, and green (dash-dot) is hard. The shaded area is the 95% CI over the datasets with five random seeds each. The curves are monotonic for most parts, and can be approximated by our framework.\n\ncost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset.\n\nThere are several promising areas for future work. These include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. Finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research.",
            "score": 0.4001090548068646,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 27435,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "266840465",
            "title": "Improving Turkish Text Sentiment Classification Through Task-Specific and Universal Transformations: An Ensemble Data Augmentation Approach",
            "text": "Post-hoc tests, such as Tukey's Honestly Significant Difference (HSD), can be employed to further investigate pairwise differences between specific datasets. The transformation function factor also exhibited a highly significant impact on model performance, as evidenced by an F-value of 297.60 (p < 0.001). This result suggests that the various text transformation techniques employed significantly influenced the performance       significant but also exert varying degrees of influence on model performance. \n\nTo assess the efficiency and effectiveness of the augmentation techniques and models, we conducted experiments on a benchmark dataset using various augmentation strategies. The results, shown in Table 22, provide insights into the training times and performance metrics associated with each combination of model and augmentation technique. From Table 22, it is evident that there exists a substantial variation in training times and performance metrics across different augmentation techniques and models. Notably, the choice of augmentation technique significantly influences both training efficiency and model performance. Augmentation strategies such as ''No Augmentation'' and ''Spelling Error (SE)'' tend to result in lower training times, while more complex techniques like ''Proposed Scheme'' lead to longer training durations. Moreover, we observe that certain models exhibit varying degrees of sensitivity to the augmentation techniques. For instance, ConvBERTurk tends to demonstrate relatively higher training times across the board, while ELECTRA shows competitive performance across multiple augmentation strategies. These findings emphasize the importance of carefully selecting augmentation techniques in conjunction with the choice of model to strike a balance between training efficiency and model performance. Researchers and practitioners should consider the specific requirements of their tasks when making these selections.",
            "score": 0.39842186399273494,
            "section_title": "D. EXPERIMENTAL RESULTS",
            "char_start_offset": 80503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1957
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262939453125
        },
        {
            "corpus_id": "256631050",
            "title": "Data Cartography for Low-Resource Neural Machine Translation",
            "text": "It is well established that data quality can have a large impact on the performance of MT systems. Khayrallah and Koehn (2018) shows how different types of training set noise impact the quality of neural MT models. Dual cross-entropy (Junczys-Dowmunt, 2018) and other techniques are routinely used to clean crawled datasets that are known to contain many types of noise (Caswell et al., 2020). \n\nTraining sample difficulty is often considered in the context of curriculum learning, which aims to present training examples to the model in an order that benefits model performance or convergence speed. Some of the earlier works by Bengio et al. (2009); Cirik et al. (2016) show that ordering training samples by sentence length can decrease training speed or improve model performance. Zhang et al. (2018);Platanios et al. (2019) go on to develop curriculum learning strategies building on the use of linguistic features such as sentence length or word frequency; but their results are mixed. Later works have looked at more model-focused techniques for deciding the difficulty of training examples such as the norm of source words (Liu et al., 2020), language model scores on monolingual text (Zhou et al., 2020) or the change in the decrease of MT model loss (Xu et al., 2020). These techniques have shown more consistent improvements in translation quality but still do not query information from the MT model itself; except in the case of the latter which still only looks at local changes in the model loss. In contrast Data Maps are based on a holistic view of the training process. \n\nPrior work on low-resource settings has primarily focused on modeling strategies or supplement-ing the training data. Araabi and Monz (2020) determined appropriate hyper-parameters for Transformer models that were initially developed in high resource settings. S\u00e1nchez-Cartagena et al. (2021) paired data augmentation with a multi-task modeling approach to strengthen the power of the model's encoder and decoder.",
            "score": 0.39781068750396187,
            "section_title": "Related Work",
            "char_start_offset": 19511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 126,
                    "matchedPaperCorpusId": "44090489"
                },
                {
                    "start": 370,
                    "end": 392,
                    "matchedPaperCorpusId": "225094586"
                },
                {
                    "start": 1131,
                    "end": 1149,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 1193,
                    "end": 1212,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 1260,
                    "end": 1277,
                    "matchedPaperCorpusId": "227227757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "271212779",
            "title": "Curriculum Learning for Small Code Language Models",
            "text": "We designed a code difficulty metric combining software measures, referred to as OM, to categorize generated programs into easy, medium and hard levels.The inverse correlation between the OM scores and the model accuracies validates its effectiveness for program difficulty assessment.An interesting observation is that conditionals posed more difficulty for models than loops, contrary to expectations.This suggests certain language features are inherently harder to learn for models.\n\nThis categorization allowed us to explore various three-stage curriculum schedules for model training.Our experiments revealed that the hybrid technique achieves much higher output accuracy compared to the conventional training baseline, especially on complex code, indicating its effectiveness in incrementally developing model capabilities.However, the sequential strategy, while helping models learn hard concepts, suffers a loss in overall accuracy.This highlights the importance of curriculum design : simply progressing from easy to hard tasks does not guarantee gains.\n\nIn the context of code completion tasks, the influence of CL is not as significant as expected.This implies that the advantages of CL may not be applicable to all tasks, but instead, they may vary based on the particular characteristics of the task.\n\nFurthermore, our fine-tuning experiments with the Code Llama 7B model further validated the effectiveness of curriculum learning.While the gains in code completion tasks were minor, the hybrid CL approach significantly improved code execution performance.These findings reinforce our findings that a well-designed curriculum can enhance model capabilities, especially for complex tasks, even when scaling to larger models.9 Related Works",
            "score": 0.3978059165024329,
            "section_title": "Discussion",
            "char_start_offset": 18620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 485
                },
                {
                    "start": 487,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 940
                },
                {
                    "start": 940,
                    "end": 1062
                },
                {
                    "start": 1064,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1313
                },
                {
                    "start": 1315,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1737
                },
                {
                    "start": 1737,
                    "end": 1752
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "266182457",
            "title": "Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
            "text": "CLIMB (Martinez et al., 2023). This submission presents a thorough comparison of different approaches to curriculum learning in the Strict-Small setting. They consider three main criteria for curriculum design: the size of the input vocabulary, the difficulty of the training sample, and the size of the output space for MLM prediction. They conduct experiments exploring eight different curricula sorted into these three main approaches. While there are many small differences in performance among these settings, curricula provide no consistent improvements over more naive training algorithms. \n\nAcquiring Linguistic Knowledge from Multimodal Input (Amariucai and Warstadt, 2023). The authors explored whether vision-language co-training helps the learning of linguistic knowledge. They trained models on Wiki texts with images using the state-of-the-art multi-modality model (FLAVA). After varying the amount of training data and how many images are used, the authors found that visual input only provides a slight improvement on grammar benchmarks for 10M-word training, but not for 100M-word training. \n\nGPT-like Models are Bad Babies (Steuer et al., 2023). This paper trains a decoder-only model, trying different hyperparameters, including reordering the training data by different orders (based on cues which did not improve over regular shuffling), different sizes, layer widths, among other features. The main focus of the paper is to test if models that perform better on BabyLM evaluation tasks are also better at modeling reading difficulty in humans. Surprisingly, models performing better on BabyLM tasks performed less well in modeling reading difficulty. \n\nBaby's CoThought (Zhang et al., 2023). This system leverages a large language model, GPT-3.5-Turbo, to reformat semantically unrelated sentences into cohesive paragraphs. In low-data settings, this approach can form better training examples for language models; the proposed approach results in improvements across BLiMP tasks, though performance is not significantly different on (Super)GLUE or MSGS. Note that the LLM is trained on far more than 100M words, so this submission technically does not qualify under any track.",
            "score": 0.3977999494351352,
            "section_title": "F Summary of Each Submission",
            "char_start_offset": 56718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 29,
                    "matchedPaperCorpusId": "265213396"
                },
                {
                    "start": 1692,
                    "end": 1712,
                    "matchedPaperCorpusId": "259188012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2413330078125
        },
        {
            "corpus_id": "231846815",
            "title": "Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning",
            "text": "On Table 3, we observe that soft edit distance (SED) yields the best performance, outperforming a model that does not use curriculum learning by as much as 2.42 BLEU. It also outperforms all other metrics by roughly 1 BLEU. In general, we see that models perform better on joint and text than on data. This correlates to how a difficulty function is related to the average bin sizes of scores it generates. We see that for models that distinguish samples in a more defined manner, it will have a smaller average bin size where probability of having more difficult samples at every confidence threshold is lower. From this, we see that length and DLD have larger average bin sizes across its difficulty scores and this makes samples less distinguishable from one another. Thus, they result in the smallest improvement over plain. We show reordered samples in Table 1 for all difficulty metrics computed jointly on data and text. We include length (L), rarity (R), Damerau-Levenshtein Distance (DLD), and the proposed soft edit distance (SED). On the other hand, we also justify the use of weighting for edit operation where PED, which is the \"hard\" variant of SED that does not weight edit operations like SED, is shown to be far inferior to that of SED. The score margin comes up to 2.81 BLEU. Moreover, we further examine the difference in sample orders and observe that SED yields more intuitive and better sample ordering as opposed to other metrics.\n\nHuman Evaluation. For human evaluation, three annotators are instructed to evaluate 100 samples from the joint variant to see (1) if the text is fluent (score 0-5 with 5 being fully fluent), (2) if it misses information contained in the source data and (3) if it includes wrong information. These scores are averaged and presented in Table 2.   On Training Speed. We define speed by the number of updates it takes to reach a performance plateau. On Figure 3, the speedup is measured by the difference between the vertical bars. It can be observed that curriculum learning reduces the training steps to converge, where it consists of up to 38.7% of the total updates for the same model without curriculum learning (on E2E",
            "score": 0.39761531169626235,
            "section_title": "Results & Analysis",
            "char_start_offset": 9001,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "We first summarize the process of creating the curriculum and then explain the strategy for training over it. As described in the previous section, sentences or documents are sorted by their complexity score, which are then distributed into nonoverlapping bins, essentially subsets of data (also known as shards ), such that samples in each bin are similar in complexity.\n\nThe training consists of t sequential phases, where t denotes the different points of time during the training, where training samples are fetched only from a subset of bins. For instance, t = 1 may correspond to the first epoch or first n steps, t = 2 may correspond to the second epoch or the next n steps, and so on. In our experiments, t denotes an epoch. A subset consists of one or more bins, and for creating and iterating over these training subsets during training, we explore two different strategies -BINNED and STEPPED.\n\n(i) BINNED: In this variant, the model is trained sequentially on each bin, one at a time. In other words, the model is first trained on the first bin and its state is saved. The training then continues from the saved checkpoint on the next bin, and so on. This is similar to the case where a subset consists of only one bin, and we iterate over it for one epoch.\n\nThe bins themselves can be accessed in order of either increasing difficulty (from easy to medium to hard), an approach that can be intuitively seen as mimicking the way humans learn, or in the reverse order of decreasing difficulty (from hard to medium to easy), a technique shown to benefit machine learning algorithms (Weinshall et al., 2018). In doing so, the question we ask is whether curriculum or anti-curriculum help in the context of language modeling, if at all.\n\nIt is worth mentioning that while the bins are accessed in a pre-defined order (i.e., easy to hard or reverse), the samples within the bins are still randomly selected, thus combining a deterministic schedule with the benefits of randomization that serve neural models well.\n\n(ii) STEPPED: Alternatively, the bins could be accessed cumulatively where the training set progressively increases in size by addition of newer bins while retaining earlier bins (also referred to as Baby Steps curriculum (",
            "score": 0.3968155922609232,
            "section_title": "Corpus Ordering for Pretraining",
            "char_start_offset": 11328,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1592,
                    "end": 1616,
                    "matchedPaperCorpusId": "5004002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "235353014",
            "title": "Annotation Curricula to Implicitly Train Non-Expert Annotators",
            "text": "As shown in recent works, the losses of a masked language model may be used to obtain an assessment of text complexity (Felice and Buttery 2019). We use the implementation of Salazar et al. (2020). \n\nAdaptive estimators. While simple heuristics or annotator-unaware models allow us to pre-compute annotation curricula, they do not consider any user-specific aspect that may influence the difficulty estimation (Lee, Meyer, and Gurevych 2020). Consequently, the resulting curriculum may not provide the optimal ordering for a specific annotator. To select the instance with the most appropriate difficulty for an annotator a i (\u2022) \n\nat the i-th iteration, we use a model \u03b8 i (\u2022) \u223c a i (\u2022) that is updated with an increasing number of annotated instances. We conjecture that using \u03b8(\u2022) to predict the relative difficulty -in contrast to non-adaptive estimators that provide an absolute difficulty estimation -may be more robust to task-specific influences as they are inherited in all instances annotated by a(\u2022). When training adaptive estimators, we use annotation time to approximate the difficulty of a specific instance due to its availability in any annotation scenario. At iteration i, we thus train the model \u03b8 i : L \u2192 T \u2286 R + to predict the annotation times t \u2208 T for all labeled instances x \u2208 L. Similar to active learning, we now encounter a decreasing number of unlabeled instances and an increasing number of labeled instances. The resulting model is then used to estimate the annotation time for all unlabeled instances x \u2208 U. The resulting scoring function is now defined as f a : \u03b8 i , U \u2192 R + . Finally, we select instance x * \u2208 U with the minimal rank according to f a . \n\nx * = arg min \n\nFollowing our strategy S, this results in selecting instances for annotation that have the lowest predicted annotation time. We specifically focus on regression models that can be trained efficiently in-between annotation and work robustly in low-data scenarios. We choose Ridge Regression (RR), Gaussian Process Regression (GP) and GBM Regression (GBM).",
            "score": 0.3957557346263929,
            "section_title": "Masked Language Modeling Loss (mlm)",
            "char_start_offset": 16720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 144,
                    "matchedPaperCorpusId": "203688153"
                },
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "218628872"
                },
                {
                    "start": 410,
                    "end": 441,
                    "matchedPaperCorpusId": "218581653"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "276574840",
            "title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models",
            "text": "\u2022 Scaling -Investigate data scaling laws on RL training. In particular, there are a number of training algorithms, and new methods continually developing, and the efficiency of each is unknown [Team et al., 2025]. For example, in very recent studies, Hou et al. [2025] demonstrate that training data and inference time compute scale in a complementary way (increasing inference is most useful when also scaling training data), and Setlur et al. [2025] demonstrate that the gap between methods trained with verifier-based methods and verifier-free methods increases as the quantity of training data increases. \u2022 Balancing -Future studies can analyze how to define and balance the quality, diversity, and complexity of a dataset [Havrilla et al., 2024a]. \u2022 Leveraging difficulty -Our difficulty classifications can be used studies on subset selection for RL, similar to the method proposed by Muennighoff et al. [2025]. In particular, it will be interesting to explore how these difficulty scores translate to other models or model families. \n\nAdditionally, a potentially fruitful avenue of research is to explore if difficulty scores can be used to form an effective curriculum, where curriculum learning has been effectively applied to RL in the past [Dennis et al., 2021, Jiang et al., 2021, inter alia], but has only successfully been applied to training language models recently [Albalak et al., 2023, Wang et al., 2024] \u2022 Distillation -As has been done with other datasets [Liu et al., 2024, Guan et al., 2025, inter alia], B ig -M at h an also be used in concert with a strong reasoning model to generate a dataset for SFT distillation. \n\nImproving future datasets: \n\n\u2022 Filter and verifier improvements -As described throughout the paper, our filters are overly strict. This allows for further improvements on our filters, enabling the inclusion of more complex problems. Additionally, throughout this paper, we have assumed the use of a very simple verifier and only retained data that can be extracted based on simple string matching.",
            "score": 0.3951331685114905,
            "section_title": "Using this dataset:",
            "char_start_offset": 42191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2041
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46337890625
        },
        {
            "corpus_id": "258947086",
            "title": "NLP Reproducibility For All: Understanding Experiences of Beginners",
            "text": "First, we examine the relationship of a student's skill level with their experience, specifically the time taken to set up and run their experiment, as well as their opinion on how difficult it was.\n\nRelationship with time. Figure 1 shows the distribution of setup times and runtimes reported by students. We observe a striking variation in setup time across all skill levels, from under an hour to 11 Summary of student responses in Appendix B.3. 12 Data analysis code shared at https://github.com/ sled-group/NLP-Reproducibility-For-Beginners. nearly 30 hours. As skill level increases, we observe that the median and minimum setup time, as well as the overall range of setup times, marginally decrease. To examine how factors used to assign their skill level contribute to setup time, we calculate the Spearman correlation between each skill level factor and setup time in the second column of Table 3. Indeed, we observe a significant correlation, with understanding of the homework assignment on LSTMs having the strongest negative association with setup time. As this assignment required implementing and training language models in PyTorch, this suggests that these hands-on NLP skills may save students' time when reproducing NLP results. However, if we interpret \u03c1 2 as a coefficient of determination, skill level factors explain only up to \u03c1 2 = 18.5% of variance in setup time. 14 The large overlap in the setup time distribution between skill levels further suggests that there are more factors at play here. Meanwhile, we see no clear differences in runtimes based on skill level, as each paper should have a consistent required runtime to train and evaluate models.\n\nRelationship with difficulty. For a more direct measure of students' experience, Figure 2 summarizes student ratings for the difficulty of each step of the experiment. For most steps of the experiment, more novice students reported slightly more difficulty. Students found code setup, data preprocessing, and system training to be the most difficult steps, and we observed a significant decrease in difficulty with respect to skill level for code setup. This suggests a relationship between students' skill level and their reported code setup difficulty. code downl. data downl. code setup data preproc.\n\nsystem training system eval. To understand this relationship, we calculate the Spearman",
            "score": 0.39497463094970986,
            "section_title": "Student Skill Level",
            "char_start_offset": 16200,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0369873046875
        },
        {
            "corpus_id": "273822116",
            "title": "Training on the Test Model: Contamination in Ranking Distillation",
            "text": "Neural approaches to ranking based on pre-trained language models are highly effective in ad-hoc search. However, the computational expense of these models can limit their application. As such, a process known as knowledge distillation is frequently applied to allow a smaller, efficient model to learn from an effective but expensive model. A key example of this is the distillation of expensive API-based commercial Large Language Models into smaller production-ready models. However, due to the opacity of training data and processes of most commercial models, one cannot ensure that a chosen test collection has not been observed previously, creating the potential for inadvertent data contamination. We, therefore, investigate the effect of a contaminated teacher model in a distillation setting. We evaluate several distillation techniques to assess the degree to which contamination occurs during distillation. By simulating a ``worst-case'' setting where the degree of contamination is known, we find that contamination occurs even when the test data represents a small fraction of the teacher's training samples. We, therefore, encourage caution when training using black-box teacher models where data provenance is ambiguous.",
            "score": 0.39474113324143334,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1153564453125
        },
        {
            "corpus_id": "266180578",
            "title": "Mmi01 at The BabyLM Challenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings",
            "text": "Our submission considers GPT-2 models (Radford et al., 2019) pretrained using curricula formed by various linguistic measures detailed in \u00a7 2. The pretraining approach involves sequentially training the model using ten different curriculum levels of the dataset, with each level building upon the previous one in terms of difficulty. Each model is pretrained three times, with a random seed used each time.",
            "score": 0.3944207125964126,
            "section_title": "Methodology",
            "char_start_offset": 5009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3037109375
        },
        {
            "corpus_id": "201698284",
            "title": "Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial Crowds",
            "text": "What is the most difficult example in the Stanford Natural Language Inference (SNLI) data set (Bowman et al., 2015) or in the Stanford Sentiment Treebank (SSTB) (Socher et al., 2013)? A priori the answer is not clear. How does one quantify the difficulty of an example and does it pertain to a specific model, or more generally?\n\nThere has been much recent work trying to assess the quality of data sets used for NLP tasks, (e.g. Lalor et al., 2016;Sakaguchi and Van Durme, * Current affiliation: Mendoza College of Business, University of Notre Dame 2018; Kaushik and Lipton, 2018). In particular, a common finding is that different examples within the same class have very different qualities such as difficulty, and these differences affect models' performance. For example, one study found that a subset of reading comprehension questions were so difficult as to be unanswerable (Kaushik and Lipton, 2018). In another work, the difficulty of specific items was found to be a significant predictor of whether a model would classify the item correctly (Lalor et al., 2018).\n\nWhile a number of methods exist for estimating difficulty, in this work we focus on Item Response Theory (IRT) (Baker, 2001;Baker and Kim, 2004), a widely used method in psychometrics. IRT models fit parameters of data points (called \"items\") such as difficulty based on a large number of annotations (\"response patterns\" or RPs), typically gathered from a human population (\"subjects\"). It has been shown to be an effective way to evaluate and analyze NLP models with respect to human populations (Lalor et al., 2016(Lalor et al., , 2018.\n\nWhile IRT models are designed to be learned with human RPs for at most 100 items, data sets used in machine learning, particularly for training deep neural networks (DNNs), are on the order of tens or hundreds of thousands of examples or more. It is not possible to ask humans to label every example in a data set of that size. In this work we hypothesize that IRT models can be fit using RPs from artificial crowds of DNN",
            "score": 0.394419863539736,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 115,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 161,
                    "end": 182,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 430,
                    "end": 449,
                    "matchedPaperCorpusId": "1330208"
                },
                {
                    "start": 449,
                    "end": 462,
                    "matchedPaperCorpusId": "46926791"
                },
                {
                    "start": 557,
                    "end": 582,
                    "matchedPaperCorpusId": "52011616"
                },
                {
                    "start": 883,
                    "end": 909,
                    "matchedPaperCorpusId": "52011616"
                },
                {
                    "start": 1054,
                    "end": 1074,
                    "matchedPaperCorpusId": "22615716"
                },
                {
                    "start": 1575,
                    "end": 1594,
                    "matchedPaperCorpusId": "1330208"
                },
                {
                    "start": 1594,
                    "end": 1615,
                    "matchedPaperCorpusId": "22615716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.",
            "score": 0.3937872470658935,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "271269996",
            "title": "metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large Language Models",
            "text": "Large Language Models (LLMs) represent a significant leap forward in our quest to emulate humanlike understanding and generation of language [1].These sophisticated algorithms are designed to process, interpret, and produce language with unprecedented accuracy and fluency [2].By training on vast datasets encompassing a myriad of linguistic patterns, LLMs have demonstrated remarkable capabilities in tasks ranging from simple text completion [3] to complex problem-solving [4] and creative writing [5].Their potential to transform various fields-education [6], customer service [7], content creation [8], and beyond-has captured the imagination of researchers and practitioners alike [2,9].However, as these models grow in complexity, so does the challenge of effectively evaluating their diverse abilities [10,11].\n\nTraditionally, research has relied on extensive benchmarks-vast arrays of tasks-to assess the abilities of LLMs [12,13].However, these benchmarks often measure overlapping skills, leading to redundancy and inefficiency.This paper introduces metabench, a sparse benchmark engineered to distill the essence of six prominent benchmarks-ARC [14], GSM8K [15], HellaSwag [16], MMLU [17], TruthfulQA [18], and WinoGrande [19]-into a more concise and informative benchmark.By analyzing data from over 5000 LLMs, we identified the most revealing items within these benchmarks, reducing their combined size to less than 3% of the original.We also provide adaptive testing simulations that suggest that the number of items administered to LLMs can be lowered even further.This volume reduction not only maintains the benchmarks' ability to measure underlying abilities but greatly enhances efficiency by eliminating redundancy.\n\nThe metabench framework leverages psychometric techniques [20,21] to estimate the latent abilities captured by these benchmarks [22].These estimates can reconstruct individual and total benchmark scores with minimal error, and reveal a single underlying factor that captures most of their variability.By refining how we measure the abilities of LLMs, metabench offers a new lens through which we can understand and improve them.",
            "score": 0.3937334222478376,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 817
                },
                {
                    "start": 819,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1735
                },
                {
                    "start": 1737,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 2038
                },
                {
                    "start": 2038,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 444,
                    "end": 447,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "225035059"
                },
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "3922816"
                },
                {
                    "start": 1184,
                    "end": 1188,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "199370376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299072265625
        },
        {
            "corpus_id": "270215134",
            "title": "SAVA: Scalable Learning-Agnostic Data Valuation",
            "text": "One can also prune data by how similar embeddings are to a cluster center or prototype (Sorscher et al., 2022) and by assessing diversity within each cluster (Abbas et al.; Tirumala et al., 2023). It has also been shown that pruning data according to how easy they are to be forgotten over the course of training -as a measure of difficulty -results in training on less data while maintaining performance (Toneva et al., 2018). These data selection methods although related, do not directly measure the importance of each training datapoint with respect to a clean validation set like LAVA (Just et al., 2023) and SAVA. Meta-learning is also used to learn datapoint importance weights by evaluating with a clean validation set (Ren et al., 2018). Similarly to LAVA and SAVA the distributional distance between a clean validation set and a large noisy dataset can be assessed using n-grams in NLP for selecting data to train large language models (Xie et al., 2023).",
            "score": 0.3922802224520986,
            "section_title": "G.2.3 SUPERVISED PROTOTYPES",
            "char_start_offset": 42060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 965
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 110,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 173,
                    "end": 195,
                    "matchedPaperCorpusId": "261076313"
                },
                {
                    "start": 590,
                    "end": 609,
                    "matchedPaperCorpusId": "258426444"
                },
                {
                    "start": 727,
                    "end": 745,
                    "matchedPaperCorpusId": "4321928"
                },
                {
                    "start": 946,
                    "end": 964,
                    "matchedPaperCorpusId": "256627727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "267782698",
            "title": "Vision-Language Navigation with Embodied Intelligence: A Survey",
            "text": "Curriculum learning [169] helps models to learn and adapt to complex tasks more effectively by gradually increasing task difficulty during the training process. One approach to implementing curriculum learning is by controlling the length of instructions in training samples. For example, BabyWalk [80] adopted this strategy, gradually increasing the length of instructions during training to allow the model to progressively adapt to more complex tasks. The core idea of this approach is that as the length of instructions increases, so does the complexity of the tasks, thereby helping the model to better handle long instructions in real scenarios. Another practice in curriculum learning is ranking task difficulty based on the attributes of trajectories. Zhang et al. [170] employed this method, reordering tasks based on the number of rooms each path in the R2R dataset passes through. This path complexity-based ranking method helps the model to gradually adapt to different levels of navigation difficulty. As an effective training strategy, curriculum learning has shown its potent potential in VLN tasks. By controlling the gradual increase in task difficulty, this method not only helps models to better adapt and handle complex navigation tasks but also provides an effective way to optimize model performance and enhance its generalization capabilities. [171] employs an instruction interpretation strategy by encoding and utilizing all instructions with a set of shared parameters to enhance text comprehension. This comprehensive text encoding strategy helps the agent to understand the information in the instructions more thoroughly and make more accurate decisions when executing navigation tasks. LWIT [172] adopts another method of interpreting instructions, explicitly indicating which objects the agent needs to interact with. This approach is particularly suited for tasks requiring the agent to interact with specific objects, such as opening doors or picking up items. Moreover, shorter and more concise instructions can provide clearer guidance to the agent compared to long and complex ones. Therefore, Hong et al.",
            "score": 0.39137400132666217,
            "section_title": "Curriculum Learning.",
            "char_start_offset": 61221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 25,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 773,
                    "end": 778,
                    "matchedPaperCorpusId": "244117255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2044677734375
        },
        {
            "corpus_id": "259593617",
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "text": "As documented in relevant CL studies in computer vision and NLP, in addition to presenting the training data in an easy-to-hard fashion, sometimes a model can achieve better prediction performance by reverting the training order to hard-to-easy (denoted as anti-curriculum). \n\nAs there lacked prior studies on applying CL to tackle the task of AST and it remained largely unknown which training paradigm would benefit the most, we included both the easy-to-hard and hard-to-easy training paradigms to measure the effectiveness of the four CL strategies described above. Through extensive evaluations on two widely-used educational datasets, i.e., one for Automatic Short Answer Scoring (ASAS) and the other for Automatic Essay Scoring (AES), our work demonstrated that: (i) with the aid of CL, the performance of state-of-the-art ATS models can be further boosted with a maximum 4.5% improvement (measured by Quadratic Weighted Kappa); (ii) among the four investigated pre-defined difficulty measurers, the number of grammatical errors tended to give the most robust performance in measuring sample difficulty; (iii) no significant difference was observed between the pre-defined and automatic CL strategies, or between the easy-to-hard and hard-to-easy training paradigms.",
            "score": 0.3908534054751986,
            "section_title": "Introduction",
            "char_start_offset": 5497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 1273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37744140625
        },
        {
            "corpus_id": "273346777",
            "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
            "text": "In this section, we introduce 4 methods based on data quality assessment and selection. \"Quality\" here refers primarily to the complexity, completeness, score, and influence of the datapoints. Different from Qin et al. (2024), we believe that the influence of a datapoint in the target dataset is also a reflection of data quality, especially in practical scenarios, where we are required to deal with diverse tasks rather than a single task. We thus regard the influence as a quality category as well. \n\nLESS Xia et al. (2024) instroduced low-rank gradient similarity search to select influential data for the target application. Concretely, a model was trained with LoRA (Hu et al., 2021) for a warmup period on a small subset D warmup \u2282 D. Then, the Adam LoRA gradient features for each data point were computed and stored in a gradient database. \n\nNext, a gradient datastore of projected low-dimensional gradient features was constructed which can be reused for different target tasks. For training datapoints z, they computed d-dimensional projection of the LoRA gradient \u2207\u2113(z; \u03b8 i ) = \u03a0 \u22a4 \u2207\u2113(z; \u03b8 i ), where \u03a0 \u22a4 is computed and applied by memory-efficient online implementation of random projections proposed by Park et al. (2023). For validation datapoint z \u2032 , they computed \n\nFinally, LESS computed max j Inf Adam (z, D \n\nval ) for the training set z across all sub-validation sets D val . Then it selected the highest score examples to construct D train . \n\nIFD introduced the Instruction-Following Difficulty (IFD) score, a metric devised to evaluate the challenge each instructional sample presents (Li et al., 2023b). Given a (Q, A) pair, they calculated the ratio between s(A) and s(A|Q): \n\nwhere s(A) means Direct Answer Score, which measures LLM's ability to generate the answer alone. s(A|Q) means Conditioned Answer Score, which is calculated by continuously predicting the next tokens given the instruction Q and their proceeding words. \n\nIn this paper, the authors first generated 100 clusters on instruction embeddings and sampled 10 instances in each cluster based on IFD score on pre-trained base LLM.",
            "score": 0.39033932908639835,
            "section_title": "QUALITY-BASED SELECTIONS",
            "char_start_offset": 5890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1955
                },
                {
                    "start": 1958,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 510,
                    "end": 527,
                    "matchedPaperCorpusId": "267522839"
                },
                {
                    "start": 1218,
                    "end": 1236,
                    "matchedPaperCorpusId": "257757261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "271051005",
            "title": "Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning",
            "text": "Various pruning methods have been explored for selecting more informative samples for model training, each tailored to different scenarios.Data clustering has been widely used as a highly effective technique for data pruning.TLDR [Wang et al., 2023] utilized KMeans clustering to group similar data points and uniformly sampled from each cluster.They employ Image-Text Matching (ITM) scores to identify suitable vision-text pairs, offering another perspective on sample selection.DEFT [Das and Khetan, 2023] utilizes unsupervised core-set selection for clustering-based dataefficient fine-tuning of LLMs.This approach significantly enhances data efficiency in fine-tuning for text-editing applications.\n\nMetrics like Hardness [Sorscher et al., 2022], Instruction Following Difficulty (IFD) [Li et al., 2023] (Li et al., 2023), and SuperFiltering [Li et al., 2024] focus on identifying \"hard\" samples that are either difficult to learn or easy to forget, tracking each data sample throughout training.In addition to these, sample influence metrics such as LESS [Xia et al., 2024] and TracIn [Pruthi et al., 2020] monitor model gradients and the impact of individual samples, albeit with significant computational overhead for large models and datasets.Quality metrics from external oracles [Chen et al., 2024, Liu et al., 2023a], leverage strong language models like ChatGPT for data selection.However, utilizing external oracles may not always be feasible due to cost constraints.",
            "score": 0.38958706072083205,
            "section_title": "Data Pruning for Efficient Training",
            "char_start_offset": 7776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 480
                },
                {
                    "start": 480,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 702
                },
                {
                    "start": 704,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1480
                }
            ],
            "ref_mentions": [
                {
                    "start": 726,
                    "end": 749,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1090,
                    "end": 1111,
                    "matchedPaperCorpusId": "211204970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "249097496",
            "title": "The Effect of Task Ordering in Continual Learning",
            "text": "Often inspired by the human classroom, curriculum learning refers to training a model with a curriculum of increasing difficulty [6,[25][26][27]. Typically, the curriculum is over data points, such that individual training examples are sorted into a progression from easy to hard according to some metric. It is a matter of ongoing debate as to whether curriculum learning improves performance [6][7][8]28], though it seems increasingly common in practice when training large state-of-the-art models [28]. Curriculum learning is closely related to both importance sampling [29][30][31][32] and to active learning [33,34]. In our work, we apply the idea of investigating ordering to whole tasks, rather than individual datapoints.",
            "score": 0.3889995495688804,
            "section_title": "Curriculum learning",
            "char_start_offset": 5706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 729
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 136,
                    "end": 140,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "13199836"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "3663876"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "6170752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.463134765625
        },
        {
            "corpus_id": "270923838",
            "title": "LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison",
            "text": "There exists a large range of literature that addresses identifying large language model training data in a \"black-box\" environment where the training corpus is unknown. For instance, LLM training data sourcing has been assessed through min-k% prob, which is a detection method based on the assumption that a member of the training data is less likely to include words that have high negative log-likelihood (and are thus outlier words) compared to a non-member of the training data (Shi et al., 2024), therefore considering \"anomalous\" vocabulary within a text. \n\nSuch an approach, based on the principles of Membership Interference Attacks (MIAs), an adversarial technique that seeks to determine whether a knowledge source is part of a model's training data, is the most common method to identify LLM training data in \"black-box\" environments. Substantial literature also exists about utilizing MIA principles to identify corpora used to fine-tune LLMs, addressing word embeddings (Mahloujifar et al., 2021), addressing NLP classification models for members of training corpora (Shejwalkar et al., 2021), and addressing source text memorization (Song and Shmatikov, 2019). \n\nHowever, such approaches based on MIA principles take a statistical and probabilistic approach to identifying LLM training data, ignoring other \"signs\" of sourcing that extend beyond simple copying or paraphrasing. Statistical measures such as only considering the likelihood of \"anomalous\" words ignore the broad relationships between ideas that exist in sentences of a source corpora that may manifest themselves in an LLM's generated answer. \n\nAdditionally, traditional plagiarism detection systems (systems that compare the similarity of multiple corpora) often rely on simple matching techniques. For instance, these systems may search direct token (word, sentence, unique phrase, paragraph, etc.) matches between a document and others, using a threshold for matches as an indicator of plagiarism/similarity. Other systems narrow down at the individual word/phrase level, analyzing semantic relationships through simple synonym/antonym detection or more complex Semantic Role Labeling techniques between words in target and source sentences (Osman et al., 2012).",
            "score": 0.38897405199567986,
            "section_title": "Literature Review",
            "char_start_offset": 3486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 1081,
                    "end": 1106,
                    "matchedPaperCorpusId": "245222525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03753662109375
        },
        {
            "corpus_id": "225066701",
            "title": "A Comprehensive Survey on Curriculum Learning",
            "text": "An illustration of CL is demonstrated in Fig. 1, where we take the task of image classification as an example. Initially, CL trains the model on a small subset of \"easy\" images, i.e., the images of apples and oranges are clear, typical, and easily recognizable. With the progress of model training, CL adds more \"harder\" images (i.e., harder to recognize) to the current subset, which is akin to the increasing difficulty of learning materials in human curricula. Finally, CL leverages the whole training dataset for training. \n\n\u2022 How to design a curriculum? A general framework for curriculum design, i.e., \"Difficulty Measurer + Training Scheduler\", is shown in Fig 2(a) (Sec. IV). We introduce the common techniques for manually designing the two core components in this framework, i.e., Difficulty Measurer and Training Scheduler, with some representative examples. These CL methods are denoted as predefined CL (Sec. IV-B). \n\n\u2022 How can we automatically design a curriculum (Sec. IV-C)? The main obstacle to the wider application of CL is that it is often hard to accurately predefine the Difficulty Measurer and Training Scheduler for a specific task or dataset by human priors. Thus, a growing trend of CL is to design automatic CL algorithms that could let the machine design the most suitable curricula to a specific task and data. We demonstrate and compare four major methodologies for automatic CL with cutting-edge representative literature, including Self-Paced Learning (SPL), Transfer Teacher, RL Teacher, and Other Automatic CL. We conclude the paper with a comparison of \"easier first\" and \"harder first\" training strategies and presenting a relation graph connecting CL and other related machine learning concepts in Sec. V. We also summarize some open questions and future directions for CL to inspire future researchers in Sec. VI. \n\nScope. There are two surveys related to our paper, both of which focus on the CL for reinforcement learning (RL).",
            "score": 0.38842459155397613,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1851
                },
                {
                    "start": 1854,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1967
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "266166181",
            "title": "Byte-ranked Curriculum Learning for BabyLM Strict-small Shared Task 2023",
            "text": "Since curriculum learning relies upon increasing the difficulty of training samples as training continues, determining what makes a training sample more difficult than another is centrally important. For language input, some proposed measures of difficulty include presence of rare words (Bengio et al., 2009), block size (Nagatsuka et al., 2021), and length (Nagatsuka et al., 2023) Amiri et al. (2017). Their work applies findings from psychology that human learners learn effectively when the same information is reviewed with increasing lengths of time between reviews. These findings suggest that human learners ability to learn information is impacted not only by repetition of material, but also by the interval of time between those repetitions. The work by Amiri et al. (2017) uses this as a basis for a curriculum learning schedule. That work created a scheduler which spends more time on difficulty training instances and less time on easy instances. This work, by contrast, by gradually increasing the size of the training set, also gradually increases the time between repetitions of the easiest training samples while saving the more difficulty samples for later in training. \n\nAs this work was part of a shared task BabyLM challenge, there will be other related works published at the same time as this work. While those works cannot be discussed here, they will also provide good comparisons of other possible approaches.",
            "score": 0.3878899843455909,
            "section_title": "Related Work",
            "char_start_offset": 5792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1437
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 309,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 322,
                    "end": 346,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 359,
                    "end": 383,
                    "matchedPaperCorpusId": "255221201"
                },
                {
                    "start": 384,
                    "end": 403,
                    "matchedPaperCorpusId": "1916665"
                },
                {
                    "start": 766,
                    "end": 785,
                    "matchedPaperCorpusId": "1916665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6572265625
        },
        {
            "corpus_id": "266166181",
            "title": "Byte-ranked Curriculum Learning for BabyLM Strict-small Shared Task 2023",
            "text": "Large language models (LLMs) have received much attention from researchers and the general public in recent years (Devlin et al., 2018;Liu et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Hoffmann et al., 2022). One distinguishing aspect of these recent models is an explosion in the size of the models and a corresponding massive increase in training data to train these large models. In particular, the Chinchilla (Hoffmann et al., 2022) work suggests that model size and training tokens should be scaled at the same rate. To demonstrate the importance of the amount of training data used to train a model, Chinchilla was trained with 1.4 trillion training tokens, nearly five times the size of the training data for other LLMs at the time. \n\nThe result was an improvement on a number of downstream tasks. \n\nWhile large models perform very well on a large variety of tasks, they also come with many drawbacks. These models require large amounts of computing resources beyond what is available to many researchers. Additionally, the amount of data used to train these models is not currently available in the majority of the world's languages. In an effort to investigate language modeling abilities and training strategies in data-limited situations, the BabyLM challenge restricts the amount of data available to models (Warstadt et al., 2023). \n\nOne approach to improve training speed and improve downstream performance is by providing training data in a specific order. In particular, gradually increasing the difficulty of the training samples provided to the model is known as curriculum learning (Elman, 1993). Human children learning language follow a similar exposure to language. Speech directed at babies is far simpler than speech directed at adults and written language data follows the same trend. The motivation behind curriculum learning is to treat a neural network in a similar manner and allow it to learn from easier training samples before being presented with more difficult training samples. \n\nThe approach taken in this current work is to apply curriculum learning in a data restricted setting, without incorporating outside knowledge or data, to see its impact on training. The preprocessing steps are kept the same across models presented to reduce their effect on the ability to compare across training runs. A byte-level byte-pairencoding tokenization is used across all models presented.",
            "score": 0.3874617957870944,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2342
                },
                {
                    "start": 2343,
                    "end": 2423
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 171,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1329,
                    "end": 1352,
                    "matchedPaperCorpusId": "266182457"
                },
                {
                    "start": 1610,
                    "end": 1623,
                    "matchedPaperCorpusId": "2105042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54736328125
        },
        {
            "corpus_id": "259593617",
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "text": "Automatic Text Scoring (ATS) is a widely-investigated task in education. Existing approaches often stressed the structure design of an ATS model and neglected the training process of the model. Considering the difficult nature of this task, we argued that the performance of an ATS model could be potentially boosted by carefully selecting data of varying complexities in the training process. Therefore, we aimed to investigate the effectiveness of curriculum learning (CL) in scoring educational text. Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms.",
            "score": 0.387382463658508,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "246996522",
            "title": "Curriculum Optimization for Low-Resource Speech Recognition",
            "text": "[9] studied the dynamics of deep neural networks trained with dynamic instance hardness and showed that the model revisits harder samples more often due to higher variance in gradient values while easier examples tend to stay in the minima as soon as the minima are reached. However, [8] finds that in different empirical settings both learning harder and easy tasks first can benefit the model. \n\nCurriculum learning has been successfully used in natural language processing tasks such as language modelling [5,10] neural machine translation (NMT) [11,12,13], keyword spotting [14], and speech recognition [15,16]. Most commonly two complexity strategies are employed: model competencebased [11,12,9] and data-driven. Complexity measures for data-driven learning include sentence/utterance length [16], language model score, n-gram size [5,10], word frequency ranking, and sentence norm [13]. Speech processing systems can rely on speech to noise ratio (SNR) as a measure of difficulty by gradually blending more and more noise into clean speech signals [15,14] ditionally, we demonstrate that both an external teacher curriculum and learner's progress are important for low-resource ASR. The teacher curriculum acts as a reliable prior while the student can construct its own curriculum in an automated manner based on progress gains. We experiment with compression ratio and text-based difficulty measures to show that the signal-based prior leads to a more optimal solution. \n\nThe remainder of the paper as organized as follows: Section 2 describes the complexity measures and methods; Sections 3 and 4 discuss the experimental setting and the results; Section 5 summarizes our contributions.",
            "score": 0.38704866882156796,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "20639213"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "231979234"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "33957080"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "20639213"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 699,
                    "end": 701,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "33957080"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 841,
                    "end": 844,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 1059,
                    "end": 1062,
                    "matchedPaperCorpusId": "231979234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "268696658",
            "title": "Symmetric Self-Paced Learning for Domain Generalization",
            "text": "Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses. \n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty. The GDM is computed with Equation 8, where \u03be x denotes the difficulty of the example x. ColorJitter. An Empirical Risk Minimization (ERM) baseline is also included, which merges data from all source domains without utilizing domain generalization techniques. Evaluation Metrics. We adopt the leave-one-out-test evaluation strategy as the evaluation metric following the prior works (Li et al. 2017b;Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2022). Specifically, we select one domain as the test domain at a time and use the remaining domains as the source domains for training. We report the accuracy for each separate domain. Performance measures are reported as top-1 classification accuracy (%) averaged over ten runs, along with their corresponding 95% confidence intervals. \n\nNetwork Structure. The network structure is chosen by following the previous work (Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2020). In the Digits dataset, images are resized to 32 \u00d7 32 and converted to RGB by replicating channels. The backbone of the neural network is constructed by 3 \u00d7 3 Conv layers (64 kernels), each followed by a",
            "score": 0.3865058553038846,
            "section_title": "Gradient-based Difficulty Measure",
            "char_start_offset": 9632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2077
                },
                {
                    "start": 2080,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2316
                },
                {
                    "start": 2317,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 1675,
                    "end": 1692,
                    "matchedPaperCorpusId": "6037691"
                },
                {
                    "start": 1692,
                    "end": 1713,
                    "matchedPaperCorpusId": "81978372"
                },
                {
                    "start": 1713,
                    "end": 1728,
                    "matchedPaperCorpusId": "59553457"
                },
                {
                    "start": 2162,
                    "end": 2184,
                    "matchedPaperCorpusId": "81978372"
                },
                {
                    "start": 2184,
                    "end": 2199,
                    "matchedPaperCorpusId": "59553457"
                },
                {
                    "start": 2199,
                    "end": 2215,
                    "matchedPaperCorpusId": "212718063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "263828737",
            "title": "Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy",
            "text": "Current research on evaluating large language model generation quality employs a diverse set of techniques, each with distinct tradeoffs [5]. Human evaluation through ratings and reviews provides nuanced assessments accounting for subjective aspects of quality, but is time-consuming, inconsistent, and doesn't scale [21]. Automated metrics like BLEU [16] are fast and consistent, but focus narrowly on n-gram overlap with reference texts. Adversarial evaluation [4] can reveal flaws invisible to standard tests, yet constructing effective adversarial examples remains challenging. Designing specific benchmark tasks can test particular skills relevant to generation quality, but requires developing comprehensive suites covering diverse skills [8]. Human-in-the-loop training iteratively improves models using human feedback, but is slow and introduces confounding factors. Overall, human evaluation remains the gold standard despite difficulties with scalability and subjectivity. Automated metrics are the most widely adopted for development due to speed and consistency, complemented by adversarial techniques and specifically designed tests to evaluate particular aspects of generation quality. But effectively incorporating human assessment during training remains an open challenge [14].",
            "score": 0.38493751375814084,
            "section_title": "LLM evaluation",
            "char_start_offset": 7181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1294
                }
            ],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "30536426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302978515625
        },
        {
            "corpus_id": "266166181",
            "title": "Byte-ranked Curriculum Learning for BabyLM Strict-small Shared Task 2023",
            "text": "A byte-level byte-pairencoding tokenization is used across all models presented. Inspired by the byte-level approach to encoding, bytes per line is used as the measure of \"difficulty\" for a given portion of the dataset. The data used to train the model came from several different datasets. The bytes per line \"difficulty\" is used to determine the order in which training datasets are provided to the models as part of a curriculum learning approach. While no additional or outside information is required to apply this approach to data, the result for this challenge was that transcribed speech was used as training data before any of the written text data. This provides another parallel to human language acquisition as speech comes before literacy in children. \n\nGiven that the limitations motivating this work and this challenge include data limitations as well as computational resources, we train each model for a set number of epochs. Models trained using the curriculum learning approach outperformed a traditional training approach baseline across several benchmark downstream tasks. When computational resources are less limited, the models also continue to improve when the model size is increased and when trained longer.",
            "score": 0.38476829206937924,
            "section_title": "Introduction",
            "char_start_offset": 2358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1234
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "272694609",
            "title": "AI Literacy for All: Adjustable Interdisciplinary Socio-technical Curriculum",
            "text": "The training process for these generative models is dissected, highlighting the importance of large data sets and parameter tuning. RAG combines generative models with traditional search engines, retrieving relevant documents to inform content generation, and enhancing the accuracy and relevance of the generated content, as seen in applications like ChatGPT. \n\nHow Do Large Language Models Work? This topic area focuses on the mechanics of Large Language Models (LLMs), such as GPT-3 and GPT-4. These models use deep learning techniques to process and generate human-like text, leveraging transformer architectures and self-attention mechanisms to understand word relationships and context. The curriculum covers the training phases of LLMs, from pre-training on vast text corpora to fine-tuning for specific tasks. Key concepts such as pre-training, parameters, features, and fine-tuning are elucidated. The evaluation of foundational AI models is discussed, emphasizing their application in various AI domains like natural language processing and attention mechanisms. \n\nInteracting with Large Language Models In this section, the curriculum addresses the interaction with LLMs, differentiating between closed-source and open-source models. Closed-source LLMs, typically proprietary with sophisticated capabilities, are compared with open-source models, which offer transparency and community-driven enhancements. Best practices for interacting with LLMs are outlined, including the formulation of effective prompts, setting response parameters, and considering ethical implications. The curriculum emphasizes the practical applications of LLMs in writing and creative tasks, advocating for responsible use to maintain authenticity and originality. \n\nAcademic/Professional Integrity, Authorship, and Ownership The importance of Academic/Professional Integrity is a pivotal theme, highlighting the necessity of ethical standards in research, writing, and assessments. The curriculum details the Code of Student Conduct and identifies behaviors constituting academic misconduct, such as plagiarism and cheating. It underscores the significance of understanding misconduct policies and sanctions, and provides strategies to avoid academic misconduct, fostering a culture of honesty and respect within academic communities. \n\nPrompt Engineering for Learning Prompt engineering is explored as a crucial technique for guiding AI responses in educational settings. This section demonstrates how AI can enhance learning interventions by providing personalized feedback and resources. It discusses the use of search engines, Gen-AI, and RAG for research and problem-solving.",
            "score": 0.38423077468769384,
            "section_title": "V. SOCIO-TECHNICAL AI LITERACY LEARNING OUTCOMES",
            "char_start_offset": 25519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1752
                },
                {
                    "start": 1755,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2323
                },
                {
                    "start": 2326,
                    "end": 2461
                },
                {
                    "start": 2462,
                    "end": 2579
                },
                {
                    "start": 2580,
                    "end": 2669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270751953125
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "Curriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009;Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia). \n\nIn this paper, we conduct an empirical exploration of curriculum learning for Neural Machine Translation (NMT). NMT is a good test case for curriculum learning as training is prohibitively slow in the large data conditions required to reach good performance (Koehn and Knowles, 2017). However, designing a curriculum for NMT training is a complex problem. First, it is not clear how to quantify sample difficulty for this task. Second, NMT systems already rely on established data organization methods to deal with the scale and varying length of training samples (Khomenko et al., 2016;Doetsch et al., 2017;Sennrich et al., 2017;Hieber et al., 2017), and it is not clear how a curriculum should interact with these existing design decisions. Kocmi and Bojar (2017) showed that constructing and ordering mini-batches based on sample length or word frequency helps when training for one epoch. It remains to be seen how curricula impact training until convergence. \n\nTo address these issues, we adopt a probabilistic view of curriculum learning that lets us explore a wide range of curricula flexibly. Our approach does not order samples in a deterministic fashion. Instead, each sample has a probability of being selected for training, and this probability changes depending on the difficulty of the sample and on the curriculum's schedule. We explore difficulty criteria based on NMT model scores as well as linguistic properties. We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018).",
            "score": 0.38419889265292506,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 409,
                    "end": 430,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 765,
                    "end": 790,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1071,
                    "end": 1094,
                    "matchedPaperCorpusId": "14333788"
                },
                {
                    "start": 1115,
                    "end": 1137,
                    "matchedPaperCorpusId": "905565"
                },
                {
                    "start": 1250,
                    "end": 1272,
                    "matchedPaperCorpusId": "26468344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "260500164",
            "title": "Test Accuracy vs. Generalization Gap: Model Selection in NLP without Accessing Training or Testing Data",
            "text": "Selecting the optimal hyperparameters, such as those for training or model size, is a critical phase in the ML pipeline. Motivated by the importance of model selection, recent years have seen a wide array of large-scale empirical studies on the various metrics used to predict the test-time performance of ML models [9,17,26,27]. These generalization metrics have been applied in a wide variety of data science tasks, including predicting the quality of pretrained learning models [23,27], designing effective training procedures [11,14], improving network efficiency [5,8], quantifying model robustness [41,47], improving ensemble learning techniques [12,13], analyzing and improving large-scale machine learning contests [26], and so on. They are typically studied using correlational analysis, measuring how strongly each metric correlates with (and therefore, can predict) model performance. In this regard, several recent works point out the deficiencies of existing generalization metrics, including a lack of \"robustness\" to the changes of environmental hyperparameters [9,17] (such as data, neural network architecture and training schemes), or the Simpson's paradox that generalization metrics perform differently (i.e., predict opposite trends) when applied to each sub-part of a collection of learning models or to the holistic study [26]. Another drawback is the over-reliance on CV models, which are relatively well-explored, and are not always representative of other types of tasks. With few exceptions [27,31,46], systematic studies in other fields, such as NLP, are largely missing. Generalization metrics for model selection in NLP. The objective of this work is to provide a systematic study of generalization metrics in NLP, addressing several deficiencies in prior studies [9,17,27]. Compared to CV, model selection in NLP has several important differences that require careful consideration. For example, the training data from standard CV benchmarks can often be easily obtained, while large language model datasets are typically web-scale and are challenging to access. Therefore, generalization metrics that can assess the quality of learning models without access to data are ideal for NLP. In this paper, we focus on",
            "score": 0.3840481631583853,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "225040271"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "204893960"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "211132500"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "59222778"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "211132500"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "222134093"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 568,
                    "end": 571,
                    "matchedPaperCorpusId": "232013680"
                },
                {
                    "start": 571,
                    "end": 573,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "220486747"
                },
                {
                    "start": 656,
                    "end": 659,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1077,
                    "end": 1080,
                    "matchedPaperCorpusId": "225040271"
                },
                {
                    "start": 1080,
                    "end": 1083,
                    "matchedPaperCorpusId": "204893960"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "211132500"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "207808916"
                },
                {
                    "start": 1525,
                    "end": 1528,
                    "matchedPaperCorpusId": "236318524"
                },
                {
                    "start": 1794,
                    "end": 1797,
                    "matchedPaperCorpusId": "225040271"
                },
                {
                    "start": 1797,
                    "end": 1800,
                    "matchedPaperCorpusId": "204893960"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "211132500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326416015625
        },
        {
            "corpus_id": "246652532",
            "title": "Assessing the Performance of Online Students -- New Data, New Approaches, Improved Accuracy",
            "text": "To answer this question we perform an empirical study using data from over 750,000 students taking a variety of courses, to study several aspects of the question including (1) which types of machine learning algorithms work best? (2) which features of a student's previous and current interactions with the ITS are most useful for predicting their current ability to solve a certain question? (3) how valuable is background information about curriculum prerequisites for improving accuracy? and (4) can accuracy be improved by training specialized models for different portions of the curriculum? We measure the quality of alternative approaches by how accurately they predict which future questions the student answers correctly. \n\nMore specifically, we present here the first comparative analysis of recent state-of-the-art algorithms for student performance modeling across four very large student log datasets that have recently become available, which are each approximately 10 times larger than earlier publicly available datasets, which cover a variety of courses in elementary mathematics, as well as teaching English as a second language, and which range across different teaching objectives such as initial assessment of student knowledge state, test preparation, and extra-curricular tutoring complementing K-12 schooling. We show that accuracy of student performance modeling can be improved beyond the current state of the art through a combination of techniques including incorporating new features from student logs (e.g., time spent on previously answered questions), incorporating background information about prerequisite/postrequisite topics in the curriculum, and training multiple specialized models for different parts of the student experience (e.g., training distinct models to assess new students during the first 10 steps of their lesson, versus students taking the post-lesson quiz). The fact that we see consistent improvements in accuracy across all four datasets suggests the lessons gained from our experiments are fairly general, and not tied to a specific type of course or specific tutoring system. \n\nTo summarize, the key contributions of this paper include: \n\n\u2022 Cross-ITS study on modern datasets. We present the first comparative analysis of state-of-the-art approaches to student performance modeling across four recently published, large and diverse student log datasets taken from four distinct intelligent tutoring systems (ITS's), resulting in the largest empirical study to date of student performance modeling.",
            "score": 0.3839998178201305,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2132
                },
                {
                    "start": 2135,
                    "end": 2193
                },
                {
                    "start": 2196,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2554
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10174560546875
        },
        {
            "corpus_id": "265506572",
            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
            "text": "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see Soviany et al. (2022); Wang et al. (2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
            "score": 0.3837117580874127,
            "section_title": "Background",
            "char_start_offset": 3761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 824
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 454,
                    "end": 471,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 490,
                    "end": 511,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 513,
                    "end": 531,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 804,
                    "end": 823,
                    "matchedPaperCorpusId": "232362223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "271039486",
            "title": "AgentInstruct: Toward Generative Teaching with Agentic Flows",
            "text": "Summarization is an important capability for Language Models, with many models achieving high quality summarization performance, yet struggling with hallucination.We assessed summarization ability using two key metrics: hallucinations and quality.For this purpose, we utilized GPT4 as our evaluator.The prompts utilized in these evaluations can be found in Appendix B. We used the following benchmarks for evaluating summarization abilities:\n\n\u2022 ACI-Bench: The Ambient Clinical Intelligence Benchmark (ACI-Bench) [32] is a dataset designed for benchmarking automatic report generation from doctor-patient conversations.The test set comprises 120 data points.\u2022 InstruSum: A dataset [15] for evaluating the generation capabilities LLMs for instruction-controllable summarization.It consists of 100 datapoints.\u2022 Orca-Sum: A newly created benchmark to evaluate LLMs' ability to follow summarization and grounded data transformation instructions.To construct this test set, we sampled data from 45 summarization datasets collected from Hugging Face across multiple domains such as news, conversations, science, health, social, e-mails, code, etc. for a total of 458 datapoints.We randomly collected, up to 1000 datapoints which then we carefully deduplicated to avoid overlapping with the training set.We then used GPT-4 to generate a set of 40 prompts for each dataset out of each we randomly sampled one for each selected datapoint.The prompts are dataset-specific and focus on summarization, grounding, and data transformation.For instance, a prompt may ask the model to generate a TikTok video out of a scientific paper or a legal contract from a Wikipedia page.This allows us to measure not only the quality of the response but also hallucination in a challenging scenario, as the model is forced to move between formats and domains.\n\nThe results are presented in Table 7.With the AgentInstruct approach, we successfully achieved a reduction in hallucinations by 31.34%, while attaining a quality level comparable to GPT4 (Teacher).",
            "score": 0.3834529174050785,
            "section_title": "Evaluation: Abstractive Summarization",
            "char_start_offset": 39686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 441
                },
                {
                    "start": 443,
                    "end": 618
                },
                {
                    "start": 618,
                    "end": 657
                },
                {
                    "start": 657,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 940
                },
                {
                    "start": 940,
                    "end": 1171
                },
                {
                    "start": 1171,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1428
                },
                {
                    "start": 1428,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1660
                },
                {
                    "start": 1660,
                    "end": 1832
                },
                {
                    "start": 1834,
                    "end": 1871
                },
                {
                    "start": 1871,
                    "end": 2031
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11083984375
        },
        {
            "corpus_id": "267740312",
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "text": "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).",
            "score": 0.3832814172581209,
            "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
            "char_start_offset": 1531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 172,
                    "matchedPaperCorpusId": "249060677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "250340652",
            "title": "Understanding Dataset Difficulty with V-Usable Information",
            "text": "Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as the lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\\mathcal{V}$. We further introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also permit the converse: for a given model $\\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.",
            "score": 0.3832814172581209,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "265466496",
            "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection",
            "text": "While data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their\"usefulness\"or\"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection.",
            "score": 0.3827494020605189,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "244908620",
            "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding",
            "text": "The key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3. \n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37,53], and they should be more difficult to learn as verified in Section 2.3. \n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process. \n\nIn particular, it should be noted that in the process of learning the augmented data we do not strictly follow the order of 1 \u2212    programs to  \u2212    programs, since we find that some programs have far more transformed program variants than others and multiple transformations could cause the data to be unbalanced. Therefore, we sample an equal number of augmented samples from the transformed program variants of each sample in the original training set for learning, and the data statistics are shown in Table 2. This method is easy to implement on general models, and we illustrate its effects in the following experiments. \n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy. \n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks. \n\nOnce the scoring function is determined, we still need to define the pace at which we transition from easy samples to harder samples.",
            "score": 0.38235100636636543,
            "section_title": "Curriculum Strategy",
            "char_start_offset": 16540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 515,
                    "end": 518,
                    "matchedPaperCorpusId": "204743994"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "221995570",
            "title": "Learn like a Pathologist: Curriculum Learning by Annotator Agreement for Histopathology Image Classification",
            "text": "Curriculum learning. One of the earliest works demonstrating the benefit of curriculum learning [1] posits that learning occurs better when examples are not randomly presented but instead organized in a meaningful order that gradually shows more concepts and complexity. Although the intuition behind this approach seems obvious in the context of human and animal learning, it is often unclear how to best apply this strategy for training neural networks. As such, a diverse set of approaches has been explored in this area of research. These approaches generally first score examples by difficulty and then train models using a schedule based on example difficulty, where easier examples are typically seen first and harder examples are seen later. For instance, Bengio et al.'s original work [1] explored a noising-based curriculum for shape detection and a vocabulary-size based task for language modeling. As popular recent examples, Weinshall et al. [10] use the confidence of a pre-trained classifier as an estimator for difficulty; Korbar et al. [11] use a schedule with self-defined easy and hard examples for learning of audio-visual temporal synchronization; Ganesh and Corso [12] propose to incrementally learn labels instead of learning difficult examples; and various teacher-student frameworks have been proposed in the context of curriculum learning [13,14]. \n\nChallenges of curriculum learning. Despite the appeal of teaching machines to learn like humans, curriculum learn-ing has been seen by some [10] as mostly remaining in the fringes of machine learning research. Based on the strategies of prior work, we broadly see two central challenges that arise when applying curriculum learning. \n\nFirst, curriculum learning assumes that a range of easy and hard examples exists. Although it could be argued that this is a true statement for any given dataset for at least some definition of easy and hard, the distribution of example difficulties likely varies based on the nature of the task and the dataset. Since the added value of curriculum learning comes from utilizing the varying degrees of difficulty in a task, tasks with a smaller range of example difficulty are less conducive to effective curriculum learning.",
            "score": 0.38198231705341673,
            "section_title": "Curriculum Learning Intuitions",
            "char_start_offset": 2918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 794,
                    "end": 797,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "53280782"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "213207497"
                },
                {
                    "start": 1369,
                    "end": 1372,
                    "matchedPaperCorpusId": "52284093"
                },
                {
                    "start": 1516,
                    "end": 1520,
                    "matchedPaperCorpusId": "5004002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "We begin by describing the unlabeled corpus of text used during training and the metrics adopted for measuring its complexity.\n\nCorpus. In this work we choose the WikiText-103 corpus (Merity et al., 2016), which has served as a popular choice of text corpus for language modeling in prior works (Khandelwal et al., 2019;Press et al., 2021). It consists of a set of verified Good and Featured articles 1 from English Wikipedia, containing a total of around 103.2 million tokens 2 . For all experiments, the predefined splits of the corpus are used.\n\nComplexity. Before we can order the unlabeled text instances in WikiText-103, we need to define some criteria for ranking them. Designing a ranking criteria in a self-supervised setup is a challenging problem, especially due to the dual pretraining and fine-tuning paradigm. We choose to estimate sample complexity in terms of human-centered notion of 'difficulty', specifically text readability and lexical richness. In other words, we construct a heuristically informed pre-defined curriculum, and adopt metrics that measure the difficulty of text, based on readability and complexity, at two levels of granularity -sentence and document. The corpus is then divided into bins, from which batches of training input are constructed at random.",
            "score": 0.3814644575567609,
            "section_title": "Corpus Complexity",
            "char_start_offset": 4372,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 339,
                    "matchedPaperCorpusId": "229924221"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "235421645",
            "title": "GPT3-to-plan: Extracting plans from text using GPT-3",
            "text": "\u2022 EASDRL and cEASDRL : Feng, Zhuo, and Kambhampati (2018)    The corresponding precision, recall and F1 scores for each method were picked directly from their respective papers. \n\nResults Given that GPT-3 is a few-shot learner we want to know how it performs given different amounts of training samples. To measure this, we query the language model with increasing numbers of examples (with a maximum of four examples) for all domains and report their F1 scores. We stop at the four-shot mark as the total amount of tokens or words that the request can contain is 2048. Additionally for the CookingTutorial and Wikihow Garden and Home datasets, 4-shot training examples already exceed this threshold, so we limit the length of input text to 10 sentences per training example. Specifically, we select the training examples as 1-shot (one datapoint is selected at random from the dataset), 2-shot (the two datapoints with the largest proportion of optional and exclusive actions from the dataset are selected), 3-shot (the three datapoints with the largest proportion of optional, exclusive and essential actions) and 4-shot (an additional random datapoint is added to 3-shot). \n\nIn Figure 1 we show how the F 1 score changes given 1, 2, 3 and 4-shot training samples when tested on the whole Win-dows Help and Support dataset. Unsurprisingly, Davinci, the model with the most amount of trainable parameters, performs best with over 80% F 1 score for each category. Both Davinci and Curie show the tendency to perform better the more examples they are given peaking at 3 and 4-shots respectively. Similarly, Babbage and Ada show their peaks given 2 and 4 examples while underperforming at one-shot training. This is unsurprising, given the fact that these models are simplified versions of GPT-3 which have also been trained on a smaller corpus of data for higher speed. Hence, they need more than one example to grasp the task.",
            "score": 0.381378533557607,
            "section_title": "Evaluation and Metrics",
            "char_start_offset": 11421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1926
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "261076515",
            "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
            "text": "In our study, we introduce a novel approach for autonomously identifying the most impactful training samples, which we refer to as \"cherry data\", from extensive open-source datasets. These data samples are particularly effective in enhancing LLM instruction tuning. Central to our hypothesis is the idea that LLMs, through initial training with a small amount of instruction data, can inherently learn to discern and follow instructions, allowing them to estimate the difficulty of instruction data. \n\nOur method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\". This methodology, which emphasizes data quality over quantity, differs markedly from existing techniques that rely on external models for data curation. \n\nExtensive experimental results validate the efficacy of our method. By applying our methodology to the Alpaca and WizardLM instruction tuning datasets, our model outperforms the official Alpaca model with only approximately 5% data selected and outperforms the reimplemented WizardLM model with approximately 10% data selected. The key contributions of this paper: \n\n\u2022 We propose a self-guided approach enabling models to autonomously select the \"cherry data\" from vast open-source datasets. This innovation minimizes manual curation and optimizes the use of existing data resources, reducing costs and streamlining training.",
            "score": 0.3800005381345688,
            "section_title": "Introduction",
            "char_start_offset": 1752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "211102338",
            "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation Models",
            "text": "To make comparisons with other existing curriculum methods, we have conducted several baseline experiments.\n\nWe take the core ideas of existing curriculum learning methods of training on data samples with gradually increasing difficulty (Platanios et al. 2019) and gradually decreasing noise ) and apply them to our setting with pre-trained models. We evaluate the following three baseline methods along with our proposed method.\n\n\u2022 Denoising is a curriculum learning method of training an NMT model in a noise-annealing fashion . They propose to measure NMT data noise with the help of a trusted dataset which contains generally cleaner data compared to the training dataset. (Kumar et al. 2019) also utilize data noise in their curriculum design and achieve similar performance as .\n\nFor the choice of the trusted dataset, we choose a subset of 500 sentences from the validation set newsdev2017 of CASICTB, CASIA2015 and NEU. \u2022 Sentence Length is an intuitive difficulty measure used in (Platanios et al. 2019), since longer sentences tend to contain more information and more complicated sentence structure.\n\n\u2022 Word Rarity is another metric for measuring the sample difficulty, as rare words appear less frequently in the training process and should be presented to the learning system more. The formula for calculating the word rarity of a sentence can be found in (Platanios et al. 2019).\n\nFor baseline experiments, the pre-trained NMT model is further trained on 20% of the original data, which are selected by the above criteria, i.e., the least noisy, the longest, and the highest word rarity, respectively. Table 3 compares the performance of our method with other baseline methods on different datasets evaluated using BLEU. The result shows that our proposed method significantly out-performs other baseline methods by a great margin. We conduct two rounds of training and update in our experiments. While the result of the first round surpasses almost all the baseline methods, our second round further improves the performance and achieves a final BLEU improvement of +0.90, +0.60, +0.59, and +0.71 on MTD, CASICTB, CASIA2015, and NEU respectively over the pretrained model. The reason of our success is due to our utilization of an RL framework to proactively",
            "score": 0.37955109953353117,
            "section_title": "Baselines",
            "char_start_offset": 22571,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 259,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 677,
                    "end": 695,
                    "matchedPaperCorpusId": "67855673"
                },
                {
                    "start": 989,
                    "end": 1012,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1369,
                    "end": 1392,
                    "matchedPaperCorpusId": "85498775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "259243771",
            "title": "Linear-Scaling Kernels for Protein Sequences and Small Molecules Outperform Deep Learning While Providing Uncertainty Quantitation and Improved Interpretability",
            "text": "We can measure feature importance using the SHAP or LIME techniques for a Gaussian process just as for any other ML model. Unlike for other ML models, we can also for a small dataset additionally construct the kernel matrix and use it to determine exactly how much each datapoint in the training dataset contributed to a given prediction. This kind of decomposition is not possible with a deep learning model.\n\nConstructing the kernel matrix explicitly however is too expensive if the dataset is larger than 5,000 datapoints. Also, for large datasets assigning a precise contribution to each point in the training set may be more information than is required or helpful. Often, practitioners may merely want to know what are the most similar datapoints in the training set -the ones which contributed the most to a prediction. We can determine this as follows. Let be the random * features representation of a test datapoint, and let be the random features representation of training datapoint; then the kernel function for the corresponding test and training datapoints * and is approximately (see Supporting Information section S1).",
            "score": 0.3788079632549406,
            "section_title": "Retrieving similar datapoints from the training set",
            "char_start_offset": 39882,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04681396484375
        },
        {
            "corpus_id": "273233280",
            "title": "Unsupervised Data Validation Methods for Efficient Model Training",
            "text": "This approach could be helpful for data validation purposes by \"forgetting\" the point which we would like to assess with the model. \n\nBesides Llama 3, (Young et al., 2024) train models for English and Chinese, but apply custom filtering rules, employing smaller models for filtering, like Quality Scorer, a classifier trained to recognize and favor pages, similar to Wikipedia in quality, Document Coherence Scorer and Safety Scorer, designed to remove undesirable content, clustering based filtering and simple ones like deduplication. \n\nThere are methods to perform filtering, but how to quantify an impact? (Blakeney, Paul, Larsen, Owen, & Frankle, 2024) describe a technique for evaluating the quality of a particular dataset on model performance by appending it to the end of the training and measuring it on downstream benchmarks, providing a FLOPS-efficient way to estimate the quality of data (given enough compute is available for pretraining). There are other works, like (Covert, Ji, Hashimoto, & Zou, 2024), where researchers test the impact of individual datapoints on model training and provide a method for evaluating that impact, but, unfortunately, it could be not used for data selection. \n\nAnother thing to consider is data contamination. (Blevins & Zettlemoyer, 2022) investigate origins of cross-lingual capabilities of large language models, finding that there are a lot of non-English data present in those datasets, helping explain the transfer of knowledge from one language to another. This finding affirms that multilingual training, even with such a small mixture, could transfer capabilities from one language to another. (Udandarao et al., 2024) complete this point of view, finding that \"zero-shot\" capabilities of multi-modal models have a direct relationship with the appearance of that concept in a training dataset. This implies that in the context of low-resource languages, a need for new architecture or adding a mixture of English data to have a transfer of that performance to low-resource language. \n\nHowever, with every technique applied, researchers should be cautious. As shown in (Goyal, Maini, Lipton, Raghunathan, & Kolter, 2024), there are scaling laws for data filtering.",
            "score": 0.37843902743890084,
            "section_title": "Data Scarcity Solutions for Low-Resource Languages",
            "char_start_offset": 6173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2039
                },
                {
                    "start": 2042,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 982,
                    "end": 1018,
                    "matchedPaperCorpusId": "270199496"
                },
                {
                    "start": 1258,
                    "end": 1287,
                    "matchedPaperCorpusId": "252780005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10052490234375
        },
        {
            "corpus_id": "20785832",
            "title": "Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples",
            "text": "Learning easier material before the harder material is often beneficial to human learning. Inspired by this observation, curriculum learning [5] has shown that learning first from easier instances can also improve neural network training. When it is not known a priori which samples are easy, examples with the lower loss on the current model can be inferred to be easier and can be used in early training. This strategy has been referred to as self-paced learning [26]. By decreasing the weight on the loss of difficult examples, the model may become more robust to outliers [34], and this method has proven to be useful in several applications, especially with noisy training data [21]. \n\nNevertheless, selecting easier examples for training often slows down the training process because easier samples usually contribute smaller gradients and the current model has already learned how to make correct predictions on these samples. On the other hand, and somewhat ironically, the opposite strategy (i.e., sampling harder instances more often) has been shown to accelerate (mini-batch) stochastic gradient descent (SGD) in some cases, where the difficulty of an example can be defined by its loss [18,30,44] or be proportional to the magnitude of its gradient [51,1,12,13]. This strategy is sometimes referred to as hard example mining [44]. \n\nIn the literature, we can see that these two opposing strategies work well in different situations. \n\nPreferring easier examples may be effective when either machines or humans try to solve a challenging task containing more label noise or outliers. Otherwise, focusing on harder samples may accelerate and stabilize the SGD in cleaner data by minimizing the variance of the gradients [1,12]. However, we often do not know how noisy our training dataset is. Motivated by this practical need, this paper explores new methods of re-weighting training examples that are robust to both scenarios. \n\nIntuitively, if our model has already predicted some examples correctly with high confidence, the samples may be too easy to contain useful information for the current model. Similarly, if some examples are always predicted incorrectly over many iterations of training, these examples may just be too difficult/noisy and would confuse the model.",
            "score": 0.3784375562409733,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1937
                },
                {
                    "start": 1940,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2285
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "10327263"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "2843566"
                },
                {
                    "start": 1270,
                    "end": 1273,
                    "matchedPaperCorpusId": "973457"
                },
                {
                    "start": 1337,
                    "end": 1341,
                    "matchedPaperCorpusId": "2843566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.623046875
        },
        {
            "corpus_id": "272827311",
            "title": "ISC4DGF: Enhancing Directed Grey-box Fuzzing with LLM-Driven Initial Seed Corpus Generation",
            "text": "Pre-trained Large Language Models (LLMs) are advanced neural networks with billions of parameters. \n\nThese models are trained on large amounts of text data using an autoregressive method, where the model learns to predict the next word in a sequence based on the previous context. This extensive training allows LLMs to act as one-shot or even zero-shot learners [32], enabling them to perform a wide variety of tasks with little or no additional training. \n\nLLMs are typically used for specific tasks through either fine-tuning [36] or prompting [29]. Fine-tuning involves further training the model on a task-specific dataset, which adjusts its internal weights to enhance performance for that task. However, this approach can be challenging due to the potential lack of suitable datasets, and as LLMs grow in size [24], the cost and complexity of fine-tuning also increase. Alternatively, prompting enables the LLM to perform tasks without altering its weights. The process involves providing the model with a detailed description of the task, sometimes including a few examples to illustrate how to solve it. By utilizing the model's existing knowledge, users can guide its performance through a technique known as prompt engineering [29], where different input instructions are tested to identify the most effective prompts.",
            "score": 0.37807581618396224,
            "section_title": "Large Language Models",
            "char_start_offset": 5501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 101,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1329
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1878662109375
        },
        {
            "corpus_id": "273654515",
            "title": "Sparse Linear Bandits with Blocking Constraints",
            "text": "In niche industrial applications such as Named Entity Recognition (Nguyen et al., 2023) and learning tasks on low-resource languages (Hedderich et al., 2021), obtaining high-quality labels is challenging due to the lack of expert annotators. However, high-quality labels are critical for effective model training (Li et al., 2023), and thus, expert annotators must provide ground truth labels. (Sorscher et al., 2022) demonstrated that selecting high-quality data can reduce the power-law association of test error with dataset size to an exponential law. In annotation-expensive tasks with large volumes of unlabeled data, the challenge is to select a representative subset of datapoints for labeling. In label-scarce tasks, where the number of expert annotators is extremely low, often only one, it is impractical to query the same datapoint multiple times. While crowd-sourcing literature reduces noise by aggregating labels from multiple annotators (Verroios & Garcia-Molina, 2015), the annotation from a single or aggregated expert is considered the final ground truth label in our setting. We term this restriction, where a datapoint cannot be re-queried after annotation, the blocking constraint. Additionally, the annotation budget is typically much smaller than the datapoint embedding dimension. An efficient annotation strategy should be sequential (instead of one-shot) in such cases, as each annotation informs future decisions and helps identify more informative datapoints. \n\nIn addition, data-pruning techniques like coreset selection emphasize selecting hard examples (Maharana et al., 2023), while (Sorscher et al., 2022) justifies this for perceptron learning. Curriculum learning (Bengio et al., 2009) also uses increasingly difficult examples, but defining the 'hardness' of unlabeled data is ambiguous. Hard examples identified by heuristics are often noisy, mislabeled, or outliers (Mindermann et al., 2022). To address this, we propose soliciting annotation difficulty feedback directly from expert annotators.",
            "score": 0.3778693502840584,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 87,
                    "matchedPaperCorpusId": "254535870"
                },
                {
                    "start": 133,
                    "end": 157,
                    "matchedPaperCorpusId": "225062337"
                },
                {
                    "start": 394,
                    "end": 417,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 953,
                    "end": 985,
                    "matchedPaperCorpusId": "8422849"
                },
                {
                    "start": 1616,
                    "end": 1639,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1700,
                    "end": 1720,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371826171875
        },
        {
            "corpus_id": "260886825",
            "title": "Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification",
            "text": "Adapters and Low-Rank Adaptation (LoRA) are parameter-efficient fine-tuning techniques designed to make the training of language models more efficient. Previous results demonstrated that these methods can even improve performance on some classification tasks. This paper complements existing research by investigating how these techniques influence classification performance and computation costs compared to full fine-tuning. We focus specifically on multilingual text classification tasks (genre, framing, and persuasion techniques detection; with different input lengths, number of predicted classes and classification difficulty), some of which have limited training data. In addition, we conduct in-depth analyses of their efficacy across different training scenarios (training on the original multilingual data; on the translations into English; and on a subset of English-only data) and different languages. Our findings provide valuable insights into the applicability of parameter-efficient fine-tuning techniques, particularly for multilabel classification and non-parallel multilingual tasks which are aimed at analysing input texts of varying length.",
            "score": 0.37785324804693665,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "247518642",
            "title": "When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation",
            "text": "-aware DA methods are capable of generating augmented samples, conditioned on the downstream task objective (Hu et al., 2019;Xie et al., 2020;Rashid et al., 2021). These methods adapt augmented examples specifically for a task in that they construct augmented examples, sometimes partly, during training. Despite their advantages, they often incur additional training costs, resulting in a prohibitively slow and a computationally expensive training.\n\nIn general, the central problems surrounding DA techniques in NLP can be summarized as follows:\n\nFirst, DA methods are mostly not sample-efficient in that they add arbitrary number of augmented samples to the training data and naively incorporate all of them into training without investigating how many of augmented samples are actually needed. Second, although more effective, taskaware methods are notoriously time-consuming to train. This is especially problematic in large-scale datasets such as SQuAD (Rajpurkar et al., 2016) and MNLI (Williams et al., 2018). Third, most DA methods are not universal as they work solely with a particular setup-e.g., training a singlenetwork (Xie et al., 2020), or training in teacherstudent settings (Rashid et al., 2021). Overall, the importance of both sample efficiency and training efficiency for DA has been often overlooked.\n\nMotivated by the above problems, in this work, we introduce a universal DA method, Glitter 2 , which can be plugged into any DA method to make them sample-efficient, and task-aware without sacrificing performance. Specifically, given a pool of augmented samples that are generated offline, our proposed method follows a minimax approach (Farnia and Tse, 2016) to select a small subset with maximal expected loss (maximization step) during training. Without any further adjustments to the training algorithm, the task objective can be optimized for this selected subset (minimization step).\n\nOur key contributions in this paper can be summarized as follows:\n\n1. Glitter is a universal method which can be effortlessly applied to any DA method to enforce sample efficiency while maintaining (or even boosting) their performance.\n\n2. We devise strategies to adapt Glitter for a variety of widely used training setups including single-network, consistency training, selfdistillation and knowledge distillation.  (",
            "score": 0.3775425220969305,
            "section_title": "Introduction",
            "char_start_offset": 1697,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 125,
                    "matchedPaperCorpusId": "56482333"
                },
                {
                    "start": 125,
                    "end": 142,
                    "matchedPaperCorpusId": "195873898"
                },
                {
                    "start": 142,
                    "end": 162,
                    "matchedPaperCorpusId": "234482764"
                },
                {
                    "start": 959,
                    "end": 983,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 993,
                    "end": 1016,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1134,
                    "end": 1152,
                    "matchedPaperCorpusId": "195873898"
                },
                {
                    "start": 1193,
                    "end": 1214,
                    "matchedPaperCorpusId": "234482764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.200439453125
        },
        {
            "corpus_id": "227227757",
            "title": "Dynamic Curriculum Learning for Low-Resource Neural Machine Translation",
            "text": "We implement our method with fairseq by simply modifying. Our DCL method measures the sample difficulty and model competence before every phase dynamically. While it results in extra time consumption (about 30%), it is acceptable for low-resource tasks. \n\nIn all experiments, we set a = 1 uniformly in Equation 6 to measure the difficulty, which means the sample difficulty takes into account two adjacent phases. For the model competence described in Equation 8, we record the best BLEU of the baseline model on the development set as the BLEU T . Although hyperparameter with careful selection can bring improvement, we set c 0 = 0.2 and \u03b2 = 0.9 universally for all experiments. It means we start training with the 20% easiest sentences and train the model with the whole training set when the performance achieves 90% of BLEU T . \n\nWe use the following notations to represent different curriculum learning strategies. For the sample difficulty, we compare our method with previous heuristic metrics and two other dynamic metrics: \n\n\u2022 Heuristic metrics: Source sentence length (Length) and source word rarity (Rarity) (Platanios et al., 2019). \u2022 Dynamic metrics: Random difficulty value (Random) and the loss at the current phase (Loss). \n\n\u2022 Our method: Loss decline between the previous phase and the current phase (Decline). \n\nFor the model competence, we experiment with the following methods: \n\n\u2022 Functional forms: Linear (Linear) and square root (Sqrt) model competence (Platanios et al., 2019). \n\n\u2022 Our method: Dynamic model competence (DMC) based on the performance.",
            "score": 0.375857175152596,
            "section_title": "Curriculum Learning Setup",
            "char_start_offset": 17883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1575
                }
            ],
            "ref_mentions": [
                {
                    "start": 1120,
                    "end": 1144,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1477,
                    "end": 1501,
                    "matchedPaperCorpusId": "85498775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.615234375
        },
        {
            "corpus_id": "247292373",
            "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
            "text": "and calculate the average predictive correctness for each instance. Finally, we compute the difficulty score by subtracting this averaged correctness value from 1. This ensures that an instance that is answered correctly with high confidence under many training configurations gets assigned a low difficulty score as it corresponds to an easy instance. In contrast, an instance that is often answered incorrectly gets assigned a high difficulty score. Algorithm 1 summarizes this approach.\n\nWe use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1).\n\nWe note that difficulty computation is not our primary contribution. Prior work (Swayamdipta et al., 2020;Xu et al., 2020) has explored different ways to achieve this. However, our approach uses 120 predictions from models trained with different configurations for its computation and hence is more reliable. Equipped with difficulty scores of evaluation instances, we now demonstrate five applications of ILDAE in the following sections.",
            "score": 0.375857175152596,
            "section_title": "Method",
            "char_start_offset": 6076,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1097,
                    "end": 1113,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "259075310",
            "title": "Adaptive and Personalized Exercise Generation for Online Language Learning",
            "text": "removing lookahead would bias the distribution toward 2 nd and 3 rd quarter. This confirms our assumption that naively applying F d would greedily select difficult words in the early steps, which is not the distribution of reference exercises. Our decoding algorithm avoids this issue by estimating the future and therefore achieves better results. Upper Bound Analysis. When we train our model, we use ground-truth difficulty d and target words C obtained from references; however, the student states s are estimated from the KT model. We conduct an upper bound analysis to understand the influence of the accuracy of s on the generation performance. Since a student's actual mastery of every vocabulary word is not available, we choose to replace the ground-truth difficulty levels d with those estimated from s. As shown in the last section of Table 2, all metrics are considerably boosted when the inconsistency between states s and difficulty d is eliminated. This again proves the effect Figure 4: Generating 50 additional exercises of specified difficulty levels for different student groups using APEG s+C+d (adaptive) and non-adaptive EG C+d models. The Y-axis is the ratio of output difficulty d out to input difficulty d in ; the closer to 1 (dotted line) the better. Solid lines are averaged results of group students at each step, and shadows represent standard deviations.  Table 4: Examples of exercises based on different controls. d in is the input difficulty while d out is the output difficulty estimated by our knowledge tracing model. The degree of highlight represents a student's mastery of vocabulary words (the darker the harder).\n\nof incorporating student states and explains how such information comes to play: the knowledge states explicitly convey the dynamics between control signals d, C, and target exercises e, which is non-trivial to learn by the model itself. Case Study. We provide a few cases in Table 4.\n\nWe can see our model can dynamically adjust the exercise content according to specified words, target difficulty, as well as students' different mastery states of the vocabulary. The exercises generated for advanced students (avg. state = 0.65) are generally more difficult than for poor students (avg. state = 0.32) under the same input difficulty.",
            "score": 0.375857175152596,
            "section_title": "Exercise Generation Evaluation",
            "char_start_offset": 23644,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "262045288",
            "title": "Anchor Points: Benchmarking Models with Much Fewer Examples",
            "text": "Sample-Efficient Model Evaluation We deviate from prior sample-efficient model evaluation literature along key axes. Many works (Kossen et al., 2021a,b;Deng and Zheng, 2021;Corneanu et al., 2020) minimize evaluation annotation costs by actively selecting points to annotate for evaluating a given model. In the era of large benchmarks and large models, labeled evaluation examples are widely-available but evaluating all of them is cumbersome. Our technique instead minimizes the number of forward-passed examples necessary for reliable model evaluation. Furthermore, our selection strategy is agnostic to the target model(s) being evaluated. The resulting evaluation set is transferable to other language models. \n\nOther works (Rodriguez et al., 2021;Ethayarajh et al., 2022;Bowman and Dahl, 2020) point out that it is often a minority of points that differentiate the performance of various models. Rodriguez et al. (2021) show that evaluating models on points identified as the most discriminative can effectively rank model performance. However, optimizing for discriminability does not result in representative subsets: very easy or very hard points will tend to be excluded, leading to a different distribution. Instead, we optimize for representativeness which we show naturally leads to discriminability. \n\nInstance-Level Model Performance Despite being noisy (Zhong et al., 2021), instance-level model predictions are a rich source of information about model behavior. Swayamdipta et al. (2020) present Data Maps, a powerful technique that leverages instance-level predictions to reveal underlying structure in the interplay of models and data points. \n\nVarious training example regions play distinct roles in guiding a classifier to its solution. Unlike our technique, Data Maps are not used for comparing model performance or isolating distinct regions of the dataset distribution where models are weak. Ethayarajh et al. (2022) further show that instance-level predictions can be used to quantify how much information a given model can extract from a dataset, providing a formal metric of dataset difficulty that exposes model behavior at the dataset and data point level.",
            "score": 0.3758561598174081,
            "section_title": "Related Work",
            "char_start_offset": 5089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 173,
                    "matchedPaperCorpusId": "235187480"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7451171875
        },
        {
            "corpus_id": "266166227",
            "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
            "text": "In this section, we briefly introduce our proposed curriculum learning method for non-compositional expression generation. Curriculum learning for efficiently leveraging available data resources consists of two main parts: a measure of difficulty of training instances, and an arrangement of the training examples using this measure. Accordingly, for non-compositional expression generation, we propose a data arrangement method for dynamically arranging the training examples according to a newly studied difficulty metric. In addition, due to the current large pre-trained language models' insufficiency in processing non-compositional expressions (Dankers et al., 2022), non-compositional expressions that are difficulty for LMs to understand would have a high perplexity score and the representations between non-compositional expressions and their constituent words would be large. Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty. \n\nMoreover, in our experiments, we observe that following the curriculum learning principle of arranging the training examples based on their difficulty levels, the problem of forgetting arises due to the gradual shift of distribution in domain difficulty. Therefore, to alleviate this forgetting problem, we propose a simple yet effective continual learning method. Figure 1 demonstrates the workflow of our proposed curriculum learning framework and its details as follows.",
            "score": 0.3755236031441558,
            "section_title": "Framework",
            "char_start_offset": 9802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1484
                }
            ],
            "ref_mentions": [
                {
                    "start": 650,
                    "end": 672,
                    "matchedPaperCorpusId": "248780588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "266180249",
            "title": "GPT-wee: How Small Can a Small Language Model Really Get?",
            "text": "The present analysis set out to investigate the influence of a usage-based factors, input ordering, and an architectural factor, model size, on the learning processes (and successes) of language models. We found that both factors have a certain influence on the training process and the model performance. \n\nWhile model size affects the performance in linguis-tic evaluation, the effect is not linear across tasks. \n\nFor zero-shot tasks, the majority show improved scores, although a few scores decrease with increasing model size. Compared to much larger baseline models, our models' performance is not considerably worse. Especially the non-linear effects of model size warrant further inspection: it remains unclear which internal factors (context length, vocabulary size, model parameters, number of training epochs, etc.) contribute to which developments, and how these factors interact with each other. For the tasks requiring additional fine-tuning, our 8k curriculum model also performed similarly to the baselines. Especially for the (SUPER)Glue benchmark, a more semantics-and pragmatics-oriented benchmark, the performance was quite in line with the baseline models, hinting at the acquisition of a fair amount of the needed information. The MSGS benchmark, however, showed that our model systematically picks up surface generalizations. Yet, this also applies to the much larger baselines. \n\nThe usage-inspired na\u00efve ordering approach to curriculum learning also has no straightforward effects on model performance. Especially during the training process, differences to traditional, randomized learning are observable. Although it appears to be somewhat detrimental to overall performance, certain specific evaluation tasks are positively influenced. The results thus remain inconclusive. From a usage-based viewpoint, Diessel (2013b) stresses the importance of deictic pointing and joint attention as (extralinguistic) language acquisition fac- The non-improvements added by the curriculum approach also further add to the debate on what language models mean for linguistic theory. For example, Pannitto and Herbelot (2022) and Piantadosi (2023) have stressed the anti-Chomskyan evidence provided by the successes of language models. Curriculum learning looks like an obvious choice when trying to implement usage-based findings in the training process for (smaller) language models.",
            "score": 0.3753327573869065,
            "section_title": "Discussion",
            "char_start_offset": 22756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2397
                }
            ],
            "ref_mentions": [
                {
                    "start": 1832,
                    "end": 1847,
                    "matchedPaperCorpusId": "146386959"
                },
                {
                    "start": 2109,
                    "end": 2137,
                    "matchedPaperCorpusId": "247768613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1395263671875
        },
        {
            "corpus_id": "271162331",
            "title": "Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments",
            "text": "The core of our language module utilizes a large pre-trained language model, which has been further finetuned on a dataset specific to the environment.This dataset contains information on how instructions translate into specific subtasks, enabling the model to understand and decompose complex commands effectively.The training leverages a specific format that maps instructions to their corresponding subtasks (e.g., Instruction -> Subtask 1, Subtask 2, Subtask 3).\n\nDue to the difficulty of obtaining comprehensive datasets for training models to translate language instructions into commands, we face additional challenges.Typically, datasets for training large language models (LLMs) on such tasks are manually curated, which is a labor-intensive process.This often limits both the size and the quality of the datasets available.In response, we employ techniques to prevent overfitting, especially when working with these limited datasets.Our experiments demonstrate that these methods effectively enhance training quality by ensuring the model can generalize well from smaller, varied linguistic datasets, leading to a more robust understanding of instructions.\n\nAugmentation with LLM.In this technique, we begin by understanding the structure and specific terminology of the dataset.Our approach involves iteratively modifying the list of subtasks necessary to execute a given instruction for each dataset element.Subsequently, ChatGPT or another LLM is tasked with rewriting the instruction to incorporate these modified subtasks.\n\nThe prompt request is structured as follows:\n\n1) Description of the Environment: Provide the LLM with a detailed understanding of the setting by explaining the overarching themes and key specific concepts.Use succinct descriptions for familiar ideas and detailed explanations for unique aspects.\n\n2) Few-shot Example: Introduce the original instruction alongside its required subtasks.Optionally, you can also provide an example of how the instruction might change if the subtasks are altered.\n\n3) Task Modification Request: Specify new subtasks for the target instruction and request the LLM to revise the instruction accordingly.\n\nIt is important to emphasize that the LLM is not creating the instruction from scratch.We aim to start with the existing instruction and suggest modifications, ensuring that the style of the original instruction is preserved as much as possible.\n\nSubtasks decomposition.",
            "score": 0.37507867178178855,
            "section_title": "Language Module Training Techniques",
            "char_start_offset": 10113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 466
                },
                {
                    "start": 468,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 759
                },
                {
                    "start": 759,
                    "end": 833
                },
                {
                    "start": 833,
                    "end": 943
                },
                {
                    "start": 943,
                    "end": 1166
                },
                {
                    "start": 1168,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1420
                },
                {
                    "start": 1420,
                    "end": 1537
                },
                {
                    "start": 1539,
                    "end": 1583
                },
                {
                    "start": 1585,
                    "end": 1744
                },
                {
                    "start": 1744,
                    "end": 1834
                },
                {
                    "start": 1836,
                    "end": 1924
                },
                {
                    "start": 1924,
                    "end": 2032
                },
                {
                    "start": 2034,
                    "end": 2170
                },
                {
                    "start": 2172,
                    "end": 2259
                },
                {
                    "start": 2259,
                    "end": 2417
                },
                {
                    "start": 2419,
                    "end": 2442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224365234375
        },
        {
            "corpus_id": "269773266",
            "title": "Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts",
            "text": "They generalize better and can be applied to texts of varying lengths.However, these classifiers are expensive to train and require more training data than we usually have for a niche domain like educational purposes.Similarly, human assessment of difficulty may provide a gold standard, but it is expensive to collect and, like all annotation tasks, suffers from disagreement.\n\nIn this paper, we introduce and evaluate a new set of PROMPT-BASED metrics for text difficulty as complements to existing STATIC metrics.PROMPT-BASED metrics are LLM prompts that exploit the general language understanding capabilities of LLMs to capture more abstract features of educational texts than STATIC metrics.For example, LLMs can flexibly classify the topic of a text, which is one adaptation technique used by teachers to adjust the content which called curriculum compacting in pedagogy (Stamps, 2004).This would be difficult to do with STATIC approaches.\n\nWe develop our selection of PROMPT-BASED metrics based on a user study, where we ask a group of university students to 1) assess the difficulty of educational texts and explain their reasoning, and 2) come up with prompts for an LLM to change the difficulty of a given text.We then translate the qualitative findings from both parts of the study into concrete LLM prompts that serve as PROMPT-BASED metrics.We incorporate prompts from other studies to manage text readability with LLMs (Imperial and Madabushi, 2023;Gobara et al., 2024).We evaluate the ability of our new PROMPT-BASED metrics to measure text appropriateness for different education levels with a series of regression experiments.\n\nWhile PROMPT-BASED metrics perform on par or better than zero-shot and few-shot LLM classifiers, they are less useful for text difficulty classification by themselves than STATIC metrics.How-ever, combining PROMPT-BASED and STATIC metrics significantly improves performance.This suggests that PROMPT-BASED metrics capture relevant signals beyond those captured by the large number of STATIC metrics.\n\nA combination of STATIC and PROMPT-BASED metrics also provides a deeper understanding of the key metrics or features that influence complexity than classifiers could.Additionally, the factors that contribute to complexity in a scientific text differ from those in a medical or a legal document.",
            "score": 0.3750251559800536,
            "section_title": "Introduction",
            "char_start_offset": 2215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 70,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 377
                },
                {
                    "start": 379,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 946
                },
                {
                    "start": 948,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1355
                },
                {
                    "start": 1355,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1644
                },
                {
                    "start": 1646,
                    "end": 1833
                },
                {
                    "start": 1833,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2045
                },
                {
                    "start": 2047,
                    "end": 2213
                },
                {
                    "start": 2213,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 878,
                    "end": 892,
                    "matchedPaperCorpusId": "52036772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1558837890625
        },
        {
            "corpus_id": "13949438",
            "title": "Robust Logistic Regression using Shift Parameters",
            "text": "Almost any large dataset has annotation errors, especially those complex, nuanced datasets commonly used in machine learning research. Low-quality annotations have become even more common in recent years with the rise of Amazon Mechanical Turk, as well as methods like distant supervision and cotraining that involve automatically generating training data. \n\nAlthough small amounts of noise may not be detrimental, in some applications the level can be high: upon manually inspecting a relation extraction corpus commonly used in distant supervision, Riedel et al. (2010) report a 31% false positive rate. In cases like these, annotation errors have frequently been observed to hurt performance. Din-gare et al. (2005), for example, conduct error analysis on a system to extract relations from biomedical text, and observe that over half of the system's errors could be attributed to inconsistencies in how the data was annotated. Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fullysupervised counterparts. \n\nDespite this prevalence, little work has been done in designing models that are aware of annotation errors. Moreover much of the previous work focuses on heuristic techniques to filter the data before training, which might discard valuable examples simply because they do not fit closely with the model assumptions. \n\nIn this work we argue that incorrect examples should be explicitly modelled during training, and present a simple extension of logistic regression that incorporates the possibility of mislabelling directly into the objective. Our model introduces sparse 'shift parameters' to allow datapoints to slide along the sigmoid, changing class if appropriate. It has a convex objective, can handle high-dimensional data, and we show it can be efficiently trained with minimal changes to the logistic regression pipeline. \n\nExperiments on large, noisy NLP datasets show that our method can provide an improvement over standard logistic regression, both in manually and automatically annotated settings. The model also provides a means to identify which examples were mislabeled: through experiments on biological data, we demonstrate how our method can be used to ac-curately identify annotation errors.",
            "score": 0.3749140390478929,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2014
                },
                {
                    "start": 2017,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2396
                }
            ],
            "ref_mentions": [
                {
                    "start": 1001,
                    "end": 1025,
                    "matchedPaperCorpusId": "13999155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17333984375
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018). \n\nWe conduct an extensive empirical exploration of curriculum learning on a German-English translation task, implementing all training strategies in the Sockeye NMT toolkit. 1 . Our experiments confirm that curriculum learning can improve convergence speed without loss of translation quality, and show that viewing curriculum learning more flexibly than strictly training on easy samples first has some benefits. We also demonstrate that curriculum learning is highly sensitive to hyperpa-rameters, and no clear single best strategy emerges from the experiments. \n\nIn this sense, our conclusions are both positive and negative: We have confirmed that curriculum learning can be an effective method for training expensive models like those in NMT, but careful design of the specific curriculum hyperparameters is important in practice. \n\n2 Related Work Bengio et al. (2009) coined the term of curriculum learning to refer to techniques that guide the training of learning systems \"by choosing which examples to present and in which order to present them in the learning system\", and hypothesize that training on easier samples first is beneficial. While organizing training samples based on difficulty has been demonstrated in NLP outside of neural models -e.g., Spitkovsky et al. (2010) bootstrap unsupervised dependency parsers by learning from incrementally longer sentences -curriculum learning has gained popularity to address the difficult optimization problem of training deep neural models (Bengio, 2012). Bengio et al. (2009) improve neural language model training using a curriculum based on increasing vocabulary size. More recently, Tsvetkov et al. (2016) improve word embedding training using Bayesian optimization to order paragraphs in the training corpus based on a range of distributional and linguistic features (diversity, simplicity, prototypicality).",
            "score": 0.3746168734416545,
            "section_title": "Introduction",
            "char_start_offset": 1954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 220,
                    "matchedPaperCorpusId": "550225"
                },
                {
                    "start": 220,
                    "end": 246,
                    "matchedPaperCorpusId": "7921428"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "20639213"
                },
                {
                    "start": 1119,
                    "end": 1139,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1529,
                    "end": 1553,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 1780,
                    "end": 1800,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "251493126",
            "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
            "text": "In particular, after acquiring the ordered training set, we cut it into three parts (easy, medium and hard level examples). We then equally mix the easy examples with some hard and medium-level ones. In this work, we have chosen to mix 20% of the easy examples with medium and hard-level ones (due to empirical results). Further implementation notes can be found in the project's github repository. A similar method was employed in [15] where the authors found that exposing the model to a few difficult examples can improve performance. We argue that this mixing procedure aids the CL system to avoid overfitting, which is a real risk at the start of each epoch, when only easy examples are shown. \n\nTo the best of our knowledge, these methods have not been explicitly utilized in the field of ASR before. Most CL-related research in ASR has focused on the metadata approach using mainly the duration [7,16] and signal-to-noise ratio statistics [8]. The main shortcoming of these approaches compared to our solutions is their simplicity, since metadata are not always good indicators of the utterances' difficulties. They completely ignore the changes happening in the model during training, which would warrant a re-evaluation of the difficulties of the examples, similar to how teachers adapt their curriculum.",
            "score": 0.3741811202319678,
            "section_title": "Scoring Function",
            "char_start_offset": 7772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1313
                }
            ],
            "ref_mentions": [
                {
                    "start": 902,
                    "end": 905,
                    "matchedPaperCorpusId": "11590585"
                },
                {
                    "start": 946,
                    "end": 949,
                    "matchedPaperCorpusId": "14928979"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.755859375
        },
        {
            "corpus_id": "264439382",
            "title": "DoGE: Domain Reweighting with Generalization Estimation",
            "text": "Data Selection for Language Modeling. Many works show how a rigorously selected training corpus can effectively   However, due to scalability issues, most traditional data selection methods fail to be applicable for pretraining. \n\nClassifier-based data filtering techniques are commonly used to construct a pretraining corpus (Gao et al., 2020;Penedo et al., 2023). Everaert & Potts (2023) propose GIO to select a subset that minimizes the KL-divergence to the target distribution, yet incurs high computation complexity. 2023) propose to build an online resampling curriculum by exploiting the dependency relationship among skills represented by a directed skill graph. While the computation cost for constructing the skill graph limits its applicability to general language model pretraining.",
            "score": 0.3735752786122921,
            "section_title": "Related Work",
            "char_start_offset": 21997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 794
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275390625
        },
        {
            "corpus_id": "273798735",
            "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
            "text": "Large language models (LLMs) (Achiam et al., 2023;Touvron et al., 2023) have garnered significant attention for their remarkable ability to generate human-like text. However, their training on extensive web-scraped datasets, exposes them to a range of security and privacy risks, from memorizing private or copyrighted content to reproducing incorrect or harmful information in the training data (Pan et al., 2020;Wei et al., 2024;Carlini et al., 2021;Huang et al., 2022). To address these concerns and to further the development of trustworthy language models, researchers have proposed unlearning techniques. These methods aim to modify an already trained model to unlearn a set of datapoints, resulting in a model that is similar to one which never included the datapoints in its training set (Blanco-Justicia et al., 2024). \n\nSince LLMs are pre-trained or fine-tuned on vast datasets, retraining from scratch after excluding target datapoints is computationally infeasible for unlearning. Consequently, the community has explored efficient methods to simulate this procedure. These approaches are typically evaluated by assessing their effectiveness in forgetting content within unlearning documents, e.g., copyrighted concepts, toxic content, or forgetting factual knowledge about concepts appeared within unlearning dataset (Maini et al., 2024;Jin et al., 2024;Zhang et al., 2024;Jang et al., 2022;Kumar et al., 2022). \n\nIn this work, we consider an alternate prerequisite for unlearning: if a model is no longer influenced by the unlearning set, it should retain the same knowledge and capabilities as before encountering documents in this set. For instance, imagine a model checkpoint that has already acquired correct knowledge about certain concepts. As pretraining continues on other datapoints -some containing incorrect facts, private information, or malicious documents related to those conceptsthe model may eventually exhibit degraded performance on tasks involving these concepts. In such scenarios, unlearning would help eliminating the influence of these datapoints, since retraining the model from scratch is computationally infeasible.",
            "score": 0.3730486392668121,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 414,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 414,
                    "end": 431,
                    "matchedPaperCorpusId": "259342528"
                },
                {
                    "start": 431,
                    "end": 452,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0784912109375
        },
        {
            "corpus_id": "273248016",
            "title": "Text processing using LLM for automatic creation of agricultural crops knowledge bases",
            "text": "The use of large language models (LLMs) for processing text documents is a classical approach based on prompts, known as Retrieval-Augmented Generation (RAG). Before the era of LLMs, models were often supplemented with new data simply by fine-tuning them. However, now that the models being used have become much larger, training them on significantly larger datasets is suitable for only a few usage scenarios. Fine-tuning is particularly effective in cases where the model needs to interact with users using a style and tone of speech different from the original. However, fine-tuning is not as effective when new data needs to be added to the model, which I have found to be a much more common business scenario. Additionally, fine-tuning LLMs requires large volumes of high-quality data, substantial computational resources, and significant time. For most LLM users, these are considered limited resources (Fig. 1).",
            "score": 0.37304661534127653,
            "section_title": "Formulation of the problem",
            "char_start_offset": 4308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 919
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08465576171875
        },
        {
            "corpus_id": "215746703",
            "title": "Curriculum Learning Strategies for IR",
            "text": "Processing training instances in a meaningful order is not unique to CL. Another related branch of research focuses on dynamic sampling strategies [22,4,39,2], which unlike CL that requires a definition of what is easy and difficult before training starts, estimates the importance of instances during the training procedure. Self-paced learning [22] simultaneously selects easy instances to focus on and updates the model parameters by solving a biconvex optimization problem. A seemingly contradictory set of approaches give more focus to difficult or more uncertain instances. In active learning [6,44,4], the most uncertain instances with respect to the current classifier are employed for training. Similarly, hard example mining [39] focuses on difficult instances, measured by the model loss or magnitude of gradients for instance. Boosting [2,59] techniques give more weight to difficult instances as training progresses. In this work we focus on CL, which has been more successful in neural models, and leave the study of dynamic sampling strategies in neural IR as future work. \n\nThe most critical part of using a CL strategy is defining the difficulty metric to sort instances by. The estimation of instance difficulty is often based on our prior knowledge on what makes each instance difficult for a certain task and thus is domain dependent (cf. Table 1 for curriculum examples). CL strategies have not been studied yet in neural ranking models. To our knowledge, CL has only recently been employed in IR within the LTR framework, using LambdaMart [3], for ad-hoc retrieval by Ferro et al. [9]. However, no effectiveness improvements over randomly sampling training data were observed. The representation of the query, document and their interactions in the traditional LTR framework is dictated by the manually engineered input features. We argue that neural ranking models, which learn how to represent the input, are better suited for applying CL in order to learn increasingly more complex concepts. \n\nTable 1: Difficulty measures used in the curriculum learning literature.",
            "score": 0.37291085150766545,
            "section_title": "Related Work",
            "char_start_offset": 7023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2016
                },
                {
                    "start": 2019,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 151,
                    "end": 153,
                    "matchedPaperCorpusId": "20785832"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 599,
                    "end": 602,
                    "matchedPaperCorpusId": "9242771"
                },
                {
                    "start": 602,
                    "end": 605,
                    "matchedPaperCorpusId": "7806109"
                },
                {
                    "start": 605,
                    "end": 607,
                    "matchedPaperCorpusId": "20785832"
                },
                {
                    "start": 851,
                    "end": 854,
                    "matchedPaperCorpusId": "550225"
                },
                {
                    "start": 1561,
                    "end": 1564,
                    "matchedPaperCorpusId": "397316"
                },
                {
                    "start": 1603,
                    "end": 1606,
                    "matchedPaperCorpusId": "53033820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.697265625
        },
        {
            "corpus_id": "266163842",
            "title": "Large Language Models are Complex Table Parsers",
            "text": "To this end, a wide range of strategies has been introduced, such as reinforcement learning, memory enhancement, type awareness, relationship awareness, etc. (Pasupat and Liang, 2015;Zhong et al., 2017;Liang et al., 2018;Yu et al., 2018;Yin et al., 2020). In recent years, there has been a notable progress of Large Language Models (LLMs), such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018), GPT-2 (Radford et al., 2019), RoBERTa (Liu et al., 2019), GPT-3 (Brown et al., 2020), and T5 (Raffel et al., 2020) in the field of NLP. This enable the Table QA tasks to generate answers directly without the need for intermediate logical forms, which leverage the rich language representations and knowledge acquired through large-scale text pre-training (Herzig et al., 2020;Chen et al., 2020;Eisenschlos et al., 2021;Yang et al., 2022;Chen, 2023). \n\nWhile significant attainment has been made in the studies above-mentioned, their primary emphasis has been on the development of simple flat tables, overlooking the ubiquitous complex tables. Although (Katsis et al., 2021;Cheng et al., 2022) endeavored to construct QA datasets, specifically tailored for complex tables, and evaluated the performance of the SOTA Table QA models, the outcomes have not met expectations. \n\nMost recently, with the advent of ChatGPT 1 , an advanced NLP model derived from GPT-3, has showcased remarkable capabilities in generation (Maddigan and Susnjak, 2023;Dong et al., 2023;Liu et al., 2023a), contextual understanding (Bang et al., 2023;Gao et al., 2023b;Amin et al., 2023), and reasoning (Liu et al., 2023b;Gao et al., 2023a), has profoundly impacted on the field of NLP.",
            "score": 0.37237829265077715,
            "section_title": "Introduction",
            "char_start_offset": 1391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1663
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 221,
                    "matchedPaperCorpusId": "52300972"
                },
                {
                    "start": 497,
                    "end": 518,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08135986328125
        },
        {
            "corpus_id": "269773266",
            "title": "Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts",
            "text": "Using large language models (LLMs) for educational applications like dialogue-based teaching is a hot topic. Effective teaching, however, requires teachers to adapt the difficulty of content and explanations to the education level of their students. Even the best LLMs today struggle to do this well. If we want to improve LLMs on this adaptation task, we need to be able to measure adaptation success reliably. However, current Static metrics for text difficulty, like the Flesch-Kincaid Reading Ease score, are known to be crude and brittle. We, therefore, introduce and evaluate a new set of Prompt-based metrics for text difficulty. Based on a user study, we create Prompt-based metrics as inputs for LLMs. They leverage LLM\u2019s general language understanding capabilities to capture more abstract and complex features than Static metrics. Regression experiments show that adding our Prompt-based metrics significantly improves text difficulty classification over Static metrics alone. Our results demonstrate the promise of using LLMs to evaluate text adaptation to different education levels.",
            "score": 0.3721991141151649,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "244908620",
            "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding",
            "text": "The difficulty of code data may be reflected in the length of the code, the use of rare tokens, the complexity of logic, etc. Although these heuristics are reasonable for people, they are not necessarily the case for models. Therefore, unlike the previous validation experiment that uses code augmentation techniques to distinguish the difficulty of the samples artificially, we let the model itself give an evaluation of the data as the difficulty scores, as shown in Algorithm 2. \n\nThe purpose of Algorithm 2 is to get the average difficulty score of each class on the training set. To get the difficulty score of each sample on the training set, we apply the leave-one-out strategy, i.e., when we compute the difficulty scores for a part of the samples, we train the model with all the other data. (line 4-9) Then we compute the average difficulty scores on each class. (line [11][12][13][14] To have a comparison with the learning order under the first hypothesis, we also apply Algorithm 2 to the state-of-the-art model CodeBERT with POJ104 dataset. POJ104 dataset contains many classes, and the task of POJ104 dataset is to predict the class for a given program. We apply Algorithm 2 to both the original training set and the augmented training set. We sort their average difficulty scores of each class according to the scores on the original training set, as shown in Fig. 3. Precision@R Class ID Calculate average difficulty scores on class ,  (, )",
            "score": 0.37198561697032345,
            "section_title": "Hypotheses",
            "char_start_offset": 11954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1457
                }
            ],
            "ref_mentions": [
                {
                    "start": 883,
                    "end": 887,
                    "matchedPaperCorpusId": "10364203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.416748046875
        },
        {
            "corpus_id": "277596006",
            "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
            "text": "Reinforcement Learning (RL) has become a key training paradigm for training large language models (LLMs) specialized in reasoning tasks, exemplified by OpenAI o1 (OpenAI et al., 2024) and DeepSeek-R1 (Guo et al., 2025). These models utilize Reasoning-Oriented Reinforcement Learning (RORL), where verifiable rewards like correctness in mathematical or logical problems serve as the primary supervision signal (Lambert et al., 2024). \n\nAs RORL increasingly targets high-complexity reasoning tasks, designing effective learning dynamics becomes crucial to help models progressively acquire the necessary capabilities. Effective learning has long been studied in the education domain, where theories such as the Zone of Proximal Development (ZPD) (Cole, 1978;Tzannetos et al., 2023) emphasize that learning is most efficient when tasks are neither too easy nor too hard, but instead fall within a learner's optimal challenge zone. This has motivated a variety of strategies in language modeling, from curriculum learning that introduces harder problems progressively (Team et al., 2025), to difficulty-aware data curation that selects or filters examples based on estimated pass rates or diversity (Muennighoff et al., 2025;Ye et al., 2025). Online filtering methods further explore this idea by dynamically adjusting the training data to match the current ability of the model (Cui et al., 2025). However, while previous work demonstrate the empirical effectiveness of such techniques, they often lack a detailed analysis of why or when certain difficulty distributions yield better learning outcomes. With G rollouts for each prompt x, we measure the pass rate p(x) as the average accuracy and filter them by predefined thresholds: e.g., 0.25 \u2264 p(x) \u2264 0.75 in this case. We recursively stack filtered prompts until the train batch size meets the fixed size N. We elaborate on the asynchronous implementation in Appendix A. \n\nIn this work, we conduct extensive experiments and provide theoretical analysis to understand how and why difficulty filtering improves learning in RORL.",
            "score": 0.37162588208855885,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2077
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "261076116",
            "title": "Curriculum Learning with Adam: The Devil Is in the Wrong Details",
            "text": "Despite these apparently sound results, we also find the approach to be brittle and inconsistent. Upon deeper investigation, we find that, rather than providing a sound data-based curriculum strategy, the learned curricula are fully data-agnostic and stem from interactions of the curriculum shape with the Adam optimiser, rather than a sound curriculum strategy (Section 4). As a result of the interaction, the parameter updates of the model are scaled in size, similar to a change in learning rate. Similar or larger learning advantages can be achieved by properly tuning hyperparameters. \n\nIn a second set of experiments, we go on to demonstrate how the curriculum-Adam-interaction is not limited to the commentaries framework. We will lay out in Section 2 how all curriculum learning approaches share a similar structure. We, therefore, continue to test common, simple hand-crafted curricula and, here, observe interactions as well. Importantly, plain Adam with properly tuned hyperparameters outperforms curricula in all of our tested settings. \n\nWe can summarise our contributions as follows: \n\n1. We transfer commentaries -an automated curriculum learning approach (Raghu et al. 2020) -from vision-to language data and provide an empirical analysis of its behaviour. 2. We showcase how commentaries work not due to a beneficial ordering of the data, but rather by a data-agnostic interaction with the optimiser. This can fully explain the learning advantages attributed to the curriculum. 3. We expand the notion to other types of curricula that are commonly used with Adam and empirically demonstrate how these curricula are affected as well. Hand-crafted curricula The simplest type of curriculum fixes the difficulty measure and schedule function prior to training, without adapting them dynamically according to the learner state. The choice of the difficulty measure is usually based on the practitioner's intuitions and experiences. Common difficulty measures in NLP include the sequence lengths of an input (or the closely related depth of the parse tree) (Tay et al. 2019;Mart\u00ednez Alonso et al. 2017;Platanios et al. 2019), the number of coordinating conjunctions (Kocmi and Bojar 2017) or the diversity of the used vocabulary (Platanios et al. 2019).",
            "score": 0.3714756387290485,
            "section_title": "Introduction",
            "char_start_offset": 1882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 1172,
                    "end": 1190,
                    "matchedPaperCorpusId": "961169"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "252596021",
            "title": "Effective Vision Transformer Training: A Data-Centric Perspective",
            "text": "For the first question, it is found that the \"difficulty\" of training samples can be adopted to measure the \"effectiveness\" of training samples. To generate \"effective\" samples, we propose to dynamically adjust the \"difficulty\" of training samples through a dynamic MixUp technique at different training stages. More concretely, we propose a novel data-centric ViT training framework, which contains a data samples processing mechanism that can solve many data-centric tasks, herein the measurement of sample \"difficulty\" and generation of \"effective\" examples. The model trained from prior iteration can provide extensive data operation guidance for the current model to learn. Different from curriculum learning [2,17], we divide the evolution period of training into three stages explicitly, and flexibly manipulate the \"difficulty\" distribution of training data according to the distinguishing properties of these stages. To further enlarge the \"effective\" samples and alleviate the overfitting problem in the late training stage of ViT, a patch-level erasing method namely PatchErasing is customized for ViTs. It relieves the ViTs from overfitting to some local patch tokens in the late T 3 period by randomly erasing some patches in the image and regularizing the corresponding label vector. And we experimentally demonstrate that it can improve the generalization ability of ViTs. \n\nIn summary, we make the following contributions in this work:",
            "score": 0.3711708730442105,
            "section_title": "Introduction",
            "char_start_offset": 1949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1451
                }
            ],
            "ref_mentions": [
                {
                    "start": 714,
                    "end": 717,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "11137059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2315673828125
        },
        {
            "corpus_id": "273233719",
            "title": "Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration",
            "text": "Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;Du et al., 2022;Chowdhery et al., 2023). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023b;Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023a;Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples. However, these methods often operate independently or in isolated settings, limiting their potential when integrated into a collaborative framework. In this work, we want to explore how to effectively, flexibly, and robustly combine these advanced data selection techniques through the dynamic pretraining process, addressing the challenges of optimizing data efficiency for LLM pretraining at scale. Nowadays, various heuristic methods have been proposed to provide measurements for the data samples used during LLM pre-training, aiming to optimize data efficiency by selecting or weighting the most informative training examples. However, we observe that integrating multiple data selection and mixing strategies presents significant challenges due to their inherent conflicts. For example, high-quality data identified by scoring functions may not align with data that strongly impact model performance as measured by influence functions (Engstrom et al., 2024); similar conflicts also exists between other methods -further details are enumerated in \u00a72. These observations actually motivate us to launch a systematic discussion about how to effectively integrate these methods during the dynamic pretraining process that provides superior data efficiency for LLM pretraining. \n\nOn the other hand, effectively integrating these data selection methods into a single framework is much harder to implement than to ask for. In fact, one may have to explore an exponential space to find the optimal combination for different data sampling schemas. Such a heavy burden will be Figure 1: Statistics of the SlimPajama dataset.",
            "score": 0.3709831737231057,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 251,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 251,
                    "end": 274,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 361,
                    "end": 380,
                    "matchedPaperCorpusId": "256627727"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "267681974"
                },
                {
                    "start": 436,
                    "end": 455,
                    "matchedPaperCorpusId": "258741043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "260125378",
            "title": "Fairness of ChatGPT and the Role Of Explainable-Guided Prompts",
            "text": "Motivation. Recent advancements in large language models such as OpenAI's GPT [3], Google's PALM [5], and Facebook's LaMDA [13] have redefined the landscape of Artificial Intelligence (AI). These behemoth models utilize billions of parameters and capitalize on the vastness of the internet data for training, leading to the generation of accurate and high-quality content. Large-scale Language Models (LLMs) have shown outstanding performance across tasks such as health diagnostics, job-seeking, and risk assessment, among others [4,6,11,1,12]. Given the transformative potential of these systems in decision-making across various contexts, their trustworthiness has drawn substantial attention. Unlike conventional ML models, LLMs leverage immense data scales, far surpassing those typically used for pretraining smaller or mid-scale models. This data, often sourced from the internet, mirrors societal norms but can also propagate prevalent societal biases. If unchecked, these biases can amplify, leading to biased outcomes that unfairly affect certain individuals or demographics. \n\nA crucial aspect of harnessing these systems is through prompt engineering, highlighted in this work. This technique mitigates the need for extensive dedicated training and offers system designers a measure of 'control' over the model's behavior by enabling direct infusion of their insights into the learning process. \n\nWhile our focus is a case study on ChatGPT, the insights, and methodologies could potentially extend to other LLM types, an avenue we plan to explore in future work. Contributions. This paper provides preliminary insights from an ongoing larger project aiming to address the challenges associated with the use of LLMs, particularly in decision-making processes through prompt engineering. We demonstrate the ability to harness the potential of utilizing pre-trained models for downstream ML tasks, thereby eliminating the need for dedicated model training. By meticulously designing prompts that embody problem-specific instructions and contexts, we direct these models toward achieving desired objectives, such as enhancing prediction accuracy and minimizing risk (and/or mitigating unfair outcomes).",
            "score": 0.37073856242328224,
            "section_title": "Introduction and Context",
            "char_start_offset": 27,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 81,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 534,
                    "end": 536,
                    "matchedPaperCorpusId": "257496827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "259501760",
            "title": "ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey",
            "text": "from text completion to translation or questionanswering. Adapting the model to the specific task requirements can greatly improve its performance. Essentially, fine-tuning is a way to make the pre-trained model more specialized and efficient in its designated task. \u2022 Scaling: Scaling LLMs involves various techniques [53]. One approach is to train them on wider and more diverse datasets. Another method involves increasing the number of parameters in the model, which enables it to recognize intricate patterns. However, this method demands significant computational resources. Parallel computing techniques and specialized hardware like GPUs or TPUs can also aid in computation scaling. Moreover, larger models have demonstrated better performance on language tasks, at times showing emergent characteristics but they may also have limitations and biases that arise during the training process [51]. \u2022 Optimization: Optimizing parameters and hyperparameters, such as the learning rate, regularization techniques, and loss functions are essential to attain optimal outcomes when training LLMs [54]. Optimizing these parameters can help the model converge to better solutions and improve its overall performance. As researchers continue to develop new optimization algorithms and techniques, the training process of LLMs becomes increasingly efficient and effective. \u2022 Human-in-the-Loop: LLMs are often developed with the help of human interactions. This means that human reviewers and annotators are involved in the process of curating data, which is essential to ensure that the training data is of high quality and relevance. Additionally, human interactions are used to provide feedback, evaluate model outputs, and address ethical concerns, including bias, fairness, and misinformation [55,56]. Identifying and addressing these issues through interactions is vital for the successful development and deployment of LLMs. \u2022 Evaluation: Evaluating the efficacy of extensive LLMs is crucial to gauge their proficiency across diverse tasks [57]. The metrics used to evaluate performance varies depending on the task but typically include accuracy, precision, recall, F1 score, and perplexity. Similar to general ML or NLP methods, it is imperative to conduct comprehensive evaluations to comprehend the constraints, prejudices, and potential hazards that come with extensive LLMs.\n\nIt is crucial to consider these when discussing LLMs. Each factor has its own intricacies and ongoing research aimed at overcoming obstacles and enhancing the capabilities of these models. Below, we",
            "score": 0.37056401965256297,
            "section_title": "Characterizing LLMs",
            "char_start_offset": 18063,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "149029614"
                },
                {
                    "start": 1797,
                    "end": 1800,
                    "matchedPaperCorpusId": "256655912"
                },
                {
                    "start": 2042,
                    "end": 2046,
                    "matchedPaperCorpusId": "258157875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359375
        },
        {
            "corpus_id": "272690054",
            "title": "Active Learning to Guide Labeling Efforts for Question Difficulty Estimation",
            "text": "Question Difficulty Estimation (QDE), also known as question calibration, is a regression task that estimates a question's difficulty directly from the question and answers' text. It is a crucial task in personalized support tools like computerized adaptive testing (Van der Linden and Glas, 2000), which tailors questions to a student's skill level. If the questions are too easy or too difficult, the student might lose motivation, negatively affecting their learning outcome (Wang et al., 2014). \n\nTraditionally, QDE has been performed with manual calibration (Attali et al., 2014) and pretesting (Lane et al., 2016), which are time-consuming and expensive. Recent studies aim to address these limitations by leveraging natural language processing (NLP) techniques. The NLP approaches train machine learning models to estimate question difficulty from its text. Once trained, the models can quickly calibrate unseen questions, reducing the need for pretesting and manual calibration. \n\nSupervised techniques dominate QDE with state-of-the-art results (Zhou and Tao, 2020;Benedetto et al., 2021) by fine-tuning the publicly available pre-trained models BERT (Devlin et al., 2018) and DistilBERT (Sanh et al., 2019). However, fine-tuning often requires a large labeled dataset containing tens of thousands of calibrated questions, almost impossible to collect for individual course instructors developing QDE tools on their exam data. An isolated study (Loginova et al., 2021) has delved into an unsupervised approach, relying solely on additional pre-training and evaluating pairwise difficulty. Although this approach is helpful, its performance cannot be directly compared to supervised methods and is more computationally expensive in practical implementations. \n\nIn this work, we explore active learning (AL) (Settles, 2009) for QDE, a data-efficient supervised approach aiming to minimize the labeling work for human annotators while matching the performance of state-of-the-art models. AL operates by iteratively training a model on an increasingly growing labeled subset by acquiring labels from an expert only for the most informative unlabeled data points.",
            "score": 0.37006002760721984,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 497,
                    "matchedPaperCorpusId": "31281"
                },
                {
                    "start": 1054,
                    "end": 1074,
                    "matchedPaperCorpusId": "227122854"
                },
                {
                    "start": 1074,
                    "end": 1097,
                    "matchedPaperCorpusId": "233365134"
                },
                {
                    "start": 1454,
                    "end": 1477,
                    "matchedPaperCorpusId": "244007367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "231979234",
            "title": "Dynamic Curriculum Learning via Data Parameters for Noise Robust Keyword Spotting",
            "text": "Acoustic modeling is essential for speech applications, such as keyword spotting and automatic speech recognition, e.g. [1][2][3][4][5][6][7][8]. Noise robustness of an acoustic model is one of critical points for successful acoustic modeling particularly in far field speech scenarios [9][10][11][12]. Various approaches have been proposed, including front-end speech enhancement [13][14][15][16][17] and data augmentation for acoustic modeling [18][19][20]. One promising approach for improving the noise robustness is data augmentation using noise signals, where the noise signals are added to clean utterances in training data. This data augmentation, also called multicondition training, artificially introduces difficult training samples, which typically leads the acoustic model to be more robust against noise. However, it is unclear what degree of difficulty would help to generalize the model the most, so it is difficult to understand how to ob-tain the maximum benefit from the multicondition training. Although the augmented training data have various degrees of difficulty for acoustic modeling, e.g. high/low SNRs, and easy/hard phone states for classification, it is difficult to design an efficient training curriculum exploiting the variability of the augmented training data. \n\nRecently, data parameters were proposed for curriculum learning, and their effectiveness was demonstrated on image classification [21]. Weight parameters are introduced for target classes and instances, i.e. image samples, in training data. The data parameters are used to scale logits depending on target classes and instances, and are optimized with the model parameters. By scaling the logits, we can control the contribution of each class/instance to the gradients. Optimizing the data parameters during training enables class-level and/or instance-level dynamic curriculum learning by dynamically updating the scaling factors. \n\nThe main contributions of this paper are: 1) an application of the data parameter to acoustic modeling for keyword spotting, and 2) a combination of the data parameter approach with a data augmentation technique to yield further performance improvement.",
            "score": 0.3699408906609444,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "1816414"
                },
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "58623492"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "13816461"
                },
                {
                    "start": 138,
                    "end": 141,
                    "matchedPaperCorpusId": "206602362"
                },
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "206742954"
                },
                {
                    "start": 286,
                    "end": 289,
                    "matchedPaperCorpusId": "14411577"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "207834305"
                },
                {
                    "start": 381,
                    "end": 385,
                    "matchedPaperCorpusId": "6865608"
                },
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "206742483"
                },
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "53582361"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "10310847"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "206602807"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "3355461"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "209438234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54638671875
        },
        {
            "corpus_id": "232233485",
            "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
            "text": "Effective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect  Table 2: Gradual curriculum augmentation with three schedules. Curriculum: temperature \u03c4 increases. Control: \u03c4 is randomly selected every fifty updates. Anti: decreasing \u03c4 . Results are shown for ten seeds. a greater improvement from curriculum learning. Figure 4 compares standard and two-stage curriculum augmentation for various temperatures, with results averaged over all four datasets. At low temperature, augmented examples remained pretty similar to original examples, and so the range of difficulty in examples was small and therefore curriculum learning showed little improvement. At higher temperatures, however, augmented examples became quite different from original examples, and so the range of difficulty in examples was much larger and therefore curriculum data augmentation improved over standard augmentation more. Whereas Wei and Zou (2019) recommend \u03c4 \u2208 {0.05, 0.1}, our curriculum framework liberates us to use much larger \u03c4 and maintain relatively robust improvements even at \u03c4 \u2208 {0.4, 0.5} when standard augmentation is no longer useful.",
            "score": 0.3690609843505156,
            "section_title": "Ablation: Augmentation Temperature",
            "char_start_offset": 12689,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "276250237",
            "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
            "text": "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;Chen et al., 2023;Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. Chen et al. (2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values. Furthermore, we demonstrate that combining domain-level methods such as DoReMi (Xie et al., 2023) or DoGE (Fan et al., 2023) with our instance-level reweighting methods results in improved performance across multiple domains. Instance-level reweighting has been used in post-training settings of LLMs (Chen et al., 2024;Jiang et al., 2024). Jiang et al. (2024) boost the self-improvement abilities of LLMs by employing sample reweighting to filter out self-generated data that have correct answers but exhibit high distribution shifts. Chen et al. (2024) reweight individual samples during continual training/instruction-tuning to focus on medium-loss samples. In contrast, our work systematically studies the effects of various sample-level, loss-based reweighting strategies on the efficiency and effectiveness of LLMs pretraining. The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE.",
            "score": 0.3690032221231281,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1930
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 106,
                    "matchedPaperCorpusId": "260203057"
                },
                {
                    "start": 464,
                    "end": 482,
                    "matchedPaperCorpusId": "260203057"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73046875
        },
        {
            "corpus_id": "273901292",
            "title": "McGill NLP Group Submission to the MRL 2024 Shared Task: Ensembling Enhances Effectiveness of Multilingual Small LMs",
            "text": "For instance, consider the passage: \"Tom went to the supermarket and bought two apples.\" If the question is \"How many apples did Tom buy?\" and the four options are \"A. 1\", \"B. 2\", \"C. 3\", and \"D. 4\", the system should return \"B\". \n\nCurriculum Learning Curriculum learning (Bengio et al., 2009) is a machine learning strategy that gradually introduces a model to progressively more challenging data pieces over multiple training iterations. This method can often produce better results compared to using a randomly shuffled training set. This approach is effective in the sense that, the model begins by learning general concepts through simpler examples, and then incrementally incorporates more detailed and complex information as more difficult examples are introduced. For our systems, we define \"difficulty\" by the length of the input text, where longer text equates to greater complexity and comes later in the epoch, as shown in Figure 1. Since curriculum learning is a paradigm that focuses solely on the selection and ordering of training data, it can be integrated with various other machine learning techniques, like Interleaving Multilingual Data Pieces which we will introduce later in this section. \n\nKnowledge Transfer Knowledge transfer in multilingual LLMs refers to the model's ability to leverage information, patterns, or representations learned in one language to enhance its performance or understanding in another. This happens because multilingual LLMs develop shared representations of concepts that can be applied across different languages. To facilitate the knowledge transfer for our base models, we fine-tuned the base models on diverse multilingual data. This includes a relatively small amount of data for the target languages, additional data for languages closely related to the target languages, and a large amount of data from high-resource languages like English. \n\nInterleaving Multilingual Data Pieces Interleaving Multilingual Data Pieces is a machine learning technique used to train multilingual models by interleaving data from various languages during training. This approach promotes cross-lingual knowledge transfer by encouraging the model to develop shared linguistic representations and structures, which improves its ability to generalize across languages. It is especially effective in crosslingual information retrieval scenarios, allowing the model to utilize common features across languages and enhance performance in low-resource language settings.",
            "score": 0.36867724998255,
            "section_title": "Background",
            "char_start_offset": 5382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2305
                },
                {
                    "start": 2306,
                    "end": 2503
                }
            ],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 293,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "265506572",
            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
            "text": "The datasets with transcribed speech had the lowest average sentence difficulty scores. Even within transcribed speech, datasets with informal speech (such as child directed speech and subtitles) had lower average difficulty scores than datasets with 4 SAP Benchmark uses Bayesian mixed effects models. We use linear mixed effects models because they are less resource intensive to fit and yield nearly identical model estimates. more formal speech (such as BNC). Additionally, as expected the proportion of transcribed speech steadily decreased over time, as the proportion of written text increased. By the last \"epoch\", the distribution of datasets was very similar to the true distribution (see Figure 2), suggesting that the crossreview method we used as our difficulty-measurer was effective, as was the root-10 training scheduler. \n\nAgreement between LSTM teachers For any given sentence, there was a lot of variance in the surprisal estimates across the teachers: the average standard deviation was 113 bits of surprisal; the mean Spearman rank correlation between any two pairs of teachers was only 0.0009. This highlights the importance of averaging the surprisal estimates across different teachers to avoid over-fitting to idiosyncrasies of any particular teacher model. \n\nOther difficulty measures Figure 7 plots the correlation between our difficulty measure computed using the cross-review method and two other simpler difficulty measures: average unigram frequency of the words in a sentence and sentence length. Our difficulty measure is moderately correlated with unigram frequency (R = 0.27, p < 0.0001) and highly correlated with sentence length (R = 0.89, p < 0.0001). We also predicted our difficulty measure as a function of unigram frequency and sentence length in a linear regression model and found that unigram frequency explains variance in our difficulty measure over and above sentence length, and together they explain most of the variance in the difficulty measure (adjusted R-squared = 0.93). This suggests that for the specific BabyLM datasets, using cross-review, while effective, might not be necessary: using faster-tocompute measures such as sentence length would have likely resulted in a comparable curriculum.",
            "score": 0.3685763966626367,
            "section_title": "What curriculum was learned?",
            "char_start_offset": 16142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2250
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5595703125
        },
        {
            "corpus_id": "269990215",
            "title": "Experimental Design of Extractive Question-Answering Systems: Influence of Error Scores and Answer Length",
            "text": "Experimental design, an indispensable cornerstone of scientific research, traces its origins to Sir Ronald A. Fisher's pioneering work in the early 20th century (Fisher, 1935).While the basic principles of conducting experiments have always been integral to science, it was Fisher who introduced rigorous statistical methodologies.Today, the design of experiments remains a dynamic field, continually refining its techniques in the face of ever-evolving technologies.In this paper, we study the experimental design of language models for EQA systems.\n\nWhile the importance of experimental design in the development and assessment of general natural language processing (NLP) systems is undeniable (Sidorov & Sidorov, 2019), so far, it is sorely underappreciated.This is also the case for EQA systems.Instead, the literature is dominated by publications that prioritize the introduction of new methods deemed to be the 'best.'In this paper, we diverge from the conventional path of proposing a novel method or selecting the superior model from a list of candidates.In contrast, we delve into a critical examination of key factors that influence the evaluation of a EQA system's performance.This will allow to establish guidelines for the design of EQA system experiments.\n\nTo be specific, it is common for publications to claim that one EQA system shows a performance improvement over another by a few percentage points.However, since performance measures are random variables, it is necessary to question whether these reported improvements are genuine or merely the result of random fluctuations, often referred to as \"noise\".Regrettably, many studies do not provide sufficient detail, which leads to problems in reproducibility.For instance, using the \"exact match\" score as a performance score is ambiguous or potentially misleading unless the post-processing steps involved are clearly described.Moreover, employing datasets with \"similar\" but unspecified answer length characteristics can yield inconsistent results.In this paper, we examine the effects of these two factors-the \"exact match\" scores and the answer length-on the evaluation of EQA systems' performance.\n\nAs a result from our investigations, we aim to answer the following research questions.\n\n1. What influence does the answer length of the training and test data have on the evaluation of a EQA system?",
            "score": 0.368227593752909,
            "section_title": "Motivation and Research Question",
            "char_start_offset": 6089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 550
                },
                {
                    "start": 552,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 925
                },
                {
                    "start": 925,
                    "end": 1064
                },
                {
                    "start": 1064,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1270
                },
                {
                    "start": 1272,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1627
                },
                {
                    "start": 1627,
                    "end": 1730
                },
                {
                    "start": 1730,
                    "end": 1900
                },
                {
                    "start": 1900,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2173
                },
                {
                    "start": 2175,
                    "end": 2262
                },
                {
                    "start": 2264,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18701171875
        },
        {
            "corpus_id": "235097217",
            "title": "Training Language Models under Resource Constraints for Adversarial Advertisement Detection",
            "text": "We ablate the effects of curriculum learning based on increasing difficulty using models trained in two control conditions. (a) Anti-curriculum learning (B_T L ACL ) using scoring function f = \u2212f where harder samples are fed first and (b) random curriculum (B_T L RCL ) where scoring function randomly scores the training samples. As seen from the Table 3 anti-curriculum and random curriculum are not as effective as the curriculum of increasing hardness. Further, random scoring function results in significant degradation of performance when compared to approaches employing a curriculum. Similar trends are observed for respective models trained in FR as well. \n\nWe further conduct ablations to rule out any other factors contributing to the gain in recall from curriculum based on lexical similarity. We perform two other experiments where we train the model in similar manner but feed the languages in reverse lexical similarity order(B_M L REV LEX ) and random order (B_M L RAN D LEX ). However, in both the experiments we feed the target language at the end to minimise domain shift. We see that the model trained in the lexical similarity order beats the performance of the other two models in Table 3. We validate statistical significance of gains from both lexical and hardness curricula using the Mc-Nemar's Test (Dietterich, 1998;McNemar, 1947) (Raschka, 2018). The gains through both curriculum are statistically significant as p-value is < 0.05 for both DE and FR.",
            "score": 0.36751621135000684,
            "section_title": "Ablations",
            "char_start_offset": 19528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1479
                }
            ],
            "ref_mentions": [
                {
                    "start": 1343,
                    "end": 1356,
                    "matchedPaperCorpusId": "46226024"
                },
                {
                    "start": 1358,
                    "end": 1373,
                    "matchedPaperCorpusId": "53237711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373291015625
        },
        {
            "corpus_id": "273346777",
            "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
            "text": "The findings from our experiments reveal three main points: \n\n\u2022 Most self-scoring data selection techniques do not significantly outperform random selection on large-scale datasets. Even though these self-scoring methods can achieve significant gains on small-scale datasets, their effectiveness will be greatly reduced when the data size increases and the data sources become complex. \u2022 Data diversity holds more significance than data quality during the SFT phase. Data quality-based selection methods are more effective than data diversity-based methods when dealing with a small-scale dataset from a single source. However, when tackling multisource data, only considering data quality is far from enough. \u2022 Through a comparative empirical analysis of two IT datasets, we find that it is useful to utilize token length as a criterion to conduct data filtering, yielding stable and efficient results for SFT when dealing with large-scale IT data. Previous work (Liu et al., 2023) has demonstrated the benefit of long texts training for models on subjective evaluation tasks such as MTbench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023c), we have further confirmed the positive effect of long texts training on objective evaluation tasks, such as Big-Bench-Hard (Suzgun et al., 2022). While utilizing token length in SFT may not yield optimal outcomes on every language model, it is highly beneficial for applying it in training with long texts, especially on a relatively weak BASE language model, like Llama3-8B. \n\nALPAGASUS model that used ChatGPT to evaluate each instruction and then selected various data based on a certain threshold. Du et al. (2023) suggested a model-oriented instruction selection approach that not only considered the quality and coverage of instruction data but also incorporated the necessity of instructions according to the capabilities of specific LLMs. Liu et al. (2023) introduced DEITA, it used ChatGPT to iteratively enhance the complexity or quality of each data sample across relevant dimensions and then requested ChatGPT to evaluate these samples for their complexity or quality. These models exceed the performance of the basic foundation models trained on complete datasets. However, they heavily depend on high-performing external LLMs to score data. \n\nSelf-scoring Method.",
            "score": 0.3673947433776696,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 62,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1524
                },
                {
                    "start": 1527,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2303
                },
                {
                    "start": 2306,
                    "end": 2326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1339111328125
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "We presented a set of experiments using training dynamics (Swayamdipta et al., 2020) as difficulty metrics for CL on several NLU tasks. Differently from existing works, we focus our evaluation on indistribution, out-of-distribution and zero-shot crosslingual transfer data by testing existing discrete and continuous schedulers as well as modifications of those in a transfer-teacher curriculum setting. \n\nOur findings offer evidence that simply reordering the training examples in a meaningful way has mostly an impact on zero-shot cross-lingual transfer and OOD data, with no improvement on ID. Our proposed Continuous scheduler with confidence and variability sampling provided a boost up to 8.5% on a challenging OOD dataset over prior work. Comparing our proposed application of training dynamics to other transfer-teacher curriculum methods that are using more than 1 teacher model, we observed greater speedups, improved performance and more stable training. In particular, we found that task-agnostic metrics do not perform better than task-specific ones on ID and ZS data but can offer good performance on OOD settings. \n\nOverall, our experiments suggest there is no curriculum outperforming others by a large margin which is consistent with findings in Zhang et al. (2018) and that task-agnostic metrics should not be rejected when transferring to challenging new domains. However we show that training dynamics are potentially better difficulty metrics for CL in both monolingual and multilingual models even with a limited budget. \n\nAlthough in this study we focused on using CL on a single language only (English), a reasonable extension is considering training data from other languages as well and investigate instance difficulties based on language or following efforts towards continual learning (Parisi et al., 2019). Finally, using TD in a dynamic rather than a static curriculum is another interesting direction that can potentially offer further training speedups as well as ways to improve model pre-training (Nagatsuka et al., 2021;Li et al., 2021).",
            "score": 0.3669511171681124,
            "section_title": "Conclusion",
            "char_start_offset": 23268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 84,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1813,
                    "end": 1834,
                    "matchedPaperCorpusId": "73497737"
                },
                {
                    "start": 2031,
                    "end": 2055,
                    "matchedPaperCorpusId": "244048238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "273963552",
            "title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs",
            "text": "We then investigate the performance of large language models (LLMs) for rewriting and RAG-augmented data. This investigation aims to understand how these methods can enhance the quality and diversity of the training data, potentially leading to further improvements in model performance. \n\n\u2022 CoT+RAG: This method involves using a larger Qwen 2.5 72B model with RAG to provide more information and CoT prompt instructions to sequentially generate answers. The prompts used for this process are detailed in Table 3.3, and no parameter tuning was involved. \u2022 Rewrite Data: In this approach, the Landmark Data is rewritten using a Qwen 2.5 72B language model, as detailed in Table 3.2. This expands the dataset to 3,000 samples from Reuters and 6,000 from Web of Science (WoS). \u2022 RAG Augmented Data: In this approach, each document is augmented using a Qwen 2.5 72B language model, as detailed in Table 3.4. This expands the dataset to 15,473 samples from Reuters and 62,886 from WoS. \n\n\u2022 Combined Data: This method combines the WordNet, LLM rewriting, and RAG augmentation techniques by merging the training data together. \n\n\u2022 Original Data: For comparison only, we also list the performance of the model when using all training data with their true labels. The original datasets contain From the results presented in Table 9, it is clear that even with a larger model like Qwen 2.5 72B employed for CoT+RAG, its performance does not match that of a much smaller, parameter fine-tuned model such as Qwen 2.5 0.5B, which outperforms the larger model across multiple metrics. Specifically, for the Reuters dataset, the RAG Augmented Data method significantly outperformed other data augmentation techniques. Notably, despite using only 300 landmark labels, the RAG Augmented Data achieved a part match rate that was only 1.52% lower than the result obtained using the full set of 5,394 training data labels, which is a remarkable achievement. On the other hand, it is worth noting that when using only RAG augmented data, the accuracy for \"All Match\" is comparatively lower.",
            "score": 0.36647189414196285,
            "section_title": "LLMs Rewrite and RAG augmentation",
            "char_start_offset": 53519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2069
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08416748046875
        },
        {
            "corpus_id": "272421662",
            "title": "A Systematic Review of Synthetic Data Generation Techniques Using Generative AI",
            "text": "For example, XLNet's ability to attain higher accuracy in tasks such as question answering and sentiment analysis illustrates its competence in dealing with complicated linguistic relationships, whereas its permutation-based training objective reduces pretrain finetune differences. By resolving both autoregressive and autoencoding pretraining limitations, XLNet establishes a new benchmark in the field, demonstrating the significance of advanced training techniques and architectural integration in improving NLP model performance. \n\nA study introduced SuperGen, a novel approach for zero-shot learning in natural language understanding (NLU) tasks using pre-trained language models (PLMs) [62]. The method consists of a unidirectional PLM that generates class-conditioned texts guided by prompts which can be used as training data for fine-tuning a bidirectional PLM. This is achieved through regularization techniques, such as label smoothing and temporal grouping to enhance the generalization and stability. SuperGen outperformed the zero-shot prompting methods and was approximately as effective as some few-shot approaches over the seven GLUE benchmark classification tasks. However, limitations include difficulties in hyperparameter tuning owing to the lack of task-specific samples and potential challenges in generating high-quality training data for tasks with distributions that differ from pretraining data. \n\nA study led by Google investigated the capabilities of large language models (LLMs) in synthesizing short Python programs from natural language descriptions, using the MBPP and MathQA-Python benchmarks [63]. Models with parameters ranging from 244M to 137B were evaluated in few-shot and fine-tuning settings. The results demonstrated that the performance was log-linearly dependent on the size of the model, with the largest model achieving 59.6% accuracy with MBPP and 83.8% accuracy with MathQA-Python after fine-tuning. This study also examined how human feedback can help reduce the error rates and limitations of these models in understanding programming semantics. A notable finding is that while LLMs can generate syntactically correct code, they struggle with semantic comprehension, indicating the need for further research in multi-modal models and grounding outside the program synthesis domain.",
            "score": 0.36615727229502276,
            "section_title": "Devlin, J. [66]",
            "char_start_offset": 94072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2333
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2474365234375
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "Pre-trained language models (PLMs) are successful in many downstream tasks regarding performance (Wang et al., 2019). But for a reliable deployment in practice, the calibration performance should also be carefully examined (Vaicenavicius et al., 2019). Well-calibrated models assign appropriate confidence scores that truly reflect the outcome probability of their predictions. However, the confidence scores of existing deep neural networks cannot serve as reliable estimates of their uncertainty (Guo et al., 2017), and a deep understanding of PLMs calibration is lacking.\n\nIn this paper, we give a systematical analysis of PLMs calibration. We consider two questions about PLMs calibration: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? We first introduce the metrics we adopt for calibration performance evaluation. The most widely used calibration metric ECE (Naeini et al., 2015) is considered. It measures the difference between confidence and accuracy by portioning samples into various confidence zones. To give a more comprehensive and practical calibration evaluation, we provide an application-driven perspective, describing two undesirable situations in practice: (1) For the first question, we consider the influence of six factors that have influence on PLMs' calibration performance, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. Some of them are overlooked in previous empirical studies (Snoek et al., 2019;Nixon et al., 2019;Minderer et al., 2021). We motivate to conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training through manipulating controlling variables. We empirically observe a consistent change in calibration performance across six factors. All six factors influence PLMs' fitness on the training distribution. This results in two states of PLMs considering calibration performance, namely under-fitted and over-fitted states (see Fig. 1). In the under-fitted state, PLMs' performance and confidence increase at different speeds when more fitted on the training distribution. In the over-fitting state, PLMs' confidence continues to",
            "score": 0.36590715052970635,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 116,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 223,
                    "end": 251,
                    "matchedPaperCorpusId": "67749814"
                },
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1566,
                    "end": 1586,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1586,
                    "end": 1605,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1605,
                    "end": 1627,
                    "matchedPaperCorpusId": "235435823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "270887630",
            "title": "Reinforcement Learning from Experience Feedback: Application to Economic Policy",
            "text": "The predominant training methodology for large language models is self-supervised pretraining on massive corpora of text data. Pretraining results in capable language models before any downstream application. \n\nTo increase model capacity further, approaches like mixture of experts (MoE) can be used during the training. This trains a model with N expert sub-models, each with parameters \u03b8n. A gating network g determines weighting   for each expert: \n\nIn this, (|) is the combined prediction,   () is the weight for the expert determined by a gating network, and (|,   ) is the prediction of the expert  \u210e with parameters. The gating network enables routing different inputs or prompts to the most relevant expert. \n\nOther approaches include multitask learning, with shared parameters  MT across tasks and task-specific   parameters: \n\nHere, the equation \u03a3    ( MT ,   ) represents a loss function in multitask learning.  is the combined loss,   is the loss for task ,  MT are shared parameters,   are task-specific parameters, and   are the task weights. \n\nOther techniques used to enhance large language model training include knowledge distillation for transferring knowledge from a larger teacher model to a smaller student model, self-distillation for transferring knowledge across time within a model, sparse training for updating only a subset of parameters each batch, progressive module stacking to gradually add layers during training, curriculum learning for organizing training data from simple to complex, meta-learning to learn optimal initialization and hyperparameters, and multi-agent learning where multiple models learn collaboratively.",
            "score": 0.36582473783943564,
            "section_title": "Training",
            "char_start_offset": 12832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1656
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451416015625
        },
        {
            "corpus_id": "254685579",
            "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
            "text": "Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model. CCM has two key differences from existing curriculum learning approaches to effectively reflect the nature of MLM. First, we introduce a novel curriculum that evaluates the MLM difficulty of each token based on a carefully-designed linguistic difficulty criterion. Second, we construct a curriculum that masks easy words and phrases first and gradually masks related ones to the previously masked ones based on a knowledge graph. Experimental results show that CCM significantly improves pre-training efficiency. Specifically, the model trained with CCM shows comparative performance with the original BERT on the General Language Understanding Evaluation benchmark at half of the training cost.",
            "score": 0.36557481361097766,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "254974529",
            "title": "Confidence-Aware Paced-Curriculum Learning by Label Smoothing for Surgical Scene Understanding",
            "text": "Inspired by human and animal learning principles, curriculum learning introduces samples from easier to the complex during training. It is observed that the learning process from easy to difficult tasks helps achieve better performance by avoiding the local minima and obtaining better generalization results [22], [23]. Curriculum learning and self-paced learning also improve the robustness and reliability of noisy samples [9], [11]. There are several ways to measure the sample difficulty in designing this learning technique. Most of the works adopt the confidence score to sort the samples. In this way, first, feed the high confidence/easy samples and subsequently introduce low confidence/difficult ones into the learning. Previous studies also utilized handcrafted features [15], multi-raters disagreement [16] and similarity scores [17]. A cutting-edge strategy [24] is introduced to use curricula to pinpoint the fundamentals of how a system learns. Domainaware Curriculum Learning [25] identifies curriculum learning as one crucial element that can reduce the multiple domain shifts in the multi-target domain adaptation. It adapts to the easier target domains first, then moves on to the more difficult ones. A curriculum based on human visual acuity [26] lessens the texture biases in models for gallbladder cancer. Most recently, the Curriculum By Smoothing (CBS) [27] employs the Gaussian filter of feature maps from a higher variance to lower across the training epochs. The higher variance smoothes the feature map heavily and limits the model to learn less information at the beginning. However, designing a curriculum by smoothing label probability is still unexplored in this domain.",
            "score": 0.3654574501471017,
            "section_title": "A. Curriculum learning / self-paced learning",
            "char_start_offset": 5014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 313,
                    "matchedPaperCorpusId": "14892153"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "2603360"
                },
                {
                    "start": 426,
                    "end": 429,
                    "matchedPaperCorpusId": "58981386"
                },
                {
                    "start": 431,
                    "end": 435,
                    "matchedPaperCorpusId": "33880888"
                },
                {
                    "start": 783,
                    "end": 787,
                    "matchedPaperCorpusId": "244119160"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "221995570"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "251647193"
                },
                {
                    "start": 993,
                    "end": 997,
                    "matchedPaperCorpusId": "233004251"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "248376979"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "220301592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "274776821",
            "title": "Do Tutors Learn from Equity Training and Can Generative AI Assess It?",
            "text": "Early ASAG approaches for assessing open-ended responses relied on traditional machine learning techniques, such as feature extraction and bag-of-words models, which provided easily interpretable results [18,27,39]. However, these early models struggled with domain shifts, where subtle differences in assessment tasks significantly impaired their performance [17]. More recently, studies have turned to deep learning models for automated assessment of open-ended responses [9,33,40]. For example, [9] explored ASAG using Sentence-BERT (SBERT) to measure textual similarity within learner response grading. Although SBERT showed promise for generalization, it encountered difficulties with unseen questions, revealing the need for models capable of deeper contextual understanding and adaptability, particularly in more complex domains such as equity training. \n\nRecent advances in generative AI, particularly large language models (LLMs) such as GPT-4, have revolutionized ASAG by enabling models trained on extensive datasets to interpret and assess nuanced textual responses [6], with applications in tutor training [16,20,26]. LLMs offer flexibility, as they can be fine-tuned for specific educational tasks or used in their pre-trained form. Studies show that, with effective prompting, LLMs can capture the subtleties in learner responses, improving grading efficiency and scalability [17]. Nevertheless, challenges persist, as LLMs operate as \"black-box\" models, making their outputs difficult to interpret [6]. Furthermore, LLMs lack consistent knowledge of the pedagogy and content and are prone to hallucinations, generating confidently inaccurate or nonsensical information [39]. There remains an urgent need to explore how LLMs can be trained to assess equity-focused tutor responses, such as recognizing when tutors advocate for underserved students or support them in addressing inequities. These advancements could significantly improve tutor training to promote equitable education.",
            "score": 0.3651864288241456,
            "section_title": "Using Generative AI to Assess Open-ended Tutor Responses",
            "char_start_offset": 12219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "269605674"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "230526602"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 1119,
                    "end": 1123,
                    "matchedPaperCorpusId": "267782884"
                },
                {
                    "start": 1126,
                    "end": 1129,
                    "matchedPaperCorpusId": "269502151"
                },
                {
                    "start": 1391,
                    "end": 1395,
                    "matchedPaperCorpusId": "269605674"
                },
                {
                    "start": 1514,
                    "end": 1517,
                    "matchedPaperCorpusId": "259360395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.156982421875
        },
        {
            "corpus_id": "270562313",
            "title": "Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning: Lessons Learned",
            "text": "Our spatio-temporal quantile curriculum learning framework primarily builds upon state-of-the-art (SOTA) models for existing spatio-temporal problems. In this section, we will detail our method by illustrating how and when to implement curriculum learning. We adapt self-paced learning (SPL) [17] from the curriculum learning branch to serve as a component within spatio-temporal models. Targeting the difficulty level assessment as performed by the  module in Algorithm 1, we utilize evaluation metrics specific to spatio-temporal challenges to further guide the adjustment of the curriculum's difficulty level. \n\nDifferent from the tasks in image classification, text recognition, and other tasks in the fields of computer vision (CV) and natural language processing (NLP), spatio-temporal forecasting problems often lack reliable prior knowledge. Therefore, traditional curriculum learning methods that rely on predefined difficulty measurer based on prior knowledge before training [35] may not be suitable for spatio-temporal prediction tasks. SPL, which assesses difficulty using data or the model itself, fits this task well. \n\nAs shown in Figure 2, SPL based on the loss function initially creates a difficulty ranking with an initial model. The model serves as its own teacher, using difficulty scores derived from this initial assessment to guide subsequent curriculum learning and training intervention. Building upon this foundation, our study introduces three curriculum learning strategies (spatial, temporal, and quantile) to extract features from different perspectives. Finally, a stacking fusion module integrates the advantages of all three strategies, enhancing the predictive model's performance.",
            "score": 0.36506722736157915,
            "section_title": "Overview",
            "char_start_offset": 9310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1717
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 986,
                    "end": 990,
                    "matchedPaperCorpusId": "232362223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497314453125
        },
        {
            "corpus_id": "277786647",
            "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients",
            "text": "Cherry LLM (Li et al., 2024e) introduces Instruction-Following Difficulty (IFD) scores-a self-guided metric for evaluating instruction difficulty without relying on additional LLMs-and uses these scores to select datasets. Motivated by Humpback (Li et al., 2023b), Selective Reflection-Tuning (Li et al., 2024a) introduces a teacher-student collaborative pipeline that is guided by the IFD score and its reverse version to select the data based on the evaluation feasibility. Du et al. (2023) and Bukharin & Zhao (2023) utilize reward models as the metric for measuring data quality and subsequently select the data. DEITA (Liu et al., 2023) employs ChatGPT to diversify datasets, then applies various data selection metrics to construct a high-quality dataset. Superfiltering (Li et al., 2024d) demonstrates that both weak and strong language models consistently assess instruction difficulty, thereby streamlining the filtering process. Instruct Mining (Cao et al.) presents a method for automatically selecting high-quality instruction-following data using natural language quality indicators. SelectIT (Liu et al., 2024) proposes an uncertainty-aware self-filtering approach that leverages an LLM's intrinsic uncertainty to select high-quality instruction-tuning data LESS (Xia et al., 2024) constructs a low-dimensional \"gradient datastore\" for candidate data, subsequently selecting examples whose gradient influence closely matches that of a limited set of target demonstration examples. Collectively, these studies focus on distinguishing high-quality data samples from lower-quality ones for effective instruction tuning.",
            "score": 0.36498851303176744,
            "section_title": "A.2 SFT and Data Selection",
            "char_start_offset": 29035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 29,
                    "matchedPaperCorpusId": "261076515"
                },
                {
                    "start": 245,
                    "end": 263,
                    "matchedPaperCorpusId": "260866107"
                },
                {
                    "start": 293,
                    "end": 310,
                    "matchedPaperCorpusId": "267682220"
                },
                {
                    "start": 777,
                    "end": 795,
                    "matchedPaperCorpusId": "267365346"
                },
                {
                    "start": 1106,
                    "end": 1124,
                    "matchedPaperCorpusId": "268032188"
                },
                {
                    "start": 1277,
                    "end": 1294,
                    "matchedPaperCorpusId": "267522839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "271865496",
            "title": "Leveraging Perceptual Scores for Dataset Pruning in Computer Vision Tasks",
            "text": "Deep learning has made tremendous progress in the past few years exploiting the scale of large training sets, among other factors. Recently data centric methods, such as training on pruned dataset [21], or using non uniform mixing strategies [24] have become standard practice for training large scale models. In these methods a score is attached to each data instance, and an instance is selected (or not) for training using the ordered scores of available instances. In this paper we focus on data pruning for computer vision tasks where a subset of the instances available is used for training with minimal loss of performance. \n\nIn previous data pruning approaches for computer vision, mostly on the classification task, scores naturally reflect the learning task at hand. That is, they are based on the distribution of input and its label(s). This implicitly pays less attention to the input itself. Also modeling this distribution accurately is expensive -usually some steps of training have to be done before the scores can be calculated [7,15,21,22]. Thus one of the key questions, raised in literature, is how early in training can the instances to be pruned identified [15]? \n\nWe start with four observations. First, the vision perception literature recognizes that images with less clutter, simpler background, iconic objects, are processed faster by the human visual system [18]. Many measures for characterising human perception of scene complexity have been proposed [10], among which the information theoretic measure of entropy is not only the simplest but also captures the clutter in an image well [18]. \n\nSecond, images with less clutter and a plain background, also lead to better labeling of pictures by young children [14]. In general child development literature shows that the nature of picture books for young children leads to different transfer learning experience. Similar studies for deep learning can yield interesting insights. \n\nThird, in natural language processing scores such as length of sentences, or word rarity, have been proposed as difficulty scores for machine translation [17]. Sentences that are shorter are easier to translate than sentences that are longer; here difficulty is a measure of only the input sentence, not of the input, output sentences. Machine translation is similar to the semantic segmentation task in computer vision.",
            "score": 0.36454515975331153,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2296
                },
                {
                    "start": 2297,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1048,
                    "end": 1051,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1051,
                    "end": 1054,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1386,
                    "end": 1390,
                    "matchedPaperCorpusId": "10847280"
                },
                {
                    "start": 1481,
                    "end": 1485,
                    "matchedPaperCorpusId": "253518465"
                },
                {
                    "start": 1616,
                    "end": 1620,
                    "matchedPaperCorpusId": "10847280"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "55786227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474609375
        },
        {
            "corpus_id": "252355060",
            "title": "CES-KD: Curriculum-based Expert Selection for Guided Knowledge Distillation",
            "text": "llation (BLKD) [7] on these different levels of curriculum. Table I shows the test accuracy of the student network trained on three different levels of difficulty with three teachers of different capacities. We see that for easy samples the student network (ResNet20) has higher accuracy when guided through distillation by the lowest capacity teacher (ResNet26) within the group of experts. As for the difficult samples, the student acquires better knowledge from the highest capacity teacher model (ResNet56), which validates our assumption in terms of quality of learning. To further study the student's learning efficiency, Figure 3 (a) shows that the student's optimization, on easy samples, converges faster when trained with the guidance of the lowest capacity teacher model (ResNet26) via knowledge distillation. Also in Figure 3 (b) we observe that on difficult samples, the student learns faster (i.e. faster convergence) from the highest capacity network (ResNet56) than from the lower capacity teachers. This comparison validates our hypothesis and motivates our technique regarding the curriculum data-model selection.  B. Methodology of our proposed method Figure 1 presents a global overview of the distillation framework CES-KD. Our method relies on two main steps: the design of the data curriculum and the selection of a single representative expert based on their expertise on a given input data (i.e. images). We provide in Figure 2 details on the distillation pipeline of our method with a back-toback comparison on current TA-based KD methods (TAKD [9] and DGKD [10]). Following the standard CL paradigm, we need to address two main questions: (1) How do we rank the dataset according to an easy-to-hard curriculum? (see subsection III-B1); (2) How do we train a student model using the ranked instances? (See subsection III-B2).\n\n1) Design of the data curriculum: In this work, we adopt the meta-network method described by Hacohen and Weinshall [17] as transfer learning-based scoring function. In particular, we consider a reference model trained on a very large dataset. Then, we fine-tune this reference model on the smaller training dataset",
            "score": 0.3643860729193906,
            "section_title": "A. Hypothesis and Motivation",
            "char_start_offset": 10710,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1571,
                    "end": 1574,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "221802641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2386474609375
        },
        {
            "corpus_id": "235097217",
            "title": "Training Language Models under Resource Constraints for Adversarial Advertisement Detection",
            "text": "In the above section we discussed augmenting data using weak signals. \n\nHere we explore how we can utilise large amounts of labeled data available in resource rich languages such as EN. We translate the ad creatives available in EN to the target language. Hence forth, this data is referred to as translated data. A trivial approach to utilise this data for tuning the model is to combine the strong and translated data and randomly sample mini-batches (B_T L RS ) from the unified set while training. Another possibility is to use the translated data to pretrain the classifier and fine-tune it with the strong data in target domain (B_T L F T ). Here, during every epoch, we initially train the model with the mini-batches sampled from the translated data followed by sampling mini-batches from strong data. This clearly has an advantage over the earlier approach as it helps model adapt to the target domain and avoid domain shift arising from the translation engine employed. \n\nWe also explore an approach leveraging curriculum learning that is agnostic of the distinction between translated and strong data for training the M-BERT model. Curriculum learning(Hacohen and Weinshall, 2019) involves using the prior knowledge of the difficulty of the training samples to sample training mini-batch. To rank the difficulty of the training sample (x i , y i ) we need a scoring function. Scoring function f : X \u2192 R is any function which scores the difficulty of a given training sample. If f (x i , y i ) > f (x j , y j ) then (x i , y i ) is more difficult than (x j , y j ). We also use a pacing function (Hacohen and Weinshall, 2019) which determines the sequence of subsets X 1 , .., X m \u2286 X of size g i from which mini-batches {B i } M i=1 are sampled. These are generally monotonically increasing functions so the likelihood of the easier samples decrease over time. \n\nIn our case, we use BOE_LIN (See Section 2.3) as our scoring function-a proxy for hardness of the sample. Samples with confident predictions by BOE_LIN for positive and negative classes are considered easy while hardness increases as the samples are closer to boundary of separation.",
            "score": 0.3642861413817694,
            "section_title": "Curriculum for leveraging resource rich domains",
            "char_start_offset": 10938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 72,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6669921875
        },
        {
            "corpus_id": "251663102",
            "title": "Human-in-the-loop machine learning: a state of the art",
            "text": "To organize a curriculum for students, teachers need to deal with two challenging tasks: \n\n\u2022 Arrange the material taking into account its complexity or difficulty, a knowledge that is not available in the training set in most of machine learning paradigms. This task is referred to as scoring function or difficulty measurer (Wang et al. 2021). \u2022 Guide the pace at which the material is presented, known as pacing function or training scheduler (Wang et al. 2021). \n\nTherefore, a general framework for curriculum design consists of these two main components. The nature of the difficulty measurer and the training scheduler lead us to two different CL categories: \n\n\u2022 Specifically, when both the difficulty measurer and the training scheduler are designed by human prior knowledge with no data-driven algorithms involved, the CL method is called predefined CL. \u2022 If any (or both) of the two components are learned by data-driven models or algorithms, then the CL method is known as automatic CL.",
            "score": 0.36428333643546773,
            "section_title": "Curriculum design",
            "char_start_offset": 74456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 91,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 995
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 342,
                    "matchedPaperCorpusId": "208248381"
                },
                {
                    "start": 445,
                    "end": 463,
                    "matchedPaperCorpusId": "208248381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24072265625
        },
        {
            "corpus_id": "245124549",
            "title": "Do Data-based Curricula Work?",
            "text": "In the last years state-of-art results in natural language processing (NLP) are often obtained with Transformer-like architectures based on the selfattention mechanism (Vaswani et al., 2017) such as BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), which could have billions of parameters. Due to many parameters, these architectures require lots of time and hardware resources to be trained.\n\nCurriculum learning (CL) is one of the popular methods to reduce training time and increase the resulting quality of the model. Inspired by the importance of adequately ordering information when teaching humans (Avrahami et al., 1997), curriculum learning increases the difficulty of training samples shown to the model over time (Elman, 1993). Previous studies have demonstrated that curriculum learning significantly impacts training time and quality in different machine learning domains, such as computer vision (Soviany, 2020) and reinforcement learning (Narvekar et al., 2020).\n\nIn NLP, some results hint that CL might be beneficial (Platanios et al., 2019;Xu et al., 2020;Kocmi and Bojar, 2017); however, these results are not as optimistic as in reinforcement learning setup.\n\nWe suggest dividing recent research in curriculum learning into two main categories: task-driven curriculum and data-driven curriculum. The idea of the task-driven curriculum was inspired by human behavior. First, the model learns how to solve a simple task, and then the difficulty is gradually increased. This type of curriculum proposed by Bengio et al. (2009) is considered to be classical, and a majority of curriculum-related results are obtained in this framework. Alternatively to the taskdriven curriculum, some curricula try to use some form of filtering or sorting of training data that could facilitate learning a model on a given task. We suggest calling these curricula data-driven and distinguishing them from the classical task-based approach.\n\nThis paper attempts to understand when datadriven curriculum learning works for transformerbased language models. Generally, data-driven curriculum learning is organized in two steps: first, estimating the complexity for the elements that comprise the dataset; second, designing a sampling strategy, thus",
            "score": 0.36389373198677744,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "266182457",
            "title": "Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
            "text": "Children can acquire language from less than 100 million words of input. Large language models are far less data-efficient: they typically require 3 or 4 orders of magnitude more data and still do not perform as well as humans on many evaluations. These intensive resource demands limit the ability of researchers to train new models and use existing models as developmentally plausible cognitive models. The BabyLM Challenge is a communal effort in which participants compete to optimize language model training on a fixed data budget. Submissions are compared on various evaluation tasks targeting grammatical ability, downstream task performance, and generalization. Participants can submit to up to three tracks with progressively looser data restrictions. From over 30 submissions, we extract concrete recommendations on how best to train data-efficient language models, and on where future efforts should (and perhaps should not) focus. The winning submissions using the LTG-BERT architecture (Samuel et al., 2023) outperformed models trained on trillions of words. Other submissions achieved strong results through training on shorter input sequences or training a student model on a pretrained teacher. Curriculum learning attempts, which accounted for a large number of submissions, were largely unsuccessful, though some showed modest improvements.",
            "score": 0.3637392311908252,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.430419921875
        },
        {
            "corpus_id": "226262184",
            "title": "Intrinsic Evaluation of Summarization Datasets",
            "text": "Open Problems and Future Directions. Our results demonstrate that a sizeable fraction of examples in most summarization datasets are low quality. However, it remains open whether modellers should simply prune these examples, manually/automatically attempt to correct them, or model them without change. We do note that research in the machine learning and learning theory communities shows that models both theoretically and empirically do substantially worse when trained using low quality examples, even when the examples are not strictly adversarially chosen (Klivans et al., 2009;Biggio et al., 2012;Koh et al., 2018). These concerns are further compounded by the evidence of Belinkov and Bisk (2018) that neural models for natural language generation are not robust to naturally noisy data.\n\nOur metrics may be repurposed to rank examples in designing curricula for curriculum learning ap-proaches (Bengio et al., 2009). Alternatively, they can serve as additional metrics for the (possibly unsupervised) evaluation of summarization systems, potentially mitigating deficiencies in standard metrics, such as ROUGE, by directly penalizing redundancy and semantic incoherence.\n\nLimitations. In this work, we restrict ourselves to single-document single-reference English language summarization datasets. While the datasets we study constitute a considerable fraction of dataset usage in the summarization community, several multi-document summarization datasets have been introduced (e.g. Fabbri et al., 2019; Antognini and Faltings, 2020) and multi-reference summarization datasets have often been argued to be desirable due to under-constrained nature of the summarization task (Kryscinski et al., 2019) and the ideal evaluation paradigm for ROUGE (Lin, 2004). Beyond English, both large summarization datasets (Nguyen and Daum\u00e9 III, 2019; Varab and Schluter, 2020) and more general language resources/technologies (Joshi et al., 2020) are less available, which may heighten the need for data quality assurance.\n\nMore broadly, the measures that we introduce are automated, and therefore non-human, judgments of the quality of summarization data. Therefore, we only",
            "score": 0.3630744380653752,
            "section_title": "Discussion",
            "char_start_offset": 24117,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09808349609375
        },
        {
            "corpus_id": "271987337",
            "title": "Evaluating the Adaptability of Large Language Models for Knowledge-aware Question and Answering",
            "text": "Google has developed a suite of LLMs optimized for different natural language tasks. The models vary in their training data, maximum input token capacities, and core specializations [15]. \n\nWhen a question is posed to the Google's LLM, it is accompanied by the user's level of understanding. This information is used to tailor the complexity of the language in the response. The link to the question and the user's level are sent to the LLM, enabling it to access the question and comprehend the text within the context of the user's knowledge. Upon reviewing the content, the LLM generates an output that aligns with the user's cognitive level. Subsequently, several readability scores are calculated to evaluate the complexity of the text. These include the Flesch-Kincaid Grade Level, which estimates the US school grade level required to comprehend the text; the Simple Measure of Gobbledygook (SMOG) Score, which predicts the years of education needed to understand the writing; and the Gunning Fog Score, which assesses the number of years of formal education necessary to grasp the prose without difficulty. Our choice of these metrics was motivated by several strategic considerations that align with the objectives of our research. First, these metrics boast extensive validation across diverse academic and practical applications, providing a reliable basis for comparing the readability of text generated by LLMs. Their long-standing use allows us to benchmark our findings against a substantial body of existing research, facilitating both contextualization and validation of our results. Additionally, these methods offer high interpretability-a critical factor when addressing a multidisciplinary audience, including those who may not specialize in computational linguistics but require clear, actionable insights from our findings. Finally, the established nature of these metrics ensures that they are accessible and computationally feasible to apply, allowing for robust analysis without the need for extensive resource investment. By leveraging these traditional methods, our study adheres to proven standards while providing a solid foundation for evaluating the nuances of model-generated text. Each of these scores offers a different perspective on the text's accessibility, ensuring that the language model's output is appropriate for the user's level of understanding.",
            "score": 0.3630132901203858,
            "section_title": "III. Methodology",
            "char_start_offset": 17128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2214
                },
                {
                    "start": 2215,
                    "end": 2391
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "30151602"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272216796875
        },
        {
            "corpus_id": "267657703",
            "title": "The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes",
            "text": "Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models. In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it sug-gests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches. We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analy-sis of memorization, mislabeled data detection, and tracing behavior in language models.",
            "score": 0.3629034642810531,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3662109375
        },
        {
            "corpus_id": "264439064",
            "title": "Irreducible Curriculum for Language Model Pretraining",
            "text": "Curriculum Learning for Language Modeling. As a popular concept in education and cognitive science, there have been plenty of attempts to apply curriculum learning on training language models, by progressively expose the neural learner with samples with ascending difficulty level [Campos, 2021, Nagatsuka et al., 2021, Li et al., 2022a]. However, most of the effective methods increase the difficulty by varying the context length of sequences, without focusing on the syntactic nor semantic properties. Li et al. [2022a] empirically proves that with a incrementally increase context length, the pretraining of language model could be more stable, reducing the number of loss spikes. Similarly, Nagatsuka et al. [2021] shows that feed training samples with increasing block size could improve the performance when pretraining BERT [Devlin et al., 2019], an encoder-only model with masked language modeling objective. Campos [2021] has conducted extensive experiments towards using linguistic-featured curriculum learning for language model pretraining. However, the conclusion shows their curriculum fails to effectively help with validation perplexity nor downstream performance. \n\nRecently, Xie et al. [2023] has introduced a domain reweighting framework by sampling from domains with various probability. They propose to use two small-scale auxiliary models to determine the domain weights through a group distributional robust optimization game. However, all the samples within one domain would share the same sampling weights, which fails to distinguish the instance-wise attributes inside the domain barrier. How to combine the domain-level reweighting and the in-domain curriculum could also be a promising direction for future exploration.",
            "score": 0.36265754981766474,
            "section_title": "B Related Work",
            "char_start_offset": 13100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1748
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 318,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 696,
                    "end": 719,
                    "matchedPaperCorpusId": "244048238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "250451514",
            "title": "A Data-Based Perspective on Transfer Learning",
            "text": "We have presented in the main paper how to compute the influences of every class in the source dataset on the predictions of the model on the target dataset. In that setup, we demonstrated multiple capabilities of our framework, such as improving overall transfer performance, detecting particular subpopulations in the target dataset, etc. Given the wide range of capabilities class-based influences provide, one natural question that arises: Can we compute the influence of every source datapoint on the predictions of the model on the target dataset? Furthermore, what do these influences tell us about the transfer learning process? Mathematically, the computation of example-based influences (i.e., the influence of every source datapoint) is very similar to the computation of class-based influences. Specifically, to compute example-based influences, we start by training a large number of models on different subsets of the source datapoints (as opposed to source classes for class-based influences). Next, we estimate the influence value of a source datapoint s on a target example t as the expected difference in the transfer model's performance on example t when datapoint s was either included or excluded from the source dataset: \n\n(2) where f (t; S) is the softmax output of a model trained on a subset S of the source dataset. Similar to classbased influences, a positive (resp. negative) influence value indicates that including the source datapoint s improves (resp. hurts) the model's performance on the target example t. \n\nWhile example-based influences provide some insights about the transfer process, we found that-in this regime-datamodels [IPE+22] provide cleaner results and better insights. Generally, influences and datamodels measure similar properties: the effect of the source datapoints on the target datapoints. For a particular target datapoint t, we measure the effect of every source datapoint s with datamodels by solving a regression problem. Specifically, we train a large number of models on different subsets of the source dataset.",
            "score": 0.36254157436292256,
            "section_title": "D Adapting our Framework to Compute the Effect of Every Source Datapoint on Transfer Learning",
            "char_start_offset": 23128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 1663,
                    "end": 1671,
                    "matchedPaperCorpusId": "246442081"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07330322265625
        },
        {
            "corpus_id": "233289916",
            "title": "Optimal Size-Performance Tradeoffs: Weighing PoS Tagger Models",
            "text": "Improvement in machine learning-based NLP performance are often presented with bigger models and more complex code. This presents a trade-off: better scores come at the cost of larger tools; bigger models tend to require more during training and inference time. We present multiple methods for measuring the size of a model, and for comparing this with the model's performance.In a case study over part-of-speech tagging, we then apply these techniques to taggers for eight languages and present a novel analysis identifying which taggers are size-performance optimal. Results indicate that some classical taggers place on the size-performance skyline across languages. Further, although the deep models have highest performance for multiple scores, it is often not the most complex of these that reach peak performance.",
            "score": 0.3625320392046954,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348876953125
        },
        {
            "corpus_id": "272398203",
            "title": "CoRA: Optimizing Low-Rank Adaptation with Common Subspace of Large Language Models",
            "text": "Reducing the parameter count necessary for model training could decrease computational expenses in maintaining equivalent training outcomes. Furthermore, optimizing the volume of parameters to achieve the same training performance is a critical objective, ensuring more efficient resource utilization. In an era characterized by numerous demands for training large language models (LLMs), cost reduction and performance enhancement are imperative for developing more advanced, sustainable, and cost-effective models. This approach could optimize resource utilization and broaden the opportunity for a wider range of individuals to train and deploy domainspecific large models in their respective fields, thereby overcoming the barrier of insufficient training resources that often hinder the development of targeted models. \n\nIn the field of Natural Language Processing (NLP), the adaptation of large language models (LLMs) for downstream tasks employs various Parameter-Efficient Fine-Tuning (PEFT) methods [19] to reduce training costs while maintaining or enhancing performance. Notable among these methods are techniques like Adapter Tuning [8]. Adapter Tuning, exemplified by methods such as Low-Rank Adaptation (LoRA) [9], involves introducing small, trainable Low-Rank matrices that adjust the model's existing weight matrices, thereby modifying the pre-trained model. This method allows the model to adapt quickly to new tasks while maintaining the stability of the pretrained model structure. Adapter models achieve personalized adjustments while maintaining efficiency. These PEFT techniques substantially reduce the number of parameters that need training, enabling effective adaptation to new tasks without the computational cost of retraining the entire model. They lower the barriers to training largescale models and expand their applicability across various tasks and domains. \n\nHowever, the computational resources required for these methods remain substantial [34]. We aim to reduce computational resource usage further and leverage existing resources to optimize large models for downstream tasks. Several methods have been developed to enhance the efficiency of LoRA, such as DyLoRA [24], which prioritizes the representations learned by the adapter modules during training. Another method, QA-LoRA [31], quantifies the weights of LLMs to reduce time and memory usage; after fine-tuning, LLM and auxiliary weights are seamlessly integrated into the quantized model without impacting accuracy.",
            "score": 0.36236454995915757,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2512
                }
            ],
            "ref_mentions": [
                {
                    "start": 1145,
                    "end": 1148,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3134765625
        },
        {
            "corpus_id": "266166801",
            "title": "Optimizing GPT-2 Pretraining on BabyLM Corpus with Difficulty-based Sentence Reordering",
            "text": "Removing these redundant inputs from the gutenberg dataset, led to improved BLiMP performances for the model trained on the sorted and cleaned dataset (Table 6). \n\nWhile we did achieve improvements in the BLiMP evaluation by training models only on the Strict-Small datasets using the described methods, the most significant intellectual contribution of this paper is highlighting the importance of considering contextual and knowledge-based similarity when reordering training inputs with any performance-enhancing metrics for language models. This concept reflects how humans learn effectively. In schools, subjects like math and English are not mixed in the same class period, regardless of the difficulty of the subjects, unless students are already proficient in both. \n\nIf we compare a context length-sized input to a human's attention span of 5 minutes, teaching math to a model or human is more effective if we present 10 similar examples of arithmetic operations that follow the same logical pattern within that 5-minute span, rather than presenting examples of different mathematical operations (like basic combinatorics mixed with calculus and geometry) without any logical pattern connecting the examples, even if these examples share the same level of difficulty. Therefore, in curriculum learning for language models, we argue that sentences that are presented together within a context length should be semantically and contextually similar beyond having the same level of difficulty.",
            "score": 0.3620284600655565,
            "section_title": "Introduction",
            "char_start_offset": 5996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1499
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "210838977",
            "title": "Single headed attention based sequence-to-sequence model for state-of-the-art results on Switchboard-300",
            "text": "In a second set of experiments, we characterize the importance of each of the regularization methods described in Sec. 2 for our model performance by switching off one training method at a time without re-optimizing the remaining settings. In these experiments, decoding is performed without an external language model. Curriculum learning is evaluated by either switching to randomized batches after 35 epochs or leaving the sorting on throughout training. We also test the importance of \u2206 and \u2206\u2206 features [49]. Sorting the results by decreasing number of absolute errors on Hub5'00, Table 2 indicates that each regularization method contributes to the improved WER. SpecAugment is by far the most important method, while using \u2206 and \u2206\u2206 features or switching off the curriculum learning in the later stage of training have marginal but positive effects. Other direct input level perturbation steps (speed/tempo perturbation and sequence noise injection) are also key techniques that can be found in the upper half of the table. If we compare the worst and baseline models, we find that the relative performance difference between them is nearly unchanged by including the external LM in decoding. Without the LM, the gap is 18% relative, while with the LM the gap is 17% relative. This clearly underlines the importance of the regularization techniques.",
            "score": 0.36189705013074924,
            "section_title": "Ablation study",
            "char_start_offset": 12481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1354
                }
            ],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "61938515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1519775390625
        },
        {
            "corpus_id": "270199394",
            "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
            "text": "A large focus of the machine learning community has been improving the performance of large language models (LLMs) while reducing their training costs.In this work, we consider how to improve the quality of an LLM by improving the quality of its pretraining data.Although there are many techniques to improve data quality, such as augmenting training samples with additional information (Li et al., 2024;Korbak et al., 2023), in this work we focus on the predominant method of data pruning: intelligently selecting a high-quality subset of a larger dataset to train on.\n\nData pruning is commonly used for quality filtering of noisy text data.Simple approaches include using symbolic rules (Bane et al., 2022;Raffel et al., 2020) or using simple classifiers to determine high-quality samples (Wenzek et al., 2020).However, in addition to basic quality filtering, more complex data pruning techniques are also applied to datasets to further improve their quality.Xie et al. (2023b) perform importance resampling where importance scores are calculated based on feature similarity to a target text.Tirumala et al. (2023) prune datasets by deduplicating and diversifying data based on a pretrained language model's embeddings of the text samples.Xie et al. (2023a) re-weight domain proportions based on learnability as determined by a smaller proxy model.Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.\n\nIn this work, we thoroughly investigate the impact that data pruning based on sample perplexity (Marion et al., 2023) has on LLM pretraining.In particular, we focus on the interplay between pretraining dataset composition and pruning methodology.We further evaluate perplexity pruning in the overtrained and data-constrained regimes.We also investigate whether evaluating the quality of data interventions based on upstream test set perplexity is a sound methodology for gauging downstream performance.",
            "score": 0.36167795445881606,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 569
                },
                {
                    "start": 571,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1094
                },
                {
                    "start": 1094,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1568
                },
                {
                    "start": 1570,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1816
                },
                {
                    "start": 1816,
                    "end": 1903
                },
                {
                    "start": 1903,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 404,
                    "matchedPaperCorpusId": "260866107"
                },
                {
                    "start": 404,
                    "end": 424,
                    "matchedPaperCorpusId": "257020046"
                },
                {
                    "start": 689,
                    "end": 708,
                    "matchedPaperCorpusId": "252186406"
                },
                {
                    "start": 708,
                    "end": 728,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 791,
                    "end": 812,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 961,
                    "end": 979,
                    "matchedPaperCorpusId": "256627727"
                },
                {
                    "start": 1094,
                    "end": 1116,
                    "matchedPaperCorpusId": "261076313"
                },
                {
                    "start": 1241,
                    "end": 1259,
                    "matchedPaperCorpusId": "258741043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "236459832",
            "title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models",
            "text": "design such as parameter sharing and parameter decomposition, and achieves better performance than original models; Q8BERT (Zafrir et al., 2019) compresses the model to 1/4 of the original model but with no more than 1% performance loss by quantizing 32bit floating point into 8bit; (Michel et al., 2019) used BERT weight pruning to compress the model and found that removing a large number of attention heads would not have a major impact on the model performance; TinyBERT (Jiao et al., 2020) reduced the model size by 7.5 times but with no more than 4% performance loss by designing a teacher-student distillation model. All above works are on language pre-training models, and most of them concern scales of model parameters. There are few works on cutting training data and light weighing multimodal pretraining model. In fact, compared with language model, multimodal pre-training models should deal with data from both language and visual modal, which demand larger amounts of data and more computational resources. Meanwhile, collections of training data are more difficult. Taking for example the size of text-image pairs used for multimodal pre-training, the frequently used MS COCO (Lin et al., 2014) is a high quality dataset with only 0.82M pairs, while LAIT (Qi et al., 2020) is already a big data with 10M pairs but with average quality. Therefore, it is significantly valuable to develop lightweight multimodal pre-training models which can make use of limited data efficiently.\n\nExisting research on curriculum learning (Bengio et al., 2009) has shown that imitating the process of human learning by gradually increasing the difficulty of a task from simple to complex in stages helps to make better use of different types of data and effectively improve the performance of learning. Many models (Qi et al., 2020) use as much as data available but few works have been done on how to arrange the tasks for better making use of limited data. We therefore borrow the idea of curriculum learning on training pre-training models. We construct a pre-training process which makes use of data from smaller units to bigger units in stages, and design appropriate pre-training tasks for each corresponding stage.\n\nSpecifically, we propose a new Multi-stage",
            "score": 0.3615762496240384,
            "section_title": "Introduction",
            "char_start_offset": 1812,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 304,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 475,
                    "end": 494,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1193,
                    "end": 1211,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 1537,
                    "end": 1558,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.143798828125
        },
        {
            "corpus_id": "269741199",
            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
            "text": "We propose introducing uncertainty to both the query selection and the RL fine-tuning steps to address these challenges. Moreover, to mitigate the hallucination problem, trustworthy language models should refrain from answering questions when the answer is unknown, which can occur due to ambiguity in the question or the questioner's intent. Cole et al., 2023 proposed that using sampling-based confidence scores to quantify repetition in model outputs is crucial for addressing hallucination issues and helps improve accuracy, especially for ambiguous questions. \n\nCurriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2022). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018;Zhang et al., 2021;Campos, 2021;Weber et al., 2023). \n\nStudies regarding the deployment of CL at the pre-training stage of LMs are focused on reducing the pre-training computational cost and the instability of the auto-regressive training emerging when increasing the models' size, batch size, sequence length, and learning rate. Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). Ranaldi et al., 2023 proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks. \n\nWang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs.",
            "score": 0.3614550781548812,
            "section_title": "I. Related Work",
            "char_start_offset": 80668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2243
                },
                {
                    "start": 2246,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 360,
                    "matchedPaperCorpusId": "258866001"
                },
                {
                    "start": 801,
                    "end": 823,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 1070,
                    "end": 1088,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1088,
                    "end": 1107,
                    "matchedPaperCorpusId": "243766208"
                },
                {
                    "start": 1945,
                    "end": 1965,
                    "matchedPaperCorpusId": "265068175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "267740312",
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "text": "In this paper, we introduce a learning percentagebased metric for assessing the difficulty of samples. We demonstrate that language models ranging from 1B to 13B sizes can self-select high-quality training data by employing this metric. Additionally, we empirically validate the transferability of data hardness across different model sizes, showcasing the efficient curation of high-quality training data by smaller models. Furthermore, we propose an optimized version of the metric that offers increased speed while maintaining equal efficacy. In future, we aim to explore methods for automatically transforming easy training samples into more challenging ones. \n\nOur examination reveals prevalence of noisy samples within the LP(1) and LP app (1) subsets of data. The detection and mitigation of noisy samples are imperative to mitigate their influence on the dataset. We leave this for future work.",
            "score": 0.3614349632155474,
            "section_title": "Conclusion",
            "char_start_offset": 22528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 902
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "250340652",
            "title": "Understanding Dataset Difficulty with V-Usable Information",
            "text": "Xu et al. (2019) show that it can be measured using the predictive V-information framework, which generalizes Shannon information to consider computational constraints. \n\nOur work extends the above framework by framing dataset difficulty as the lack of V-usable information. 1 The higher the V-usable information, the easier the dataset is for V. Not only does this framework allow comparisons of models w.r.t. the same dataset, but also of different datasets w.r.t. the same model. Figure 1 illustrates that different datasets provide different amounts of usable information for the same model, even when the task is identical (i.e., natural language inference in the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) datasets). \n\nBuilding on the aggregate estimate of dataset difficulty, we introduce a measure called pointwise V-information (PVI) for estimating the difficulty of each instance w.r.t. a given distribution ( \u00a73). PVI estimates allow us to compare not only individual instances, but also the difficulty of slices of data w.r.t V. On datasets containing more usable information (e.g., SNLI), PVI estimates are highly correlated (Pearson r \u2265 0.75) across different models, seeds, and training time, and with human judgments of difficulty. \n\nComparisons of V-usable information before and after isolating an input attribute shed light on why the dataset is easy or difficult for V ( \u00a74), which has significant implications for interpretability in AI 2 (Miller, 2019). Specifically, we use V-usable information to identify some limitations in benchmarks that are widely used in NLP to test for a model's understanding of different language phenomena: \n\n\u2022 Word ordering has a limited impact on the difficulty of a popular natural language entailment benchmark, SNLI (Bowman et al., 2015), even though entailment describes a causal relationship. \n\n\u2022 Some of the most difficult instances in SNLI and a popular grammaticality detection benchmark, CoLA (Warstadt et al., 2018), are mislabelled.",
            "score": 0.3614349632155474,
            "section_title": "Introduction",
            "char_start_offset": 1846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 171,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 695,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 1481,
                    "end": 1495,
                    "matchedPaperCorpusId": "36024272"
                },
                {
                    "start": 1793,
                    "end": 1814,
                    "matchedPaperCorpusId": "14604520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "276574659",
            "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
            "text": "The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the\"emergence phenomenon\", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.",
            "score": 0.361267713854594,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "276258596",
            "title": "Investigating the Zone of Proximal Development of Language Models for In-Context Learning",
            "text": "In this paper, we introduce a learning analytics framework to analyze the in-context learning (ICL) behavior of large language models (LLMs) through the lens of the Zone of Proximal Development (ZPD), an established theory in educational psychology. ZPD delineates the space between what a learner is capable of doing unsupported and what the learner cannot do even with support. We adapt this concept to ICL, measuring the ZPD of LLMs based on model performance on individual examples with and without ICL. Furthermore, we propose an item response theory (IRT) model to predict the distribution of zones for LLMs. Our findings reveal a series of intricate and multifaceted behaviors of ICL, providing new insights into understanding and leveraging this technique. Finally, we demonstrate how our framework can enhance LLM in both inference and fine-tuning scenarios: (1) By predicting a model's zone of proximal development, we selectively apply ICL to queries that are most likely to benefit from demonstrations, achieving a better balance between inference cost and performance; (2) We propose a human-like curriculum for fine-tuning, which prioritizes examples within the model's ZPD. The curriculum results in improved performance, and we explain its effectiveness through an analysis of the training dynamics of LLMs.",
            "score": 0.36119126241405364,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "46939075",
            "title": "Estimating Linguistic Complexity for Science Texts",
            "text": "Our work differs from existing models that estimate text difficulty since we do not use engineered features. There are advantages and disadvantages to both approaches, which we briefly discuss here. Models using engineered features based on research on language acquisition offer interpretability and insight into which specific linguistic features are contributing to text difficulty. An additional advantage of using engineered features in a regression or classification model is that less training data is required. \n\nHowever, given both the evolving theories in language acquisition and the large number of variables that impact second language acquisition, the methodologies used in language acquisition research have certain limitations. For example, the number of variables that can be considered in a study is practically limited, the sample population is often small, and the question of qualitative vs. quantitative methodologies used can influence outcomes (more details in (Larsen-Freeman and Long, 2014; Mitchell et al., 2013)). These limitations can carry into the feature engineering process. Using a model with text as input ensures that these constraints are not inherently part of the model; the performance of the system is not limited by the features provided. Of course, performance is limited by the training data, both in terms of the cost of collection and any biases inherent in the data. In addition, with advances in neural architectures such as attention modeling, there may be opportunities for identifying specific aspects of texts that are particularly difficult, though research in this direction is still in early stages.",
            "score": 0.36104528284367043,
            "section_title": "Discussion",
            "char_start_offset": 24079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1654
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1865234375
        },
        {
            "corpus_id": "232233485",
            "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
            "text": "Motivation. Inspired by human and animal learning, curriculum learning (Bengio et al., 2009) posits that neural networks train better when examples are not randomly presented but instead organized in a meaningful order that gradually shows more concepts and complexity. Traditionally, curriculum learning approaches first assume that a range of example difficulty exists in the data and then leverage various heuristics to sort examples by difficulty and train models on progressively harder examples (Bengio et al., 2009;Tsvetkov et al., 2016;Weinshall et al., 2018). A newer school of thought, however, has noted that instead of discovering a curriculum in existing data, data can be intentionally modified to dictate an artificial range of difficulty (Korbar et al., 2018;Ganesh and Corso, 2020)this is the approach we will take here.\n\nOur approach. Unlike data augmentation in computer vision where augmented data undoubtedly resembles original data, in text, data augmentation techniques might introduce linguistic adversity and therefore can be seen as a form of noising (Li et al., 2017;, where noised data is harder to learn from than unmodified original data. As such, we can create an artificial curriculum in the data by leveraging controlled application of data augmentation, starting by training on only original data and then adding augmented data with a higher levels of noising as training progresses. Specifically, we propose two simple schedules.\n\n(1) Two-stage curriculum data augmentation calls for one stage of training with only original data, followed by one stage of training with augmented data of fixed temperature.\n\n(2) Gradual curriculum data augmentation involves one stage of training with only original data, followed by multiple stages of training with augmented data where the temperature of augmented data (i.e., fraction of perturbed tokens) gradually increases each stage.  (Wei and Zou, 2019). Our proposed twostage curriculum (second stage starts at four-thousand updates) trains faster and achieves slightly higher performance compared with standard augmentation while using the same number of updates. Our proposed gradual curriculum (which here linearly increases augmentation temperature \u03c4 by 0.1 at {4, 8, 12, 16, 20}thousand updates) outperforms both standard augmentation and the",
            "score": 0.36102031208258933,
            "section_title": "Curriculum Data Augmentation",
            "char_start_offset": 2351,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 91,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 501,
                    "end": 522,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 522,
                    "end": 544,
                    "matchedPaperCorpusId": "3963337"
                },
                {
                    "start": 544,
                    "end": 567,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 754,
                    "end": 775,
                    "matchedPaperCorpusId": "53280782"
                },
                {
                    "start": 1077,
                    "end": 1094,
                    "matchedPaperCorpusId": "17730607"
                },
                {
                    "start": 1910,
                    "end": 1929,
                    "matchedPaperCorpusId": "59523656"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53466796875
        },
        {
            "corpus_id": "243865335",
            "title": "Self-Supervised Curriculum Learning for Spelling Error Correction",
            "text": "Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model\u2019s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for: 1) scoring the difficulty of training data, and 2) evaluating the competence of the model. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38% F1).",
            "score": 0.3609391653047017,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "269982744",
            "title": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum",
            "text": "Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk). Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch-size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to 6x faster training compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.",
            "score": 0.360871494438965,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424560546875
        },
        {
            "corpus_id": "273901573",
            "title": "Curriculum Consistency Learning for Conditional Sentence Generation",
            "text": "Curriculum learning (Bengio et al., 2009) introduces a human-like learning process for models, progressively transitioning from simpler to complex samples, contingent on the model's ability to learn. The challenge lies in defining the sample complexity and model capability. While sentence length (Platanios et al., 2019) has been utilized for the former, it is limited to text-related instances. \n\nOther research has used the norm value of the training corpus to characterize sample complexity and model capability (Liu et al., 2020). Techniques incorporating domain knowledge, LLMs' generative abilities, and an integrated curriculum scheduler have been effective in training a genetic programming agent player (Jorgensen et al., 2024). \n\nStronger LLM teachers creating a structured curriculum for student LLMs have demonstrated notable improvements in language generation quality, evaluated by GPT-4 (Feng et al., 2023). The instructional tuning phase in ICCL (Liu et al., 2024) progressively adjusts prompt complexity, leading to better LLM's performances. Strategies such as dataset decomposition (to the union of buckets) and variable batch size utilization in conjunction with a curriculum have proven to be time-efficient in LLM training (Pouransari et al., 2024). (Erak et al., 2024) show the automation of curriculum design leveraging LLMs' generative capabilities, successfully improving the agent's performance and convergence rate while reducing manual effort. Despite advances, limitations like high manual intervention and the need for sturdy guiding models affect the scalability of these methods. How to optimally use CL in LLMs instructional tuning for dynamic sample recognition is still an unresolved issue. In this paper, we propose an innovative complementary approach that leverages the strengths of both consistency learning and curriculum learning to enhance the capabilities of CSG.",
            "score": 0.36036379064204277,
            "section_title": "Curriculum Learning",
            "char_start_offset": 21355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 297,
                    "end": 321,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 713,
                    "end": 737,
                    "matchedPaperCorpusId": "271065259"
                },
                {
                    "start": 1246,
                    "end": 1271,
                    "matchedPaperCorpusId": "273162336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "264288956",
            "title": "Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers",
            "text": "We use dataset cartography to examine the impact of training dynamics on curriculum learning. Curriculum learning is a strategy that trains models on instances from easy to hard, based on the assumption that this order facilitates learning. However, we also explore the opposite strategy, which trains models on instances from hard to easy, and compare it with the conventional curriculum learning approach. This way, we can study how different training schedules affect the model performance. \n\nFigure 3 depicts accuracy plots showing the performance of various CL strategies based on (Hacohen and Weinshall, 2019) on the CFQ dataset. The figure legends indicate the ranking scheme and the employed confidence measure. For instance, hardto-learn (Inv PPL) refers to the case where Inv PPL is being used as the confidence measure, and the inclusion of the hard-to-learn samples is prioritized within the curriculum. Our analysis reveals that no single curriculum consistently outperforms others on the CFQ dataset. Exponential pacing leads to stagnant performance in the final 2/7 th of the training process due to surpassing the training size percentages of 33% and 50%. Surprisingly, initiating training with hard-to-learn samples yields superior performance compared to easy-to-learn samples, contrary to common curriculum learn-  ing expectations. This aligns with our previous findings, emphasizing the benefits of starting with challenging examples for improved adaptation. \n\nFigure 4 examines the impact of leveraging data maps within the CL strategy proposed by Zhang et al. (2019) for compositional generalization. The hard-to-learn (BLEU) configuration outperforms the no curriculum strategy, albeit with no notable improvement in convergence speed. This outcome mirrors our observations using the CL framework developed by Hacohen and Weinshall (2019), where initiating training with harder samples leads to better performance. However, the ambiguous configurations perform similarly to no curriculum, while the easy-to-learn configurations yield worse results than the no curriculum approach. \n\nIn Figures 5 and 6, we gain deeper insights into the contributions of dataset cartography. Overall, hard-to-learn (BLEU) emerges as the most effective configuration in the plots.",
            "score": 0.3603219165469226,
            "section_title": "Impact of Cartography-Based Curriculum Learning",
            "char_start_offset": 18134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2104
                },
                {
                    "start": 2107,
                    "end": 2197
                },
                {
                    "start": 2198,
                    "end": 2285
                }
            ],
            "ref_mentions": [
                {
                    "start": 1570,
                    "end": 1589,
                    "matchedPaperCorpusId": "155089817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "255941901",
            "title": "Dynamic data-free knowledge distillation by easy-to-hard learning strategy",
            "text": "In this section, we design a dynamic training module of DFKD, specifically in relation to Q2. Therefore, we adjust the difficulty when updating   in DFKD. As discussed in section 2.2, automatic curriculum learning (CL) is a technique that dynamically adjusts the difficulty of the training process, thereby enhancing both training speed and convergence. By implementing CL through appropriate pacing functions or transferring from the teacher, we can further improve the performance of DFKD. To achieve this, the pretrained teacher model serves as a better scheduler for the guidance of DFKD. This approach improves the effectiveness of DFKD by ensuring that the difficulty of DFKD process is adapted in a dynamic and efficient manner. \n\nFirst of all, we construct a difficulty measurer for the above dynamic information. Compared with previous CL methods [3], the dataset \ue230 is not available. Therefore, we build a difficulty measurer (  (), ) with respect to the built distribution estimator   () and timestamp . It should satisfy the following criteria: \n\nwhere  is a type of loss function. \n\nInspired by boundary-based methods [8,9], we consider pseudo samples close to the decision boundary as hard samples. Therefore, we define a tractable difficulty scheduler for a given dataset \ue230 as (\ue230, ) \n\nwhere   and   denote the parameters of the teacher and student models, respectively. Here  is used to minimize the objective function (, ) in the DFKD method. In the setting of DFKD, the dataset \ue230 represents the distribution of the trained dynamic generation module. The dynamic strategy  * is then calculated to control the level of difficulty at a fine-grained level, and it is considered as another dynamic target for the training stage of DFKD. To this end, we define () as a linearly increasing function with respect to the epoch . The objective function for optimizing   can then be expressed as follows: \n\nThe logit output at the final layer of the teacher and student models is denoted by   and   , respectively. The temperature,  , defined in the original KD method [17] is used to soften the output distribution of   and   . The loss function  in Equ. 4 is given by (",
            "score": 0.3602485995164384,
            "section_title": "Dynamic Training Module",
            "char_start_offset": 14017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1055
                },
                {
                    "start": 1058,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1909
                },
                {
                    "start": 1912,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 856,
                    "end": 859,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1130,
                    "end": 1133,
                    "matchedPaperCorpusId": "242757321"
                },
                {
                    "start": 1133,
                    "end": 1135,
                    "matchedPaperCorpusId": "218571034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2425537109375
        },
        {
            "corpus_id": "252735228",
            "title": "Generative Entity Typing with Curriculum Learning",
            "text": "As most of the previous entity typing models (Choi et al., 2018;Lee et al., 2020;Gong et al., 2021), our GET model is also trained upon the heterogeneous data consisting of a small portion of human-annotated data and a large portion of autogenerated data, due to the difficulty and high cost of human annotation. We will introduce how to obtain our auto-generated data in Section 4.1. The frame-work of our model learning includes the following three steps, as shown in Figure 2. \n\n1. Prompt Construction: To better leverage the knowledge obtained from the pre-training of PLM, we employ the prompt mechanism (Liu et al., 2021a) to guide the learning of our PLMbased GET model; \n\n2. Curriculum Instruction: As a key component of CL, the curriculum instruction is responsible for measuring the difficulty of each sample in the heterogeneous training data, and then designing a suitable curriculum for the model training process; \n\n3. CL-based Learning: In this step, our PLMbased GET model is trained with the designed curriculum, which is capable of adjusting its learning progress dynamically through selfpaced learning (SPL).",
            "score": 0.3600123558255167,
            "section_title": "Framework",
            "char_start_offset": 7558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449462890625
        },
        {
            "corpus_id": "254685579",
            "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
            "text": "CoLA Recent NLP studies have shown that curriculum learning (CL), presenting examples in an easyto-difficult order rather than presenting them randomly, can accelerate the model convergence and improve the generalization performance (Zhang et al., 2018;Tay et al., 2019;Zhan et al., 2021). There mainly exist two criteria for assessing the difficulty of examples, 1) model-based criteria  and 2) linguisticinspired criteria (Sachan and Xing, 2016;Tay et al., 2019;Nagatsuka et al., 2021;Campos, 2021). Model-based criteria measure the difficulty of each example using task-specific models. However, these criteria are unsuitable for reducing the computation cost of pre-training, given that they require calculating the loss of every example in a large pretraining corpus using language models. In contrast, linguistic-inspired criteria can efficiently assess the difficulty of examples based on prior knowledge and rules. Therefore, we adopt CL with linguistic difficulty criteria into MLM to improve the efficiency of pre-training.\n\nHowever, we argue that existing linguisticsinspired criteria, such as length, rarity, and masking ratio of a sequence, do not effectively reflect the nature of MLM, as verified empirically in Table 1.\n\nThe difficulty associated with MLM is significantly affected by the choice of tokens to be masked in the given sequence, rather than by the given sequence itself. For example, given \"The man is a Stanford <mask> student\", we can easily predict that the masked token would be University, whereas given \"The man is a <mask> University student\", it would be relatively difficult to predict the original token due to the insufficient clues in the context. Then, how can we measure the MLM difficulty? MLM can be viewed as predicting masked tokens based on other contextual tokens related to masked tokens. Therefore, if a word is related to many other words and phrases, it is likely that it has several clues in the context that make MLM easier.\n\nIn this paper, we propose a novel concept-based curriculum masking (CCM) for improving pretraining efficiency by considering the nature of MLM. We consider words and phrases that are related to several other concepts as easy ones and define them as the initial concepts to be masked first. To identify them,",
            "score": 0.35921931896715387,
            "section_title": "Methods",
            "char_start_offset": 784,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 270,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 270,
                    "end": 288,
                    "matchedPaperCorpusId": "232104951"
                },
                {
                    "start": 424,
                    "end": 447,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 447,
                    "end": 464,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 464,
                    "end": 487,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 487,
                    "end": 500,
                    "matchedPaperCorpusId": "236912520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "270620190",
            "title": "Reconciling Kaplan and Chinchilla Scaling Laws",
            "text": "1 Introduction Kaplan et al. (2020) ('Kaplan') and Hoffmann et al. (2022) ('Chinchilla') provided two influential studies measuring the impact of scale in large language models (LLMs). Both informed large-scale efforts on how to trade off model parameters (N ) and training tokens (D) for a given compute budget (C), but with conflicting advice. Kaplan's finding that N optimal \u221d C 0.73 , D optimal \u221d C 0.27 led to the conclusion that \"big models may be more important than big data\", and LLMs trained in the ensuing years committed more resources to",
            "score": 0.35910966566027447,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 550
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10595703125
        },
        {
            "corpus_id": "53295019",
            "title": "Analysing Dropout and Compounding Errors in Neural Language Models",
            "text": "However, more recent methods in machine learning have looked to learn the probabilities using variational inference methods Kingma et al. (2015); Gal et al. (2017). However, these novel techniques have been primarily used in the context of other challenges in computer vision and reinforcement learning, but not for natural language tasks. Other dropout techniques such as curriculum dropout Morerio et al. (2017) require no tuning, instead a schedule that increases dropout throughout training. This can be beneficial when there is some information known about the underlying distribution or the loss space. \n\nHence, this paper carries out an extensive overview of various dropout techniques and how adapting probabilities can be used to somewhat deter exposure bias. Additionally, we aim to identify commonalities between the networks that benefit the most from each variant and recommend regularization settings when training neural language models. Experiments are carried out on WikiText-2 and Penn Treebank language modelling datasets. Lastly, we analyse the accumulation of errors for a generated sequence of fixed size and conclude how each dropout variant effects the confidence interval. \n\nContributions Our contributions can be summarized as follows: \n\n\u2022 An evaluation of dropout variants and the first application of concrete dropout and curriculum dropout to neural language models. \u2022 A novel extension of variational dropout Gal & Ghahramani (2016) to concrete dropout Gal et al. (2017) and curriculum dropout Morerio et al. (2017) with various schedules. \u2022 An analysis of test perplexities at each time step, providing an insight as to why and where most errors occur in generating sequences at test time on defacto langauge modelling datasets. \u2022 An argument for the characteristics of loss surfaces for discrete kronecker delta outputs of high dimensionality where the output space is loosely structured (e.g such as modelling language) and its influence on modelling choices (e.g choosing a curriculum strategy when using curriculum dropout). \n\nBefore discussing the methodology we give a brief overview of the aforementioned regularization techniques and introduce related research.",
            "score": 0.35885816296960205,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 2059
                },
                {
                    "start": 2062,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 144,
                    "matchedPaperCorpusId": "46343823"
                },
                {
                    "start": 146,
                    "end": 163,
                    "matchedPaperCorpusId": "19840332"
                },
                {
                    "start": 1439,
                    "end": 1462,
                    "matchedPaperCorpusId": "15953218"
                },
                {
                    "start": 1483,
                    "end": 1500,
                    "matchedPaperCorpusId": "19840332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1805419921875
        },
        {
            "corpus_id": "259203337",
            "title": "Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values",
            "text": "Large language models (LMs) have achieved stateof-the-art performance on many natural language processing (NLP) tasks (Radford et al., 2019;Brown et al., 2020;Sanh et al., 2022). To adapt these models to new datasets and tasks, the standard approach is to fine-tune a pre-trained LM on a targeted downstream task. This allows the pre-trained general linguistic knowledge to be leveraged while fine-tuning to learn the task-specific information. However, during fine-tuning, pre-trained LMs are prone to significant performance degradation in the presence of noisy data (Srivastava et al., 2020). This effect may be further amplified when noisy or otherwise harmful instances are highly influential to the model parameters (Koh and Liang, 2017). As a result, it is important to identify harmful in- Figure 1: An overview of TS-DSHAPLEY: 1) Process the data using the target LM; 2) Compute sampling chains using a subset of the training set and aggregate the resulting Shapley values; and 3) Transfer the estimated data value information for use with the target LM by estimating the optimal low value data removal index. stances in the fine-tuning data that may obfuscate the task information and degrade performance.\n\nTo automatically identify harmful data, prior works have used training dynamics (Swayamdipta et al., 2020) and estimation of marginal contributions via leave-one-out retraining (Cook, 1977) or influence functions (Koh and Liang, 2017). Shapley values, which satisfy certain desirable fairness guarantees, have also recently been adopted from cooperative game theory to measure datum contributions, where a data point's Shapley value is the average marginal contribution to every possible data subset (Ghorbani and Zou, 2019).\n\nIn practice, Shapley-based data values are approximated using various techniques (Ghorbani and Zou, 2019;Jia et al., 2019bJia et al., , 2021Kwon and Zou, 2022;Schoch et al., 2022), as exact Shapley value computation over a dataset would require exhaustively retraining the model for every datum on",
            "score": 0.35884878354801136,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 140,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 140,
                    "end": 159,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 159,
                    "end": 177,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 569,
                    "end": 594,
                    "matchedPaperCorpusId": "224801132"
                },
                {
                    "start": 722,
                    "end": 743,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 1297,
                    "end": 1323,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1394,
                    "end": 1406,
                    "matchedPaperCorpusId": "2556544"
                },
                {
                    "start": 1430,
                    "end": 1451,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 1717,
                    "end": 1741,
                    "matchedPaperCorpusId": "102350503"
                },
                {
                    "start": 1825,
                    "end": 1849,
                    "matchedPaperCorpusId": "102350503"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "247292373",
            "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
            "text": "Performance Metric: We measure the efficacy of an instance selection technique by computing accuracies of candidate models on the selected instances and calculating their Kendall's correlation (Kendall, 1938) with accuracies obtained on the full evaluation dataset. High correlation implies that the performance scores obtained using the selected instances display the same behavior as the performance scores obtained using the complete dataset. Hence, high correlations values are preferred. Figure 3: Demonstrating difficulty score generalization. Difficulty scores computed using RoBERTa-large show negative correlation with accuracy averaged over 27 other models, hence satisfying the desiderata mentioned in Section 2.1. Note that we depict this trend for a few datasets only to avoid cluttering the image. Similar trend is observed for other dataset also 2 .\n\nDatasets: We experiment with a total of 23 datasets across Natural Language Inference, Duplicate Detection, Sentiment Analysis, Question Answering, Commonsense Reasoning, and several other tasks. Refer to Appendix section B for an exhaustive list of datasets for each task.  (Iandola et al., 2020), ELECTRA (Clark et al., 2020) in our experiments. We also use different variants of ConvBert (small, mediumsmall, base) and ELECTRA (small, base) models. For comprehensive experiments, we train each of the above models with training data of three different sizes (2k, 5k, and 10k examples) resulting in 27 candidate models for each dataset. We intentionally exclude RoBERTa from this list as we use it for computing the difficulty scores.\n\nInstance Selection Baselines: We compare the proposed instance selection approach with the following baselines:\n\nRandom Selection: Select a random subset of instances from the evaluation dataset.\n\nHeuristic Selection: Select instances based on the length heuristic (number of characters in the instance text) instead of the difficulty scores.",
            "score": 0.35865129896655534,
            "section_title": "Experimental Details",
            "char_start_offset": 10918,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 208,
                    "matchedPaperCorpusId": "120478295"
                },
                {
                    "start": 1141,
                    "end": 1163,
                    "matchedPaperCorpusId": "219966938"
                },
                {
                    "start": 1173,
                    "end": 1193,
                    "matchedPaperCorpusId": "208229926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "271710435",
            "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
            "text": "The easiest and hardest samples from the TruthfulQA (Lin et al., 2021) via these indicators are confirmed positively correlated with the actual complexity. The selection of difficult instructionresponse pairs via Eq. 5 allows the wider performance distribution of models under investigation, making it accurate to keep the relative rank of different models unchanged on subsets S b . \n\nRemark The computing of difficulty indices helps comprehensively analyze the robustness of models across samples and datasets. In addition, it also presents guidelines in curating and constructing discriminating NLP benchmarks.",
            "score": 0.3579152567309107,
            "section_title": "Technical",
            "char_start_offset": 60535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 613
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6650390625
        },
        {
            "corpus_id": "247292373",
            "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
            "text": "Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students\u2019 potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5% instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2% higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.",
            "score": 0.3579152567309107,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "53096414",
            "title": "Evolutionary Data Measures: Understanding the Difficulty of Text Classification Tasks",
            "text": "When their models do not achieve good results, ML practitioners could potentially calculate thousands of statistics to see what aspects of their datasets are stopping their models from learning. Given this, how do practitioners tell which statistics are the most useful to calculate? Which ones will tell them the most? What changes could they make which will produce the biggest increase in model performance? \n\nIn this work, we have presented two measures of text classification dataset difficulty which can be used as analysis tools and performance estimators. We have shown that these measures generalise to unseen datasets. Our recommended measure can be calculated simply by counting the words and labels of a dataset and is formed by adding five different, unweighted statistics together. As the difficulty measure is an unweighted sum, its components can be examined individually to analyse the sources of difficulty in a dataset. \n\nThere are two main benefits to this difficulty measure. Firstly, it will reduce the time that practitioners need to spend analysing their data in order to improve model scores. As we have demonstrated which statistics are most indicative of dataset difficulty, practitioners need only calculate these to discover the sources of difficulty in their data. Secondly, the difficulty measure can be used as a performance estimator. When practitioners approach new tasks they need only calculate these simple statistics in order to estimate how well models are likely to perform. \n\nFurthermore, this work has shown that for text classification the areas of Class Diversity, Balance and Interference are essential to measure in order to understand difficulty. Data Complexity is also important, but to a lesser extent. \n\nFuture work should firstly experiment with nonlinear but interpretable methods of combining statistics into a difficulty measure such as decision trees. Furthermore, it should apply this difficulty measure to other NLP tasks that may require deeper linguistic knowledge than text classification, such as named entity recognition and parsing. Such tasks may require more advanced features than simple word counts as were used in this work.",
            "score": 0.3579152567309107,
            "section_title": "Conclusion",
            "char_start_offset": 28723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1752
                },
                {
                    "start": 1755,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2193
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "234338422",
            "title": "Self-Guided Curriculum Learning for Neural Machine Translation",
            "text": "Inspired by the learning behavior of human, Curriculum Learning (CL) for neural network training starts from a basic idea of starting small, namely better to start from easier aspects of a task and then progress towards aspects with increasing level of difficulty (Elman, 1993). Bengio et al. (2009) achieves significant performance boost on tasks by forcing models to learn training examples following an order from \"easy\" to \"difficult\". They further explain CL method with two important constituents, how to rank training examples by learning difficulty, and how to schedule the presentation of training examples based on that rank. * Partial of this work was done when the first author was visiting at CLSP, JHU.\u0177  Figure 1: The NMT model is well-trained on parallel corpus D, {(x 1 , y 1 ), (x 2 , y 2 )} \u2208 D. Taking x 1 and x 2 as the input, the recovery degrees of y 1 are significantly better than that of y 2 . Note that the distance between y i and\u0177 i represents the recovery degrees, which indicate by dashed arrows.\n\nIn the scenario of neural machine translation (NMT), empirical studies have shown that CL strategy contributes to convergence speed and model performance (Zhang et al., 2018;Platanios et al., 2019;Zhang et al., 2019;Liu et al., 2020;Zhan et al., 2021;Ruiter et al., 2020). In these approaches, the learning difficulty of each training example is measured by different difficulty criteria. Early approaches depend on prior knowledge from various sources, including manually crafted features like sentence length and word rarity (Kocmi and Bojar, 2017). The drawback lies in the fact that human understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples, to approximate the perspective of an NMT model. For example, Platanios et al. (2019) turns discrete numerical difficulty scores into relative probabilities and then construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derives",
            "score": 0.3577488156236219,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 277,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 1203,
                    "end": 1226,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1226,
                    "end": 1245,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 1245,
                    "end": 1262,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 1262,
                    "end": 1280,
                    "matchedPaperCorpusId": "232104951"
                },
                {
                    "start": 1280,
                    "end": 1300,
                    "matchedPaperCorpusId": "222176797"
                },
                {
                    "start": 1556,
                    "end": 1579,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1853,
                    "end": 1876,
                    "matchedPaperCorpusId": "85498775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "273162483",
            "title": "How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics",
            "text": "To show that our method is model-agnostic, we further provide a comparison between the dataset characterizations obtained by RoBERTa and De-BERTa. Table 4 showcases the accuracies of the two models on each others' data characterizations. The difficulty splits are maintained cross-model. Across datasets and difficulty levels, the performance sharply drops for the \"hard\" split for both models. DeBERTa achieved higher accuracy for \"hard\" set, most likely due to better overall performance compared to RoBERTa. In Figure 5, we show that overall heuristic values for \"Contains Negation\" are maintained across both models. Extended results for are presented in Appendix A. \n\nOur proposed methodology is general and independent of the underlying encoder model since we process training dynamics computed from raw logit scores. This characterization procedure may be adapted to using Large Language Models (LLMs) (Lee et al., 2023) in a zero-shot classification setting by manipulating the log-likelihood for the tokens of the correct classes. However, using LLMs requires a different approach than the one presented here since the networks are usually used without further training, in an in-context-learning manner (Dong et al., 2022). Furthermore, even if the LLMs are fine-tuned (Hu et al., 2021), it is not straightforward how the logits of each of the three classes are tracked across training. We leave this approach for future work. Our method highlights significant shortcomings in widely used NLI evaluation datasets (SNLI and MultiNLI) due to spurious correlations in the annotation process. To address these issues, we proposed an automatic method for constructing more challenging test sets, effectively filtering out problematic instances and providing a more realistic measure of model performance. Our approach, which categorizes examples in increasing difficulty levels using a wide range of training dynamics features, enhances evaluation reliability and offers insights into underlying challenges in NLI. Importantly, our methodology is general and model-agnostic, and can be applied across different datasets and models, promising improved evaluation practices in NLP.",
            "score": 0.3576182156119998,
            "section_title": "Impact of the Underlying Encoder",
            "char_start_offset": 20609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "258841424"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "265696189",
            "title": "Teaching Specific Scientific Knowledge into Large Language Models through Additional Training",
            "text": "The model size of LLMs crucially affects their inferencing capabilities, with larger models demonstrating superior performance in tasks like few-shot prompting using prompt tuning. 1,3,4,15 Ho ever, the training efficiency when subjecting these models to additional training with text data is not entirely clear. Also, the impact of quantization techniques, which compress the size of larger models, on the learning process has not been fully elucidated. 21  trained 7, 13, and 70b models using optimized training parameters using a 7b model. Each model was trained with either 4-bit or 16-bit quantization (Fig. 4, Task 3). When only the original text was learned without augmentation, scores for all models, including the 7b model with full-parameter fine-tuning, remained low, ranging between 0.1 and 0.5 (Fig. 4a,b, number of contents = 1). This outcome suggests that larger models demonstrate superior performance in few-shot prompting tasks, but their effectiveness in conventional training processes may be limited. For the 7b and 13b models, scores generally increased monotonically with the addition of more context texts through augmentation, supporting the importance of providing multiple contexts. The largest 70b model initially showed consistently low scores of around 0.2 across all tasks. However, a significant improvement was observed when LoRA adapters were added to every trainable layer, even in the 70b model (Fig. 4c,d). This indicates the potential of LoRA adapters in enhancing model performance. Yet, there was a notable decline in performance when the number of irrelevant texts increased to 1000, with scores plummeting to 0. \n\nThe sudden drop in performance may also suggest a possibility of model collapse due to excessive training parameters, highlighting the delicate balance required in model architecture and training dataset size. \n\nThere is a need for continued research to understand how to tune LLMs larger than 13b appropriately. The study highlights how subtle differences in learning conditions can significantly impact performance, as evidenced by the performance differences under various learning conditions in the 7b model (refer to Fig. 3a and Fig. 4b,d). Such exploration is crucial not only for advancing our understanding of large-scale language models but also for their practical applications.",
            "score": 0.3574285226275268,
            "section_title": "Effect of model size and quantization on learning efficiency",
            "char_start_offset": 28492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2345
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.234375
        },
        {
            "corpus_id": "257378516",
            "title": "A Challenging Benchmark for Low-Resource Learning",
            "text": "Previous literature in curriculum learning (Hacohen & Weinshall, 2019), data pruning (Paul et al., 2021), and continual learning (Toneva et al., 2018) propose metrics for data sample difficulty based on loss or gradient norms. Here we restate three metrics: Loss score, GradNorm score and explain how they can be applied in our problem scenario. \n\nLoss Score Paul et al. (2021) and Sorscher et al. (2022) state this metric in the EL2N method, which intuitively measure data samples difficulty by looking at whether they can be learned correctly. Data samples with a higher loss score after training are more likely to be near the decision boundary. Therefore, we can select the hardest samples by ranking the loss score on the dataset. We call datasets constructed via loss scores as Hard-Bench (Loss). Examples with higher losses are selected as hard examples. \n\nGradient Norm Score Paul et al. (2021) discussed using gradient norm as an indicator of data importance. Samples with larger gradient norms shape the training geometry. However, there is little discussion on the connection between gradient norm and data difficulty. Here we give a brief and casual explanation. Based on previous analysis, we can find hard samples by checking their margin to the decision boundary of our model f , f (x 0 ) = 0. Therefore, we can define the L p norm margin as, \n\nWe can use Taylor's approximation to find an approximate solution to the problem, following (Elsayed et al., 2018). \n\nWhen the numerator is constrained (For a classification problem, we can constraint logits f (x) within 1 using sigmoid function. ), we can maximize the gradient norm to minimize margin. We call datasets constructed via gradient norm scores as Hard-Bench (GradNorm). Examples with higher gradient norm scores are selected as hard examples.",
            "score": 0.3572977303189419,
            "section_title": "Metrics Measuring Data Difficulty",
            "char_start_offset": 12703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1816
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 70,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 85,
                    "end": 104,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 359,
                    "end": 377,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 884,
                    "end": 902,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1452,
                    "end": 1474,
                    "matchedPaperCorpusId": "3927382"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "251493126",
            "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
            "text": "In the context of machine learning, CL aims to rank the training examples based on their difficulty. Each example is assigned a score and then the data set is ordered accordingly (from the easiest to the hardest example) with the goal of assisting the learning process. Empirical results show that this approach, when done properly, can speed up convergence and improve the stability of the training process of neural networks [12]. \n\nWe can think of CL as a way to provide guidance to a training model. A commonly used analogy to the real world, is that of the teacher-student relationship. The teacher has to create the curriculum in a way so that the student will neither get bored of the easy material, nor get overwhelmed by the hard examples. Instead, there should be an increasing level of difficulty throughout the curriculum. Note that this has nothing to do with the teacher-student transfer learning technique that is commonly used in deep learning. \n\nTo create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch [13]. The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model. \n\nTo continue the analogy of CL with human learning, the transfer-learning approach is equivalent to a teacher network trying to help a student, while in the adaptive approach the student is an autodidact trying to adapt to the learning difficulty.",
            "score": 0.357211827885274,
            "section_title": "Curriculum Learning",
            "char_start_offset": 2609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2263
                },
                {
                    "start": 2266,
                    "end": 2512
                }
            ],
            "ref_mentions": [
                {
                    "start": 1631,
                    "end": 1635,
                    "matchedPaperCorpusId": "102350936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "266180578",
            "title": "Mmi01 at The BabyLM Challenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings",
            "text": "Curriculum learning (CL) was first proposed by Bengio et al. (2009). The idea behind curriculum learning comes from the pedagogical observation that animals and humans learn better when knowledge is presented in a meaningfully organised way. For instance, starting with simple examples and gradually advancing to more complex ones (Skinner, 1958;Sweller, 1994;Krueger and Dayan, 2009). In the language modelling experiment carried out by Bengio et al. (2009), a corpus replacement method was used to make the data increasingly difficult. This way of pertaining was found to be more effective, producing improved results. \n\nThere have been then numerous works have explored using CL as the pretraining approach for language models. Whilst some works reported CL as beneficial to pretraining, others have reported the opposite results. Nagatsuka et al. (2021) investigated a CL-based pretraining scheme that utilises the length of the input text as the measure of \"difficulty\" in curriculum design. It was found that using length-based curriculum training alongside using the maximum available batch size, models achieved drastically faster convergence speed, and higher scores on downstream tasks ( Nagatsuka et al., 2021( Nagatsuka et al., , 2022) ) . \n\nCurriculum design greatly varies in each work. Linguistic features that have been used in curriculum formation include Parts-of-Speech (POS) information, n-gram frequency (Platanios et al., 2019), average number of dependents per word in the sentence parse tree (Jafarpour et al., 2021), edit distance (Kadotani et al., 2021;Chang et al., 2021). However, arguably, the most common curriculum formations are based on measures of frequency (Liu et al., 2018) and text length (Tay et al., 2019;Cirik et al., 2016). \n\nComparing curriculum learning studies becomes challenging due to the inherent variability in curriculum choices across different tasks. However, it is undeniable that the arrangement of data holds significance. As a result, in distinction from prior research, our work is oriented towards investigating diverse linguistic features in curriculum formation. Notably, we investigate 5 different measures of linguistic complexity. They are: \n\n\u2022 Average dependency distance (ADD) \n\n\u2022 Dependents per word (DPW)",
            "score": 0.3571072042675982,
            "section_title": "Related Works",
            "char_start_offset": 2045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2242
                },
                {
                    "start": 2245,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 67,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 360,
                    "end": 384,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 438,
                    "end": 458,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 834,
                    "end": 857,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 1196,
                    "end": 1220,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 1220,
                    "end": 1249,
                    "matchedPaperCorpusId": "255221201"
                },
                {
                    "start": 1425,
                    "end": 1449,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1516,
                    "end": 1540,
                    "matchedPaperCorpusId": "236486249"
                },
                {
                    "start": 1556,
                    "end": 1579,
                    "matchedPaperCorpusId": "237329957"
                },
                {
                    "start": 1579,
                    "end": 1598,
                    "matchedPaperCorpusId": "231846815"
                },
                {
                    "start": 1692,
                    "end": 1709,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1727,
                    "end": 1745,
                    "matchedPaperCorpusId": "166228313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "11966916",
            "title": "Management and Evaluation of Interactive Dialog in the Air Travel Domain",
            "text": "One of our most interesting findings was our ability to quantify the lack of convergence of the ATIS data, both in terms of grammar rules and in terms of lexicon. Starting with the direction-assistance application, we developed techniques for quantifying the growth of the system as ~This data was collected from the TI debriefing questionaires. We thank Charles Hemphill of TI for making these questionaires available to us. We recorded the rate of growth in terms of grammar rules and lexical items as a measure of convergence for both ATIS and directionassistance ([1], [2]) versions of PUNDIT. Our expectation is that the rate of growth should level off as more and more training is seen. To the extent that it does not, significant gaps in coverage can be expected. Figure 3 shows the incremental growth of the grammar for both domains and Figure 4 shows the incremental growth of the lexicon. It is interesting to note that after 600 sentences from the direction-assistance domain the rate of growth in both grammar and vocabulary is quite slow, indicating that this amount of training data is enough to provide a good sample of the kinds of constructions used in the domain. In contrast, we do not see any leveling off in ATIS growth after 600 sentences. From this we can conclude that a larger set of data will be required to provide a good sample of the constructions needed for ATIS. It is important for future evaluations to develop some better methods for estimating the amount of training data needed for a given application. Since the vocabulary growth curve is similar to the grammar growth curve in both applications it may be that simple measurement of vocabulary convergence would serve as a crude measure of amount of training data needed. We are just beginning to assemble some data points in terms of training data for multiple applications. The direction-assistance vs. ATIS applications illustrate that two seemingly similar kinds of applications can have very different characteristics, perhaps reflecting how the actual data collection was carried out. As we look at more spoken language applications, our ability to make reasonable estimates on training data should improve significantly.",
            "score": 0.35685516199458805,
            "section_title": "Data",
            "char_start_offset": 20842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "16248365"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036773681640625
        },
        {
            "corpus_id": "235097217",
            "title": "Training Language Models under Resource Constraints for Adversarial Advertisement Detection",
            "text": "We explore various techniques that can be used to train generalised language models(GLM) like BERT and multilingual variants with significant performance gains over baseline models described in Section 2.3. We look at resource constraints during training of machine learning models in a supervised setting attributed to the following cases: \n\n\u2022 Lack of labeled data. \n\n\u2022 Lack of large in-domain monolingual corpora. \n\n\u2022 Linguistic resources insufficient for building reliable statistical NLP applications. \n\nWe leverage product catalog to source data for weak and semi-weak supervision training in monolingual setting. We also explore how curriculum strategies and multilingual training can benefit training text classifiers for low resource languages. \n\nOur experiment show that generalised language models like BERT or multilingual variants like M-BERT can be trained using these techniques with significant performance gains over baseline model described in Section 2.3.",
            "score": 0.35673726166259123,
            "section_title": "Finetuning BERT under low resource constraints",
            "char_start_offset": 6590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2379150390625
        },
        {
            "corpus_id": "269982112",
            "title": "Super Tiny Language Models",
            "text": "The recent rise in popularity of large language models (LLMs) has mostly been fueled by the invention of the attention based autoregressive transformer [1,2].These models are trained to predict the next token (a subword unit) on very large corpora of data.There exists an extensive body of research on how scaling these architectures to more parameters and more data, predictably, improves model performance [3,4,5].Thus, in practice, this is the recipe for performance improvement most companies (notably OpenAI, Google DeepMind, Anthropic) follow.However, with ever larger models, a number of problems arise.Firstly, the state-of-the-art models have been scaled to an extent where it is impossible for academic researchers to train competitive models, making safety and capability research harder.Secondly, training models of this magnitude requires so much compute and energy that industry players plan to build nuclear power plants, solely for the purpose of training models [6,7].Lastly, when models are large, both inference time and applications that require running models on edge devices suffer.\n\nSmall language models on the other hand, on the order of 1 billion parameters, have been trained with increasingly impressive performance.This includes models like TinyLlama (1.1B) [8], Phi-3-mini (3.3B) [9], and MobiLlama (0.5B) [10].While these models are able to reasonably compete with large foundation models, they still require thousands of GPU hours to train, which puts them out of the reach of many researchers, and thus prohibits fast experimentation.\n\nRather than focusing on recreating foundation models at a smaller scale, in this series of papers, we aim to use the small model size as a test bed for an open exploration of effective methods for improving parameter and sample efficiency.Specifically, we will focus on methods related to tokenizer-free models, weight tying, self-play based training, alternative training objectives, and data sampling techniques, arXiv:2405.14159v2[cs.CL] 26 Jun 2024",
            "score": 0.35626359362391974,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1104
                },
                {
                    "start": 1106,
                    "end": 1244
                },
                {
                    "start": 1244,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1567
                },
                {
                    "start": 1569,
                    "end": 1808
                },
                {
                    "start": 1808,
                    "end": 2002
                },
                {
                    "start": 2002,
                    "end": 2021
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19482421875
        },
        {
            "corpus_id": "275471669",
            "title": "What Matters for In-Context Learning: A Balancing Act of Look-up and In-Weight Learning",
            "text": "As described in Section 4.2, we believe the look-up mechanism on its own is not sufficient for a stable ICL performance and a choice of the appropriate IWL objective plays an important role. In particular, we hypothesize that if the in-weight task is too simplistic, the look-up mechanism does not emerge or results in a transient ICL performance since the model can optimize for the IWL objective without needing to attend to the context. Therefore, the in-weight task must have a minimum level Preprint. \n\nof complexity to give rise to the look-up mechanism. A related study (Chan et al., 2022) showed that a long-tail distribution and an increase in the number of classes improve ICL, connecting these properties to natural language data. In this work, we additionally provide an explanation of why these properties enhance ICL -by increasing the complexity of the IWL task. \n\nWe propose four different ways of regulating the IWL task difficulty -changing the number of training classes, changing the number of samples used for training, training with noisy labels, and switching to the instance discrimination task. Here, we show how each of the proposed techniques influences the ICL and IWL performance.",
            "score": 0.3562456456526089,
            "section_title": "DOES IWL OBJECTIVE MATTER FOR ICL?",
            "char_start_offset": 24395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1209
                }
            ],
            "ref_mentions": [
                {
                    "start": 577,
                    "end": 596,
                    "matchedPaperCorpusId": "248665718"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497802734375
        },
        {
            "corpus_id": "220045816",
            "title": "Curriculum Learning for Natural Language Understanding",
            "text": "Curriculum Learning (CL) is first proposed by (Bengio et al., 2009) in machine learning area, where the definition of easy examples is established ahead, and an easy-to-difficult curriculum is arranged accordingly for the learning procedure. Recent developments have successfully applied CL in computer vision areas (Jiang et al., 2017;Guo et al., 2018;Hacohen and Weinshall, 2019). It is observed in these works that by excluding the negative impact of difficult or even noisy examples in early training stage, an appropriate CL strategy can guide learning towards a better local minima in parameter space, especially for highly non-convex deep models. We argue that language models like transformer, which is hard to train (Popel and Bojar, 2018), should also benefit from CL in the context of learning NLU tasks, and such idea still remains unexplored. \n\nThe key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks. \n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model. In general, our CL approach is not constrained to any particular task, and does not rely on human prior heuristics about the task or dataset.",
            "score": 0.35605017868841315,
            "section_title": "Introduction",
            "char_start_offset": 2107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2309
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 67,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 336,
                    "end": 353,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 725,
                    "end": 748,
                    "matchedPaperCorpusId": "4556964"
                },
                {
                    "start": 1129,
                    "end": 1147,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 1202,
                    "end": 1223,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "219829",
            "title": "Scalable Language Processing Algorithms for the Masses: A Case Study in Computing Word Co-occurrence Matrices with MapReduce",
            "text": "Over the past couple of decades, the field of computational linguistics (and more broadly, human language technologies) has seen the emergence and later dominance of empirical techniques and datadriven research. Concomitant with this trend is a coherent research thread that focuses on exploiting increasingly-large datasets. Banko and Brill (2001) were among the first to demonstrate the importance of dataset size as a significant factor governing prediction accuracy in a supervised machine learning task. In fact, they argued that size of training set was perhaps more important than the choice of machine learning algorithm itself. Similarly, experiments in question answering have shown the effectiveness of simple pattern-matching techniques when applied to large quantities of data (Brill et al., 2001;Dumais et al., 2002). More recently, this line of argumentation has been echoed in experiments with Web-scale language models. Brants et al. (2007) showed that for statistical machine translation, a simple smoothing technique (dubbed Stupid Backoff) approaches the quality of the Kneser-Ney algorithm as the amount of training data increases, and with the simple method one can process significantly more data. \n\nChallenges in scaling algorithms to increasinglylarge datasets have become a serious issue for researchers. It is clear that datasets readily available today and the types of analyses that researchers wish to conduct have outgrown the capabilities of individual computers. The only practical recourse is to distribute the computation across multiple cores, processors, or machines. The consequences of failing to scale include misleading generalizations on artificially small datasets and limited practical applicability in real-world contexts, both undesirable. This paper focuses on two barriers to developing scalable language processing algorithms: challenges associated with parallel programming and access to hardware. Google's MapReduce framework (Dean and Ghemawat, 2004) provides an attractive programming model for developing scalable algorithms, and with the release of Hadoop, an open-source implementation of MapReduce lead by Yahoo, cost-effective cluster computing is within the reach of most academic research groups. It is emphasized that this work focuses on largedata algorithms from the perspective of academiacolleagues in commercial environments have long enjoyed the advantages of cluster computing. However, it is only recently that such capabilities have become practical for academic research groups.",
            "score": 0.35593486374491345,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2445
                },
                {
                    "start": 2446,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 348,
                    "matchedPaperCorpusId": "6645623"
                },
                {
                    "start": 790,
                    "end": 810,
                    "matchedPaperCorpusId": "17951257"
                },
                {
                    "start": 810,
                    "end": 830,
                    "matchedPaperCorpusId": "9391992"
                },
                {
                    "start": 937,
                    "end": 957,
                    "matchedPaperCorpusId": "633992"
                },
                {
                    "start": 1977,
                    "end": 2001,
                    "matchedPaperCorpusId": "67055872"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1201171875
        },
        {
            "corpus_id": "3963337",
            "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning",
            "text": "It is well established that in language acquisition, there are robust patterns in the order by which phenomena are acquired. For example, prototypical concepts are acquired earlier; concrete words tend to be learned before abstract ones (Rosch, 1978). The acquisition of lexical knowledge in artificial systems proceeds differently. In general, models will improve during the course of parameter learning, but the time course of acquisition is not generally studied beyond generalization error as a function of training time or data size. We revisit this issue of choosing the order of learning-curriculum learning-framing it as an optimization problem so that a rich array of factors-including nuanced measures of difficulty, as well as prototypicality and diversity-can be exploited. \n\nPrior research focusing on curriculum strategies in NLP is scarce, and has conventionally been following a paradigm of \"starting small\" (Elman, 1993), i.e., initializing the learner with \"simple\" examples first, and then gradually increasing data complexity (Bengio et al., 2009;Spitkovsky et al., 2010). In language modeling, this preference for increasing complexity has been realized by curricula that increase the entropy of training data by growing the size of the training vocabulary from frequent to less frequent words (Bengio et al., 2009). In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010). These case studies have demonstrated that carefully designed curricula can lead to better results. However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990;Skinner, 1938). Had different heuristics been chosen, the results would have been different. In this paper, we use curriculum learning to create improved word representations. However, rather than testing a small number of curricula, we search for an optimal curriculum using Bayesian optimization. A curriculum is defined to be the ordering of the training instances, in our case it is the ordering of paragraphs in which the representation learning model reads the corpus. We use a linear ranking function to conduct a systematic exploration of interacting factors that affect curricula of representation learning models.",
            "score": 0.35578301513780414,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 250,
                    "matchedPaperCorpusId": "15633758"
                },
                {
                    "start": 924,
                    "end": 937,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 1046,
                    "end": 1067,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1067,
                    "end": 1091,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 1315,
                    "end": 1336,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1471,
                    "end": 1496,
                    "matchedPaperCorpusId": "1363892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74755859375
        },
        {
            "corpus_id": "259376868",
            "title": "BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition",
            "text": "Analysis of our experiments motivates us to hypothesize that different training techniques like masking rate and masking strategies might help broaden the model's experience (Mitchell, 1997) and gain more diversified knowledge of textual input. In addition, during the experimental stage, we've observed that pre-training with specific techniques like WWM and increased masking rate slow down the training process if introduced right from the beginning due to increased task complexity. That brings us to the idea that introducing more complex tasks gradually, using CL's easy-to-hard strategy, assists in guiding the model's learning process more smoothly (Bengio et al., 2009). Using the results of our experiments, we implement a CL method for pre-training a biomedical LM. In Masked Language Modeling (MLM), the objective is to predict the masked token based on the surrounding context:\n\nIt remains a challenging task to measure the difficulty of a task or a training sample in CL. It is primarily an issue in the case of pre-training masked language models. There are a few newly proposed approaches to tackle this challenge. Nagatsuka et al. (2021) pre-trains RoBERTa by increasing the block size of input text.  proposed a curriculum based on masking easy-to-predict tokens first. We formulate our curriculum strategy from the perspective of the complexity of the prediction task. Predicting whole words is more complex than predicting just tokens which may be a part of the word, giving the LM more hints from the surrounding context (Gu et al., 2021;Cui et al., 2021). Increasing the masking rate makes prediction more challenging (Wettig et al., 2023) since less context is available for prediction.\n\nWe use a pre-trained model of the same architecture and number of parameters to measure the prediction task difficulty and evaluate its performance as MLM (Dudy and Bedrick, 2020;. We use a corpus from a different domain to account for domain shift. In our case, we evaluate the performance of the BERT-baseuncased 4 on the RealNews dataset (Raffel et al., 2019). The evaluation results are presented in Figure 4. Based on this evaluation, we divide our curriculum into four phases (see Table 4).\n\nWe start with pre-",
            "score": 0.3556272991616176,
            "section_title": "Motivation and Method",
            "char_start_offset": 16204,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 657,
                    "end": 678,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1131,
                    "end": 1154,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 1542,
                    "end": 1559,
                    "matchedPaperCorpusId": "220919723"
                },
                {
                    "start": 1559,
                    "end": 1576,
                    "matchedPaperCorpusId": "195068911"
                },
                {
                    "start": 1866,
                    "end": 1890,
                    "matchedPaperCorpusId": "222310253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "229923196",
            "title": "FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging",
            "text": "Language understanding systems are becoming ever more powerful with the recent advances in 1 Code is available at https://github.com/ salesforce/fast-influence-functions. large-scale pre-training. As these systems become widely adopted for real-world applications, the ability to interpret the model decisions grows increasingly important. An array of interpretation methods now exist for shedding light on models' decisionmaking processes, with the bulk of work focusing on estimating feature importance (Li et al., 2016a;Ribeiro et al., 2016b;Lundberg and Lee, 2017;Sundararajan et al., 2017a;Ribeiro et al., 2018a) and interpreting internal model representations (Simonyan et al., 2014;Kim et al., 2018;Olah et al., 2018Olah et al., , 2020. These approaches aim to identify features a model uses to make decisions or analyze representations obtained from trained models. In contrast, one might also want to know how particular training data points influence model behavior at test time. This kind of goal is an instance of what Lipton (2016) terms \"algorithmic transparency,\" or, transparency \"at the level of the learning algorithm itself.\" The ability to do so would allow researchers to identify and respond to data responsible for problematic test-time behavior.\n\nOne simple brute-force way to estimate the importance of a training data-point to a test-time de-cision is the leave-one-out approach (Hastie et al., 2009), i.e., train two models, one on the full training dataset, and the other on all but the training example to be considered. The difference in the behavior on the test data-point between these two models serves as a measure of the data influence. However, the computational cost of leave-one-out increases with model size and data size. To find the most influential data-point in the training set of size N , practitioners have to train O(N ) models.\n\nAlternatively, influence functions provide a tractable estimate of the effect without the need to repeatedly retrain the model. In a nutshell, the influence function estimates the impact on the model parameters when the loss of the training example is up-weighted by a small delta. Developed decades ago as a classic technique",
            "score": 0.3554849134091227,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 505,
                    "end": 523,
                    "matchedPaperCorpusId": "14099741"
                },
                {
                    "start": 545,
                    "end": 568,
                    "matchedPaperCorpusId": "21889700"
                },
                {
                    "start": 568,
                    "end": 595,
                    "matchedPaperCorpusId": "16747630"
                },
                {
                    "start": 595,
                    "end": 617,
                    "matchedPaperCorpusId": "3366554"
                },
                {
                    "start": 666,
                    "end": 689,
                    "matchedPaperCorpusId": "1450294"
                },
                {
                    "start": 1031,
                    "end": 1044,
                    "matchedPaperCorpusId": "5981909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2493896484375
        },
        {
            "corpus_id": "264935274",
            "title": "People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection",
            "text": "In this work, we compare manual CADs to automated CADs generated from Polyjuice (Wu et al., 2021), ChatGPT,2 and Flan-T5 (Chung et al., 2022) as training data for harmful language detection models, specifically sexism and hate speech detection. With recent advances in generative NLP approaches, specifically Large Language Models (LLMs), their dominance in many NLP bench-marks (Chung et al., 2022) and their use in social computing generative tasks (Josifoski et al., 2023;Veselovsky et al., 2023), we expect these generation techniques to produce high-quality CADs, especially recent variants of Large Language models which can be accessed via prompt-based interfaces like  This work aims to shed light on (i) RQ1: the extent to which different automated CAD generation methods are capable of generating efficient CADs that boost model performance. We then use the complementary perspective of informationtheoretic formulations of dataset difficulty (Ethayarajh et al., 2022) to obtain the 'difficulty' of manual and automated CADs, i.e., how hard it is for a model family to learn them or how much useful information they entail for learnability; We then explore (ii) RQ2: the properties of CADs making them effective as training data -i.e., those that help improve model performance; Using instancelevel difficulty scores, we study links between various properties of CADs (such as the semantic and lexical distance from the original instance or their label) to surface the properties of effective CADs. \n\nWe find that models trained on a combination of original and manual CADs outperform models trained on just originals on several OOD datasets for both sexism and hate speech detection. While ChatGPT CADs are better training data than other automated CADs, they still fare worse than manual CADs. In terms of CAD properties, the generation mechanism plays a strong role in the efficacy of CADs -Flan-T5 and Polyjuice CADs have high difficulty, indicating too little usable information is available from them for a model to learn.",
            "score": 0.35547707784066795,
            "section_title": "Data type",
            "char_start_offset": 1437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 97,
                    "matchedPaperCorpusId": "235266322"
                },
                {
                    "start": 953,
                    "end": 978,
                    "matchedPaperCorpusId": "250340652"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16064453125
        },
        {
            "corpus_id": "203951409",
            "title": "Multiple-objective Reinforcement Learning for Inverse Design and Identification",
            "text": "CL is a strategy for multi-task learning, which draws parallels to human knowledge. A sequence of subtasks is ranked in difficulty ascending order and conducted within different phases (Bengio et al. 2009). Its success in learning performance improvement can be seen in many domains, such as learning language modeling and pattern recognition (Bengio et al. 2009;Graves et al. 2017). The difficulty level of subtasks is usually defined with respect to the training data. In this study, we design a measurement (difficulty score) to differentiate task difficulties and tune the agent model to satisfy a larger number of constraints with multiple tuning phases. \n\nThe goal for the agent model is to generate valid SMILES The difficulty score is to measure the difficulty of each constraint. For one constraint j, the difficulty score D j is the percentage of it being captured in the prior distribution with the pretrained prior model, as calculated with Eq7: \n\nwhere F j (y) is 1 if the sequence y contains the target constraint, otherwise, -1. |Y | is the size of SMILES test dataset. Using 1000 SMILES string generated by the prior model, we report the average inherent difficulty score D as shown as Fig 4 . The inherent difficulty score can be interpreted as the \"default\" probability that a certain constraint is satisfied. For example, the FG constraint for benzene is about 0.8, which means that satisfying this constraint is relatively easy since it can be generated 80% of the time. With the difficulty score (D), the constraints can be ranked from easiest to the most difficult. Progressively more difficult tasks are designated by having a greater divergence from the inherent distribution relative to the prior model. \n\nIn this work, we propose a novel approach of combining CL heuristics in a RL context. We hypothesize that the agent model can be more effectively trained to satisfy multiple constraints using this approach. This RL-based CL process is demonstrated as Fig 5 . There are multiple curriculum training phases. In each phase, the agent model is initialized from the prior model in the beginning, and the prior model is synchronized with the agent model at the end of each phase for memorizing the previously learned constraints.",
            "score": 0.355475076309141,
            "section_title": "Curriculum Learning (CL)",
            "char_start_offset": 11964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 205,
                    "matchedPaperCorpusId": "76665438"
                },
                {
                    "start": 343,
                    "end": 363,
                    "matchedPaperCorpusId": "76665438"
                },
                {
                    "start": 363,
                    "end": 381,
                    "matchedPaperCorpusId": "25494718"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2685546875
        },
        {
            "corpus_id": "269762685",
            "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models",
            "text": "A general framework for curriculum design consists of two core components: Difficulty Measurer and Training Scheduler, which decide two things respectively: 1) What kind of training data is supposed to be easier than other data?2) When should we present harder data for training, and how much more?Thus, we can divide existing CL methods into two types: when both the Difficulty Measurer and Training Scheduler are designed by human prior knowledge with no data-driven algorithms involved, we call the CL method predefined CL.If any (or both) of the two components are learned by data-driven models or algorithms, then we denote the CL method as automatic CL.\n\nIn the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning.All representative approaches of both categories will be reviewed and discussed in this tutorial.",
            "score": 0.3554091959757717,
            "section_title": "Approaches",
            "char_start_offset": 5464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 659
                },
                {
                    "start": 661,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1422
                },
                {
                    "start": 1422,
                    "end": 1519
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "227343966",
            "title": "When Do Curricula Work?",
            "text": "Are examples learned in a consistent order across different runs, architectures, and tasks? If such a robust notion exists, is it possible to change the order in which the examples are learned by presenting them in a different order? The answer to this question determines if there exists a robust notion of example difficulty that could be used to influence training. \n\nWe then look into different ways of associating difficulty to examples using scoring functions and a variety of schedules known as pacing functions for introducing examples to the training procedure. We investigate if any of these choices can improve over the standard full-data i.i.d. training procedure commonly used in machine learning. Inspired by the success of curriculum learning in large scale training scenarios, we train in settings intended to emulate these large scale settings. In particular, we study the effect of curricula when training with a training time budget and training in the presence of noise. \n\nContributions In this paper, we systematically design and run extensive experiments to gain a better understanding of curricula. We train over 20,000 models over three datasets, CIFAR10,100, and FOOD101, covering a wide range of choices in designing curricula and arrive at the following conclusions: \n\n\u2022 Implicit Curricula: Examples are learned in a consistent order (Section 2). We show that the order in which examples are learned is consistent across runs, similar training methods, and similar architectures. Furthermore, we show that it is possible to change this order by changing the order in which examples are presented during training. Finally, we establish that well-known notions of sample difficulty are highly correlated with each other. \n\n\u2022 Curricula achieve (almost) no improvement in the standard setting (Section 4). We show curriculum learning, random, and anti-curriculum learning perform almost equally well in the standard setting.1 Furthermore, we establish that using similar techniques to remove examples from the training set (as opposed to introducing them) also does not help.",
            "score": 0.3551566188921895,
            "section_title": "introduction",
            "char_start_offset": 2029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "271903477",
            "title": "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning",
            "text": "Recent studies highlight the effectiveness of using in-context learning (ICL) to steer large language models (LLMs) in processing tabular data, a challenging task given the structured nature of such data. Despite advancements in performance, the fairness implications of these methods are less understood. This study investigates how varying demonstrations within ICL prompts influence the fairness outcomes of LLMs. Our findings reveal that deliberately including minority group samples in prompts significantly boosts fairness without sacrificing predictive accuracy. Further experiments demonstrate that the proportion of minority to majority samples in demonstrations affects the trade-off between fairness and prediction accuracy. Based on these insights, we introduce a mitigation technique that employs clustering and evolutionary strategies to curate a diverse and representative sample set from the training data. This approach aims to enhance both predictive performance and fairness in ICL applications. Experimental results validate that our proposed method dramatically improves fairness across various metrics, showing its efficacy in real-world scenarios.",
            "score": 0.35475623461704975,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30224609375
        },
        {
            "corpus_id": "255941901",
            "title": "Dynamic data-free knowledge distillation by easy-to-hard learning strategy",
            "text": "Curriculum learning (CL) has been widely used to train models by an easy-to-hard strategy [3,38,32]. Bengio [3] provides a clear illustration of its convergence of it. \n\nAutomatic CL is a dynamic strategy to adjust the difficulty by the feedback of training process. Self-paced learning (SPL) [19] is the most widely used automatic CL method assigning data with different difficulties based on the training losses at each timestamp (or epoch). Several theoretical studies [24,26] provide a deep understanding of SPL and are categorized into majorization minimization (MM) [6] algorithm and concave optimization. They use transfer learning [32] and uncertainty [50] as specific representative techniques to transfer from teacher. Recently, SPL is also applied to the field of unsupervised learning [22], clustering [47], anomaly detection [44] and graph [15]. \n\nBesides, some data-driven KD methods also use CL to enhance student learning from the teacher. For example, Xiang et al. [42] use SPL for instance selection in longtailed datasets, and Li et al. [21] use a similar uncertainty curriculum to distill models from large pretrained language models. In this work, CL provides an adaptive training target for the generation process of DFKD. Additionally, it provides a theoretical understanding of accelerating the convergence of DFKD methods and contributes to the usage of CL in knowledge distillation.",
            "score": 0.3546960986741304,
            "section_title": "Curriculum Learning",
            "char_start_offset": 8283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1408
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "263862224"
                },
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "533152"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "5708790"
                },
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 660,
                    "end": 664,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "245131303"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "249917174"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "247598732"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "209862398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32275390625
        },
        {
            "corpus_id": "150373675",
            "title": "When deep learning met code search",
            "text": "Because we wanted to understand the extent to which the complex sequence-of-words based networks help, we also developed a minimal extension to the NCS technique, just adding supervision and nothing else: \n\n\u2022 UNIF A supervised extension of the base NCS technique of our own creation. UNIF uses a bag-of-words-based network, which has significantly lower complexity compared to sequence-ofwords-based networks. This simple model is a new contribution of this paper. \n\nOur evaluation is structured using the following three research questions; we also give the summary of our findings along with the question. \n\nResearch Question 1. : Does extending an effective unsupervised code search technique with supervision based on a corpus of paired code and natural language descriptions improve performance? \n\nOur results show that UNIF performed better than NCS on our benchmark queries, though the improvement was not seen uniformly across all data sets. \n\nResearch Question 2. : Do more sophisticated networks improve performance in supervised neural code search? \n\nOur results show that UNIF, a simple, bag-of-words-based network, outperformed the sequence-of-words-based CODEnn and SCS. The additional sophistication did not add value. \n\nResearch Question 3. : How effective is supervision based on docstrings as the natural language component of the training corpus? \n\nWe found that supervision based on docstrings -which is commonly the natural language component of an aligned corpus -did not always improve performance, contrary to expectations. To understand the possible room for improvement, we constructed an ideal alternate training corpus, where the code snippets and natural language, while disjoint from our benchmark queries, were drawn from the same source. \n\nWhen trained on this corpus, all supervised techniques improved significantly, showing that, as a proof of concept, if given a training corpus that matches expected user evaluation, these techniques can provide impressive search performance. \n\nContributions. 1. We believe this is the first paper to compare recent neural code search systems running on the same platform and evaluation using the same corpora.",
            "score": 0.35447254430955355,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 2013
                },
                {
                    "start": 2016,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "267759615",
            "title": "Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection",
            "text": "Motivation. The goal of data selection tasks is to find the instructions with the highest quality and diversity. In assessing the quality of instruction data, prior research (Xu et al., 2023a;Zhao et al., 2023;Cao et al., 2023) has indicated that the most challenging samples, or \"hardest\" samples, can provide significant insights and are therefore valuable for model training. These \"hardest\" samples are defined as those that present substantial learning challenges to VLMs during the training process. Motivated by this observation, we aim to develop an evaluation metric capable of quantifying the learning difficulty d i (with VLMs) of each sample x i to accurately reflect its quality. \n\nWhile many metric-based data selection methods (Chen et al., 2023;Cao et al., 2023;Liu et al., 2023c) have been applied to large language models (LLMs), directly transferring them to more complex multimodal contexts proves challenging, often yielding suboptimal outcomes as evidenced by our experiments. Given the difficulty of evaluating multimodal instructions with only a singular metric, we propose to combine these features via a lightweight scoring network s, which can be either a Multi-Layer Perceptron (MLP) or a linear layer. \n\nTo increase the diversity within the filtered instruction dataset, we introduce a penalty mechanism targeting similar examples. Specifically, when a sample is chosen for the filtered dataset, we reduce the likelihood of selecting its k-nearest neighbors from the raw dataset. This approach is designed to ensure a broader variety of instructional content by actively reducing redundancy among the chosen samples. \n\nIn the rest part of this section, we introduce SELF-FILTER, which consists of two stages: a) score net training, and b) data selecting by quality and diversity. \n\nUse L to backpropagate and optimize the vision-language model and the score net ically exhibiting higher loss. Motivated by data re-weighting approaches (Gao et al., 2023;Zhang and Pfister, 2021), we propose utilizing learnable weights, produced by a score net, within the VLM loss framework to assess the difficulty of training instructions. Given that more challenging instructions typically incur higher loss, their corresponding learnable weights are inclined to decrease during the training process.",
            "score": 0.3544289123930477,
            "section_title": "SELF-FILTER",
            "char_start_offset": 8537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 1964,
                    "end": 1982,
                    "matchedPaperCorpusId": "257219618"
                },
                {
                    "start": 1982,
                    "end": 2006,
                    "matchedPaperCorpusId": "237431056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "274886046",
            "title": "Assessing and Scoring Difficulty of Hard-to-Solve Data in Summarization Tasks",
            "text": "In recent years, data-driven and machine learning-based natural language processing (NLP) technologies have effectively addressed various challenges. To further enhance the performance of NLP models, it is crucial to understand the types of data that a model can handle well and those it struggles with. This study introduces a method to discern which types of data can be effectively processed by given neural network-based models and which pose difficulties. We define the criteria for hard-to-solve data, construct a pairwise easy-hard dataset, and propose a neural scoring model. This model ascertains the difficulty level of each data instance. To utilize the proposed difficulty level as an application, we employed curriculum learning. The experimental results show that our methodology can effectively distinguish between easy and hard data, and performance improves when applying the curriculum learning approach.",
            "score": 0.3544289123930477,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "Tables 1 show the performance of different models when test samples are grouped based on word rarity. The results show that the performance of the baseline models severely drops compared to standard training (No-CL). This is while our Ling-CL approach results in 4.5 absolute points improvement in accuracy over the best-performing baseline averaged across tasks, owing to its effective use of linguistic indices. Appendix D shows the overall results on the entire test sets, and results when test samples are grouped based on their loss; we use loss because it is a widely-used measure of difficulty in curriculum learning. These groupings allow for a detailed examination of the model's performance across samples with varying difficulty, providing insights into the strengths and weaknesses of models. For example, the performance on SNLI varies from 89.8 to 90.6. However, when word rarity is used to group data based on difficulty, the performance range significantly drops from 74.4 to 83.6, indicating the importance of the proposed measure of evaluation. We observe that such grouping does not considerably change the performance on ANLI, which indicates the high quality of the dataset. In addition, it increases model performance on AN-Pair and GED, which indicates a greater prevalence of harder examples in these datasets. \n\nOn average, the optimization approach outperforms correlation by 1.6% \u00b11.9% accuracy in our experiments. Also notably, on average, the argmax index aggregation outperforms the weighted average by 1.9% \u00b11.9%, and the filtered indices outperform the full list of indices by 1.4% \u00b11.1%.",
            "score": 0.3544289123930477,
            "section_title": "Enhanced Linguistic Performance",
            "char_start_offset": 18411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1334
                },
                {
                    "start": 1337,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1620
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78759765625
        },
        {
            "corpus_id": "276249756",
            "title": "PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts",
            "text": "Modern foundation models, such as large language models (LLMs), have demonstrated impressive generalization and multitask learning capabilities by pretraining on diverse datasets across multiple domains [32,58,8,45]. The effectiveness of these models is heavily influenced by the composition of their training data [16,25]. However, determining the optimal data mixture (across different tasks and data sources) remains a fundamental challenge due to the substantial size of both models and datasets, as well as the high computational cost of training. In most cases, training large models is limited to a single experimental run, making it impractical to iteratively fine-tune the weights of different data sources/tasks. Current approaches to multitask learning typically rely on fixed dataset weights (aka mixing or sampling strategies), often determined heuristically or based on the performance of smaller proxy models. For example, mT5 [69] assigns dataset weights based on their relative abundance, GLaM [16] selects weights by evaluating downstream performance on smaller models, and the 405B LLaMA-3 model [17] heuristically constructs its training corpus from diverse sources. More recently, DoReMi [67] introduced a method that pretrains a small model using group distributionally robust optimization to determine dataset weights for larger-scale training. However, the optimality of these approaches is unclear, as the capabilities of large and small models differ significantly [59,65]. \n\nMoreover, the loss landscape evolves throughout training [75,30], meaning that static dataset weights determined at initialization may not remain optimal (as we will further elaborate in Section 3.1). \n\nAnother line of research addresses multitask optimization by modifying gradient updates to mitigate gradient conflicts, where task gradients point in opposing directions, slowing down optimization. Techniques such as PCGrad [73], GradNorm [7], and MGDA [14] attempt to minimize these conflicts by adjusting gradient directions during training. While these methods improve performance, they introduce significant computational and memory overhead, making them impractical for large-scale models with numerous tasks [68].",
            "score": 0.35367498626294,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "258741043"
                },
                {
                    "start": 1563,
                    "end": 1566,
                    "matchedPaperCorpusId": "13739955"
                },
                {
                    "start": 1929,
                    "end": 1933,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1944,
                    "end": 1947,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 1958,
                    "end": 1962,
                    "matchedPaperCorpusId": "120459561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454833984375
        },
        {
            "corpus_id": "265456857",
            "title": "SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling",
            "text": "Alternatively, a scaled-down model can be trained to serve as a proxy for selecting important data samples based on predefined metrics (Coleman et al., 2020). Other metrics include the number of times a model misclassifies a sample and the high variance of gradients, indicating example difficulty (Dadalto et al., 2023). Prediction depth, the layer at which a k-NN classifier can successfully classify an example, can also serve as a measure of computational difficulty (Ma and Karaman, 2018). \n\nCurriculum learning methods revolve around identifying the optimal order of data samples to accelerate model convergence. Unlike dataset pruning, these methods do not remove data samples but emphasize the importance of the order in which data samples are presented during training. Curriculum learning methods define metrics to quantify the importance of each data sample, and the order of learning is determined based on these scores. These metrics can be crafted manually or generated automatically (Zhang and Pfister, 2021;de Mathelin et al., 2022). While handcrafted metrics may be task-specific, they often outperform automated metrics. Nevertheless, both types of metrics remain valuable for various tasks. It is worth noting that most dataset reduction techniques have primarily been explored in image classification tasks, with limited exploration in text-based tasks. \n\nIn this paper, we propose SwiftLearn, a technique that could be considered as a combination of dataset pruning and curiculum learning. The proposed algorithm is described in the following sections.",
            "score": 0.35365419522738994,
            "section_title": "Introduction",
            "char_start_offset": 3928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1573
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "271874686",
            "title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering",
            "text": "Level of improvements The highest accuracy gains in our study (1.81% and 1.44%) align with previous findings on curriculum learning. Xu et al. [37] reported a 1.3% improvement in natural language understanding tasks by using multiple teacher models to define question difficulty for BERT fine-tuning, while Maharana and Bansal [24] showed a 2% gain in common sense reasoning with RoBERTa fine-tuned on fixed and adaptive curricula. Our results are lower than Lee et al. [21], who reported gains of 3.28% and 1.73% on World Knowledge and Commonsense Reasoning benchmarks for Llama-13B using Interleaved Curriculum. \n\nTwo key factors may explain the difference. First, Lee et al. [21] used a broader curriculum, covering subjects from secondary to graduate level, while we focused on a specialised graduate-level medical curriculum, with a narrower scope of content. Second, their approach used Bloom's taxonomy to classify questions by distinct difficulty levels, whereas our dataset, with more semantically similar medical questions, exhibits subtler difficulty variations. These differences in curriculum scope and difficulty categorisation may account for the variation in performance gains. \n\nLimited generalisation Most previous studies demonstrated the effectiveness of curriculum learning using a single model, showing consistent improvements across several benchmarks [37,24,21]. However, these results are insufficient to extrapolate that curriculum learning generalises well to other models, even though it is often assumed in these studies. Our findings suggest that a strategy effective for one model does not generalise to others, emphasising the need for caution when generalising results and drawing conclusions from limited model-specific evidence. This limited generalisation across models and datasets may be due to variations in how different LLMs perceive question difficulty and inconsistencies in question categories across medical datasets. \n\nBenefits of LLM-defined labels Our results show that using LLM-generated difficulty modestly improves the performance of curriculum-based learning strategies. This aligns with previous studies, which found that language-model-ranked difficulty led to consistent accuracy gains in curriculum learning across benchmarks [24,37].",
            "score": 0.35354963614923995,
            "section_title": "Discussion",
            "char_start_offset": 12904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "250391006"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1379,
                    "end": 1382,
                    "matchedPaperCorpusId": "250391006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43798828125
        },
        {
            "corpus_id": "269921463",
            "title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus",
            "text": "We explore additional baselines based on the categorization produced by data cartography. Following the work of Swayamdipta et al. (2020), we fine-tune the Ro-BERT model on subsets of the training set, determined by the difficulty categories: easy-tolearn, ambiguous and hard-to-learn. We train one Ro-BERT model for each of the three categories of samples. In addition, we train a Ro-BERT model on easy-to-learn and ambiguous (E2L+A) samples, based on the intuition that hard-to-learn samples suffer from high rates of labeling noise, and should thus be discarded. \n\nRo-BERT + Length-CL. We employ a Ro-BERT model based on curriculum learning, a training paradigm proposed by Bengio et al. (2009). \n\nThe technique emulates the human learning process, which typically progresses from simpler to more complex concepts, and applies it to the training of machine learning models. The idea behind the technique is to expose the model to easier examples first, thus building a foundational understanding of the task, which will in turn allow it to accommodate more complex samples faster. Curriculum learning has received significant attention from researchers due to its vast applicability, as shown by Soviany et al. (2022). Following Nagatsuka et al. ( 2023), the considered baseline applies the curriculum with respect to text length. For a fair comparison with the conventional training paradigm, we ensure that the total number of training iterations is equal with those of the base model. We perform the curriculum training for half the number of total iterations. Subsequently, we continue training under the standard paradigm, using all available data for the remaining iterations. As for the baseline Ro-BERT, we introduce early stopping to prevent overfitting. We apply the same training procedure to all curriculum models described below. Ro-BERT + STS-CL. An intuitive way to introduce curriculum learning with respect to the NLI task is to use the semantic text similarity (STS) between sentences to measure difficulty. We thus add a baseline Ro-BERT model that uses curriculum learning based on STS. To calculate STS, we utilize the cosine similarity based on the fine-tuned Ro-BERT embeddings of the sentences.",
            "score": 0.3531844501571377,
            "section_title": "Models",
            "char_start_offset": 18987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 137,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 677,
                    "end": 697,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1199,
                    "end": 1220,
                    "matchedPaperCorpusId": "231709290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "204949786",
            "title": "Distribution Density, Tails, and Outliers in Machine Learning: Metrics and Applications",
            "text": "Machine learning (ML) is now applied to problems with sufficiently large datasets that it is difficult to manually inspect each training and test point. This drives interest in research that seeks to understand the dataset and the underlying data distribution. Potential uses of these techniques are numerous. On the one hand, they contribute to improving how ML is perceived by end users (e.g., one of the motivations behind interpretability efforts). On the other hand, they also help ML practitioners glean insights into the learning procedure. This surfaces the need for tools that enable one to (1) measure and characterize the contribution of each training point to the learning procedure and (2) explain the different failure modes observed on individual test points when the model infers.\n\nTowards this goal, prior work has investigated model interpretability, identifying training and test points that are prototypical (Kim et al., 2014), or applying influence functions to measure the contribution of individual training points to the final model (Koh & Liang, 2017). While defining precisely a prototype remains an open problem, a common intuitive definition of the notion is that prototypes should be \"a relatively small number of samples from a data set which, if well chosen, can serve as a summary of the original data set\" (Bien & Tibshirani, 2011). In addition to these two examples, there is a wealth of related efforts discussed below.\n\nIn this work, we take an orthogonal direction and show that rather than trying to identify a single metric or technique to identify \"prototypes\", simultaneously considering a variety of metrics can be more effective to discover properties of the training data. In particular, we introduce five metrics for measuring to what extent a specific point is well represented or an outlier in a dataset.\n\nWe explicitly do not define what we mean by well-represented or outlier specifically because we are interested in the interplay between different metrics that may fall under that definition. Indeed, we find that while the different metrics are highly correlated for most training and test inputs, their disagreement are highly informative.\n\nIn more detail, our metrics are based on adversarial robustness, retraining stability, ensemble agreement, and differentially-private learning. We demonstrate that in addition to supporting use cases previously studied in the literature (e.g., identifying prototypes)",
            "score": 0.353050866785338,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1339,
                    "end": 1364,
                    "matchedPaperCorpusId": "11541349"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.168212890625
        },
        {
            "corpus_id": "259243960",
            "title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
            "text": "Large Language Models (LLMs) have demonstrated superior performance in a myriad of NLP tasks [4,9,43,42,55,61].These models have captured vast amounts of knowledge from enormous and diverse corpora during pre-training.After instruction fine-tuning [10,44,2], they have demonstrated impressive capabilities in information-seeking question answering [65,26].Despite their remarkable performance, LLMs face several challenges.For example, they are susceptible to hallucinationsgenerating plausible yet ungrounded information-which can mislead users and affect content integrity [66,19,5].Additionally, they exhibit weaknesses in numerical reasoning, an essential skill in numerous real-life applications [14,36,41,28,51,13].These limitations highlight the need for techniques that can enhance LLMs' question-answering abilities.\n\nRecent research has shown that these issues can be mitigated by augmenting LLMs with external tools, such as retrieval augmentation [58,17], math tools [56,76,32], and code interpreters [13,63].For example, a Wolfram math plugin can enhance numerical reasoning [68], and a verified database can mitigate hallucinations by providing up-to-date fact-checked knowledge [49].However, existing evaluation methodologies struggle to distinguish whether the model is simply recalling pre-trained information or truly utilizing external tools for problem-solving [37].This challenge arises, in part, because the external data used for evaluation may have already been exposed to LLMs during the pre-training phase [53].This exposure can lead to a biased evaluation of LLMs' tool-use abilities, as the models could just use their ingrained knowledge and their reasoning abilities, bypassing the use of external tools.As a result, these evaluations cannot accurately reflect the true competency of the  models.We need a fair and explicit way to check if LLMs are really good at problem-solving with tools or if they are just using their memorized information.\n\nTo fill this gap, we introduce ToolQA, a question answering (QA) benchmark to evaluate LLMs' ability in using external tools for answering questions.",
            "score": 0.352795071549013,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 721
                },
                {
                    "start": 721,
                    "end": 825
                },
                {
                    "start": 827,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1386
                },
                {
                    "start": 1386,
                    "end": 1537
                },
                {
                    "start": 1537,
                    "end": 1734
                },
                {
                    "start": 1734,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1975
                },
                {
                    "start": 1977,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 252,
                    "end": 255,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 983,
                    "end": 986,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.090576171875
        },
        {
            "corpus_id": "208006818",
            "title": "MML: Maximal Multiverse Learning for Robust Fine-Tuning of Language Models",
            "text": "Recently, there has been an increasing number of studies suggesting the use of general language models, for improving natural language processing tasks (Dai and Le, 2015;Peters et al., 2018;Radford et al.;Howard and Ruder, 2018). Among the most promising techniques, the unsupervised pretraining approach (Dai and Le, 2015;Radford et al.) has emerged as a very successful method, that achieves state-of-the-art results on many language tasks, including sentiment analysis (Socher et al., 2013), natural language infer-ence (Williams et al., 2017) and similarity and paraphrase tasks (Dolan and Brockett, 2005;Cer et al., 2017). This approach incorporates a twophase training procedure. The first phase utilizes an unsupervised training of a general language model on a large corpus. The second phase applies supervision to fine-tune the model for a given task. \n\nMore lately, unsupervised pretraining models such as BERT (Devlin et al., 2018), XLNET (Yang et al., 2019) and RoBERTa (Liu et al., 2019), have achieved unprecedented performance, even exceeding human level of performance on some language tasks. For example, in the GLUE benchmark (Wang et al., 2018), BERT (Devlin et al., 2018) reported to achieve performance that exceeds human level on a few different datasets, such as QNLI (Rajpurkar et al., 2016), QQP (Chen et al., 2018) and MRPC (Dolan and Brockett, 2005). However, although the great progress achieved by these task-specific and dataset-specific models, it is not yet clear how well they can generalize to different tasks, and how robust they are when evaluating the same task on different datasets. \n\nThe most direct way to estimate the specificity of a learned model is by employing crossbenchmark experiments. These evaluations can be done by using datasets of the same task the model was specialized on (to measure robustness), or by utilizing datasets from different tasks (to measure generalization).",
            "score": 0.35225755030293515,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1928
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 170,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 305,
                    "end": 323,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 472,
                    "end": 493,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 583,
                    "end": 609,
                    "matchedPaperCorpusId": "16639476"
                },
                {
                    "start": 1350,
                    "end": 1376,
                    "matchedPaperCorpusId": "16639476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235107421875
        },
        {
            "corpus_id": "266977266",
            "title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks",
            "text": "Training on Easy Data? \n\nWe now examine how well models generalize from easy training data to hard test data. \n\nDesign. For each of our hardness measures, we test models on exclusively hard test data (according to that hardness measure), while varying whether they are finetuned on easy or hard data. 3 esults. Surprisingly, Llama-2-70b with ICL shows comparable generalization to hard test data regardless of whether it is fit to easy or hard data (Fig. 4). In fact, across all six hardness measures, the Supervision Gap Recovered is between 70% and 100%. These results are statistically significant, with CIs and p-values shown in Appendix Table 2. Interestingly, for ARC and MMLU, there is no difference in easy vs. hard generalization using ICL. Results are also robust across finetuning methods and additional hardness measures (Appendix Figs. 13,18). With QLoRA, for example, the SGR remains within 70%-100% for ARC, MMLU and StrategyQA. While GSM8k appears to exhibit worse easy-to-hard generalization, we note that easy-to-all generalization is actually equally good to hard-to-all generalization (see Fig. 11). Thus it seems like easy data provides surprisingly good supervision for LMs. These results contrast notably with past work in curriculum learning and compositional generalization (Bengio et al., 2009;Lake and Baroni, 2018). This is likely because models like Llama-2-70b have learned much more during pretraining than models commonly used in work on curriculum learning and compositional generalization. So, it would seem that finetuning these models on relatively small amounts of easy data successfully elicits the relevant task knowledge from the models in a way that is largely invariant to datapoint hardness.",
            "score": 0.3522303003489973,
            "section_title": "RQ2: Can We Do Well on Hard Data by",
            "char_start_offset": 17416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 109
                },
                {
                    "start": 112,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1734
                }
            ],
            "ref_mentions": [
                {
                    "start": 1299,
                    "end": 1320,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1320,
                    "end": 1342,
                    "matchedPaperCorpusId": "46761158"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298095703125
        },
        {
            "corpus_id": "216641819",
            "title": "Training Curricula for Open Domain Answer Re-Ranking",
            "text": "We proposed three weighting heuristics to train neural rankers using curriculum learning that boost performance when ranking answers on three datasets. Our proposed heuristics boost ranking performance through training samples weighting, without changing the sequence in which the data are presented to the model for training. Generally, the reciprocal rank (RECIP) and kernel density estimation (KDE) curricula were the most effective, although when working with inferred relevance labels with TREC CAR, the normalized score (NORM) curriculum was more effective. Although these gains were not always enough to achieve state-of-the-art performance, they were often able to approach the level of performance of larger or more complicated approaches (such as using BERT (large) or re-training BERT with a different pre-training objective). We experimentally showed that the convergence of the curriculum to equal weighting is important when manually-labeled test data are used, otherwise resulting in inferior effectiveness. Finally, we found that focusing on the easiest samples first (rather than the hardest samples) was also an important characteristic of this approach. Future work could explore alternative difficultly degradation functions or explore how well the method applies to other approaches, such as performing additional domain fine-tuning. It could also combine the weighting strategies with more intelligent sampling approaches for relevant and non-relevant training pairs. We note that our proposed difficulty heuristics may be an effective starting point for sampling strategies. Even with more effective sampling, our weighting approach may be beneficial for ensuring that 'easy' samples are ranked effectively. Another possible direction for future work could explore the use of self-paced learning techniques [19,20], allowing the model to learn which training samples characteristics make a samples easy or difficult.",
            "score": 0.35221692630022405,
            "section_title": "CONCLUSIONS AND FUTURE WORK",
            "char_start_offset": 40862,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1830,
                    "end": 1834,
                    "matchedPaperCorpusId": "9686483"
                },
                {
                    "start": 1834,
                    "end": 1837,
                    "matchedPaperCorpusId": "10891229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "265150191",
            "title": "Simple and Effective Input Reformulations for Translation",
            "text": "Our work can be viewed as a data efficiency technique for translation. Past works in translation have explored data augmentation (Sennrich et al., 2016;Fadaee et al., 2017), sample re-weighting (Shu et al., 2019;Ren et al., 2019;Gu et al., 2018), and curriculum learning (Kocmi and Bojar, 2017;Zhang et al., 2018;Platanios et al., 2019;Zhang et al., 2019;NLLB-Team et al., 2022). These approaches vary in effectiveness, are not generalizable, and introduce complexity into the training process. Curriculum learning approaches in particular are typically complicated and unsuccessful, because they are designed using intuition on how humans treat inputs, which may differ from how models treat inputs. In contrast, our input reformulations are simple and can be directly applied to any sequence-to-sequence task. \n\nPrevious work has explored prompting a frozen language model using manually curated prompts (Brown et al., 2020;Touvron et al., 2023;Petroni et al., 2019). Results are typically sensitive to the exact prompt used. This technique cannot be applied to larger corpora because it is limited by the number of examples that can feasibly fit into a single input context. Other works have explored finetuning with a fixed prompt without leveraging the target output as a part of the input (Radford et al., 2018(Radford et al., , 2019;;Dong et al., 2019;Devlin et al., 2019;Lewis et al., 2019;Sun et al., 2019;Liu et al., 2019;Clark et al., 2020;Yang et al., 2020;Raffel et al., 2020;Gao et al., 2021;Schick and Sch\u00fctze, 2021;au2 et al., 2021;Xue et al., 2021;He et al., 2021;Taori et al., 2023).",
            "score": 0.35205377868079857,
            "section_title": "Related work",
            "char_start_offset": 4478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1601
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 152,
                    "matchedPaperCorpusId": "15600925"
                },
                {
                    "start": 152,
                    "end": 172,
                    "matchedPaperCorpusId": "3291104"
                },
                {
                    "start": 271,
                    "end": 294,
                    "matchedPaperCorpusId": "26468344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07080078125
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "In this section, we extend the scope of pruning methods traditionally applied to BERT (Devlin et al., 2019) and evaluate their efficacy on larger language models (LLMs).Table 20 provides an overview of these pre-existing pruning methods, primarily utilized for BERT.A notable difference between these methods and our approach is their integration of pruning with the fine-tuning process.Additionally, BERT-specific pruning techniques typically focus on downstream task performance, in contrast to our aim of preserving the general language modeling capabilities of pre-trained LLMs.\n\nAdapting these methods for LLM pruning, we employ the pre-training auto-regressive loss as the metric to guide the pruning process.Our evaluation considers two scenarios: post-training pruning and post-training pruning followed by a constrained period of fine-tuning, limited to one day.The effectiveness of the pruning is determined using the metrics outlined in Table 20.In the post-training pruning scenario, these metrics are directly applied to the LLMs.The pruned models are then subjected to fine-tuning within the stated computational constraints.The results, summarized in Table 21, reveal that the traditional pruning methods, when adapted to LLMs, do not yield effective outcomes.This observation underscores the necessity for developing pruning techniques that are more suited to the unique characteristics of large-scale language models.",
            "score": 0.3520319155356405,
            "section_title": "E.3. Comparison with Previous Pruning Methods",
            "char_start_offset": 62704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 582
                },
                {
                    "start": 584,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 957
                },
                {
                    "start": 957,
                    "end": 1043
                },
                {
                    "start": 1043,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1434
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 107,
                    "matchedPaperCorpusId": "226096901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.525390625
        },
        {
            "corpus_id": "215746703",
            "title": "Curriculum Learning Strategies for IR",
            "text": "Curriculum Learning (CL) is motivated by the way humans teach complex concepts: teachers impose a certain order of the material during students' education. Following this guidance, students can exploit previously learned concepts to more easily learn new ones. This idea was initially applied to machine learning over two decades ago [8] as an attempt to use a similar strategy in the training of a recurrent network by starting small and gradually learning more difficult examples. More recently, Bengio et al. [1] provided additional evidence that curriculum strategies can benefit neural network training with experimental results on different tasks such as shape recognition and language modelling. Since then, empirical successes were observed for several computer vision [14,49] and natural language processing (NLP) tasks [36,42,60]. \n\nIn supervised machine learning, a function is learnt by the learning algorithm (the student) based on inputs and labels provided by the teacher. The teacher typically samples randomly from the entire training set. In contrast, CL imposes a structure on the training set based on a notion of difficulty of instances, presenting to the student easy instances before difficult ones. When defining a CL strategy we face two challenges that are specific to the domain and task at hand [14]: (1) arranging the training instances by a sensible measure of difficulty, and, (2) determining the pace in which to present instances-going over easy instances too fast or too slow might lead to ineffective learning. \n\nWe conduct here an empirical investigation into those two challenges in the context of IR. Estimating relevance-a notion based on human cognitive processes-is a complex and difficult task at the core of IR, and it is still unknown to what extent CL strategies are beneficial for neural ranking models. This is the question we aim to answer in our work. \n\nGiven a set of queries-for instance user utterances, search queries or questions in natural language-and a set of documents-for instance responses, web documents or passages-neural ranking models learn to distinguish relevant from non-relevant query-document pairs by training on a large number of labeled training pairs. Neural models have for some time struggled to display significant and additive gains in IR [53]. In a short time though, BERT [7] (released in late 2018) and its derivatives (e.g.",
            "score": 0.3519461241297904,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2321
                },
                {
                    "start": 2322,
                    "end": 2404
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 337,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 833,
                    "end": 836,
                    "matchedPaperCorpusId": "397556"
                },
                {
                    "start": 2316,
                    "end": 2320,
                    "matchedPaperCorpusId": "126166925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "In the first part of this section, we focus on the general curriculum learning solutions that have been tested in multiple domains. Two of the main works in this category are the papers that first formulated the vanilla curriculum learning and the self-paced learning paradigms. These works highly influenced the progress of easy-to-hard learning strategies and led to multiple approaches, which have been successfully employed in all domains and in a wide range of tasks. Bengio et al. (2009) introduce a set of easy-to-hard learning strategies for automatic models, referred to as curriculum learning (CL). The idea of presenting the examples in a meaningful order, starting from the easiest samples, then gradually introducing more complex ones, was inspired by the way humans learn. To show that automatic models benefit from such a training strategy, achieving faster convergence, while finding a better local minimum, the authors conduct multiple experiments. They start with toy experiments with a convex criterion in order to analyze the impact of difficulty on the final result. They find that, in some cases, easier examples can be more informative than more complex ones. Additionally, they discover that feeding a perceptron with the samples in increasing order of difficulty performs better than the standard random sampling approach or than a hardto-easy methodology (anti-curriculum). Next, they focus on shape recognition, generating two artificial data sets: Basic-Shapes and GeomShapes, with the first one being designed to be easier, with less variability in terms of shape. They train a neural network on the easier set until a switch epoch when they start training on the GeomShapes set. The evaluation is conducted only on the difficult data, with the curriculum approach generating better results than the standard training method. The methodology above can be considered an adaptation of transfer learning, where the network was pretrained on a similar, but easier, data set. Finally, the authors conduct language modeling experiments for predicting the best word which could follow a sequence of words in correct English. The curriculum strategy is built by iterating over Wikipedia and selecting the most frequent 5000 words from the vocabulary at each step. This vocabulary enhancement method compares favorably to conventional training. Still, their experiments are constructed in a way that enables the easy and the difficult examples to be easily separated. In practice, finding a way to rank the training examples can be a complex task",
            "score": 0.35187208307433787,
            "section_title": "Multi-Domain Approaches",
            "char_start_offset": 29136,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 473,
                    "end": 493,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66259765625
        },
        {
            "corpus_id": "253734400",
            "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
            "text": "Curriculum learning (CL) proposed by Bengio et al. (2009) provides performance improvements on a number of machine learning tasks. It mimics the learning process of humans by training models with samples in a more meaningful order, i.e., from the easy ones to the hard ones. Therefore, ranking training samples by difficulty lies in the core of CL, which is also the key challenge when it's applied to natural language generation (NLG) tasks.\n\nPrevious work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018;Kocmi and Bojar, 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020;Kumar et al., 2019;Shen and Feng, 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks. For * The corresponding author. example, summarization focuses more on generating concise outputs while style transfer emphasizes style changes. So the former should pay attention to the ratio between the lengths of the output and the input (the more compressed the more difficult), while the latter should focus on differences in style between the input and output (the more different the more difficult). Designing a comprehensive or universal scoring function is difficult or even impossible under this definition of CL.\n\nIn this paper, we propose an alternative to sample-wise CL, which we call in-sample CL (ICL). ICL re-orders the learning sequence within the sample. One particular ICL re-ordering strategy which we find effective is to predict the last few tokens given a long prefix first from the original output, and then gradually increase the number of tokens at the end while shortening the prefix, to create an easy-to-hard training order. Such a curriculum learning strategy focuses more on the difficulty of language generation itself, leading to a better generalization ability among tasks.\n\nActually, we are not the first to propose the idea of ICL. Liang et al. (2021) introduced the notion of \"token-wise curriculum learning(TCL)\". Illustrations",
            "score": 0.3518616325930283,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 641,
                    "end": 659,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 659,
                    "end": 681,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 784,
                    "end": 803,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 822,
                    "end": 842,
                    "matchedPaperCorpusId": "218470266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "225066701",
            "title": "A Comprehensive Survey on Curriculum Learning",
            "text": "In this section, we take a further step on the curriculum design by introducing automatic CL methods to break through the limits of predefined CL. A general comparison of predefined CL and automatic CL is presented in Table III. We summarize the four major methodologies for automatic CL: Self-Paced Learning (SPL), Transfer Teacher, RL Teacher, and Other Automatic CL. In predefined CL, the teacher designing the curriculum is a human expert, and the student getting trained by the curriculum is the machine learning model. To reduce the need for human teachers, the four methodologies take different ideas, which can be intuitively summarized in the bullets. Note that while SPL and Transfer Teacher methods are semi-automatic with automatic Difficulty Measurer and predefined Training Scheduler, the RL Teacher and Other Automatic CL methods are fully automatic and adopt the extended definition of CL as a sequence of data selection, example reweighting, or training criteria (Sec. II). \n\n\u2022 SPL methods let the student himself act as the teacher and measure the difficulty of training examples according to its losses on them. This strategy is analogous to the self-study of human students: one decides his/her own learning pace based on his/her current status. \n\n\u2022 Transfer Teacher methods invite a strong teacher model to act as the teacher and measure the difficulty of training examples according to the teacher's performance on them. Since the teacher model is pretrained and transfers its knowledge to measure example difficulty for student model training, we denote this strategy as Transfer Teacher. \u2022 RL Teacher methods adopt reinforcement learning (RL) models as the teacher to play dynamic data selection according to the feedback from the student. This strategy is the most ideal scene in human education, where the teacher and the student improve together through benign interactions: the student makes the biggest progress based on the tailored learning materials that the teacher selects for him, while the teacher also effectively adjusts her teaching strategy to teach better. \u2022 Other Automatic CL methods include various automatic CL strategies except for the above-mentioned. The works take different optimization techniques to automatically find the best curriculum for model training, including Bayesian Optimization, meta-learning, hypernetworks, etc. The curriculum in these methods often refers to a sequence of loss weights or even loss functions on data batches.",
            "score": 0.3514402572404825,
            "section_title": "C. Automatic CL",
            "char_start_offset": 37804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2377
                },
                {
                    "start": 2378,
                    "end": 2492
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455322265625
        },
        {
            "corpus_id": "273653865",
            "title": "Rethinking Data Synthesis: A Teacher Model Training Recipe with Interpretation",
            "text": "Instruction design, exemplified by OpenAI's approach with real-world user data (Ouyang et al., 2022), has become a key data curation technique in LLM post-training. However, the traditional approach of collecting human-generated instructions faces substantial limitations due to labor costs. \n\nRecent approaches have explored synthetic data generation using powerful teacher LLM models, primarily focusing on prompt-engineering methodologies (Taori et al., 2023;Honovich et al., 2023;Xu et al., 2023;Wang et al., 2023;Lee et al., 2023;Xu et al., 2024). They usually begin with a small seed pool of example tasks, gradually generating, filtering and refining new prompts. However, these approaches typically rely on standard instructionmasked supervised fine-tuning (SFT) models designed for general question-answering. Therefore, we argue that current models have key limitations: they prioritize solving problems accurately over generating novel ones, lack question-generationspecific design, and can generate contextually incomplete questions in chat formats. This motivates our core investigation: Should we train a specialized model specifically for data synthesis instead of the current post-training recipe, and if so, how? \n\nThis paper addresses this question by investigating two critical aspects that differentiate data synthesis from standard language model training: 1. The Role of Prompt Masking: We address a tiny yet long-ignored question in standard SFT: the impact of prompt masking. While traditional approaches mask prompts to improve response quality, we demonstrate that learning from prompts is crucial for generating better synthetic data.1 2. Training Data Optimization: We explore the counterintuitive finding that larger training sets don't always yield better results. Our research shows that carefully selecting a smaller subset of training data often produces more effective supplementary synthetic data. \n\nBuilding on these insights, we propose NOMAD (No Masking Data Synthesizer), a novel approach that specifically addresses these challenges. In particular, when only small size train samples are available, synthetic data generated by NOMAD outperforms baselines (i.e., using train set only) by 1.5% on average, with >4% gains in TriviaQA and >2% in GSM8K.",
            "score": 0.35137844703635185,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1932
                },
                {
                    "start": 1935,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 462,
                    "end": 484,
                    "matchedPaperCorpusId": "254853659"
                },
                {
                    "start": 518,
                    "end": 535,
                    "matchedPaperCorpusId": "264484010"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.097412109375
        },
        {
            "corpus_id": "270440133",
            "title": "ECBD: Evidence-Centered Benchmark Design for NLP",
            "text": "While benchmarking has long been seen as critical to gauging progress in natural language processing (NLP) and guiding model selection for downstream applications, assessing the quality of a benchmark remains a persistent challenge.Do benchmark measurements-most often in the form of numerical scores-provide meaningful insights about the evaluated models and their capabilities?How valid are these measurements?For what purposes are they useful?The field of NLP lacks a systematic way of reflecting on these important questions.At the same time, as NLP models are increasingly believed to be more performant and to exhibit a wider range of capabilities, evaluation in NLP has shifted from measuring model performance on a specific dataset for a single task to using large benchmarks that cover multiple tasks (e.g., GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), BIG-Bench (Srivastava et al., 2022), HELM (Liang et al., 2022)).These benchmarks are increasingly larger and more ambitious (e.g., HELM aims to \"assess language models in their totality\"), covering ever-growing numbers of tasks, datasets, and metrics aimed at measuring an increasing number of capabilities-i.e., abilities or behaviors that researchers believe the models exhibit or might exhibit.This trend further increases the complexity of assessing benchmark quality.\n\nSuch issues with assessing measurement quality do not only concern NLP practitioners.Researchers and practitioners in educational testing often face similar questions: do students' exam results provide meaningful insights about their ability in, for example, reading comprehension?Can these results be used to determine whether a student needs remedial classes?\n\nIn this work, we take inspiration from the Evidence-Centered Design (ECD) framework in educational testing (Mislevy et al., 2003).ECD views testing as the process of gathering evidence about students' abilities, and provides guidance on the creation, documentation, and validation of educational tests.",
            "score": 0.3509754580961078,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1271
                },
                {
                    "start": 1271,
                    "end": 1346
                },
                {
                    "start": 1348,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1709
                },
                {
                    "start": 1711,
                    "end": 1841
                },
                {
                    "start": 1841,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 853,
                    "end": 872,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 1818,
                    "end": 1840,
                    "matchedPaperCorpusId": "144723860"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1766357421875
        },
        {
            "corpus_id": "263134555",
            "title": "Qwen Technical Report",
            "text": "The size of data has proven to be a crucial factor in developing a robust large language model, as highlighted in previous research (Hoffmann et al., 2022;Touvron et al., 2023b). To create an effective pretraining dataset, it is essential to ensure that the data are diverse and cover a wide range Figure 2: Performance of GPT-4, GPT-3.5, the previous 13B SOTA, as well as QWEN-14B. We demonstrate the results on 12 datasets covering multiple domains, including language understanding, knowledge, reasoning, etc. QWEN significantly outperforms the previous SOTA of similar model sizes, but still lag behind both  of types, domains, and tasks. Our dataset is designed to meet these requirements and includes public web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.\n\nTo ensure the quality of our pretraining data, we have developed a comprehensive data preprocessing procedure. For public web data, we extract text from HTML and use language identification tools to determine the language. To increase the diversity of our data, we employ deduplication techniques, including exact-match deduplication after normalization and fuzzy deduplication using MinHash and LSH algorithms. To filter out low-quality data, we employ a combination of rule-based and machine-learning-based methods. Specifically, we use multiple models to score the content, including language models, text-quality scoring models, and models for identifying potentially offensive or inappropriate content. We also manually sample texts from various sources and review them to ensure their quality. To further enhance the quality of our data, we selectively up-sample data from certain sources, to ensure that our models are trained on a diverse range of high-quality content. In recent studies (Zeng et al., 2022;Aribandi et al., 2021;Raffel et al., 2020), it has been demonstrated that pretraining language models with multi-task instructions can enhance their zero-shot and few-shot performance. To further enhance the performance of our model, we have incorporated high-quality instruction data into our pretraining process. To safeguard the integrity of",
            "score": 0.3509754580961078,
            "section_title": "DATA",
            "char_start_offset": 3597,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1912,
                    "end": 1932,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18505859375
        },
        {
            "corpus_id": "273185541",
            "title": "Rule-based Data Selection for Large Language Models",
            "text": "The quality of training data significantly impacts the performance of large language models (LLMs). There are increasing studies using LLMs to rate and select data based on several human-crafted metrics (rules). However, these conventional rule-based approaches often depend too heavily on human heuristics, lack effective metrics for assessing rules, and exhibit limited adaptability to new tasks. In our study, we introduce an innovative rule-based framework that utilizes the orthogonality of score vectors associated with rules as a novel metric for rule evaluations. Our approach includes an automated pipeline that first uses LLMs to generate a diverse set of rules, encompassing various rating dimensions to evaluate data quality. Then it rates a batch of data based on these rules and uses the determinantal point process (DPP) from random matrix theory to select the most orthogonal score vectors, thereby identifying a set of independent rules. These rules are subsequently used to evaluate all data, selecting samples with the highest average scores for downstream tasks such as LLM training. We verify the effectiveness of our method through two experimental setups: 1) comparisons with ground truth ratings and 2) benchmarking LLMs trained with the chosen data. Our comprehensive experiments cover a range of scenarios, including general pre-training and domain-specific fine-tuning in areas such as IMDB, Medical, Math, and Code. The outcomes demonstrate that our DPP-based rule rating method consistently outperforms other approaches, including rule-free rating, uniform sampling, importance resampling, and QuRating, in terms of both rating precision and model performance.",
            "score": 0.3509754580961078,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "270764411",
            "title": "DataGen: Unified Synthetic Dataset Generation via Large Language Models",
            "text": "Difficulty Enhancement. Given that the dataset is produced by LLMs, the complexity of the generated data is occasionally insufficient to challenge LLMs as their capabilities evolve. To address this, and inspired by prior research [14,13], we implement several strategies to increase the data's difficulty. These strategies are designed to elevate the challenges faced by LLMs in processing and responding to the data. The applied policies include:  option cannot be paraphrased without altering its meaning, it should remain unchanged. (4) Adding A New Choice: Introduce a plausible but incorrect option to the existing choices to create ambiguity and require deeper understanding. \n\nGroup Checking. To mitigate the issue of high similarity among generated data items, a groupchecking mechanism is implemented to identify and eliminate duplicates. Specifically, we utilize OpenAI's text-embedding-ada-002 [59] to compute embeddings for all generated items. Let X = {x 1 , x 2 , . . . , x n } be the set of generated data items, and e i be the embedding of item x i computed via text-embedding-ada-002. We define the similarity matrix S where the element s ij is given by \n\n, representing the Euclidean distance between the embeddings of items x i and x j . Data items exhibiting a similarity exceeding a predefined threshold \u03b8 are filtered out to ensure diversity within the dataset. Formally, if s ij < \u03b8 for any pair (i, j), at least one of the items x i or x j is randomly removed from the final dataset. Figure 3: Human evaluation of overall quality assessment and enhancement. To thoroughly evaluate the effectiveness of UNIGEN, we have carefully selected four representative benchmark datasets: GSM8K [19], TruthfulQA [72], MMLU [73], and HellaSwag [74]. Each dataset uniquely contributes to language model assessment, covering dimensions from mathematical problemsolving and factual accuracy verification to extensive language understanding and commonsense reasoning.",
            "score": 0.3509754580961078,
            "section_title": "Post-Processing",
            "char_start_offset": 15477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 1735,
                    "end": 1739,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "159041722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "265610064",
            "title": "A machine learning approach towards SKILL code autocompletion",
            "text": "The selected models from Table 5 were evaluated on the test split of input-output pairs. Figure 10 shows the scores assigned to each model for each sample in the human evaluation study and the mean scores for each pair type. Function-completion pairs were given a score of 1 for every sample. The survey results gave mean scores of 4.4 and 4.2 out of 5 for the quality of the input prompt and output references, respectively, for the functioncompletion samples so the comprehensiveness of the function-completion pairs is most likely not the problem. \n\nThe results for comment-function and comment-code pairs were better for most of the models. Both T5-SKILL models performed poorly which supports the hypothesis that the custom SKILL dataset is too small to train a model from scratch. These results also suggest that the reason for the poor results on the function-completion pairs might be because the pre-trained models have been trained on a lot of NL and so this knowledge has transferred much better than the models have been able to learn the SKILL language. \n\nThe CodeT5 model 28 achieved the highest overall mean score of 2.27. The CodeTrans model 27 achieved the next-best mean score of 1.93. The T5-NL model 12 with supervised training achieved a mean score of 1.67, a better score than the T5-NL model not trained in a supervised manner and the CodeTrans model without self-supervised training. This suggests that both self-supervised and supervised training are beneficial. When asked to score whether the best outputs for each sample would be useful if the evaluator was tasked with writing SKILL code for the corresponding input prompts, the human evaluator gave scores of 1, 2, and 1 out of 5 for comment-function, comment-code, and function-completion pairs, respectively. This refutes the idea that the models trained on the custom SKILL dataset are ready to be deployed in a real-world setting to assist SKILL developers. \n\nThe two automatic evaluation metrics, BLEU using different tokenizers and lint IQ, were measured on the same samples from the human evaluation survey to calculate the correlation between them. Figure 11 shows that the correlation between the change in the lint IQ score correlates well with human scores compared to BLEU scores.",
            "score": 0.3509754580961078,
            "section_title": "Final Evaluation Results",
            "char_start_offset": 38492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1941
                },
                {
                    "start": 1944,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05511474609375
        },
        {
            "corpus_id": "265985194",
            "title": "Too Much Information: Keeping Training Simple for BabyLMs",
            "text": "Features like these and their different combinations are often used to measure text complexity and/or readability (Bengio et al., 2009;Spitkovsky et al., 2009;Cirik et al., 2016;Kocmi and Bojar, 2017;Zhang et al., 2018;Platanios et al., 2019;Chang et al., 2021). \n\nIn our experiments, we scale all these features to fit into the [0,1] interval (with MinMax scaler) and use their mean as our complexity measure. \n\nTo assess the role of data ordering along the complexity scale based on the measure above, we trained triples of minimally different models, keeping everything apart from the data ordering fixed: \n\n\u2022 Curriculum model: All training data is ordered by increasing complexity. \n\n\u2022 No-curriculum model: No particular order is imposed on the training data. \n\n\u2022 Reversed-curriculum model: Training data is ordered by decreasing complexity. \n\nAll models in this set of experiments are RoBERTa-base models trained following the twostage procedure described in Section 4.1 -first, the models are trained on context size 32, then the context is increased to 128. Unlike in other experiments, however, each of the stages was further divided into three consecutive phases: \n\n\u2022 Phase 1: The first 1/3 of the data is used in training, the other 2/3 are withheld. The curriculum model just sees the 'easiest' data here; the reversed-curriculum model sees the 'most difficult' portion; the baseline, no-curriculum model sees 1/3 of data without any particular selection; \n\n\u2022 Phase 2: Another 1/3 of the data is unlocked. Now all models are being trained on 2/3 of all training data. Both the curriculum model and the reversed-curriculum model now have access to the middle of the complexity range. \n\n\u2022 Phase 3: The final 1/3 of data is unlocked. Now all models are being trained on the whole range of complexity. The data-unlocking procedure above happens twice: first, on a small context size (32 tokens), and later when the context size is increased (128 tokens).",
            "score": 0.3509754580961078,
            "section_title": "Curriculum learning",
            "char_start_offset": 8402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 135,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 219,
                    "end": 242,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 242,
                    "end": 261,
                    "matchedPaperCorpusId": "231846815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359375
        },
        {
            "corpus_id": "266177072",
            "title": "Data Selection Curriculum for Abstractive Text Summarization",
            "text": "Abstractive Text Summarization (ATS) aims to generate concise summaries while preserving essential content. Recent studies (Liu et al., 2023b;Zhang et al., 2023;Liu et al., 2023a) have shown that Large Language Models (LLMs), including GPT-3.5 (Ouyang et al., 2022), can produce summaries more favored by human annotators compared to reference summaries from well-established datasets, such as CNN/DailyMail (Hermann et al., 2015). Additionally, Liu et al. (2023a) experimentally assert that contrastive learning based methods applied to smaller summarization models, like BART (Lewis et al., 2020), can deliver performance on par with LLMs. This finding renews the importance of training smaller models via contrastive learning, as it offers the advantage of reducing computational costs. However, ATS models are commonly trained using large-scale data that is randomly shuffled. \n\nThis study aims to explore the potential for optimizing ATS models through the strategic utilization of data selection and curriculum learning (Bengio et al., 2009) in conjunction with contrastive learning techniques. It is important to note that not all data is equally valuable, and the presence of redundant or even detrimental examples can impede the performance of ATS systems (Mohiuddin et al., 2022). Furthermore, the ordering in which data is presented during model training, as emphasized by curriculum learning principles, can have a significant impact on both the efficiency and effectiveness of the learning process. Consequently, there is a fundamental necessity to develop a scoring system that can accurately assess the learning difficulty of individual samples and furtherly can be used for data selection and curriculum learning. \n\nIn this study, we introduce a novel scoring system called the Data Selection Curriculum (DSC) score, which serves as a measure of learning difficulty. The DSC score considers two essential factors: the difficulty of enhancing the ATS model through a specific instance and the expected performance of the model on that particular instance. Instances on which the current model already exhibits promising performance and stands to benefit significantly from slight adjustments are assigned as a lower learning difficulty. This is because these instances have the potential to contribute more to the final performance.",
            "score": 0.3506161605087486,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2251
                },
                {
                    "start": 2252,
                    "end": 2347
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 142,
                    "matchedPaperCorpusId": "254685611"
                },
                {
                    "start": 244,
                    "end": 265,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1026,
                    "end": 1047,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1265,
                    "end": 1289,
                    "matchedPaperCorpusId": "247762191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "258967447",
            "title": "Test-Time Training on Nearest Neighbors for Large Language Models",
            "text": "We present a system for test-time training on nearest neighbors, based on a large-scale index as illustrated in Figure 1. Our distributed index can serve each nearest neighbor query to approximately 200 million vectors and 1TB of data in approximately one second on standard hardware. The vectors represent text embeddings of all training sequences in the Pile dataset (Gao et al., 2020). We evaluate our method on all 22 tasks for language modeling from the Pile benchmark. We focus on three causal language models of increasing size: a small GPT-2 model with 117M parameters, a large GPT-2 model with 774M parameters, and a GPT-Neo model with 1.3B parameters. We find that training for only one gradient iteration on as few as 50 neighbors reduces a normalized perplexity measurement, the bits per byte metric, by 20%. In fact, most of the gain is already realized after only 20 neighbors. When it comes to some tasks, especially code generation as in the pile github task, the bits per byte metric goes down by more than 60% during test-time training. See Figure 2. g p t 2 g p t 2 -t t t g p t 2 -l a r g e g p t 2 -l a r g e -t t  The GPT-Neo model was specifically trained to convergence on the entire Pile dataset. In contrast, GPT-2 was trained only on some subsets of the dataset, roughly pile pile-cc, but not on others such as pile github. Therefore, we can see how TTT-NN on a model that was not pre-trained on a specific Pile task stacks up to a model that was. We see that the improvements due to test-time training on tasks unseen during training, such as pile github, can be dramatic. TTT-NN still helps on seen tasks, such as pile-cc, but the improvements are more moderate. \n\nOur results suggest that test-time training can increase the effective capacity of a model. This comes at the obvious cost of increased inference time, like other retrieval augmented techniques.",
            "score": 0.35051895962609847,
            "section_title": "OUR CONTRIBUTIONS",
            "char_start_offset": 1699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1888
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336669921875
        },
        {
            "corpus_id": "246652398",
            "title": "Transferable Student Performance Modeling for Intelligent Tutoring Systems",
            "text": "Student performance models estimate a student's ability to solve different questions based on sequential log data that describes their prior interactions with the system. The student proficiency estimates produced by such performance models are a key component, which allows the ITS to adapt to each student's personal needs as they go through the curriculum [12]. In the literature there are three major categories of student performance modeling techniques: (i) Markov process based inference, (ii) logistic regression and (iii) deep learning based approaches. Markov process based techniques, such as Bayesian Knowledge Tracing (BKT) [11] and BKT+ [21], are well established and can for example be found in the Cognitive Tutor [25]. Most probabilistic approaches determine a student's proficiency level by performing inference in a two state Hidden Markov Model -one state to represent mastery and one for non-mastery. Logistic regression models rely on a set of manually specified features which summarizes the student's interaction sequence. Given an input vector with feature values, the regression based performance model estimates the probability that the student is proficient in a certain question or KC. Some approaches in this class are IRT [40], PFA [36], DAS3H [9], Best-LR [16] and its recent extension Best-LR+ [42]. Deep learning based approaches take as input the same interaction sequence data, but unlike logistic regression techniques can learn suitable features on their own without requiring human feature engineering. Deep learning models benefit from large-scale training data and might in the future also include additional video and text data into their performance predictions. As of today, BKT and logistic regression models are still competitive with deep learning based approaches in multiple domains [16,21,42]. Two comprehensive surveys on recent deep learning based performance models are provided by Liu et al. [26] and Sarsa et al. [41]. \n\nImportantly, all of the above mentioned student performance modeling approaches rely on course-specific parameters (e.g., parameters that represent the difficulty of individual questions and KCs in the target course) which need to be learned from target course data. This makes these models inapplicable in our coldstart setting where a new course is first introduced and there is no training data available.",
            "score": 0.35041179917534004,
            "section_title": "Student Performance Modeling",
            "char_start_offset": 8573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1973
                },
                {
                    "start": 1976,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "14826104"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "19228797"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "3587872"
                },
                {
                    "start": 1263,
                    "end": 1267,
                    "matchedPaperCorpusId": "16644912"
                },
                {
                    "start": 1275,
                    "end": 1278,
                    "matchedPaperCorpusId": "155099780"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "226253566"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "237420476"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "226253566"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "3587872"
                },
                {
                    "start": 1839,
                    "end": 1842,
                    "matchedPaperCorpusId": "237420476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10894775390625
        },
        {
            "corpus_id": "273821612",
            "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
            "text": "We conduct extensive evaluations of Hunyuan-Large to demonstrate its effectiveness. The following experiments concentrate on our pre-trained language model (in Sec. 4.1) and post-trained language model (in Sec. 4.2) on various tasks in Chinese and English, including math and reasoning, code, reading comprehension, commonsense, long context, and aggregated task, etc., where Hunyuan-Large achieves excellent performance among tasks in both pre-training and post-training.",
            "score": 0.35017963846545797,
            "section_title": "Model Evaluations",
            "char_start_offset": 30302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 472
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07781982421875
        },
        {
            "corpus_id": "270620248",
            "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
            "text": "To address this, our survey provides a fine-grained review of data selection methods for instruction fine-tuning LLMs, rethinking existing approaches, proposing a unified comparison method, and outlining key trends and challenges in the field. \n\nWe begin by reviewing existing data selection techniques, organizing them within a three-stage scheme based on the key components of the data selection pipeline: feature extraction, criteria design, and selector evaluation (Figure 2). In the feature extraction stage (Section 2), we categorize methods into three types based on the form of the candidate data: human-designed features, model-oriented features, and raw text. In the criteria design stage (Section 3), we categorize methods based on the source of the sample quality label into two groups: internal information and external information. The latter is further divided into methods obtaining criteria from model preference or sample influence. In the selector evaluation stage (Section 4), we outline three key aspects to reliably evaluate a selector's effectiveness: candidate datasets, counterpart models, and evaluation metrics. \n\nWe also introduce a unified comparison method for evaluating existing works, incorporating both ratio-based efficiency indicators and ranking-based feasibility indicators (Section 5). Specifically, we first construct a quantitative comparison of twodimensional efficiency based on the performance improvement ratio (PIR) and selected dataset fraction (SDF), aligning them through the efficiency curve assumption, effectively addressing the chal-lenge of comparing different methods under inconsistent configurations. Then, we consider the feasibility of the method from the perspectives of flexibility and simplicity indicators. It qualitatively ranks existing models by manually considering algorithm complexity and reproducibility (the number of training models, algorithm steps, and opensource availability), as well as their transferability and scalability (dependence on data and models). \n\nFinally, we discuss the main trends and challenges faced for data selection. We first sort out the existing works chronologically from three aspects (Candidate Dataset, Quality Measurement, and Selected Feature) to grasp the current research focus (Section 6). We then point out the most important open question (How can we design effective sample quality measurement for data selection?)",
            "score": 0.350175248426279,
            "section_title": "Introduction",
            "char_start_offset": 1721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 2034
                },
                {
                    "start": 2037,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2297
                },
                {
                    "start": 2298,
                    "end": 2425
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3115234375
        },
        {
            "corpus_id": "267522839",
            "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
            "text": "Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022;2023;Anthropic, 2023;Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023;Wang et al.;Mukherjee et al., 2023;Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. \n\nMany real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: \n\nGiven just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? \n\nWe approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020;Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020;Han et al., 2023), we design an optimizer-aware approach to select such data.",
            "score": 0.350046966156849,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 116,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 805,
                    "end": 824,
                    "matchedPaperCorpusId": "259108263"
                },
                {
                    "start": 1637,
                    "end": 1662,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1662,
                    "end": 1680,
                    "matchedPaperCorpusId": "256627727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "234338422",
            "title": "Self-Guided Curriculum Learning for Neural Machine Translation",
            "text": "construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derives criterion from the NMT model in the training process. And according to different way of scheduling the curriculum, these difficulty criteria are apply to either fixed schedule (Cirik et al., 2016) or dynamic one (Platanios et al., 2019;Liu et al., 2020;Xu et al., 2020;Zhou et al., 2020b).\n\nA well-trained NMT model learns an optimal probability distribution mapping sentences from source language to target language, which is expected to be capable of recovering training labels . However if we test on the training set, we can observe inconsistent predictions against the target reference sentences, reflecting the discrepancy between the model distribution and the empirical distribution of training corpus, as Figure 1 illustrated. For a training example, high recovery degree between prediction and target reference sentence means it's easier to be masted by the NMT model, while low recovery degree means it's more difficult (Ding and Tao, 2019;. Taking recovery degree as the difficulty criterion, we propose a CL strategy to schedule curriculum learning with a well-trained vanilla NMT model. We put forward an analogy of this method that a person can schedule a personal and effective curriculum after skimming over the whole textbook, namely self-guided curriculum.\n\nIn this work, we cast recovery degree of each training example as its learning difficulty, enforcing an NMT model to learn from examples with higher recovery degree to the lower ones, and we analyze the coordination of this criterion with fixed and dynamic curriculum schedules. We conduct experiments on widely-used benchmarks, including WMT14 En-De and WMT17 Zh-En. Experimental results demonstrate that our proposed self-guided CL strategy can boost the performance of an NMT model against strong baseline Transformer.",
            "score": 0.3498998111192519,
            "section_title": "Introduction",
            "char_start_offset": 1972,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 132,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 132,
                    "end": 149,
                    "matchedPaperCorpusId": "215415842"
                },
                {
                    "start": 149,
                    "end": 165,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 192,
                    "end": 212,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 214,
                    "end": 230,
                    "matchedPaperCorpusId": "227227757"
                },
                {
                    "start": 451,
                    "end": 475,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 475,
                    "end": 492,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 492,
                    "end": 508,
                    "matchedPaperCorpusId": "227227757"
                },
                {
                    "start": 508,
                    "end": 527,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 1170,
                    "end": 1190,
                    "matchedPaperCorpusId": "195767109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "272524329",
            "title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications",
            "text": "The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in telecommunications remain limited, often relying on general-purpose models that lack domain-specific specialization. This lack of specialization results in underperformance, particularly when dealing with telecommunications-specific technical terminology and their associated mathematical representations. This paper addresses this gap by first creating and disseminating Tele-Data, a comprehensive dataset of telecommunications material curated from relevant sources, and Tele-Eval, a large-scale question-and-answer dataset tailored to the domain. Through extensive experiments, we explore the most effective training techniques for adapting LLMs to the telecommunications domain, ranging from examining the division of expertise across various telecommunications aspects to employing parameter-efficient techniques. We also investigate how models of different sizes behave during adaptation and analyze the impact of their training data on this behavior. Leveraging these findings, we develop and open-source Tele-LLMs, the first series of language models ranging from 1B to 8B parameters, specifically tailored for telecommunications. Our evaluations demonstrate that these models outperform their general-purpose counterparts on Tele-Eval and telecommunications-related literature tasks while retaining their previously acquired capabilities, thus avoiding the catastrophic forgetting phenomenon.",
            "score": 0.3497247407836217,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373046875
        },
        {
            "corpus_id": "276259405",
            "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
            "text": "In this paper, we introduced a novel approach for detecting overfit to benchmarks datasets in LLMs by applying parametric transformations to these datasets. Our method revealed that many models rely heavily on surface features of public test sets, leading to significant performance drops when these features are altered. This finding underscores a critical insight: what appears to be robust performance may, in fact, be largely driven by memorization rather than true generalization. \n\nWe demonstrated the effectiveness of our approach across multiple LLM families. Notably, larger models tend to exhibit more pronounced performance declines under perturbation, while certain models (such as Llama) show greater stability. These observations suggest that training strategies and architectural choices play a significant role in mitigating overfitting, prompting a necessary rethinking of how we evaluate and benchmark LLMs. \n\nBy providing a practical, dataset-agnostic framework, our work equips the community with a powerful tool to uncover overfitting and to drive the development of benchmarks that better capture genuine generalization. Incorporating these parametric transformations into the evaluation process not only exposes hidden vulnerabilities in current LLMs but also suggests a way for the creation of more resilient models that can adapt to the evolving challenges of language tasks. While C-BOD serves as a promising framework for detecting overfitting in LLMs and has successfully identified overfitting in most evaluated models, it remains subject to several limitations. First, our approach primarily targets textual rephrasings that preserve semantic content. Consequently, it may overlook deeper forms of overfitting, such as factual inaccuracies or logical inconsistencies, which may require more specialized probing techniques. Moreover, incorporating \u00b5-based transformations into the training or fine-tuning loop can significantly increase computational cost. Iteratively rephrasing large datasets and retraining with multiple \u00b5 values imposes a heavy resource burden, which may not be feasible for LLMs or under restricted computational budgets. Future work should investigate more lightweight or partial-integration strategies. In summary, while C-BOD provides an effective means of detecting surface-level overfitting, further advancements are necessary to enhance its efficiency, scalability, and ability to capture more nuanced forms of model overfitting.",
            "score": 0.34959182199373057,
            "section_title": "Conclusion",
            "char_start_offset": 21924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2486
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363525390625
        },
        {
            "corpus_id": "2300724",
            "title": "Semi-Supervised Model Training for Unbounded Conversational Speech Recognition",
            "text": "There are many approaches to building speech training sets, including acoustic data perturbation and data synthesis [11]. Our survey of the literature will be restricted to unsupervised and semi-supervised approaches to corpora assembly. \n\nGoogle takes advantage of their large scale in constructing a training set for their Voice Search and Voice Input tasks for low-resource languages such as Brazilian Portuguese, Italian, Russian and French [10]. Their unsupervised approach makes use of a slow but accurate decoder, confidence scores, transcript length and transcript flattening heuristics to select the utterances for acoustic modeling. \n\nIn conjunction with owner-uploaded transcripts, Youtube apply \"island of confidence\" filtering heuristics to generate additional semi-supervised training data for the deep neural network (DNN) based acoustic model (AM) driving their closed captions feature [16]. \n\nKapralova et al. and Yu et al. [10][29] train acoustic models on a Mandarin language Broadcast News (BN) and Broadcast Conversation (BC) dataset created with semisupervised techniques. Due to the prevalence of English loan words and code-switching, data selection starts with a dual Mandarin-English language classifier, followed by the computation of utterance and word-level decoder confidence scores for the Mandarin-only utterances. \n\nRagni et al. [21] use a semi-supervised system to build corpora for low-resource languages Zulu and Assamese task, using weighted word-confusion-network confidences for data selection. \n\nLi et al. [15] employ semi-supervised methods to construct a Mandarin training corpus based on a Chinese television spoken lectures series, using conditional random fields (CRF) for confidence estimation instead of the raw ASR decoder confidence measure. \n\nEnarvi et al. [6][12] tackle a conversational Finnish language ASR task with a novel semi-supervised approach to training text selection. In lieu of adding new transcribed candidate utterances to the corpus based on low in-domain LM perplexity, they score utterances by the decrease in indomain perplexity when the utterance is removed from the set of candidate utterances.",
            "score": 0.34955843200097925,
            "section_title": "Building ASR Training Corpora",
            "char_start_offset": 5009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "7360763"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "5917427"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "12510405"
                },
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "5917427"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "16992423"
                },
                {
                    "start": 1546,
                    "end": 1550,
                    "matchedPaperCorpusId": "17559102"
                },
                {
                    "start": 1807,
                    "end": 1810,
                    "matchedPaperCorpusId": "12904215"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1795654296875
        },
        {
            "corpus_id": "238744333",
            "title": "Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese",
            "text": "Designing effective criteria for language modeling is one of the major topics in training pre-trained models, which decides how the model captures the knowledge from large-scale unlabeled data. Recent studies have investigated denoising strategies (Raffel et al., 2020;Lewis et al., 2020), model architecture (Yang et al., 2019), and auxiliary objectives (Lan et al., 2020;Joshi et al., 2020) to enhance the model capacity in pre-training. However, the cutting-edge researches mainly focus on English; there are a few studies in other languages like Chinese (Wei et al., 2019;Cui et al., 2020;Zhang et al., 2021b;Zeng et al., 2021). Besides, the application requirements in specific domains, e.g., financial analysis and multimodal tasks, further urge the development of effective Chinese pre-trained models. \n\nTo the end of efficiency, recent studies have investigated knowledge distillation (Sanh et al., 2019;Jiao et al., 2020;Wang et al., 2020) and model compression techniques (Gordon et al., 2020;Shen et al., 2020;Xu et al., 2020a). However, they are not optimal for real-world applications. Knowledge distillation methods train a light model with the guidance of a largescale teacher model, which requires two stages of training, and training a teacher model still consumes massive computing resources. Similarly, model compression aims to train a simplified and optimized model from the original one without significantly diminished accuracy. The widely-used techniques include parameter sharing (Lan et al., 2020), module replacement (Xu et al., 2020a), pruning (Gordon et al., 2020), and quantization (Shen et al., 2020). Such a line of methods still needs abundant training. Also, these methods suffer from dramatic changes in the model architecture, so that it would be hard for easy real-world practice as it is incompatible with commonly deployed frameworks like the Transformers toolkit (Wolf et al., 2020).",
            "score": 0.34946020030049046,
            "section_title": "Introduction",
            "char_start_offset": 1786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 269,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 269,
                    "end": 288,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 309,
                    "end": 328,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 355,
                    "end": 373,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 373,
                    "end": 392,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 576,
                    "end": 593,
                    "matchedPaperCorpusId": "216641856"
                },
                {
                    "start": 593,
                    "end": 613,
                    "matchedPaperCorpusId": "227238757"
                },
                {
                    "start": 893,
                    "end": 912,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 912,
                    "end": 930,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 982,
                    "end": 1003,
                    "matchedPaperCorpusId": "211171709"
                },
                {
                    "start": 1003,
                    "end": 1021,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1021,
                    "end": 1038,
                    "matchedPaperCorpusId": "211066200"
                },
                {
                    "start": 1505,
                    "end": 1523,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1544,
                    "end": 1562,
                    "matchedPaperCorpusId": "211066200"
                },
                {
                    "start": 1572,
                    "end": 1593,
                    "matchedPaperCorpusId": "211171709"
                },
                {
                    "start": 1612,
                    "end": 1631,
                    "matchedPaperCorpusId": "202565587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0701904296875
        },
        {
            "corpus_id": "269982256",
            "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
            "text": "For all our quantitative counterfactual experiments, we project gradients onto a low-dimensional space using LOGRA with k i = k o = 128.We used the same experimental setup, including the configurations for the baseline data valuation techniques, from Park et al. [42] and Bae et al. [2].We used one A100 GPU with 80GB VRAM for all our counterfactual evaluation experiments.For model training, we used hyperparameters in Brittleness Test.For classification tasks, we first selected 100 correctly classified test examples when the model is trained on the full dataset (across all 5 random seeds).Then, for each test example x te , we identified the top-k influential data points using the data valuation algorithm, removed these training data points, retrained the model, and examined if this removal causes misclassification of x te on average (across 3 random seeds).In Figure 4, we reported the fraction of test examples (out of 100) that get misclassified after removing at most k training data points.For the language modeling task, we selected the 50 test sequences, obtained the top influential training sequences using the data valuation method, and reported the mean test perplexity after removing the top-k influential sequences and retraining the model.\n\nLinear Datamodeling Score (LDS).We measured LDS by generating 100 data subsets of size\n\nFor each data subset, we retrained the model 10 times for FashionMNIST, 20 times for CIFAR-10, and 5 times for WikiText to construct the ground truth.The LDS results in Figure 4 show the mean and standard deviation of LDS obtained from 5 distinctly trained models.A more detailed description of the LDS evaluation can be found in Park et al. [42].",
            "score": 0.3494304429435363,
            "section_title": "C.1 Quantitative Counterfactual Experiments",
            "char_start_offset": 57842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 287
                },
                {
                    "start": 287,
                    "end": 373
                },
                {
                    "start": 373,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 594
                },
                {
                    "start": 594,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 1004
                },
                {
                    "start": 1004,
                    "end": 1262
                },
                {
                    "start": 1264,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1350
                },
                {
                    "start": 1352,
                    "end": 1502
                },
                {
                    "start": 1502,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1699
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "257757261"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "257757261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11279296875
        },
        {
            "corpus_id": "220047761",
            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
            "text": "Neural machine translation (NMT) has advanced the state-of-the-art on various translation tasks (Hassan et al., 2018;Chen et al., 2018). A wellperformed NMT is trained using an end-to-end framework (Sutskever et al., 2014) that profits from large-scale training corpus and various optimization tricks (Ott et al., 2018;Xu et al., 2019;Li et al., 2020). These techniques boost the translation quality, in the meanwhile, leading to massive hyper-parameters to be tuned and expensive development costs (Popel and Bojar, 2018). Recent studies (Zhang et al., 2018(Zhang et al., , 2019;;Platanios et al., 2019;Liu et al., 2020) have proven that feeding training examples in a meaningful order rather than considering them randomly can accelerate the model Figure 1: The change of confidence in an area during the learning. Humans (red) experience the process of overconfidence\u21d2despair\u21d2enlightenment (Dunning-Kruger Curve), while prior work that exploits CL in NMT assumes a monotonically increased curve (green, Platanios et al., 2019). Interestingly, our model automatically draws a similar tendency as humans (blue). \n\nconvergence thus reducing the computational cost. Such methods refer to curriculum learning (CL, Bengio et al., 2009), in which a model is taught as a human from simple concepts to complex ones. \n\nThere exists two open problems in the integration of CL with NMT, i.e. the assessment of data difficulty and the programme of learning schedule. Considering the former, prior studies (Kocmi and Bojar, 2017;Platanios et al., 2019) intuitively treat human linguistic knowledge, e.g. either sentence length or word rarity, as the measure of difficulty. Nevertheless, each linguistic feature merely considers an aspect of sentences which fails to fully cope with the data difficulty for a model (Jiang et al., 2015). For the latter, existing methods pre-define the duration of curriculum based on an assumption that the model confidence monotonically increases with the training (Zhang et al., 2018(Zhang et al., , 2019)).",
            "score": 0.3492623223729793,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 222,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 301,
                    "end": 319,
                    "matchedPaperCorpusId": "44131019"
                },
                {
                    "start": 319,
                    "end": 335,
                    "matchedPaperCorpusId": "196206239"
                },
                {
                    "start": 335,
                    "end": 351,
                    "matchedPaperCorpusId": "208248001"
                },
                {
                    "start": 499,
                    "end": 522,
                    "matchedPaperCorpusId": "4556964"
                },
                {
                    "start": 558,
                    "end": 581,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 581,
                    "end": 604,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1212,
                    "end": 1232,
                    "matchedPaperCorpusId": "14386564"
                },
                {
                    "start": 1518,
                    "end": 1541,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1803,
                    "end": 1823,
                    "matchedPaperCorpusId": "10891229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "216641819",
            "title": "Training Curricula for Open Domain Answer Re-Ranking",
            "text": "We already observed in Figure 3 that when using a training curriculum, ranking performance not only peaks higher sooner, but also leaves the model in a better starting point for when all samples are weighted equally. However, an important question remains: Is it important to train with equal weight for all samples or can the difficulty weights be used exclusively? To this end, we perform a test that forgoes the curriculum convergence parameter m, directly using D(\u00b7) as the training sample weight, regardless of training iteration (i.e., m = \u221e, or equivalently W = D instead of Eq. 3).\n\nWe report the performance for this experiment on each dataset for each top-performing curriculum in Table 6 (m = \u221e setting). We observe that for all models on the TREC DL and ANTIQUE datasets, this approach leads to a drop in ranking effectiveness, suggesting that it is important to eventually perform equal sample weighting. Intuitively, this is important because if easy samples are always weighted higher than difficult samples, the model will be hindered in learning the more complicated function to rank difficult samples. Curiously, for TREC CAR, this setting sometimes leads to improved ranking effectiveness (though not a statistically significant improvement). One possible explanation is that in situations where weak labels are used (rather than human-judged labels from top retrieved results), it may be better to always apply the weighting, as some inferred positive labels may be too distant from what the model will typically encounter at inference time.\n\nTo answer RQ3 (whether shifting to difficult samples is important), we find that it is indeed beneficial to use our proposed weighting technique given in Eq. 3, rather than always applying the difficulty weighting when using manually-assessed relevance labels.",
            "score": 0.3489935697620879,
            "section_title": "End of curriculum evaluation",
            "char_start_offset": 37551,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "201070342",
            "title": "Transductive Auxiliary Task Self-Training for Neural Multi-Task Models",
            "text": "Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995;Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. \n\nIn multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (S\u00f8gaard and Goldberg, 2016;Ruder et al., 2019;Lin et al., 2019). One of the few examples where multitask learning is combined with other methods is the semi-supervised approach by Chao and Sun (2012), where main task labels are assigned to unlabelled instances which are then added to the main task dataset. However, to the best of our knowledge, no one has applied self-training to label additional instances with auxiliary task labels.",
            "score": 0.34898698236627135,
            "section_title": "Related Work",
            "char_start_offset": 11991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1301
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 89,
                    "matchedPaperCorpusId": "7464925"
                },
                {
                    "start": 127,
                    "end": 143,
                    "matchedPaperCorpusId": "1487550"
                },
                {
                    "start": 159,
                    "end": 181,
                    "matchedPaperCorpusId": "8066499"
                },
                {
                    "start": 345,
                    "end": 361,
                    "matchedPaperCorpusId": "1487550"
                },
                {
                    "start": 361,
                    "end": 381,
                    "matchedPaperCorpusId": "1164969"
                },
                {
                    "start": 444,
                    "end": 463,
                    "matchedPaperCorpusId": "406684"
                },
                {
                    "start": 595,
                    "end": 616,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 863,
                    "end": 891,
                    "matchedPaperCorpusId": "16661147"
                },
                {
                    "start": 891,
                    "end": 910,
                    "matchedPaperCorpusId": "115985550"
                },
                {
                    "start": 1044,
                    "end": 1063,
                    "matchedPaperCorpusId": "10688474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1925048828125
        },
        {
            "corpus_id": "268793544",
            "title": "Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning",
            "text": "Data augmentation is one of the regularization strategies for the training of deep learning models, which enhances generalizability and prevents overfitting, leading to performance improvement. Although researchers have proposed various data augmentation techniques, they often lack consideration for the difficulty of augmented data. Recently, another line of research suggests incorporating the concept of curriculum learning with data augmentation in the field of natural language processing. In this study, we adopt curriculum data augmentation for image data augmentation and propose colorful cutout, which gradually increases the noise and difficulty introduced in the augmented image. Our experimental results highlight the possibility of curriculum data augmentation for image data. We publicly released our source code to improve the reproducibility of our study.",
            "score": 0.34895545743176315,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338623046875
        },
        {
            "corpus_id": "264825487",
            "title": "Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery",
            "text": "Curriculum learning (CL) is a learning procedure where the training data's complexity gradually increases during training instead of randomly choosing the training data. Since it closely resembles how people actually learn, this approach to train deep learning models is natural. This paper focuses on developing a dynamic curriculum at the task and class level (explained in Section 3.4). So our most significant contribution to the paper is in this area. Curriculum learning can be divided into several subfields: vanilla CL, self-paced learning (SPL), self-paced CL (SPCL), teacher-student CL, and implicit CL. \n\nIn the early works of vanilla curriculum learning, (Bengio et al., 2009) suggested curriculum learning as a way to increase the entropy of training distributions and apply to geometric shapes and language modeling gradually increasing the number of complex geometric shapes and frequent words. In Spitkovsky et al. (2010), sentence length is gradually increased as a measure of difficulty. Results in these initial methods were successful and gave the green light to CL. However, these methods require difficulty to be priorly known and hand-picked. Self-paced learning does not require difficulty to be known a priori and can be calculated during training. In this way, the difficulty is periodically calculated during training, changing the samples' order in the process (Kumar et al., 2010). Self-paced Curriculum Learning combines both prior and measurement during training (Jiang et al., 2015). Self-paced learning is applied to object detection (Soviany et al., 2021;Zhang et al., 2019), semantic segmentation (Pan et al., 2021;Zhang et al., 2020;Peng et al., 2021), and visual question answering (Yuan et al., 2021b). \n\nTeacher-student CL is another method where a pretrained teacher model chooses the curriculum for a student model. This method was used by Hacohen & Weinshall (2019) to lay foundational work on common CL concepts such as pacing and difficulty with different case studies. Later research, however, indicates that it is mostly used in reinforcement learning (Matiisen et al., 2020), (el-Bouri et al., 2020;Jiang et al., 2018).",
            "score": 0.34889833263971437,
            "section_title": "Curriculum Learning",
            "char_start_offset": 14367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1740
                },
                {
                    "start": 1743,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 667,
                    "end": 687,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 913,
                    "end": 937,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 1389,
                    "end": 1409,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1567,
                    "end": 1589,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 1589,
                    "end": 1608,
                    "matchedPaperCorpusId": "52114770"
                },
                {
                    "start": 1632,
                    "end": 1650,
                    "matchedPaperCorpusId": "231683968"
                },
                {
                    "start": 1650,
                    "end": 1669,
                    "matchedPaperCorpusId": "218901076"
                },
                {
                    "start": 1669,
                    "end": 1687,
                    "matchedPaperCorpusId": "236493246"
                },
                {
                    "start": 1719,
                    "end": 1739,
                    "matchedPaperCorpusId": "238750638"
                },
                {
                    "start": 1881,
                    "end": 1907,
                    "matchedPaperCorpusId": "102350936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382080078125
        },
        {
            "corpus_id": "273025893",
            "title": "Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective",
            "text": "The efficacy of large language models (LLMs) is extensively influenced by both the volume and quality of the training data, as established by the widely acknowledged scaling laws (Kaplan et al., 2020). Given the inherent sparsity of data available during the post-training phases of LLMs, synthetic data plays a critical role, particularly during fine-tuning and alignment processes. Over the past decades, the LLM community has increasingly employed synthetic data to augment training in scenarios where real data is scarce. As of September 2024, there are over 1,000 datasets labeled as \"synthetic\" on the Hugging Face platform 1 . Several leading-edge large language models, including LLaMA (Dubey et al., 2024), Falcon (Almazrouei et al., 2023), Qwen (Bai et al., 2023), and GPT-4 (OpenAI et al., 2024), have also reported utilizing synthetic data during their post-training stages. These instances underscore the pivotal role of synthetic data in enhancing the post-training of LLMs. \n\nNumerous methodologies for synthetic data generation have been advanced (Patel et al., 2024;M\u00f8ller et al., 2023;Park et al., 2024), yet the most prevalent and efficacious approach within the community involves generating synthetic data through sampling from a proficiently trained generative model, often another LLM tailored for specific domain tasks. To delineate this process more precisely, Long et al. (2024) describe the generation of synthetic data as follows: a well-trained generative model M is utilized, and synthetic data S gen is produced by sampling from M , conditioned on a set of prompts p, just as illustrated in the lower part of Figure 1 (a). \n\nIn this paper, we endeavor to examine the influence of synthetic data on the post-training phases of large language models (LLMs) through an analytical lens focused on data distribution and information content. Our investigation seeks to address the following theoretical questions: \n\n\u2022 What underlies the effectiveness of synthetic data? How can we model the data generation process and connect it with the generalization capabilities of post-trained models? \n\n\u2022 What is the reason for the effectiveness of synthetic data in LLM post-training?",
            "score": 0.34864176092648924,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1938
                },
                {
                    "start": 1941,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1951904296875
        },
        {
            "corpus_id": "272593364",
            "title": "Reranking Laws for Language Generation: A Communication-Theoretic Perspective",
            "text": "Large language models (LLMs) have shown remarkable performance across many tasks in natural language processing, computer vision, and speech recognition. Despite their capabilities, instances of hallucinations and other critical errors occasionally arise, casting doubt on the reliability of their predictions, without clear indication of when and how badly they might fail (Ji et al., 2023;Guerreiro et al., 2023). This is particularly concerning as these models are increasingly used in high-stakes applications such as those within the medical or legal domains (Hung et al., 2023) or as agents that can perform multiple tasks, including generating and executing code (Wang et al., 2024). \n\nThe most common mitigation strategy is to \"steer\" the LLM with the aid of a reward model or directly from human preferences, either at training time (Stiennon et al., 2020;Yuan et al., 2024;Rafailov et al., 2024) or during decoding (Liu et al., 2024;Huang et al., 2024). A simple and effective decoding-time strategy is first to generate multiple hypotheses and then use a reranker to select the most appropriate one. Several generation techniques used with modern LLMs, including voting procedures (Borgeaud and Emerson, 2020;Wang et al., 2023;Li\u00e9vin et al., 2024;Shi et al., 2022), minimum Bayes risk decoders (Eikema and Aziz, 2020;Freitag et al., 2022), quality-aware decoders (Fernandes et al., 2022), or other types of hypothesis ensembling/reranking techniques (Farinhas et al., 2023;Ni et al., 2023;Bertsch et al., 2023), embody this idea. An essential aspect of these procedures is that they all add redundancy as an intermediate step (by generating multiple hypotheses) to increase the chances of returning an acceptable answer as the final output. \n\nThe idea of adding redundancy to decrease the error rate in noisy channels is a cornerstone of communication theory, more specifically in forward error correction methods. In its simplest",
            "score": 0.3486355380016767,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 391,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 391,
                    "end": 414,
                    "matchedPaperCorpusId": "257771892"
                },
                {
                    "start": 564,
                    "end": 583,
                    "matchedPaperCorpusId": "265456842"
                },
                {
                    "start": 670,
                    "end": 689,
                    "matchedPaperCorpusId": "267406155"
                },
                {
                    "start": 842,
                    "end": 865,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 865,
                    "end": 883,
                    "matchedPaperCorpusId": "258059818"
                },
                {
                    "start": 883,
                    "end": 905,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1192,
                    "end": 1220,
                    "matchedPaperCorpusId": "201070821"
                },
                {
                    "start": 1220,
                    "end": 1238,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1258,
                    "end": 1275,
                    "matchedPaperCorpusId": "248377325"
                },
                {
                    "start": 1305,
                    "end": 1328,
                    "matchedPaperCorpusId": "218763425"
                },
                {
                    "start": 1328,
                    "end": 1349,
                    "matchedPaperCorpusId": "248392447"
                },
                {
                    "start": 1374,
                    "end": 1398,
                    "matchedPaperCorpusId": "248496443"
                },
                {
                    "start": 1461,
                    "end": 1484,
                    "matchedPaperCorpusId": "264172844"
                },
                {
                    "start": 1484,
                    "end": 1500,
                    "matchedPaperCorpusId": "256900680"
                },
                {
                    "start": 1500,
                    "end": 1521,
                    "matchedPaperCorpusId": "263605610"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.103759765625
        },
        {
            "corpus_id": "246652398",
            "title": "Transferable Student Performance Modeling for Intelligent Tutoring Systems",
            "text": "(ii) In the inductive transfer setting where small-scale target course data is available, we tune pre-trained course-agnostic performance models to the target course by learning target course-specific question and knowledge component (KC) difficulty parameters. This inductive transfer setting mimics the case where the course designer can run a pilot with a small number of students before large-scale deployment. \n\nWe evaluate the proposed techniques using student log data collected from five different mathematics courses describing learning trajectories from over 47,000 students in a real world large-scale ITS. In both settings, the proposed transfer learning techniques mitigate the cold-start problem for all courses successfully. We hope that transfer learning techniques will become a standard tool for ITS course designers and improve the learning experience of early students. To summarize, the key contributions of this paper include: \n\n\u2022 Course-agnostic student performance modeling. We present the first course-agnostic modeling techniques for predicting student performance on future questions in newly introduced courses where no previous students have yet taken this target course. Even though our agnostic models have no access to training data logs of students taking the new target course, they exhibit predictive performance comparable to conventional BKT and PFA student performance models -found in many real world ITSs -which were trained on data from thousands of students taking the new target course. Our course-agnostic models can enable effective personalized learning experiences even when introducing a new course for which no prior student log data is available. \u2022 Inductive transfer learning for effective tuning. We use transfer learning techniques to efficiently adapt our pre-trained course-agnostic performance models to individual target courses by learning question-and KC-specific parameters. \n\nOur experiments show how our approach leads to more accurate performance predictions than conventional modeling techniques in settings in which only limited student log data from the target course is available (< 100 students). \u2022 Guidance for practice. By analyzing data from five different courses offered by a large-scale ITS this work provides various insights which can inform the design of future ITSs. \n\nAmong others, our experiments show how manually assigned difficulty ratings and information about different learning contexts provided by domain experts during content creation can be used to boost the prediction accuracy of course-agnostic models.",
            "score": 0.3485592821518426,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1934
                },
                {
                    "start": 1937,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2344
                },
                {
                    "start": 2347,
                    "end": 2595
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1304931640625
        },
        {
            "corpus_id": "227227757",
            "title": "Dynamic Curriculum Learning for Low-Resource Neural Machine Translation",
            "text": "Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014;Bahdanau et al., 2015;Wu et al., 2016;Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a;Cheng et al., 2016;Fadaee et al., 2017), transfer learning (Zoph et al., 2016;Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018;Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. \n\nIn general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009;Kumar et al., 2010;Shrivastava et al., 2016). For example, Arpit et al. (2017) have pointed out that deep neural networks tend to prioritize learning \"easy\" samples first. This agrees with the idea of curriculum learning (Bengio et al., 2009) in that an easy-to-hard learning strategy can yield better convergence for training. \n\nIn NMT, curriculum learning is not new. Several research groups have applied it to large-scale translation tasks although few of them discuss the issue in a low-resource setup (Zhang et al., 2018;Platanios et al., 2019;Liu et al., 2020). The first question here is how to define the \"difficulty\" of a training sample. Previous work resorts to functions that produce a difficulty score for each training sample. This score is then used to reorder samples before training.",
            "score": 0.34828039882938133,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 153,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 153,
                    "end": 175,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 175,
                    "end": 191,
                    "matchedPaperCorpusId": "3603249"
                },
                {
                    "start": 191,
                    "end": 212,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 349,
                    "end": 374,
                    "matchedPaperCorpusId": "8822680"
                },
                {
                    "start": 479,
                    "end": 503,
                    "matchedPaperCorpusId": "15600925"
                },
                {
                    "start": 503,
                    "end": 522,
                    "matchedPaperCorpusId": "256189"
                },
                {
                    "start": 522,
                    "end": 542,
                    "matchedPaperCorpusId": "3291104"
                },
                {
                    "start": 562,
                    "end": 581,
                    "matchedPaperCorpusId": "16631020"
                },
                {
                    "start": 581,
                    "end": 603,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 628,
                    "end": 649,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 649,
                    "end": 669,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1100,
                    "end": 1121,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1121,
                    "end": 1140,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1140,
                    "end": 1165,
                    "matchedPaperCorpusId": "2843566"
                },
                {
                    "start": 1180,
                    "end": 1199,
                    "matchedPaperCorpusId": "11455421"
                },
                {
                    "start": 1342,
                    "end": 1363,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1647,
                    "end": 1670,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1670,
                    "end": 1687,
                    "matchedPaperCorpusId": "219260306"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "274908356",
            "title": "M.I.N.I.-KID interviews with adolescents: a corpus-based language analysis of adolescents with depressive disorders and the possibilities of continuation using Chat GPT",
            "text": "In order to analyze whether leveraging large language models (LLMs) in this research context is a promising method to augment available datasets substantially, we established several baselines for our small corpus of transcriptions using \"traditional\" classifiers4 that are less data hungry than LLMs and compared them with a classifier using BERT embeddings as input. Below we list all the classifiers in our setup along with the hyperparameters we configured the classifiers with. \n\n\u2022 Multinomial Na\u00efve Bayes: scikit's default settings, with smoothing set to a=1.0. \u2022 Decision Tree: scikit's default settings, in particular Gini impurity as measure for the quality of a split. We compared these baselines to a Deep Neural Network consisting of a BERT Transformer and a single subsequent feed forward layer with softmax-activation for our binary classification task. We trained batches of 16 datapoints for 5 epochs. For implementation, we used HuggingFace's TFAutoModelFor SequenceClassification class5 with the pretrained model googlebert/bert-base-german-cased. We adhered to the common practice and applied BERT's tokenizer on our question-answer pairs and used its output as input for the classifier. \n\nFor all other classifiers, we computed unigram, bigram, and trigram frequencies as features. The classifier was trained using all these frequencies, as experiments that utilized only unigrams, bigrams, or trigrams individually resulted in decreased performance across all training processes. The results reported below are based on training with the combined set of n-gram frequencies. \n\nWe trained these classifiers on several datasets described below in order to: \n\n1. Evaluate their performance for our classification task. 2. Determine whether augmenting datasets with the help of large language models (LLMs) has a positive effect on training our classifiers and can outperform a more traditional method of data augmentation, namely oversampling data using the SMOTE approach (39). \n\nTo reach these objectives, we constructed the following datasets to evaluate different strategies for data augmentation.:",
            "score": 0.3476229714204193,
            "section_title": "Baselines and datasets",
            "char_start_offset": 18464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09600830078125
        },
        {
            "corpus_id": "277787795",
            "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments",
            "text": "The cost of training large language models (LMs) necessitates methods of trying out options at small scale, but it also makes it expensive to validate the accuracy of development decisions made with such methods. We focus on the question of choosing between pretraining datasets to use-one of the most impactful development decisions. Common practice (e.g., Li et al., 2024) uses a single, small scale of experiments to cheaply test pretraining data intended for larger-scale models, where scale is determined by number of model parameters and training tokens. The other predominant approach is to fit scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022;Choshen et al., 2024) to the trend in performance observed over multiple small scales, with recent work extending this to the prediction of downstream performance instead of language modeling loss (Gadre et al., 2024;Dubey et al., 2024;Bhagia et al., 2024). \n\nSo far decision-making approaches have only been validated without observing the counterfactual outcome, either by producing a single large model on the chosen decision with impressive performance or by low error in predicting the magnitude of observed performance of a small number of large models. Knowing what amount of error in predicting performance over scale is a low enough to actually make a correct decision among datasets, requires a suite of comparable models trained on many datasets. Although a wide variety of open-source pretraining corpora are available, the scaling behavior of data is difficult to assess from off-the-shelf models that vary simultaneously in data, optimizer, and modeling decisions. \n\nTo make it possible to empirically study what methods make the best decisions over data, we build DATADECIDE1 -a suite of models we pretrain on 25 corpora up to 100B tokens, over 14 different model sizes ranging from 4M parameters up to 1B parameters (more than 30K model checkpoints in total). We evaluate all models across a suite of 10 downstream tasks and calculate how accurately small models predict which pretraining corpora lead to better performance at our largest scale. Our conclusions provide practical recommendations for the best benchmarks, prediction methods, and metrics to use to make decisions.",
            "score": 0.3475544306000783,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263916015625
        },
        {
            "corpus_id": "235353014",
            "title": "Annotation Curricula to Implicitly Train Non-Expert Annotators",
            "text": "Strategy. Due to the large number of n! possible curricula C resulting from n = |U| instances, solving Eq. ( 1) is intractable for large n even if a(\u2022) was known. We can furthermore only assess the true effectiveness of a curriculum C post-study, making it impossible to find the optimal curriculum C * beforehand. We hence require a strategy S \u223c C * that specifies how instances of U should be ordered optimally. Similar to educational approaches, we rely on estimating the \"difficulty\" of an instance to generate our curriculum (Taylor 1953;Beinborn, Zesch, and Gurevych 2014;Lee, Schwan, and Meyer 2019). In this work, we investigate an easy-instances-first strategy which has been shown to be a reasonable strategy in previous work (Tauchmann, Daxenberger, and Mieskes 2020); thereby sorting instances in ascending order according to their difficulty. Our C * is thus approximated by the ordered set S = {x 1 , . . . , x n |\u2200x 1\u2264i\u2264n \u2208 S : f (x i ) \u2264 f (x i+1 )} with f (\u2022) being the difficulty estimator. \n\nNon-adaptive estimators. We define non-adaptive estimators as heuristics or pre-trained models that are not updated interactively. The respective annotation curriculum can thus be pre-computed and does not impose any additional changes to the underlying annotation platform. To estimate the annotation difficulty, non-adaptive estimators define a scoring function f \u0101 : U \u2192 R. In this work, we evaluate non-adaptive estimators that are commonly used in readability assessment to score the reading difficulty of a text (Xia, Kochmar, and Briscoe 2016;Deutsch, Jasbi, and Shieber 2020). Although they are not capable of capturing any task-specific difficulties, they have the advantage of being applicable to a wide range of tasks with low effort for study conductors. \n\nThe following heuristics and pre-trained models are investigated to obtain difficulty estimations for the easy-instances-first curriculum: \n\nSentence Length (sen) The average number of tokens in the annotated instance.",
            "score": 0.3475544306000783,
            "section_title": "Approach",
            "char_start_offset": 14561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 543,
                    "end": 578,
                    "matchedPaperCorpusId": "13822684"
                },
                {
                    "start": 578,
                    "end": 606,
                    "matchedPaperCorpusId": "189927987"
                },
                {
                    "start": 736,
                    "end": 778,
                    "matchedPaperCorpusId": "212675597"
                },
                {
                    "start": 1529,
                    "end": 1561,
                    "matchedPaperCorpusId": "5617603"
                },
                {
                    "start": 1561,
                    "end": 1594,
                    "matchedPaperCorpusId": "219177150"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4033203125
        },
        {
            "corpus_id": "277780682",
            "title": "Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms",
            "text": "In this study, we investigate the use of GPT-based large language models to estimate the difficulty of K-5 mathematics and reading assessment items. Item difficulty prediction at this level poses unique challenges (young learners' skills are rapidly developing, and small differences in phrasing or context can greatly affect difficulty), making this grade range an ideal target for evaluating LLM's capabilities and limitations. Building on prior research, we employ two approaches to leverage GPT for item difficulty estimation. \n\nFirst, a direct LLM estimation approach is used: we craft a detailed prompt that describes the task and asks the model (GPT-4o) to analyze a given item's content and predict its difficulty on a numerical scale. This approach treats the LLM as an expert evaluator that generates a difficulty rating based on its internal knowledge of language and common curricular expectations. Notably, this is done in a single-shot manner-the model is not fine-tuned on any item data, but rather instructed to simulate the role of an educational expert. We examine the estimation error of GPT's predicted difficulty scores using root mean square error (RMSE) and mean absolute error (MAE), and the correlations between predicted and \"true\" difficulties derived from IRT calibration using field-test data collected from actual examinees. \n\nSecond, a feature-based modeling approach is implemented, in which we instruct GPT to extract interpretable features from each item. The features are determined based on extensive discussions with math and reading SMEs as well as the prior literature on the item characteristics that correlate with item difficulty. We then use those LLM-generated features together with item meta-data (e.g., subject domain, word count) to train ensemble tree-based regressors (specifically, random forests and gradient boosting machines) to predict difficulty. The features are designed to capture various aspects of the item that might influence difficulty. This featurebased method is structurally in line with previous text-based difficulty modeling approaches discussed earlier; however, the use of a LLM allowed us to extract abstract features in ways that might not be feasible for simpler text-based methods (e.g., dictionary-based approaches). The same set of items with known difficulty values were used for both methods.",
            "score": 0.3475544306000783,
            "section_title": "Present Research",
            "char_start_offset": 6377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62255859375
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "Effective curricula improve learning in humans (Tabibian et al., 2019;Nishimura, 2018) and machines (Bengio et al., 2009;Kumar et al., 2010;Zhou et al., 2020;Castells et al., 2020). Curriculum learning has been found effective in many NLP tasks (Settles and Meeder, 2016;Amiri et al., 2017;Platanios et al., 2019;Zhang et al., 2019;Amiri, 2019;Xu et al., 2020;Lalor and Yu, 2020;Jafarpour et al., 2021;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). A multiview curriculum is a curriculum able to integrate multiple difficulty scores simultaneously and leverage their collective value (Vakil and Amiri, 2023). \n\nWe assume there exists a subset of linguistic complexity indices that are most influential to learning an NLP task by a particular model. To identify these indices for each model and NLP task, we derive a weight factor \u03c1 i \u2208 [\u22121, 1] for each linguistic index that quantifies how well the index estimates the true difficulty of data samples to the model, determined by model instantaneous loss against validation data. By learning these weight factors, we obtain precise estimations that shed light on the core linguistic complexity indices that each model needs at different stages of its training to learn an NLP task. In addition, these estimates can be readily used for linguistic curriculum development, e.g., by training models with linguistically easy samples (with respect to the model) and gradually introducing linguistically challenging samples. \n\nTo achieve the above goals, we should address two gaps in the existing literature: First, existing curricula are often limited to a single criterion of difficulty and are not applicable to multiview settings. This is while difficulty is a condition that can be realized from multiple perspectives, can vary across a continuum for different models, and can dynamically change as the model improves. Second, existing approaches quantify the difficulty of data based on instantaneous training loss.",
            "score": 0.3475544306000783,
            "section_title": "Introduction",
            "char_start_offset": 1615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 70,
                    "matchedPaperCorpusId": "58947495"
                },
                {
                    "start": 70,
                    "end": 86,
                    "matchedPaperCorpusId": "52157421"
                },
                {
                    "start": 100,
                    "end": 121,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 140,
                    "end": 158,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 158,
                    "end": 180,
                    "matchedPaperCorpusId": "227275456"
                },
                {
                    "start": 271,
                    "end": 290,
                    "matchedPaperCorpusId": "1916665"
                },
                {
                    "start": 290,
                    "end": 313,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 313,
                    "end": 332,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 332,
                    "end": 344,
                    "matchedPaperCorpusId": "174799918"
                },
                {
                    "start": 344,
                    "end": 360,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 360,
                    "end": 379,
                    "matchedPaperCorpusId": "226226711"
                },
                {
                    "start": 379,
                    "end": 402,
                    "matchedPaperCorpusId": "236486249"
                },
                {
                    "start": 402,
                    "end": 424,
                    "matchedPaperCorpusId": "238856825"
                },
                {
                    "start": 424,
                    "end": 450,
                    "matchedPaperCorpusId": "247518847"
                },
                {
                    "start": 450,
                    "end": 476,
                    "matchedPaperCorpusId": "250391006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "247528476",
            "title": "Adaptive Curriculum Learning for Video Captioning",
            "text": "We experiment with the difficulty measurements proposed in Section III-A by training the ARB model solely on different ranges of sorted continuous data, and the results are shown in Table 2. For example, the data range ''0 th -25 th '' means that the video-caption pairs used in model training are the data with the least difficulties and consist of 25% of the total training data from the dataset. The hyperparameter of difficulty balance (\u03b1) is set to 0.2 when sorting the data pairs, and all models are trained with random sampling from the designated data range. We can observe that as the subset of data ranking higher in percentile, the model attains fewer scores, which proves that our difficulty measurements distinguish the easy from the hard, and indicates that the model struggles to learn from complex video-caption pairs. Furthermore, compared with a subset size of 25%, training on the subset of more data shows greater model performance, and hence we would continue training the model on the whole training set after the curriculum learning stage completes.",
            "score": 0.3475544306000783,
            "section_title": "2) EFFECT OF DIFFICULTY MEASUREMENTS",
            "char_start_offset": 19179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494140625
        },
        {
            "corpus_id": "268032846",
            "title": "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies",
            "text": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization.",
            "score": 0.3474731065812062,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44677734375
        },
        {
            "corpus_id": "277398866",
            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
            "text": "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes. \n\nModel compression techniques like KD, pruning, quantization, and adaptive KD each have unique pros and cons. KD transfers knowledge from a larger teacher model to a smaller student model, significantly reducing size while maintaining accuracy [6], especially in natural language processing [16]. However, it relies on large labeled datasets and struggles with very small models [6]. Pruning removes redundant weights or neurons for better compression and lower computational demands, but often sacrifices accuracy and requires fine-tuning [48], [49]. Quantization reduces model precision, saving memory and speeding up inference, though it can degrade accuracy in complex tasks without specialized hardware [50]. Adaptive KD customizes the distillation process based on input difficulty or model alignment, improving performance but increasing training complexity [47], [51]. While KD and its adaptive forms balance performance and size, pruning and quantization excel in extreme compression scenarios, highlighting the complementary nature of these methods.",
            "score": 0.3474169228328696,
            "section_title": "1) ADDITIONAL TECHNIQUES AND APPLICATIONS",
            "char_start_offset": 26928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 1042,
                    "end": 1045,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1177,
                    "end": 1180,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "212628335"
                },
                {
                    "start": 1506,
                    "end": 1510,
                    "matchedPaperCorpusId": "67855262"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "278033087",
            "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training",
            "text": "In this paper, we introduce a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries with 40 million model-generated responses spanning various capability gradients. Leveraging this dataset, we conduct large-scale reasoning-focused training based on pass rates and the Coefficient of Variation (CV) to identify and utilize the most valuable training data. Additionally, we observed a training pattern shift, indicating that higher learning rates are necessary for effectively fitting complex reasoning tasks. \n\nMoreover, preliminary experiments revealed that further increasing data complexity through an annealed second-stage training could enhance model performance. Starting from a base model, our reasoning-focused fine-tuning method surpasses most open-source distilled models on several complex reasoning benchmarks, such as AIME2024, achieving performance close to or at state-ofthe-art levels. These results validate the effectiveness and generalization capability of both our dataset and training strategies. \n\nIn future work, we aim to develop more refined methods for evaluating data quality to better identify data most beneficial for model training. Additionally, we will investigate how models with varying initial capabilities influence subsequent reinforcement learning (RL) training outcomes, thus illuminating the interplay between foundational model capability and data quality. We hope that our open-sourced data and methodologies will facilitate further advancements in constructing open-source large language models with exceptional long-form reasoning capabilities, contributing significantly to the broader research community.",
            "score": 0.34705307729523877,
            "section_title": "Conclusion and Limitations",
            "char_start_offset": 26185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1697
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "268724039",
            "title": "$\\forall$uto$\\exists$val: Autonomous Assessment of LLMs in Formal Synthesis and Interpretation Tasks",
            "text": "Foundation Models such as Large Language Models (LLMs) have been demonstrated to successfully perform many natural language tasks such as translation with human-like performance [Xue et al., 2021]. Furthermore, LLMs have been shown to be versatile and easily transferrable to several downstream, domain-specific tasks (e.g., French-to-English Translation [Zhu et al., 2024]) using approaches such as supervised fine tuning (SFT) [Lu et al., 2023, Zeng et al., 2024]. Recently, there has been research on using LLMs to translate natural language (NL) to formal syntax (FS) for robotics [Liang et al., 2023] and for constructing planning problems solvable using existing solvers [Guan et al., 2023]. Although these methods have been successful in small-scale scenarios, their effectiveness in accurately translating NL to/from FS remains uncertain. This uncertainty mainly stems from the difficulty in assessing how good LLMs are for truth maintenance in such tasks. This paper addresses three key questions for performing this evaluation: (1) How do we accurately assess an LLM's translation abilities? (2) Can we avoid relying on datasets that require humanannotators? (3) Can we construct dynamic datasets that can be scaled as LLMs are retrained? Existing methods for LLM assessment are limited along one or more of these dimensions. Firstly, the datasets used are static, making them prone to be overfitted/memorized as they are likely included in the training sets when LLMs are retrained (e.g., [Han et al., 2022, Sinha et al., 2019]). Secondly, these using \u03a3 and operators like the Kleene star * which represents zero or more occurrences of a given pattern. For example, the regex 2(01) * 2 using \u03a3 = {0, 1, 2} matches all strings possible using \u03a3 that begin with a two, contain zero or more instances of alternating zeroes, and end with a two. Regular expressions can be converted to Deterministic Finite Automata (DFAs) which are finite-state machines that can easily determine whether a given string matches the regex.",
            "score": 0.3466774288710832,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2027
                }
            ],
            "ref_mentions": [
                {
                    "start": 445,
                    "end": 465,
                    "matchedPaperCorpusId": "259501202"
                },
                {
                    "start": 585,
                    "end": 605,
                    "matchedPaperCorpusId": "252355542"
                },
                {
                    "start": 677,
                    "end": 696,
                    "matchedPaperCorpusId": "258865907"
                },
                {
                    "start": 1517,
                    "end": 1538,
                    "matchedPaperCorpusId": "198184456"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147216796875
        },
        {
            "corpus_id": "264127877",
            "title": "Table-GPT: Table-tuned GPT for Diverse Table Tasks",
            "text": "It was observed in the NLP community [8,40,59], that earlier versions of pre-trained language models, such as GPT-3, is able to complete a sentence with the next likely token (e.g., \"write a bed-time\" \u2192 \"story\"), but cannot reliable follow higher-level instructions from humans (e.g., \"write a bed-time story for a 3 years-old, in 100 words\"), a behavior that is only demonstrated in later models such as ChatGPT. \n\nInstruction-tuning was the key technique invented that continues to train GPT-like models into ChatGPT-like models, in a process shown on the left of Figure 3. Diverse training data in the form of \"(instruction, completion)\" pairs are constructed, often manually annotated by human labellers [40], e.g. (\"write a bed-time story\" \u2192 an-actual-story), to continue train language-models on these explicit demonstrations of how to follow high-level human instructions, leading to well-known models such as ChatGPT/In-structGPT [2,40], as well as their open-source counterparts like Stanford- Alpaca [4] and LLaMa-chat [50]. \n\nTable-tuning: train language-models to understand tables. We believe that the research on instruction-tuning in NLP, which successfully enhances language-models ability to follow human instructions, holds lessons for us when we aim to enhance languagemodels ability to understand tables and perform table-tasks. \n\nIn this work, we propose a \"table-tuning\" paradigm analogous to instruction-tuning, where we continue to train language-models, using diverse training data in the form of (instruction, table, completion), which we synthesize using large amounts of real tables. This process is illustrated on the right of Figure 3. \n\nThrough extensive experiments, we show that \"table-tuning\" is a promising new direction, as our resulting Table-GPT models are: \n\n(1) Strong table models, which substantially outperform 175B GPT-3.5 and ChatGPT, on a wide range of seen and unseen table-tasks, as we summarize in Table 2 and Figure 9;",
            "score": 0.34667587383912035,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 40,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 40,
                    "end": 43,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242431640625
        },
        {
            "corpus_id": "271903168",
            "title": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments",
            "text": "Acceptability judgments have been widely used to measure grammatical knowledge of language models (LMs) (Lau et al., 2017;Warstadt et al., 2019). Of two major categories of acceptability judgment benchmarks, we focus on the minimalpair (MP) benchmark, where LMs are tested to see if they will prefer the more acceptable sentence from a pair of minimally different sentences. An example minimal pair extracted from Warstadt et al. (2020) is shown below. \n\n(a) These casseroles disgust Kayla. \n\n(b) *These casseroles disgusts Kayla. \n\n1 Our codes and templates will be publicly available upon acceptance. \n\nHere, sentence (a) is acceptable or grammatically correct, while (b) is not, as its underlined verb violates the subject-verb agreement. As such, MP benchmarks can evaluate any LMs including ase models and nstruct models, without fine-tuning for acceptability judgments. \n\nMeanwhile, recent scaling up of model sizes and training data for LMs has made it possible to solve a wide range of tasks by few-shot or zeroshot prompting, without task-specific finetuning (Brown et al., 2020;Liu et al., 2021), popularizing the term large language models (LLMs). Incorporating learning techniques such as instructiontuning (Wei et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2023) has further improved the alignment of LLM outputs with human preferences and expectations. The LLMs trained by such techniques achieve good performance through prompting. In other words, LLMs show high performance when provided with guidance on what knowledge to elicit. \n\nIn this light, one can conceive various methods of obtaining acceptability judgments from LLMs, including prompting. However, no previous studies have thoroughly explored them; most of them naively input the given sentences to an (L)LM, calculate their probabilities, and deem the sentence with the higher probability of the pair to be acceptable for the (L)LM. Consequently, it is unclear what methods are effective in obtaining acceptability judgments using LLMs and what their strengths or weaknesses are.",
            "score": 0.34656762823099696,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 122,
                    "matchedPaperCorpusId": "1056628"
                },
                {
                    "start": 122,
                    "end": 144,
                    "matchedPaperCorpusId": "44072099"
                },
                {
                    "start": 414,
                    "end": 436,
                    "matchedPaperCorpusId": "208527435"
                },
                {
                    "start": 1219,
                    "end": 1237,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1279,
                    "end": 1302,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18359375
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "More recently, Tsvetkov et al. (2016) improve word embedding training using Bayesian optimization to order paragraphs in the training corpus based on a range of distributional and linguistic features (diversity, simplicity, prototypicality). \n\nWhile curriculum learning often refers to organizing examples from simple to difficult, other data ordering strategies have also shown to be beneficial: Amiri et al. ( 2017) improve the convergence speed of neural models using spaced repetition, a technique inspired by psychology findings that human learners can learn efficiently and effectively by increasing intervals of time between reviews of previously seen materials. \n\nCurriculum design is also a concern when deciding how to schedule learning from samples of different tasks either in a sequence from simpler to more difficult tasks (Collobert and Weston, 2008) or in a multi-task learning framework (Graves et al., 2017;Kiperwasser and Ballesteros, 2018). In this work, we focus on the question of organizing training samples for a single task. \n\nIn NMT, curriculum learning has not yet been explored systematically. In practice, training protocols randomize the order of sentence pairs in the training corpus (Sennrich et al., 2017;Hieber et al., 2017). There are works that speed training up by batching the samples of similar lengths (Khomenko et al., 2016;Doetsch et al., 2017). Such works attempt to improve the computational efficiency, while curriculum learning is supposed to improve the statistical efficiency -fewer batches of training examples are needed to achieve a given performance. Kocmi and Bojar (2017) conducted the first study of curriculum learning for NMT by exploring the impact of several criteria for curriculum design on the training of a Czech-English NMT system for one epoch. They ensure samples within each mini-batch have similar linguistic properties, and order mini-batches based on complexity. They show translation quality can be improved by presenting samples from easy to hard based on sentence length and vocabulary frequency. However, it remains to be seen whether these findings hold when training until convergence. \n\nPrevious work has focused on dynamic sampling strategies, emphasizing training on samples that are expected to be most useful based on model scores or domain relevance.",
            "score": 0.3462301699516405,
            "section_title": "Introduction",
            "char_start_offset": 3850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2161
                },
                {
                    "start": 2164,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 865,
                    "matchedPaperCorpusId": "2617020"
                },
                {
                    "start": 904,
                    "end": 925,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1215,
                    "end": 1238,
                    "matchedPaperCorpusId": "905565"
                },
                {
                    "start": 1342,
                    "end": 1365,
                    "matchedPaperCorpusId": "14333788"
                },
                {
                    "start": 1603,
                    "end": 1625,
                    "matchedPaperCorpusId": "26468344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7451171875
        },
        {
            "corpus_id": "252873115",
            "title": "Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities",
            "text": ", the remove \u2192 training data approaches we found span both static and dynamic mechanisms. While learning-based mechanisms typically produce the best results, cheaper-to-compute domain-knowledge-based mechanisms like random pruning can be effective, particularly when applied dynamically. Significant progress has been made on pruning smaller datasets like CIFAR-10 and CIFAR-100, but we are still far from achieving large amounts of data pruning on ImageNet-scale data.\n\nRestrict Restrict \u2192 training data can be done at both the sample and dataset levels. These techniques tend to achieve speedup by reducing n iteration while keeping T i fixed. The major idea behind this class of speedup techniques is to restrict the statistical properties of the data.\n\nOn a data sample level, popular normalization techniques include centering, scaling, decorrelating, standardizing, and whitening. A more detailed review of these techniques is provided in .\n\nOn a dataset level, there are several theoretical works that show that significant speedups can be achieved by imposing certain restrictions on the training data generation process. For example,  show that training datasets with optimized spectral properties, e.g., space-filling designs, produce models with superior generalization. Jin et al. (2020) introduce cover complexity (CC) to measure the difficulty of learning a dataset as a function of the richness of the whole training set and the degree of separation between different labeled subsets. They found that the error increases linearly with the cover complexity both in theory and on MNIST and CIFAR-10. The major obstacle preventing the use of these theoretical findings is that one rarely has control over the data-generating distribution.\n\nMost of the above restrict \u2192 training data approaches are static and based on domain knowledge.\n\nReorder Reorder \u2192 training data changes the order in which examples or aspects of examples are presented to the model during training. The central challenge of this set of approaches is outperforming the random default ordering typically used in training. We discuss ordered learning via curriculum and progression approaches and suggest  for further discussion of ordered learning.\n\nReordering the dataset using a curriculum can aid learning (Bengio et al., 2009), allowing a larger performance gain per iteration that reduces the n iteration required to reach a desired performance level. Hacohen and Wei",
            "score": 0.34620667930791243,
            "section_title": "Training Data",
            "char_start_offset": 62991,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1282,
                    "end": 1299,
                    "matchedPaperCorpusId": "263796258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244384765625
        },
        {
            "corpus_id": "269899809",
            "title": "Automated Radiology Report Generation: A Review of Recent Advances",
            "text": "Supervised learning is defined by training a model using datasets containing ground truth labels.In ARRG, the label is the radiology report which is the target output the model is trained to predict.For ARRG datasets, some training samples are more challenging than others as the radiographic image could contain a subtle pathology or the radiology report could be particularly long.Curriculum learning is a training strategy where, instead of randomly shuffling the entire dataset, the model is exposed to increasingly complex examples over time.This method of training a model is thought to closely resemble how a radiologist learns throughout their career, enabling a clinician to gradually become more comfortable with more complex cases.The novelty of curriculum learning techniques relates to how a researcher measures how challenging an example case is.\n\nLiu et al. [27] developed a competence-based multimodal curriculum learning strategy which evaluated the complexity of each sample in the dataset using heuristic metrics to quantify the visual and textual difficulty.To measure the degree of visual difficulty, they implemented a ResNet-50 architecture, fine-tuned on the CheXpert dataset.Their model extracted the normal image embeddings of all normal training images from ResNet-50's last average pooling layer.Then, given an input image, an image embedding was obtained from their model which was compared against the normal images using average cosine similarity to provide a heuristic measure of the visual complexity of that training instance.For the textual difficulty, they utilised a count of the number of sentences describing abnormal findings within the report to establish how arduous it is to generate.Their method considered sentences without the keywords \"no\", \"normal\", \"clear\" or \"stable\" as sentences describing abnormal findings.\n\nA multi-criteria supervised approach to training was proposed by Wang et al. [9], with auxiliary objectives embedded into their training strategy.An image-text matching objective is used to better correlate image and text features while an image classification objective is also developed to improve the feature extraction capabilities of their model.For the classification objective, the MeSH labels provided with the MIMIC-CXR dataset are adopted as ground truths.Li et al. [20] also implemented auxiliary objectives within their model, naming the process auxiliary signal-guided learning.",
            "score": 0.34601159479128185,
            "section_title": "A. Supervised Learning",
            "char_start_offset": 19617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 860
                },
                {
                    "start": 862,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1560
                },
                {
                    "start": 1560,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1860
                },
                {
                    "start": 1862,
                    "end": 2008
                },
                {
                    "start": 2008,
                    "end": 2213
                },
                {
                    "start": 2213,
                    "end": 2328
                },
                {
                    "start": 2328,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 873,
                    "end": 877,
                    "matchedPaperCorpusId": "236460239"
                },
                {
                    "start": 1939,
                    "end": 1942,
                    "matchedPaperCorpusId": "248514098"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "225066701",
            "title": "A Comprehensive Survey on Curriculum Learning",
            "text": "The curriculum in these methods often refers to a sequence of loss weights or even loss functions on data batches. The comparison of these automatic CL methodologies are in Table IV. Note that as aforementioned, automatic CL is broadly applied to Deep RL tasks, and we refer readers to the recent surveys [64], [69] for further reading. The automatic CL methods discussed in this section are mostly designed for supervised learning settings, though some of them are also shown to be effective in RL tasks [37], [59]. 1) Self-Paced Learning: Self-paced Learning (SPL) is a primary branch of CL which automates the Difficulty Measurer by taking the example-wise training loss of the current model as criteria. The concept of \"self-paced learning\" originates from human education, where the student is able to control the learning curriculum, including what to study, how to study, when to study, and how long to study [93]. Under machine learning settings, SPL refers in particular to a training strategy initially proposed by Kumar et al. [41], which trains the model at each iteration with the easiest proportion of training set according to the model's current performance, i. The most valuable advantages of SPL over predefined CL is mainly two-fold: 1) SPL is semi-automatic CL with loss-based automatic Difficulty Measurer and dynamic curriculum, which makes it more flexible and adaptive for various tasks and data distributions. 2) SPL embeds the curriculum design into the learning objective of the original machine learning tasks, which makes it widely applicable as a plug-in tool. \n\n1-a) The Original Version of SPL. The original SPL algorithm [41] is formally defined as follows. Let D = {x i , y i } N i=1 denotes the training set, where x i and y i is the feature and label of example i, respectively. The machine learning model f w with parameters w maps each x i to the model prediction f w (x i ), and gets a loss l i = L(f w (x i ), y i ), where L is the learning objective. The original goal is then to minimize the empirical loss on the whole training set:",
            "score": 0.3459475639052277,
            "section_title": "C. Automatic CL",
            "char_start_offset": 40182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1590
                },
                {
                    "start": 1593,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2075
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 515,
                    "matchedPaperCorpusId": "8432394"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "21323057"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1654,
                    "end": 1658,
                    "matchedPaperCorpusId": "1977996"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443359375
        },
        {
            "corpus_id": "234338422",
            "title": "Self-Guided Curriculum Learning for Neural Machine Translation",
            "text": "Model\n\n, where a language model pre-trained on source sentences of the parallel corpus, D S , is adopted to measure the uncertainly of each source sentence x = w 1 , . . . , w i , . . . , w I by per-word crossentropy (Zhang et al., 2019). d(x) and d(y) can be used separately or jointly. Both n-gram language model and neural language model are adopted in Zhou et al. (2020b).\n\n. . , w I is a distributed representation of source sentence x mapped through a independently trained word embedding model. In the case of Liu et al. (2020) the norm of word vector on the source side is used as the difficulty criterion. They also use the CDF function to assure the difficulty scores are within [0, 1].\n\nNMT Model d(z n ; \u03b8 k ) = l(z n ;\u03b8 k )\u2212l(z n ;\u03b8 k\u22121 ) l(z n ;\u03b8 k\u22121 )\n\n, l(z n ; \u03b8 k ) = \u2212 log P (y n |x n ; \u03b8 k ), where \u03b8 k represents the NMT model parameters at the kth training phase. The decline of loss is defined as the difficulty criterion in Xu et al. (2020). Besides, the score of cross-lingual patterns may also be a proper difficulty criterion for NMT Zhou et al., 2020a;Wu et al., 2021), which we leaves as future work.\n\nWe now turn to curriculum scheduling. There are two controlling factors, extraction of training set and training phase duration, namely how to split training corpus into subsets and when to load them. Given difficulty scores d(z n ), z n \u2208 D, D is split into K mutual exclusive subsets {D 1 , . . . , D K }, which are loaded in order as training phase progresses. There are two general regimens. In one pass regimen, k subsets D k are loaded as training set one by one, while in baby steps regimen, these subsets are merged into the current training set one by one (Cirik et al., 2016). According to Cirik et al. (2016), baby steps outperforms one pass. Later approaches generally take",
            "score": 0.34562089400980667,
            "section_title": "Language",
            "char_start_offset": 6396,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 237,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 356,
                    "end": 375,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 517,
                    "end": 534,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 948,
                    "end": 964,
                    "matchedPaperCorpusId": "227227757"
                },
                {
                    "start": 1061,
                    "end": 1080,
                    "matchedPaperCorpusId": "222291089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "262054095",
            "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
            "text": "Large language models inherently possess a structure that supports parallelization, especially when optimized using techniques that allow for batch training. When computational resources permit, large batch sizes are favored to expedite the training of large models containing potentially millions or billions of parameters. At a fundamental level, larger batch sizes enhance the quality of each gradient update since they consider a more considerable chunk of the dataset. Conversely, a smaller batch size means that model parameter updates are based on gradients derived from a limited dataset portion. This smaller dataset slice might not comprehensively capture the intricate relationships between features and labels. Therefore, it might seem that larger batch sizes consistently offer advantages in training. However, [20] pointed out that this perspective does not factor in the model's capacity to generalize to new, unseen data, nor the intricate, non-convex optimization landscape of contemporary large models. In practice, multiple studies [18,20] have demonstrated that while larger batch sizes might hasten convergence, they can impair a model's generalization to new datasets, irrespective of the deep network type. This observed disparity has been named as the Generalization Gap. A method [18] to address this gap involves starting from a smaller batch size and gradually enlarging it as training advances. In our study, we explore this problem through a new and unique angle of progressive weight decay training.",
            "score": 0.3454027035632735,
            "section_title": "Large Batch Training for Large Language Models",
            "char_start_offset": 23030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1529
                }
            ],
            "ref_mentions": [
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "7967806"
                },
                {
                    "start": 1305,
                    "end": 1309,
                    "matchedPaperCorpusId": "7967806"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.364013671875
        },
        {
            "corpus_id": "254564561",
            "title": "Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging",
            "text": "Large Pre-trained Language Models (PLMs) like BERT (Devlin et al., 2019) and GPT (Brown et al., 2020) have achieved state-of-the-art (SOTA) performance on a number of NLP applications. A large model pre-trained on a massive text corpus learns general-purpose features of the language and can be easily adapted to different downstream tasks by fine-tuning with task-specific objectives (Devlin et al., 2019;Liu et al., 2019;Kaplan et al., 2020). Multiple papers (Brown et al., 2020;Zeng et al., 2021;Wang et al., 2022) demonstrated that the improvement in pre-training can be transferred favorably to most downstream tasks with little or no data, which leads to considerable efforts to scale up PLMs and increase the size of the pre-training dataset. However, a recent work (Abnar et al., 2021) empirically shows that as the upstream accuracy increases, the performance of downstream tasks gradually saturates. Furthermore, although PLMs perform well in a task-agnostic few-shot setting, it is still desirable to fine-tune them to achieve SOTA performance, especially for models with limited capacity. This encourages more efforts to improve the generalization of PLMs through additional techniques like Knowledge Distillation or better optimization. \n\nKnowledge Distillation (Hinton et al., 2015) is an effective method to improve the generalization of compact PLMs. One can use a larger model (a teacher) to fine-tune a smaller model (a student) by making a student learn the teacher's output. This technique has helped in achieving SOTA performance on multiple Natural Language Understanding (NLU) problems for compact models (Sun et al., 2020;Jiao et al., 2020;Sanh et al., 2019). However, KD is not an efficient method as it requires training a separate teacher model for every new task. Alternatively, some works focus on boosting the performance of the optimization algorithm directly.",
            "score": 0.3453252273876851,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 101,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 461,
                    "end": 481,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 481,
                    "end": 499,
                    "matchedPaperCorpusId": "233394012"
                },
                {
                    "start": 1628,
                    "end": 1646,
                    "matchedPaperCorpusId": "215238853"
                },
                {
                    "start": 1646,
                    "end": 1664,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355712890625
        },
        {
            "corpus_id": "267627801",
            "title": "Autonomous Data Selection with Zero-shot Generative Classifiers for Mathematical Texts",
            "text": "However, constructing such a continuous scoring system poses nontrivial challenges. Large language models often struggle with generating reliable numerical values or sampling consistently from intricate distributions (Hopkins et al., 2023;Hu et al., 2024). Drawing inspiration from the Direct Preference Optimization (DPO) framework (Rafailov et al., 2023), we propose a simpler yet effective solution: leveraging the model's own logits associated with targeted tokens (e.g., \"YES\" vs. \"NO\") to produce a quantitative score function. This approach avoids costly labeling efforts and bypasses the need for training an additional classifier on human-annotated data. \n\nConcretely, we introduce Autonomous Data Selection (AutoDS), which uses zero-shot metaprompts to evaluate the quality of mathematical texts for continual pretraining. Instead of relying on aligned or fine-tuned models, we take a strong base model and prompt it with two yes/no questions assessing (1) the level of \"mathematical intelligence\" in the text, and (2) its utility for future math learning. From the resulting logits on \"YES\" and \"NO,\" we compute a single real-valued LM-SCORE that captures the text's educational value. This enables a more fine-grained assessment than binary filtering approaches (Li et al., 2023;Paster et al., 2024), thus amplifying token efficiency by selectively training on the most instructive samples. \n\nAnother distinguishing factor of our method is its ability to autonomously curate data: no separate human-annotated corpus or reward model is needed. Techniques like supervised fine-tuning (SFT) (Radford et al., 2019), Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), or specialized preference modeling (Rafailov et al., 2023) are not required. By directly applying a softmax-based score on the base model's logits, AUTODS orchestrates a form of active, self-directed learning, where the model itself identifies and harnesses the best materials for continual pretraining. This paves the way for a more dynamic and scalable data selection pipeline, especially relevant for highly specialized fields like mathematics.",
            "score": 0.3453044415190214,
            "section_title": "Introduction",
            "char_start_offset": 1991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 255,
                    "matchedPaperCorpusId": "263830173"
                },
                {
                    "start": 333,
                    "end": 356,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1291,
                    "end": 1311,
                    "matchedPaperCorpusId": "263829563"
                },
                {
                    "start": 1600,
                    "end": 1622,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1732,
                    "end": 1755,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21044921875
        },
        {
            "corpus_id": "277148541",
            "title": "Subset selection for domain adaptive pre-training of language model",
            "text": "While the pre-training 22,23 or continual pre-training(CPT) 14 is a promising way to build a general or domainadaptive language model, it demands significant computational resources. 24 To mitigate this issue, there have been studies on model-centric and data-centric approaches to efficiently pre-train language models with a limited computational budget. As a model-centric approach, Chen et al. 10 designed a way of training models based on the lottery ticket hypothesis. Lan et al. 9 and Dehghani et al. 25 introduced parameter sharing techniques for making the models to have fewer parameters. Such parameter sharing enables better performance in language modeling and allows to efficiently construct the language models. In this paper, we chose the ALBERT 9 utilizing such efficient parameter sharing technique as our base model for experiments. As a data-centric approach, there have been subset selection techniques that extract informative examples from a large dataset to construct a small but effective or representative subset. The subset allows efficient training without losing much performance compared to the total dataset. Yao et al. 11 employed BM25 26 by treating the textual data of down-stream task as a query. This allows the similarity measurement between the pre-training data and the query to facilitate subset selection. Wang et al. 12 introduced a subset selection based on influence function 27,28 , and showed that their method outperforms other previous domain-specific language models. It finds samples that have the most positive impact on the down-stream task using the influence function. These studies commonly rely on the dataset of target task Dt; they are of down-stream dependent or down-stream fully-dependent groups. Suzuki et al. 13 , belonging to down-stream independent group, defined scores based on the difference in length-normalized per-word cross-entropy between a domain-adaptive language model M d and a general language model Mg. They proved that there exist representative subsets for a given domain, but their method did not exhibit much performance improvement over randomly extracted subsets. We believe that the biggest reason for their marginal performance gain might be that they strongly rely on the model loss, so it became difficult to distinguish between noisy samples and informative samples 29,30 .",
            "score": 0.3452171859720128,
            "section_title": "Efficient pre-training",
            "char_start_offset": 1595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 26,
                    "end": 28,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 60,
                    "end": 62,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 398,
                    "end": 400,
                    "matchedPaperCorpusId": "230438816"
                },
                {
                    "start": 486,
                    "end": 487,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 508,
                    "end": 510,
                    "matchedPaperCorpusId": "49667762"
                },
                {
                    "start": 762,
                    "end": 763,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1151,
                    "end": 1153,
                    "matchedPaperCorpusId": "243847293"
                },
                {
                    "start": 1168,
                    "end": 1170,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1359,
                    "end": 1361,
                    "matchedPaperCorpusId": "258832673"
                },
                {
                    "start": 1423,
                    "end": 1425,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 1772,
                    "end": 1774,
                    "matchedPaperCorpusId": "255883539"
                },
                {
                    "start": 2356,
                    "end": 2359,
                    "matchedPaperCorpusId": "235898952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2486572265625
        },
        {
            "corpus_id": "271089080",
            "title": "Training on the Test Task Confounds Evaluation and Emergence",
            "text": "We introduce the term training on the test task to group a growing repertoire of practices that utilize knowledge about evaluation tasks at training time. We study its impact on present-day benchmark evaluations by critically examining the performance improvements of recent language models. Our analysis spans 56 different language models and two major active benchmarks, MMLU and GSM8K. We start in Section 2 by dividing models into those trained before November 2023 and those trained after. We find that for the same amount of pretraining compute, newer models strongly outperform older ones, on average by 7 percentage points in MMLU and 19 points in GSM8K. We then fine-tune all models on the same amount of task-specific data before evaluation. After fine-tuning on the same task data, newer models no longer outperform older ones. Rather, their performance equalizes. See Figure 1. This outcome suggests that the main difference between newer and older models is the extent to which they train on the test task. \n\nWe propose a simple and effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model on the same, sufficient amount of task-specific data before evaluation. To validate our method, we demonstrate its effectiveness in a controlled setting: we take the older models and fine-tune them on the test task. Remarkably, this recreates the kind of performance differences observed between newer and older models, further suggesting that training on the test task explains the improvements of newer models. We then show that we can undo the advantage of the fine-tuned models over the other models by further fine-tuning all models on the test task (Section 3.1, Figure 3). \n\nNext, we provide evidence that training on the test task may be a more dominant factor in benchmark performance than data contamination. To argue this point, we consider ARC and HellaSwag. Here, at first, there appears to be no sign of newer models having an unfair advantage over older models. But after reformulating these benchmarks as MMLU-style multiple choice question answering tasks (MCQA), we see the same confounded results as for MMLU (Section 3.2, Figure 4).",
            "score": 0.3451952452581092,
            "section_title": "Our contributions",
            "char_start_offset": 3497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1632080078125
        },
        {
            "corpus_id": "254366520",
            "title": "Memorization of Named Entities in Fine-Tuned BERT Models",
            "text": "Training data extraction attacks aim to reconstruct training datapoints, but unlike model inversion attacks, the goal is to retrieve verbatim training examples and not just \"fuzzy\" class representatives [8]. These attacks are best suited for generative sequence models such as LMs. Initially these attacks have been designed for small LMs using academic datasets [7,62,52]. The aim of these studies was to measure the presence of specific training datapoints in the text samples generated by the models. A common approach to measure the extent of this unintended memorization is to insert socalled \"canaries\" (artificial datapoints) into the training datasets and quantify their occurrence during sequence completion [7]. Since these initial studies were based on smaller models trained with a high number of epochs, it was assumed that this kind of privacy leakage must be correlated with overfitting [62]. However, a follow-up study using the GPT-2 model, which is trained on a very large corpus for only a few epochs, showed that even state-of-the-art large LMs are susceptible to these kinds of attacks. Using the pre-trained GPT-2 model, Carlini et al. [8] were able to generate and select sequence samples which contained low k-eidetic data-points (data points that occur k times in the training corpus). A study by Lehman et al. [29] on Clinical BERT attempted to extract patient-condition association using both domain-specific template infilling and the text generation methods inspired by the text extraction research done on GPT-2 [8] and the BERT specific text generation technique proposed by Wang and Cho [56]. Their methods were not successful in reliably extracting privacy sensitive information (patient-condition associations) from Clinical BERT, but it remains inconclusive whether it is due to the limitations in their method or in the linguistic capabilities of BERT.",
            "score": 0.3446521646683427,
            "section_title": "Training Data Extraction Attacks",
            "char_start_offset": 9512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 206,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "219450111"
                },
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "219450111"
                },
                {
                    "start": 1158,
                    "end": 1161,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1542,
                    "end": 1545,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061981201171875
        },
        {
            "paperId": "f6061f4615b069084a3fb5fd8a5deaf7f378dfe3",
            "corpusId": 279243528,
            "title": "Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework",
            "venue": "",
            "year": 2025,
            "referenceCount": 42,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05695, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366142077",
                    "name": "Lingyuan Liu"
                },
                {
                    "authorId": "2366090070",
                    "name": "Mengxiang Zhang"
                }
            ],
            "abstract": "Knowledge Distillation (KD) compresses large language models (LLMs) by transferring the teacher model's capabilities to a smaller student model, reducing inference cost and memory usage while maintaining performance. However, existing KD methods for LLMs often fail to prevent significant shifts in the student model's distribution during training, leading to issues such as catastrophic forgetting, mode collapse, and training-inference mismatch. To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of\"progressive overload\"(POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures. By starting with the easiest samples and progressively increasing the difficulty, the approach enhances both the stability and efficiency of learning. Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families. Our findings highlight the effectiveness of sorted training samples in KD for LLMs. More generally, our work demonstrates how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.",
            "corpus_id": "279243528",
            "text": "Knowledge Distillation (KD) compresses large language models (LLMs) by transferring the teacher model's capabilities to a smaller student model, reducing inference cost and memory usage while maintaining performance. However, existing KD methods for LLMs often fail to prevent significant shifts in the student model's distribution during training, leading to issues such as catastrophic forgetting, mode collapse, and training-inference mismatch. To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of\"progressive overload\"(POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures. By starting with the easiest samples and progressively increasing the difficulty, the approach enhances both the stability and efficiency of learning. Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families. Our findings highlight the effectiveness of sorted training samples in KD for LLMs. More generally, our work demonstrates how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.95458984375
        },
        {
            "paperId": "524df152cc9a4d351f137302989fb5dd888376ba",
            "corpusId": 271742786,
            "title": "Large Language Model Influence on Management Reasoning: A Randomized Controlled Trial",
            "venue": "medRxiv",
            "year": 2024,
            "referenceCount": 27,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11326321",
                "status": "GREEN",
                "license": "CCBYND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11326321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1561496247",
                    "name": "Ethan Goh"
                },
                {
                    "authorId": "2214628844",
                    "name": "Robert J Gallo"
                },
                {
                    "authorId": "2294527929",
                    "name": "Eric Strong"
                },
                {
                    "authorId": "2284201578",
                    "name": "Yingjie Weng"
                },
                {
                    "authorId": "2294535245",
                    "name": "Hannah Kerman"
                },
                {
                    "authorId": "2314908376",
                    "name": "Jason Freed"
                },
                {
                    "authorId": "2290999020",
                    "name": "Jos\u00e9phine A Cool"
                },
                {
                    "authorId": "2269132492",
                    "name": "Zahir Kanjee"
                },
                {
                    "authorId": "2316179559",
                    "name": "Kathleen P Lane"
                },
                {
                    "authorId": "2316180115",
                    "name": "Andrew S Parsons"
                },
                {
                    "authorId": "2291413200",
                    "name": "Neera Ahuja"
                },
                {
                    "authorId": "2234484064",
                    "name": "Eric Horvitz"
                },
                {
                    "authorId": "2294748901",
                    "name": "Daniel Yang"
                },
                {
                    "authorId": "2274260540",
                    "name": "Arnold Milstein"
                },
                {
                    "authorId": "2315089306",
                    "name": "Andrew P J Olson"
                },
                {
                    "authorId": "2284201252",
                    "name": "Jason Hom"
                },
                {
                    "authorId": "2295846554",
                    "name": "Jonathan H Chen"
                },
                {
                    "authorId": "2265754991",
                    "name": "Adam Rodman"
                }
            ],
            "abstract": "Importance: Large language model (LLM) artificial intelligence (AI) systems have shown promise in diagnostic reasoning, but their utility in management reasoning with no clear right answers is unknown. Objective: To determine whether LLM assistance improves physician performance on open-ended management reasoning tasks compared to conventional resources. Design: Prospective, randomized controlled trial conducted from 30 November 2023 to 21 April 2024. Setting: Multi-institutional study from Stanford University, Beth Israel Deaconess Medical Center, and the University of Virginia involving physicians from across the United States. Participants: 92 practicing attending physicians and residents with training in internal medicine, family medicine, or emergency medicine. Intervention: Five expert-developed clinical case vignettes were presented with multiple open-ended management questions and scoring rubrics created through a Delphi process. Physicians were randomized to use either GPT-4 via ChatGPT Plus in addition to conventional resources (e.g., UpToDate, Google), or conventional resources alone. Main Outcomes and Measures: The primary outcome was difference in total score between groups on expert-developed scoring rubrics. Secondary outcomes included domain-specific scores and time spent per case. Results: Physicians using the LLM scored higher compared to those using conventional resources (mean difference 6.5 %, 95% CI 2.7-10.2, p<0.001). Significant improvements were seen in management decisions (6.1%, 95% CI 2.5-9.7, p=0.001), diagnostic decisions (12.1%, 95% CI 3.1-21.0, p=0.009), and case-specific (6.2%, 95% CI 2.4-9.9, p=0.002) domains. GPT-4 users spent more time per case (mean difference 119.3 seconds, 95% CI 17.4-221.2, p=0.02). There was no significant difference between GPT-4-augmented physicians and GPT-4 alone (-0.9%, 95% CI -9.0 to 7.2, p=0.8). Conclusions and Relevance: LLM assistance improved physician management reasoning compared to conventional resources, with particular gains in contextual and patient-specific decision-making. These findings indicate that LLMs can augment management decision-making in complex cases. Trial Registration ClinicalTrials.gov Identifier: NCT06208423; https://classic.clinicaltrials.gov/ct2/show/NCT06208423",
            "corpus_id": "271742786",
            "text": "Importance: Large language model (LLM) artificial intelligence (AI) systems have shown promise in diagnostic reasoning, but their utility in management reasoning with no clear right answers is unknown. Objective: To determine whether LLM assistance improves physician performance on open-ended management reasoning tasks compared to conventional resources. Design: Prospective, randomized controlled trial conducted from 30 November 2023 to 21 April 2024. Setting: Multi-institutional study from Stanford University, Beth Israel Deaconess Medical Center, and the University of Virginia involving physicians from across the United States. Participants: 92 practicing attending physicians and residents with training in internal medicine, family medicine, or emergency medicine. Intervention: Five expert-developed clinical case vignettes were presented with multiple open-ended management questions and scoring rubrics created through a Delphi process. Physicians were randomized to use either GPT-4 via ChatGPT Plus in addition to conventional resources (e.g., UpToDate, Google), or conventional resources alone. Main Outcomes and Measures: The primary outcome was difference in total score between groups on expert-developed scoring rubrics. Secondary outcomes included domain-specific scores and time spent per case. Results: Physicians using the LLM scored higher compared to those using conventional resources (mean difference 6.5 %, 95% CI 2.7-10.2, p<0.001). Significant improvements were seen in management decisions (6.1%, 95% CI 2.5-9.7, p=0.001), diagnostic decisions (12.1%, 95% CI 3.1-21.0, p=0.009), and case-specific (6.2%, 95% CI 2.4-9.9, p=0.002) domains. GPT-4 users spent more time per case (mean difference 119.3 seconds, 95% CI 17.4-221.2, p=0.02). There was no significant difference between GPT-4-augmented physicians and GPT-4 alone (-0.9%, 95% CI -9.0 to 7.2, p=0.8). Conclusions and Relevance: LLM assistance improved physician management reasoning compared to conventional resources, with particular gains in contextual and patient-specific decision-making. These findings indicate that LLMs can augment management decision-making in complex cases. Trial Registration ClinicalTrials.gov Identifier: NCT06208423; https://classic.clinicaltrials.gov/ct2/show/NCT06208423",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.05560302734375
        },
        {
            "paperId": "be1985ec9f0bce3b45c81e7b28b93acbaf7e20eb",
            "corpusId": 278782199,
            "title": "Self-Evolving Curriculum for LLM Reasoning",
            "venue": "",
            "year": 2025,
            "referenceCount": 65,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.14970, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323913703",
                    "name": "Xiaoyin Chen"
                },
                {
                    "authorId": "2257319389",
                    "name": "Jiarui Lu"
                },
                {
                    "authorId": "2362621125",
                    "name": "Minsu Kim"
                },
                {
                    "authorId": "113087412",
                    "name": "Dinghuai Zhang"
                },
                {
                    "authorId": "2363320556",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "2334741390",
                    "name": "Alex Pich'e"
                },
                {
                    "authorId": "2334740526",
                    "name": "Nicolas Gontier"
                },
                {
                    "authorId": "1865800402",
                    "name": "Y. Bengio"
                },
                {
                    "authorId": "2334740521",
                    "name": "Ehsan Kamalloo"
                }
            ],
            "abstract": "Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.",
            "corpus_id": "278782199",
            "text": "Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7783203125
        },
        {
            "paperId": "7af7364311c00adcf5a82591cac0a9a17880aa66",
            "corpusId": 271855275,
            "title": "P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for Optimizing LLM Training",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 37,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.05541?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.05541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2224854311",
                    "name": "Yingxuan Yang"
                },
                {
                    "authorId": "2315948077",
                    "name": "Huayi Wang"
                },
                {
                    "authorId": "2111875607",
                    "name": "Muning Wen"
                },
                {
                    "authorId": "2315940339",
                    "name": "Weinan Zhang"
                }
            ],
            "abstract": "In the rapidly evolving field of Large Language Models (LLMs), selecting high-quality data for fine-tuning is essential. This paper focuses on task-specific data pruning and selection to enhance fine-tuning. We introduce an innovative framework, termed P3 , which improves LLM performance through a dynamic, adaptive training strategy. Specifically, P3 comprises the following components: (1) P olicy-driven Difficulty Measurement: we begin by measuring the difficulty of data based on the model\u2019s real-time performance, transitioning from static, predefined metrics to more dynamic and adaptable ones. (2) P ace-adaptive Selection: we employ self-paced learning (SPL) to gradually select increasingly challenging data, thereby progressively enhancing the model\u2019s performance. (3) Diversity P romotion: we integrate Determinantal Point Process (DPP) into the selection process to promote the diversity within and be-tween samples, enriching the learning process. We have validated our method on two well-known LLM datasets, APPS and MATH, designed for logical reasoning scenarios. The results show that our P3 framework significantly improves training outcomes compared to traditional methods. By fundamentally refining data selection and utilization strategies, P3 not only advances theoretical understanding of dynamic training approaches but also provides a versatile framework that can revolutionize model training in natural language processing.",
            "corpus_id": "271855275",
            "text": "In the rapidly evolving field of Large Language Models (LLMs), selecting high-quality data for fine-tuning is essential. This paper focuses on task-specific data pruning and selection to enhance fine-tuning. We introduce an innovative framework, termed P3 , which improves LLM performance through a dynamic, adaptive training strategy. Specifically, P3 comprises the following components: (1) P olicy-driven Difficulty Measurement: we begin by measuring the difficulty of data based on the model\u2019s real-time performance, transitioning from static, predefined metrics to more dynamic and adaptable ones. (2) P ace-adaptive Selection: we employ self-paced learning (SPL) to gradually select increasingly challenging data, thereby progressively enhancing the model\u2019s performance. (3) Diversity P romotion: we integrate Determinantal Point Process (DPP) into the selection process to promote the diversity within and be-tween samples, enriching the learning process. We have validated our method on two well-known LLM datasets, APPS and MATH, designed for logical reasoning scenarios. The results show that our P3 framework significantly improves training outcomes compared to traditional methods. By fundamentally refining data selection and utilization strategies, P3 not only advances theoretical understanding of dynamic training approaches but also provides a versatile framework that can revolutionize model training in natural language processing.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.94580078125
        },
        {
            "paperId": "e3cde6119ab59d98a9ca56da14cbd786a71fc150",
            "corpusId": 276782441,
            "title": "Enhancing Vietnamese VQA through Curriculum Learning on Raw and Augmented Text Representations",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 29,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.03285, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349303430",
                    "name": "Khoi Anh Nguyen"
                },
                {
                    "authorId": "2348544032",
                    "name": "Linh Yen Vu"
                },
                {
                    "authorId": "2348544376",
                    "name": "Thang Dinh Duong"
                },
                {
                    "authorId": "2348544378",
                    "name": "Thuan Nguyen Duong"
                },
                {
                    "authorId": "2279672202",
                    "name": "Huy Thanh Nguyen"
                },
                {
                    "authorId": "3104927",
                    "name": "V. Q. Dinh"
                }
            ],
            "abstract": "Visual Question Answering (VQA) is a multimodal task requiring reasoning across textual and visual inputs, which becomes particularly challenging in low-resource languages like Vietnamese due to linguistic variability and the lack of high-quality datasets. Traditional methods often rely heavily on extensive annotated datasets, computationally expensive pipelines, and large pre-trained models, specifically in the domain of Vietnamese VQA, limiting their applicability in such scenarios. To address these limitations, we propose a training framework that combines a paraphrase-based feature augmentation module with a dynamic curriculum learning strategy. Explicitly, augmented samples are considered\"easy\"while raw samples are regarded as\"hard\". The framework then utilizes a mechanism that dynamically adjusts the ratio of easy to hard samples during training, progressively modifying the same dataset to increase its difficulty level. By enabling gradual adaptation to task complexity, this approach helps the Vietnamese VQA model generalize well, thus improving overall performance. Experimental results show consistent improvements on the OpenViVQA dataset and mixed outcomes on the ViVQA dataset, highlighting both the potential and challenges of our approach in advancing VQA for Vietnamese language.",
            "corpus_id": "276782441",
            "text": "Visual Question Answering (VQA) is a multimodal task requiring reasoning across textual and visual inputs, which becomes particularly challenging in low-resource languages like Vietnamese due to linguistic variability and the lack of high-quality datasets. Traditional methods often rely heavily on extensive annotated datasets, computationally expensive pipelines, and large pre-trained models, specifically in the domain of Vietnamese VQA, limiting their applicability in such scenarios. To address these limitations, we propose a training framework that combines a paraphrase-based feature augmentation module with a dynamic curriculum learning strategy. Explicitly, augmented samples are considered\"easy\"while raw samples are regarded as\"hard\". The framework then utilizes a mechanism that dynamically adjusts the ratio of easy to hard samples during training, progressively modifying the same dataset to increase its difficulty level. By enabling gradual adaptation to task complexity, this approach helps the Vietnamese VQA model generalize well, thus improving overall performance. Experimental results show consistent improvements on the OpenViVQA dataset and mixed outcomes on the ViVQA dataset, highlighting both the potential and challenges of our approach in advancing VQA for Vietnamese language.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.478515625
        },
        {
            "paperId": "1bfeeb84c442f303d43071e8c9d9c739dcd4864f",
            "corpusId": 270068038,
            "title": "Revisit, Extend, and Enhance Hessian-Free Influence Functions",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303494624",
                    "name": "Ziao Yang"
                },
                {
                    "authorId": "2106738332",
                    "name": "Han Yue"
                },
                {
                    "authorId": "2300247985",
                    "name": "Jian Chen"
                },
                {
                    "authorId": "2300252559",
                    "name": "Hongfu Liu"
                }
            ],
            "abstract": "Influence functions serve as crucial tools for assessing sample influence in model interpretation, subset training set selection, noisy label detection, and more. By employing the first-order Taylor extension, influence functions can estimate sample influence without the need for expensive model retraining. However, applying influence functions directly to deep models presents challenges, primarily due to the non-convex nature of the loss function and the large size of model parameters. This difficulty not only makes computing the inverse of the Hessian matrix costly but also renders it non-existent in some cases. Various approaches, including matrix decomposition, have been explored to expedite and approximate the inversion of the Hessian matrix, with the aim of making influence functions applicable to deep models. In this paper, we revisit a specific, albeit naive, yet effective approximation method known as TracIn. This method substitutes the inverse of the Hessian matrix with an identity matrix. We provide deeper insights into why this simple approximation method performs well. Furthermore, we extend its applications beyond measuring model utility to include considerations of fairness and robustness. Finally, we enhance TracIn through an ensemble strategy. To validate its effectiveness, we conduct experiments on synthetic data and extensive evaluations on noisy label detection, sample selection for large language model fine-tuning, and defense against adversarial attacks.",
            "corpus_id": "270068038",
            "text": "Influence functions serve as crucial tools for assessing sample influence in model interpretation, subset training set selection, noisy label detection, and more. By employing the first-order Taylor extension, influence functions can estimate sample influence without the need for expensive model retraining. However, applying influence functions directly to deep models presents challenges, primarily due to the non-convex nature of the loss function and the large size of model parameters. This difficulty not only makes computing the inverse of the Hessian matrix costly but also renders it non-existent in some cases. Various approaches, including matrix decomposition, have been explored to expedite and approximate the inversion of the Hessian matrix, with the aim of making influence functions applicable to deep models. In this paper, we revisit a specific, albeit naive, yet effective approximation method known as TracIn. This method substitutes the inverse of the Hessian matrix with an identity matrix. We provide deeper insights into why this simple approximation method performs well. Furthermore, we extend its applications beyond measuring model utility to include considerations of fairness and robustness. Finally, we enhance TracIn through an ensemble strategy. To validate its effectiveness, we conduct experiments on synthetic data and extensive evaluations on noisy label detection, sample selection for large language model fine-tuning, and defense against adversarial attacks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.485107421875
        },
        {
            "paperId": "e435669c91821358f6e622e2abb5cf4a9d3ba994",
            "corpusId": 270122652,
            "title": "Constructing knowledge: the role of AI in medical learning",
            "venue": "J. Am. Medical Informatics Assoc.",
            "year": 2024,
            "referenceCount": 1,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1093/jamia/ocae124?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/jamia/ocae124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315920127",
                    "name": "Aaron Lawson McLean"
                }
            ],
            "abstract": "The integration of large language models (LLMs) like ChatGPT into medical education presents potential benefits and challenges. These technologies, aligned with constructivist learning theories, could potentially enhance critical thinking and problem-solving through inquiry-based learning environments. However, the actual impact on educational outcomes and the effectiveness of these tools in fostering learning require further empirical study. This technological shift necessitates a reevaluation of curriculum design and the development of new assessment methodologies to measure its effects accurately. Additionally, the use of LLMs introduces significant ethical concerns, particularly in addressing inherent AI biases to ensure equitable educational access. LLMs may also help reduce global disparities in medical education by providing broader access to contemporary medical knowledge and practices, though their deployment must be managed carefully to truly support the training of competent, ethical medical professionals.",
            "corpus_id": "270122652",
            "text": "The integration of large language models (LLMs) like ChatGPT into medical education presents potential benefits and challenges. These technologies, aligned with constructivist learning theories, could potentially enhance critical thinking and problem-solving through inquiry-based learning environments. However, the actual impact on educational outcomes and the effectiveness of these tools in fostering learning require further empirical study. This technological shift necessitates a reevaluation of curriculum design and the development of new assessment methodologies to measure its effects accurately. Additionally, the use of LLMs introduces significant ethical concerns, particularly in addressing inherent AI biases to ensure equitable educational access. LLMs may also help reduce global disparities in medical education by providing broader access to contemporary medical knowledge and practices, though their deployment must be managed carefully to truly support the training of competent, ethical medical professionals.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.14990234375
        },
        {
            "paperId": "e8db25fc84442d27305675bc67f101ee2cad8d49",
            "corpusId": 277983852,
            "title": "Abstract 5084: Evaluation of single-cell foundation models for cancer outcome predictions",
            "venue": "Cancer Research",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1158/1538-7445.am2025-5084?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1158/1538-7445.am2025-5084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151021595",
                    "name": "Haitham A. Elmarakeby"
                },
                {
                    "authorId": "2356742559",
                    "name": "Ahmed Roman"
                },
                {
                    "authorId": "1388622178",
                    "name": "S. Johri"
                },
                {
                    "authorId": "2124329712",
                    "name": "Eliezer Van Allen"
                }
            ],
            "abstract": "\n \n Tumor single-cell data provide unique insights into the cellular dynamics of cancer progression and therapeutic outcomes. Inspired by advancements in natural language processing (NLP), foundation models are increasingly used in single-cell analyses to uncover biological insights and identify clinical biomarkers. However, the performance of these models on cancer-specific tasks remains underexplored. We present an extensive evaluation of foundation models trained on millions of single-cell profiles for cancer-focused investigations.\n \n \n \n We evaluated nine predictive models-three traditional methods and six foundation models (four Geneformer and two scGPT models), across various downstream tasks. Traditional models used embeddings from high variable genes (HVG), principal components (PCA), and generative model embedding (scVI), each paired with a random forest classifier for downstream classification. The evaluated foundation models varied in architecture sizes, pretraining dataset sizes (30M to 95M cell profiles), and training datasets, which were either generic or cancer-specific. We assessed the quality of representations by measuring their predictive performance on lung cancer treatment status (TKI treated vs. treatment-naive patients) and breast cancer subtypes (ER+ vs. TNBC). We examined how model characteristics influenced their performance in these domains for all tasks.\n \n \n \n Zero-shot evaluations of the foundation models showed superior single-cell data clustering, measured by average silhouette width (ASW), compared to traditional methods (HVG, PCA, scVI), suggesting that pre-trained embeddings from millions of single-cell profiles may offer enhanced representations. We thus evaluated the learned representation on downstream predictive tasks, showing that foundation models with a larger pretraining dataset (95M cells), cancer-specific training, larger input size (4096 genes), and deeper model (12 layers) achieved the best performance among foundation models (F1=0.97) for predicting treatment status in lung cancer. In contrast, smaller and generic foundation models had significantly lower performance, however, additional self-supervised training on task-specific data rescued their performance to match larger foundation models. Similar trends were observed in predicting breast cancer subtypes. Notably, the best foundation model\u2019s performance was not significantly different from the best traditional method (PCA) in predicting breast cancer subtypes.\n \n \n \n These results highlight key factors driving foundation model performance on cancer-specific tasks. While current foundation models show inconsistent contributions to cancer-specific questions, observed performance trends highlight their potential to enhance single-cell data representation and drive significant advancements in cancer research and precision medicine.\n \n \n \n Haitham Elmarakeby, Ahmed Roman, Shreya Johri, Eliezer M. Van Allen. Evaluation of single-cell foundation models for cancer outcome predictions [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2025; Part 1 (Regular Abstracts); 2025 Apr 25-30; Chicago, IL. Philadelphia (PA): AACR; Cancer Res 2025;85(8_Suppl_1):Abstract nr 5084.\n",
            "corpus_id": "277983852",
            "text": "\n \n Tumor single-cell data provide unique insights into the cellular dynamics of cancer progression and therapeutic outcomes. Inspired by advancements in natural language processing (NLP), foundation models are increasingly used in single-cell analyses to uncover biological insights and identify clinical biomarkers. However, the performance of these models on cancer-specific tasks remains underexplored. We present an extensive evaluation of foundation models trained on millions of single-cell profiles for cancer-focused investigations.\n \n \n \n We evaluated nine predictive models-three traditional methods and six foundation models (four Geneformer and two scGPT models), across various downstream tasks. Traditional models used embeddings from high variable genes (HVG), principal components (PCA), and generative model embedding (scVI), each paired with a random forest classifier for downstream classification. The evaluated foundation models varied in architecture sizes, pretraining dataset sizes (30M to 95M cell profiles), and training datasets, which were either generic or cancer-specific. We assessed the quality of representations by measuring their predictive performance on lung cancer treatment status (TKI treated vs. treatment-naive patients) and breast cancer subtypes (ER+ vs. TNBC). We examined how model characteristics influenced their performance in these domains for all tasks.\n \n \n \n Zero-shot evaluations of the foundation models showed superior single-cell data clustering, measured by average silhouette width (ASW), compared to traditional methods (HVG, PCA, scVI), suggesting that pre-trained embeddings from millions of single-cell profiles may offer enhanced representations. We thus evaluated the learned representation on downstream predictive tasks, showing that foundation models with a larger pretraining dataset (95M cells), cancer-specific training, larger input size (4096 genes), and deeper model (12 layers) achieved the best performance among foundation models (F1=0.97) for predicting treatment status in lung cancer. In contrast, smaller and generic foundation models had significantly lower performance, however, additional self-supervised training on task-specific data rescued their performance to match larger foundation models. Similar trends were observed in predicting breast cancer subtypes. Notably, the best foundation model\u2019s performance was not significantly different from the best traditional method (PCA) in predicting breast cancer subtypes.\n \n \n \n These results highlight key factors driving foundation model performance on cancer-specific tasks. While current foundation models show inconsistent contributions to cancer-specific questions, observed performance trends highlight their potential to enhance single-cell data representation and drive significant advancements in cancer research and precision medicine.\n \n \n \n Haitham Elmarakeby, Ahmed Roman, Shreya Johri, Eliezer M. Van Allen. Evaluation of single-cell foundation models for cancer outcome predictions [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2025; Part 1 (Regular Abstracts); 2025 Apr 25-30; Chicago, IL. Philadelphia (PA): AACR; Cancer Res 2025;85(8_Suppl_1):Abstract nr 5084.\n",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.044586181640625
        },
        {
            "paperId": "c0006d5fbe295a06a176766b3be3d856125d5097",
            "corpusId": 278886084,
            "title": "An End-to-End Approach for Child Reading Assessment in the Xhosa Language",
            "venue": "",
            "year": 2025,
            "referenceCount": 26,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.17371, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52010784",
                    "name": "S. Chevtchenko"
                },
                {
                    "authorId": "2363347266",
                    "name": "Nikhil Navas"
                },
                {
                    "authorId": "2205563843",
                    "name": "Rafaella Vale"
                },
                {
                    "authorId": "2363342127",
                    "name": "Franco Ubaudi"
                },
                {
                    "authorId": "2363340909",
                    "name": "Sipumelele Lucwaba"
                },
                {
                    "authorId": "6839112",
                    "name": "Cally Ardington"
                },
                {
                    "authorId": "2363343938",
                    "name": "Soheil Afshar"
                },
                {
                    "authorId": "2363341986",
                    "name": "Mark Antoniou"
                },
                {
                    "authorId": "2196937620",
                    "name": "Saeed Afshar"
                }
            ],
            "abstract": "Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual's life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children's voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained.",
            "corpus_id": "278886084",
            "text": "Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual's life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children's voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0775146484375
        },
        {
            "paperId": "cc7964b5bae289fd39fb8531446b0af541b909a4",
            "corpusId": 277667305,
            "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 106,
            "citationCount": 20,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07934, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238396806",
                    "name": "Xiyao Wang"
                },
                {
                    "authorId": "2149231840",
                    "name": "Zhengyuan Yang"
                },
                {
                    "authorId": "2355366375",
                    "name": "Chao Feng"
                },
                {
                    "authorId": "2280118198",
                    "name": "Hongjin Lu"
                },
                {
                    "authorId": "50703697",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "2249709105",
                    "name": "Chung-Ching Lin"
                },
                {
                    "authorId": "2249717753",
                    "name": "K. Lin"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2273909761",
                    "name": "Lijuan Wang"
                }
            ],
            "abstract": "We introduce ThinkLite-VL, a family of visual reasoning models that achieve state-of-the-art (SoTA) performance using an order of magnitude fewer training samples, relying purely on reinforcement fine-tuning (RFT) self-improvement without any knowledge distillation. Our central insight is that sample difficulty critically influences RFT effectiveness: appropriately challenging examples can drive substantial reasoning improvements, even in low-data regimes. However, quantifying sample difficulty in a reliable and scalable manner remains non-trivial. To address this, we repurpose Monte Carlo Tree Search (MCTS) to measure sample difficulty via the number of reasoning iterations a vision-language model (VLM) requires to solve each instance. This MCTS-based selection procedure identifies samples that induce deeper reasoning while remaining solvable, allowing us to filter a high-quality subset from 70k open-source examples spanning math, natural image understanding, and chart comprehension. Using this approach, we select just 11k challenging samples for RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly outperform their respective base models across eight visual reasoning benchmarks. In particular, ThinkLite-VL-7B improves the average performance of Qwen2.5-VL-7B-Instruct by 7\\% and surpasses all existing 7B-level models, as well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a new SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark improvement of 4.42 over the open-source SOTA. These results demonstrate that MCTS-guided difficulty filtering provides a scalable and effective path toward data-efficient self-improvement in multimodal reasoning.",
            "corpus_id": "277667305",
            "text": "We introduce ThinkLite-VL, a family of visual reasoning models that achieve state-of-the-art (SoTA) performance using an order of magnitude fewer training samples, relying purely on reinforcement fine-tuning (RFT) self-improvement without any knowledge distillation. Our central insight is that sample difficulty critically influences RFT effectiveness: appropriately challenging examples can drive substantial reasoning improvements, even in low-data regimes. However, quantifying sample difficulty in a reliable and scalable manner remains non-trivial. To address this, we repurpose Monte Carlo Tree Search (MCTS) to measure sample difficulty via the number of reasoning iterations a vision-language model (VLM) requires to solve each instance. This MCTS-based selection procedure identifies samples that induce deeper reasoning while remaining solvable, allowing us to filter a high-quality subset from 70k open-source examples spanning math, natural image understanding, and chart comprehension. Using this approach, we select just 11k challenging samples for RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly outperform their respective base models across eight visual reasoning benchmarks. In particular, ThinkLite-VL-7B improves the average performance of Qwen2.5-VL-7B-Instruct by 7\\% and surpasses all existing 7B-level models, as well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a new SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark improvement of 4.42 over the open-source SOTA. These results demonstrate that MCTS-guided difficulty filtering provides a scalable and effective path toward data-efficient self-improvement in multimodal reasoning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.72412109375
        },
        {
            "paperId": "99ffecac1445b61e16821bcbe08e69453621809b",
            "corpusId": 269350362,
            "title": "ChatGPT in Education \u2013 Understanding the Bahraini Academics Perspective",
            "venue": "Electronic Journal of e-Learning",
            "year": 2024,
            "referenceCount": 35,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic-publishing.org/index.php/ejel/article/download/3250/2222",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.34190/ejel.22.2.3250?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.34190/ejel.22.2.3250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298171891",
                    "name": "Amal Alrayes"
                },
                {
                    "authorId": "114613796",
                    "name": "Tara Fryad Henari"
                },
                {
                    "authorId": "2298121927",
                    "name": "Dalal Abdulkarim Ahmed"
                }
            ],
            "abstract": "This paper provides a thorough examination of the role of Artificial Intelligence (AI), particularly ChatGPT and other AI language models, in the realm of education. Drawing insights from existing literature and a novel study on educator perspectives, the paper delves into the potential advantages, ethical dilemmas, and factors shaping educators' attitudes towards AI integration in education. AI language models have the potential to revolutionize educational content creation, personalize learning experiences, and streamline assessment and feedback processes. These capabilities hold the potential to enhance teaching and learning outcomes while catering to the diverse needs of students. However, ethical concerns loom large in the adoption of AI in education. Bias in generated content is a chief concern, as it can perpetuate societal biases and lead to unfair treatment or the dissemination of inaccurate information. The solution lies in rigorous data curation to ensure equitable educational experiences for all students. Moreover, the potential for generating inappropriate or misleading content poses a significant ethical challenge, impacting students' well-being, civic understanding, and social interactions. Safeguards must be implemented to detect and rectify biased or inappropriate content, fostering inclusive and unbiased learning environments. Transparency emerges as a crucial ethical consideration. The opacity of AI models like ChatGPT makes it difficult to comprehend their decision-making processes. Enhancing model interpretability and explainability is vital for accountability and addressing embedded ethical issues. Privacy concerns related to data collection and usage are emphasized in the literature. Clear policies and guidelines must govern data collection, use, and protection, ensuring data is solely employed for educational purposes and maintaining robust data security measures. Our study expands upon these insights by exploring socio-demographic factors, motivations, and social influences affecting educators' AI adoption in higher education. These findings inform institutions on tailoring AI integration strategies, emphasizing responsible usage through training, and assessing the impact on learning outcomes. As educational institutions increasingly embrace AI, including advanced models like GPT-4, a cautious and thoughtful approach is vital. Balancing potential benefits with ethical challenges ensures that AI enhances teaching and learning while upholding fairness, equity, and accountability. In summary, this paper illuminates the potential of AI in education, accentuates ethical concerns, and highlights the significance of understanding educators' perspectives. Collaboration between educators and policymakers is essential to navigate the complexities of AI integration, ensuring that education remains a realm of equitable, efficient, and accountable learning experiences.",
            "corpus_id": "269350362",
            "text": "This paper provides a thorough examination of the role of Artificial Intelligence (AI), particularly ChatGPT and other AI language models, in the realm of education. Drawing insights from existing literature and a novel study on educator perspectives, the paper delves into the potential advantages, ethical dilemmas, and factors shaping educators' attitudes towards AI integration in education. AI language models have the potential to revolutionize educational content creation, personalize learning experiences, and streamline assessment and feedback processes. These capabilities hold the potential to enhance teaching and learning outcomes while catering to the diverse needs of students. However, ethical concerns loom large in the adoption of AI in education. Bias in generated content is a chief concern, as it can perpetuate societal biases and lead to unfair treatment or the dissemination of inaccurate information. The solution lies in rigorous data curation to ensure equitable educational experiences for all students. Moreover, the potential for generating inappropriate or misleading content poses a significant ethical challenge, impacting students' well-being, civic understanding, and social interactions. Safeguards must be implemented to detect and rectify biased or inappropriate content, fostering inclusive and unbiased learning environments. Transparency emerges as a crucial ethical consideration. The opacity of AI models like ChatGPT makes it difficult to comprehend their decision-making processes. Enhancing model interpretability and explainability is vital for accountability and addressing embedded ethical issues. Privacy concerns related to data collection and usage are emphasized in the literature. Clear policies and guidelines must govern data collection, use, and protection, ensuring data is solely employed for educational purposes and maintaining robust data security measures. Our study expands upon these insights by exploring socio-demographic factors, motivations, and social influences affecting educators' AI adoption in higher education. These findings inform institutions on tailoring AI integration strategies, emphasizing responsible usage through training, and assessing the impact on learning outcomes. As educational institutions increasingly embrace AI, including advanced models like GPT-4, a cautious and thoughtful approach is vital. Balancing potential benefits with ethical challenges ensures that AI enhances teaching and learning while upholding fairness, equity, and accountability. In summary, this paper illuminates the potential of AI in education, accentuates ethical concerns, and highlights the significance of understanding educators' perspectives. Collaboration between educators and policymakers is essential to navigate the complexities of AI integration, ensuring that education remains a realm of equitable, efficient, and accountable learning experiences.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.049041748046875
        },
        {
            "paperId": "68a8a2e1f2df2e43c1c85d7250f915cdbcd3e8d2",
            "corpusId": 245335681,
            "title": "Proposal of a multivariate analysis model to evaluate the learning outcomes of students in higher education",
            "venue": "Technological Ecosystem for Enhancing Multiculturality",
            "year": 2021,
            "referenceCount": 54,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3486011.3486536?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3486011.3486536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2024334292",
                    "name": "M\u00f3nica Hern\u00e1ndez-Campos"
                },
                {
                    "authorId": "1813009",
                    "name": "Antonio Gonz\u00e1lez Torres"
                },
                {
                    "authorId": "1387451194",
                    "name": "F. Garc\u00eda-Pe\u00f1alvo"
                }
            ],
            "abstract": "Accreditation agencies request evidence of the graduates learning outcomes during quality processes. Universities have to provide exhaustive empirical proof of their achievement and development through the academic training program, which is a significant challenge because it implies a substantial improvement and change of the educational model and evaluation method. There are still theoretical gaps to explain from an empirical perspective which elements of the university curriculum are associated with academic programs learning outcomes. The lack of empirical evidence prevents us from identifying which factors are associated with an efficient achievement of learning results in higher education institutions to implement improvement actions based on valid and objective data. In this context, learning analytics is a valuable tool that can support collecting, measuring, analyzing, and reporting data to understand the different factors involved in the educational process and their influence on learning. The primary purpose of this study is to answer how to define and validate a multivariate metamodel based on the factors involved in learning outcomes and academic performance results. In this article, we present the objectives and their phases, activities, procedures, and instruments to achieve the goal of the study. We expect to validate a metamodel on a large-scale population with different academic engineering programs at the Costa Rica Institute of Technology.",
            "corpus_id": "245335681",
            "text": "Accreditation agencies request evidence of the graduates learning outcomes during quality processes. Universities have to provide exhaustive empirical proof of their achievement and development through the academic training program, which is a significant challenge because it implies a substantial improvement and change of the educational model and evaluation method. There are still theoretical gaps to explain from an empirical perspective which elements of the university curriculum are associated with academic programs learning outcomes. The lack of empirical evidence prevents us from identifying which factors are associated with an efficient achievement of learning results in higher education institutions to implement improvement actions based on valid and objective data. In this context, learning analytics is a valuable tool that can support collecting, measuring, analyzing, and reporting data to understand the different factors involved in the educational process and their influence on learning. The primary purpose of this study is to answer how to define and validate a multivariate metamodel based on the factors involved in learning outcomes and academic performance results. In this article, we present the objectives and their phases, activities, procedures, and instruments to achieve the goal of the study. We expect to validate a metamodel on a large-scale population with different academic engineering programs at the Costa Rica Institute of Technology.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.015777587890625
        },
        {
            "paperId": "6fc6f469e12656691cb31c118cdcd1b164630426",
            "corpusId": 266560348,
            "title": "The tools of the future are the challenges of today: The use of ChatGPT in problem-based learning medical education",
            "venue": "Medical Teacher",
            "year": 2023,
            "referenceCount": 6,
            "citationCount": 24,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.tandfonline.com/doi/pdf/10.1080/0142159X.2023.2290997?needAccess=true",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/0142159X.2023.2290997?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/0142159X.2023.2290997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276498952",
                    "name": "Christopher B Divito"
                },
                {
                    "authorId": "2276500601",
                    "name": "Bryan M. Katchikian"
                },
                {
                    "authorId": "2276498950",
                    "name": "Jenna E Gruenwald"
                },
                {
                    "authorId": "11575177",
                    "name": "Jennifer M Burgoon"
                }
            ],
            "abstract": "Abstract What is the educational challenge? Incorporation of large language model (LLM) or generative artificial intelligence (AI) software poses a challenge to various areas of medical education, including problem-based learning (PBL). LLMs, such as ChatGPT, have incredible potential to transform educational systems and enhance student learning outcomes when used responsibly. What are the proposed solutions? ChatGPT can provide several ways to support students and assist facilitators with course responsibilities. Here we address factors of implementation and describe how ChatGPT can be responsibly utilized to support key elements of PBL. How was the solution implemented? Providing reasonable access is an essential element of novel software implementation. Additionally, training for both faculty and staff is vital to foster responsible usage, provide base-line proficiency, and guide users to critically evaluate the quality of output. What lessons were learned that are relevant to a wider audience? The use of LLMs or other generative AI is dramatically rising in the world. Appropriate and conscientious incorporation of AI into educational programs can foster responsible use and potentially enhance student learning. What are the next steps? Assessment of learning outcomes, student self-efficacy, group dynamics, and stakeholder feedback are required to measure the effects of ChatGPT in the PBL curriculum. Additionally, software programs competitive with ChatGPT are currently under development and will also need to be investigated for their potential role in education.",
            "corpus_id": "266560348",
            "text": "Abstract What is the educational challenge? Incorporation of large language model (LLM) or generative artificial intelligence (AI) software poses a challenge to various areas of medical education, including problem-based learning (PBL). LLMs, such as ChatGPT, have incredible potential to transform educational systems and enhance student learning outcomes when used responsibly. What are the proposed solutions? ChatGPT can provide several ways to support students and assist facilitators with course responsibilities. Here we address factors of implementation and describe how ChatGPT can be responsibly utilized to support key elements of PBL. How was the solution implemented? Providing reasonable access is an essential element of novel software implementation. Additionally, training for both faculty and staff is vital to foster responsible usage, provide base-line proficiency, and guide users to critically evaluate the quality of output. What lessons were learned that are relevant to a wider audience? The use of LLMs or other generative AI is dramatically rising in the world. Appropriate and conscientious incorporation of AI into educational programs can foster responsible use and potentially enhance student learning. What are the next steps? Assessment of learning outcomes, student self-efficacy, group dynamics, and stakeholder feedback are required to measure the effects of ChatGPT in the PBL curriculum. Additionally, software programs competitive with ChatGPT are currently under development and will also need to be investigated for their potential role in education.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.063720703125
        },
        {
            "paperId": "dea8633907093609c7b20295f1760d2f3855a603",
            "corpusId": 268276918,
            "title": "A two-stage framework for Arabic social media text misinformation detection combining data augmentation and AraBERT",
            "venue": "Social Network Analysis and Mining",
            "year": 2024,
            "referenceCount": 24,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s13278-024-01201-4.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s13278-024-01201-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s13278-024-01201-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2157981242",
                    "name": "Ebtsam Mohamed"
                },
                {
                    "authorId": "10714919",
                    "name": "Walaa N. Ismail"
                },
                {
                    "authorId": "2181072883",
                    "name": "Osman Ali Sadek Ibrahim"
                },
                {
                    "authorId": "2316456456",
                    "name": "Eman M. G. Younis"
                }
            ],
            "abstract": "Misinformation can profoundly impact the reputation of an entity, and eliminating its spread has become a critical concern across various applications. Social media, often a primary source of information, can significantly influence individuals\u2019 perspectives through content from less credible sources. The utilization of machine-learning (ML) algorithms can facilitate automated, large-scale analysis of textual content, contributing to the rapid and efficient processing of extensive datasets for informed decision-making. Since the performance of ML models is highly affected by the size of the training data, many research papers have presented different approaches to solve the problem of limited dataset size. The data augmentation (DA) approach is one of these strategies, aiming to enhance ML model performance by increasing the amount of training data. DA generates new instances by applying different transformations to the original data instances. While many DA techniques have been investigated for various languages, such as English, achieving an enhancement of the classification model\u2019s performance on the new augmented dataset compared to the original dataset, there is a lack of studies on the Arabic language due to its unique characteristics. This paper introduces a novel two-stage framework designed for the automated identification of misinformation in Arabic textual content. The first stage aims to identify the optimal representation of features before feeding them to the ML model. Diverse representations of tweet content are explored, including N -grams, content-based features, and source-based features. The second stage focuses on investigating the DA effect through the back-translation technique applied to the original training data. Back-translation entails translating sentences from the target language (in this case, Arabic) into another language and then back to Arabic. As a result of this procedure, new examples for training are created by introducing variances in the text. The study utilizes support vector machine (SVM), naive Bayes, logistic regression (LR), and random forest (RF) as baseline algorithms. Additionally, AraBERT transformer pre-trained language models are used to relate the instance\u2019s label and feature representation of the input. Experimental outcomes demonstrate that misinformation detection, coupled with data augmentation, enhances accuracy by a noteworthy margin 5 to 12% compared to baseline machine-learning algorithms and pre-trained models. Remarkably, the results show the superiority of the N -grams approach over traditional state-of-the-art feature representations concerning accuracy, recall, precision, and F -measure metrics. This suggests a promising avenue for improving the efficacy of misinformation detection mechanisms in the realm of Arabic text analysis.",
            "corpus_id": "268276918",
            "text": "Misinformation can profoundly impact the reputation of an entity, and eliminating its spread has become a critical concern across various applications. Social media, often a primary source of information, can significantly influence individuals\u2019 perspectives through content from less credible sources. The utilization of machine-learning (ML) algorithms can facilitate automated, large-scale analysis of textual content, contributing to the rapid and efficient processing of extensive datasets for informed decision-making. Since the performance of ML models is highly affected by the size of the training data, many research papers have presented different approaches to solve the problem of limited dataset size. The data augmentation (DA) approach is one of these strategies, aiming to enhance ML model performance by increasing the amount of training data. DA generates new instances by applying different transformations to the original data instances. While many DA techniques have been investigated for various languages, such as English, achieving an enhancement of the classification model\u2019s performance on the new augmented dataset compared to the original dataset, there is a lack of studies on the Arabic language due to its unique characteristics. This paper introduces a novel two-stage framework designed for the automated identification of misinformation in Arabic textual content. The first stage aims to identify the optimal representation of features before feeding them to the ML model. Diverse representations of tweet content are explored, including N -grams, content-based features, and source-based features. The second stage focuses on investigating the DA effect through the back-translation technique applied to the original training data. Back-translation entails translating sentences from the target language (in this case, Arabic) into another language and then back to Arabic. As a result of this procedure, new examples for training are created by introducing variances in the text. The study utilizes support vector machine (SVM), naive Bayes, logistic regression (LR), and random forest (RF) as baseline algorithms. Additionally, AraBERT transformer pre-trained language models are used to relate the instance\u2019s label and feature representation of the input. Experimental outcomes demonstrate that misinformation detection, coupled with data augmentation, enhances accuracy by a noteworthy margin 5 to 12% compared to baseline machine-learning algorithms and pre-trained models. Remarkably, the results show the superiority of the N -grams approach over traditional state-of-the-art feature representations concerning accuracy, recall, precision, and F -measure metrics. This suggests a promising avenue for improving the efficacy of misinformation detection mechanisms in the realm of Arabic text analysis.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0548095703125
        },
        {
            "paperId": "5203b6fb8068efa7e0e68e57c059e0491b1003af",
            "corpusId": 255331751,
            "title": "Sparse Graph Transformer With Contrastive Learning",
            "venue": "IEEE Transactions on Computational Social Systems",
            "year": 2024,
            "referenceCount": 44,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSS.2022.3232117?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSS.2022.3232117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47423401",
                    "name": "Chun-Yang Zhang"
                },
                {
                    "authorId": "2199040176",
                    "name": "Wu-Peng Fang"
                },
                {
                    "authorId": "2198899876",
                    "name": "Hai-Chun Cai"
                },
                {
                    "authorId": "2152858950",
                    "name": "C. L. P. Chen"
                },
                {
                    "authorId": "2155853908",
                    "name": "Yue-Na Lin"
                }
            ],
            "abstract": "Information aggregation and propagation over networks via graph neural networks (GNNs) plays an important role in node or graph representation learning, which currently depend on the calculation with a fixed adjacency matrix, facing over-smoothing problem, and difficulty to stack multiple layers for high-level representations. In contrast, Transformer calculates an importance score for each node to learn its embedding via the attention mechanism and has achieved great successes in many natural language processing (NLP) and computer vision (CV) tasks. However, Transformer is inflexible to extend to graphs, as its input and output must have the same dimension. It will also become intractable to allocate attention over a large-scale graph due to distractions. Moreover, most graph Transformers are trained in supervised ways, which consume additional resources to annotate samples with potentially wrong labels and have limited generalization of representations. Therefore, this article attempts to build a new Sparse Graph Transformer with Contrastive learning for graph representation learning, called SGTC. Specifically, we first employ centrality measures to remove the redundant topological information from input graph according to the influences of nodes and edges, then disturb the pruned graph to get two different augmentation views, and learn node representations in a contrastive manner. Besides, a novel sparse attention mechanism is also proposed to capture structural features of graphs, which effectively save memory and training time. SGTC can produce low-dimensional and high-order node representations, which have better generalization for multiple tasks. The proposed model is evaluated on three downstream tasks over six networks, and experimental results confirm its superior performance against the state-of-the-art baselines.",
            "corpus_id": "255331751",
            "text": "Information aggregation and propagation over networks via graph neural networks (GNNs) plays an important role in node or graph representation learning, which currently depend on the calculation with a fixed adjacency matrix, facing over-smoothing problem, and difficulty to stack multiple layers for high-level representations. In contrast, Transformer calculates an importance score for each node to learn its embedding via the attention mechanism and has achieved great successes in many natural language processing (NLP) and computer vision (CV) tasks. However, Transformer is inflexible to extend to graphs, as its input and output must have the same dimension. It will also become intractable to allocate attention over a large-scale graph due to distractions. Moreover, most graph Transformers are trained in supervised ways, which consume additional resources to annotate samples with potentially wrong labels and have limited generalization of representations. Therefore, this article attempts to build a new Sparse Graph Transformer with Contrastive learning for graph representation learning, called SGTC. Specifically, we first employ centrality measures to remove the redundant topological information from input graph according to the influences of nodes and edges, then disturb the pruned graph to get two different augmentation views, and learn node representations in a contrastive manner. Besides, a novel sparse attention mechanism is also proposed to capture structural features of graphs, which effectively save memory and training time. SGTC can produce low-dimensional and high-order node representations, which have better generalization for multiple tasks. The proposed model is evaluated on three downstream tasks over six networks, and experimental results confirm its superior performance against the state-of-the-art baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06292724609375
        },
        {
            "paperId": "3526c824391bf919f75f5f586ce08da5302aad28",
            "corpusId": 277968005,
            "title": "The Impact of Innovative Strategies on Improving Writing Skills of English Learners at University Level",
            "venue": "Inverge Journal of Social Sciences",
            "year": 2025,
            "referenceCount": 36,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.63544/ijss.v4i1.120?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.63544/ijss.v4i1.120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351765628",
                    "name": "Qamar Abbas"
                },
                {
                    "authorId": "10421603",
                    "name": "Rabia Nawaz"
                },
                {
                    "authorId": "2356664823",
                    "name": "Mawra Tariq Malik"
                },
                {
                    "authorId": "2356700382",
                    "name": "Ahmad Yar"
                },
                {
                    "authorId": "2356665824",
                    "name": "Haji Muhammad Arif"
                }
            ],
            "abstract": "This study examined how creative strategies such as artificial intelligence (AI) tools, collaborative writing, and digital storytelling activities impacted the enhancement of academic writing skills of English learners at the university level. The research sought to understand the writing problems that were most common, evaluate the implementation and effectiveness of the measures, and monitor the students' reception to the measures being implemented. A quantitative approach using surveys was used with a sample of 346 students from different fields of study. Structured questionnaires which had been tested in a pilot study (Cronbach\u2019s Alpha = 0.792) were used to gather data and were then processed in SPSS (Version 28). Descriptive statistics and cross tabulation were used to analyse the data and find the patterns pertaining to difficulties in writing, use of tools, and outcomes. The most critical findings were the remaining issues: grammar was a problem for 62.5%, coherence for 55.2%, and vocabulary for 49.4% of the students. In spite of this, self-reported use of innovative strategies was still high such as: AI tools usage (69.7%) and peer collaboration (65.4%) as well as engaging interactive methods (82.6%). Additionally, there was strong support for institutional adoption, with 87% of participants in favour; however, there was less support for advanced digital storytelling techniques (39.3% engagement). \nThe ethical considerations such as anonymity and voluntary participation were followed. Testing in the pilot phase reduced bias and no personal information was stored. This study addresses the gap in the integration of technology and collaborative teaching processes in the teaching of academic writing. It provides empirical data on the effectiveness of modern approaches while also revealing gaps in adoption across levels of proficiency and disciplines. The results make it possible for curriculum developers and decision makers to take tangible steps toward solving the discrepancies between offer and demand in the context of use. \nReferences \nAbahussain, M. O.\u00a0(2020). Investigating EFL learners\u2019 perceptions of collaborative writing.\u00a0International Journal of English Linguistics, 10(3), 32\u201347.\u00a0 \nAlawaji, N. N. M.\u00a0(2020). Students' perceptions of collaborative summary writing.\u00a0Theory and Practice in Language Studies, 10(6), 700\u2013707.\u00a0 \nBelyaeva, E. G.\u00a0(2022). Methodological model of teaching academic writing to undergraduate students.\u00a0Focus on Language Education and Research, 3(1), 36\u201351. \nButt, S.\u00a0(2023). Employees\u2019 perception regarding in-house training programs in Pakistani organizations.\u00a0Journal of Workplace Behavior, 4(1), 35\u201350. \nButt, S., & Yazdani, N.\u00a0(2023). Implementation of quality management practices and firm\u2019s innovation performance: Mediation of knowledge creation processes and moderating role of digital transformation.\u00a0Pakistan Journal of Humanities and Social Sciences, 11(4), 3881\u20133902. \nButt, S., Umair, T., & Tajammal, R.\u00a0(2024). Nexus between key determinants of service quality and students\u2019 satisfaction in higher education institutions (HEIs).\u00a0Annals of Human and Social Sciences, 5(2), 659\u2013671. \nChiew, M. T. L., & Ismail, H. H.\u00a0(2021). Exploring vocabulary learning strategies in a second language setting: A review.\u00a0International Journal of Academic Research in Business and Social Sciences, 11(12), 1298\u20131309. \nChubko, N., Morris, J. E., McKinnon, D. H., Slater, E. V., & Lummis, G. W.\u00a0(2020). Digital storytelling as a disciplinary literacy enhancement tool for EFL students.\u00a0Educational Technology Research and Development, 68, 3587\u20133604.\u00a0https://doi.org/10.1007/s11423-020-09833-x \nDavoodifard, M.\u00a0(2022). An overview of writing process research: Using innovative tasks and techniques for a better understanding of L2 writing processes in assessment contexts.\u00a0Studies in Applied Linguistics and TESOL, 21(2). \nDodigovic, M., & Jeaco, S.\u00a0(2021). Technology in applied linguistics.\u00a0International Journal of TESOL Studies, 3(2), 1\u20135. \nFerris, D. R.\u00a0(2023).\u00a0What error correction can (not) accomplish for second language writers: Dispelling myths, discussing options.\u00a0University of Michigan Press. \nFerris, D. R., & Hedgcock, J. S.\u00a0(2023).\u00a0Teaching L2 composition: Purpose, process, and practice\u00a0(4th ed.). Routledge. \nFlowerdew, L.\u00a0(2021). Learner corpora for disciplinary writing. In\u00a0Research questions in language education and applied linguistics: A reference guide\u00a0(pp. 475\u2013479). Springer. \nFlowerdew, L., & Petri\u0107, B.\u00a0(2024). A critical review of corpus-based pedagogic perspectives on thesis writing: Specificity revisited.\u00a0English for Specific Purposes, 76, 1\u201313.\u00a0https://doi.org/10.1016/j.esp.2024.01.001 \nHinkel, E.\u00a0(2022).\u00a0Teaching academic L2 writing: Practical techniques in vocabulary and grammar\u00a0(2nd ed.). Routledge. \nHyland, K.\u00a0(2024). Ken Hyland's essential bookshelf: Academic writing.\u00a0Language Teaching, 57(3), 399\u2013407.\u00a0https://doi.org/10.1017/S0261444824000109 \nHyland, K., & Hyland, F.\u00a0(2019).\u00a0Feedback in second language writing: Contexts and issues\u00a0(2nd ed.). Cambridge University Press. \nKang, E. Y., & Han, Z.\u00a0(2021). Written corrective feedback. In\u00a0The Routledge handbook of second language acquisition and writing\u00a0(pp. [page range]). Routledge. \nKarim, K., & Nassaji, H.\u00a0(2020). The effects of written corrective feedback.\u00a0Instructed Second Language Acquisition, 3(1), 28\u201352. \nKessler, G.\u00a0(2020). Professionalizing your use of technology in English language teaching. In\u00a0Professionalizing your English language teaching\u00a0(pp. 163\u2013173). Springer. \nKessler, M.\u00a0(2023). Written corrective feedback in an online community: A typology of English language learners\u2019 requests and interlocutors\u2019 responses.\u00a0Computers and Composition, 67, 102752.\u00a0https://doi.org/10.1016/j.compcom.2023.102752 \nKim, N. J., & Kim, M. K.\u00a0(2022). Teacher\u2019s perceptions of using an artificial intelligence-based educational tool for scientific writing.\u00a0Frontiers in Education, 7, 755914.\u00a0https://doi.org/10.3389/feduc.2022.755914 \nLi, J.\u00a0(2017). Automated writing evaluation: A pedagogical tool.\u00a0TESOL Quarterly, 51(2), 427\u2013432.\u00a0 \nLi, M.\u00a0(2021).\u00a0Researching and teaching second language writing in the digital age.\u00a0Palgrave Macmillan. \nLi, M., & Zhang, M.\u00a0(2023). Collaborative writing in L2 classrooms: A research agenda.\u00a0Language Teaching, 56(1), 94\u2013112.\u00a0https://doi.org/10.1017/S0261444821000318 \nMihaylova, M., Gorin, S., Reber, T. P., & Rothen, N.\u00a0(2022). A meta-analysis on mobile-assisted language learning applications: Benefits and risks.\u00a0Psychologica Belgica, 62(1), 252\u2013267.\u00a0 \nMulyono, H., & Saskia, R.\u00a0(2021). Affective variables contributing to Indonesian EFL students\u2019 willingness to communicate within face-to-face and digital environments.\u00a0Cogent Education, 8(1), 1911282.\u00a0https://doi.org/10.1080/2331186X.2021.1911282 \nPoole, R.\u00a0(2022). \u201cCorpus can be tricky\u201d: Revisiting teacher attitudes towards corpus-aided language learning and teaching.\u00a0Computer Assisted Language Learning, 35(7), 1620\u20131641.\u00a0https://doi.org/10.1080/09588221.2020.1868533 \nReagan, D., Fell, E., & Mackey, A.\u00a0(2023). Applied linguistics in the age of anxiety.\u00a0Annual Review of Applied Linguistics, 43, 1\u20136.\u00a0 \nReppen, R.\u00a0(2022). Building a corpus: What are key considerations? In\u00a0The Routledge handbook of corpus linguistics\u00a0(pp. 13\u201320). Routledge. \nStorch, N.\u00a0(2021). Theoretical perspectives on L2 writing and language learning in collaborative writing and the collaborative processing of written corrective feedback. In\u00a0The Routledge handbook of second language acquisition and writing\u00a0(pp. 22\u201334). Routledge. \nUllah, A., & Usman, M.\u00a0(2023). Role of libraries in ensuring quality education at higher education institutions: A perspective of Pakistan.\u00a0Inverge Journal of Social Sciences, 2(4), 13\u201322. \nUllah, A.\u00a0(2024). Analyzing the students\u2019 attitudes and behavior towards traditional classes and technology-enhanced online learning.\u00a0International Journal of Social Science Archives.\u00a0https://www.ijssa.com/index.php/ijssa/article/view/498 \nUsman, M., Asif, M., Ullah, A., & Ullah, W.\u00a0(2024). User\u2019s habits and attitudes towards Chinese books reading in Pakistan.\u00a0Inverge Journal of Social Sciences, 3(2), 11\u201328. \nWei, W., Cheong, C. M., Zhu, X., & Lu, Q.\u00a0(2024). Comparing self-reflection and peer feedback practices in an academic writing task: A student self-efficacy perspective.\u00a0Teaching in Higher Education, 29(4), 896\u2013912.\u00a0https://doi.org/10.1080/13562517.2024.2316724 \nWiboolyasarin, W., Wiboolyasarin, K., Suwanwihok, K., Jinowat, N., & Muenjanchoey, R.\u00a0(2024). Synergizing collaborative writing and AI feedback: An investigation into enhancing L2 writing proficiency in wiki-based environments.\u00a0Computers and Education: Artificial Intelligence, 6, 100228.\u00a0https://doi.org/10.1016/j.caeai.2024.100228 \nWoodrow, L.\u00a0(2022).\u00a0Introducing researching English for specific purposes.\u00a0Routledge. \nXu, L., Naserpour, A., Rezai, A., Namaziandost, E., & Azizi, Z.\u00a0(2022). Exploring EFL learners\u2019 metaphorical conceptions of language learning: A multimodal analysis.\u00a0Journal of Psycholinguistic Research, 51(2), 323\u2013339.\u00a0https://doi.org/10.1007/s10936-022-09842-2 \nYALA, A.\u00a0(2022).\u00a0The use of mobile-assisted language learning to foster students\u2019 self-editing in sentence writing: Case of 1st year EFL students at Setif 2 University\u00a0[Doctoral dissertation, Universit\u00e9 de Batna 2]. \nZhang, Y. O., & Hyland, K.\u00a0(2021). Elements of doctoral apprenticeship: Community feedback and the acquisition of writing expertise.\u00a0Journal of Second Language Writing, 53, 100835.\u00a0https://doi.org/10.1016/j.jslw.2021.100835",
            "corpus_id": "277968005",
            "text": "This study examined how creative strategies such as artificial intelligence (AI) tools, collaborative writing, and digital storytelling activities impacted the enhancement of academic writing skills of English learners at the university level. The research sought to understand the writing problems that were most common, evaluate the implementation and effectiveness of the measures, and monitor the students' reception to the measures being implemented. A quantitative approach using surveys was used with a sample of 346 students from different fields of study. Structured questionnaires which had been tested in a pilot study (Cronbach\u2019s Alpha = 0.792) were used to gather data and were then processed in SPSS (Version 28). Descriptive statistics and cross tabulation were used to analyse the data and find the patterns pertaining to difficulties in writing, use of tools, and outcomes. The most critical findings were the remaining issues: grammar was a problem for 62.5%, coherence for 55.2%, and vocabulary for 49.4% of the students. In spite of this, self-reported use of innovative strategies was still high such as: AI tools usage (69.7%) and peer collaboration (65.4%) as well as engaging interactive methods (82.6%). Additionally, there was strong support for institutional adoption, with 87% of participants in favour; however, there was less support for advanced digital storytelling techniques (39.3% engagement). \nThe ethical considerations such as anonymity and voluntary participation were followed. Testing in the pilot phase reduced bias and no personal information was stored. This study addresses the gap in the integration of technology and collaborative teaching processes in the teaching of academic writing. It provides empirical data on the effectiveness of modern approaches while also revealing gaps in adoption across levels of proficiency and disciplines. The results make it possible for curriculum developers and decision makers to take tangible steps toward solving the discrepancies between offer and demand in the context of use. \nReferences \nAbahussain, M. O.\u00a0(2020). Investigating EFL learners\u2019 perceptions of collaborative writing.\u00a0International Journal of English Linguistics, 10(3), 32\u201347.\u00a0 \nAlawaji, N. N. M.\u00a0(2020). Students' perceptions of collaborative summary writing.\u00a0Theory and Practice in Language Studies, 10(6), 700\u2013707.\u00a0 \nBelyaeva, E. G.\u00a0(2022). Methodological model of teaching academic writing to undergraduate students.\u00a0Focus on Language Education and Research, 3(1), 36\u201351. \nButt, S.\u00a0(2023). Employees\u2019 perception regarding in-house training programs in Pakistani organizations.\u00a0Journal of Workplace Behavior, 4(1), 35\u201350. \nButt, S., & Yazdani, N.\u00a0(2023). Implementation of quality management practices and firm\u2019s innovation performance: Mediation of knowledge creation processes and moderating role of digital transformation.\u00a0Pakistan Journal of Humanities and Social Sciences, 11(4), 3881\u20133902. \nButt, S., Umair, T., & Tajammal, R.\u00a0(2024). Nexus between key determinants of service quality and students\u2019 satisfaction in higher education institutions (HEIs).\u00a0Annals of Human and Social Sciences, 5(2), 659\u2013671. \nChiew, M. T. L., & Ismail, H. H.\u00a0(2021). Exploring vocabulary learning strategies in a second language setting: A review.\u00a0International Journal of Academic Research in Business and Social Sciences, 11(12), 1298\u20131309. \nChubko, N., Morris, J. E., McKinnon, D. H., Slater, E. V., & Lummis, G. W.\u00a0(2020). Digital storytelling as a disciplinary literacy enhancement tool for EFL students.\u00a0Educational Technology Research and Development, 68, 3587\u20133604.\u00a0https://doi.org/10.1007/s11423-020-09833-x \nDavoodifard, M.\u00a0(2022). An overview of writing process research: Using innovative tasks and techniques for a better understanding of L2 writing processes in assessment contexts.\u00a0Studies in Applied Linguistics and TESOL, 21(2). \nDodigovic, M., & Jeaco, S.\u00a0(2021). Technology in applied linguistics.\u00a0International Journal of TESOL Studies, 3(2), 1\u20135. \nFerris, D. R.\u00a0(2023).\u00a0What error correction can (not) accomplish for second language writers: Dispelling myths, discussing options.\u00a0University of Michigan Press. \nFerris, D. R., & Hedgcock, J. S.\u00a0(2023).\u00a0Teaching L2 composition: Purpose, process, and practice\u00a0(4th ed.). Routledge. \nFlowerdew, L.\u00a0(2021). Learner corpora for disciplinary writing. In\u00a0Research questions in language education and applied linguistics: A reference guide\u00a0(pp. 475\u2013479). Springer. \nFlowerdew, L., & Petri\u0107, B.\u00a0(2024). A critical review of corpus-based pedagogic perspectives on thesis writing: Specificity revisited.\u00a0English for Specific Purposes, 76, 1\u201313.\u00a0https://doi.org/10.1016/j.esp.2024.01.001 \nHinkel, E.\u00a0(2022).\u00a0Teaching academic L2 writing: Practical techniques in vocabulary and grammar\u00a0(2nd ed.). Routledge. \nHyland, K.\u00a0(2024). Ken Hyland's essential bookshelf: Academic writing.\u00a0Language Teaching, 57(3), 399\u2013407.\u00a0https://doi.org/10.1017/S0261444824000109 \nHyland, K., & Hyland, F.\u00a0(2019).\u00a0Feedback in second language writing: Contexts and issues\u00a0(2nd ed.). Cambridge University Press. \nKang, E. Y., & Han, Z.\u00a0(2021). Written corrective feedback. In\u00a0The Routledge handbook of second language acquisition and writing\u00a0(pp. [page range]). Routledge. \nKarim, K., & Nassaji, H.\u00a0(2020). The effects of written corrective feedback.\u00a0Instructed Second Language Acquisition, 3(1), 28\u201352. \nKessler, G.\u00a0(2020). Professionalizing your use of technology in English language teaching. In\u00a0Professionalizing your English language teaching\u00a0(pp. 163\u2013173). Springer. \nKessler, M.\u00a0(2023). Written corrective feedback in an online community: A typology of English language learners\u2019 requests and interlocutors\u2019 responses.\u00a0Computers and Composition, 67, 102752.\u00a0https://doi.org/10.1016/j.compcom.2023.102752 \nKim, N. J., & Kim, M. K.\u00a0(2022). Teacher\u2019s perceptions of using an artificial intelligence-based educational tool for scientific writing.\u00a0Frontiers in Education, 7, 755914.\u00a0https://doi.org/10.3389/feduc.2022.755914 \nLi, J.\u00a0(2017). Automated writing evaluation: A pedagogical tool.\u00a0TESOL Quarterly, 51(2), 427\u2013432.\u00a0 \nLi, M.\u00a0(2021).\u00a0Researching and teaching second language writing in the digital age.\u00a0Palgrave Macmillan. \nLi, M., & Zhang, M.\u00a0(2023). Collaborative writing in L2 classrooms: A research agenda.\u00a0Language Teaching, 56(1), 94\u2013112.\u00a0https://doi.org/10.1017/S0261444821000318 \nMihaylova, M., Gorin, S., Reber, T. P., & Rothen, N.\u00a0(2022). A meta-analysis on mobile-assisted language learning applications: Benefits and risks.\u00a0Psychologica Belgica, 62(1), 252\u2013267.\u00a0 \nMulyono, H., & Saskia, R.\u00a0(2021). Affective variables contributing to Indonesian EFL students\u2019 willingness to communicate within face-to-face and digital environments.\u00a0Cogent Education, 8(1), 1911282.\u00a0https://doi.org/10.1080/2331186X.2021.1911282 \nPoole, R.\u00a0(2022). \u201cCorpus can be tricky\u201d: Revisiting teacher attitudes towards corpus-aided language learning and teaching.\u00a0Computer Assisted Language Learning, 35(7), 1620\u20131641.\u00a0https://doi.org/10.1080/09588221.2020.1868533 \nReagan, D., Fell, E., & Mackey, A.\u00a0(2023). Applied linguistics in the age of anxiety.\u00a0Annual Review of Applied Linguistics, 43, 1\u20136.\u00a0 \nReppen, R.\u00a0(2022). Building a corpus: What are key considerations? In\u00a0The Routledge handbook of corpus linguistics\u00a0(pp. 13\u201320). Routledge. \nStorch, N.\u00a0(2021). Theoretical perspectives on L2 writing and language learning in collaborative writing and the collaborative processing of written corrective feedback. In\u00a0The Routledge handbook of second language acquisition and writing\u00a0(pp. 22\u201334). Routledge. \nUllah, A., & Usman, M.\u00a0(2023). Role of libraries in ensuring quality education at higher education institutions: A perspective of Pakistan.\u00a0Inverge Journal of Social Sciences, 2(4), 13\u201322. \nUllah, A.\u00a0(2024). Analyzing the students\u2019 attitudes and behavior towards traditional classes and technology-enhanced online learning.\u00a0International Journal of Social Science Archives.\u00a0https://www.ijssa.com/index.php/ijssa/article/view/498 \nUsman, M., Asif, M., Ullah, A., & Ullah, W.\u00a0(2024). User\u2019s habits and attitudes towards Chinese books reading in Pakistan.\u00a0Inverge Journal of Social Sciences, 3(2), 11\u201328. \nWei, W., Cheong, C. M., Zhu, X., & Lu, Q.\u00a0(2024). Comparing self-reflection and peer feedback practices in an academic writing task: A student self-efficacy perspective.\u00a0Teaching in Higher Education, 29(4), 896\u2013912.\u00a0https://doi.org/10.1080/13562517.2024.2316724 \nWiboolyasarin, W., Wiboolyasarin, K., Suwanwihok, K., Jinowat, N., & Muenjanchoey, R.\u00a0(2024). Synergizing collaborative writing and AI feedback: An investigation into enhancing L2 writing proficiency in wiki-based environments.\u00a0Computers and Education: Artificial Intelligence, 6, 100228.\u00a0https://doi.org/10.1016/j.caeai.2024.100228 \nWoodrow, L.\u00a0(2022).\u00a0Introducing researching English for specific purposes.\u00a0Routledge. \nXu, L., Naserpour, A., Rezai, A., Namaziandost, E., & Azizi, Z.\u00a0(2022). Exploring EFL learners\u2019 metaphorical conceptions of language learning: A multimodal analysis.\u00a0Journal of Psycholinguistic Research, 51(2), 323\u2013339.\u00a0https://doi.org/10.1007/s10936-022-09842-2 \nYALA, A.\u00a0(2022).\u00a0The use of mobile-assisted language learning to foster students\u2019 self-editing in sentence writing: Case of 1st year EFL students at Setif 2 University\u00a0[Doctoral dissertation, Universit\u00e9 de Batna 2]. \nZhang, Y. O., & Hyland, K.\u00a0(2021). Elements of doctoral apprenticeship: Community feedback and the acquisition of writing expertise.\u00a0Journal of Second Language Writing, 53, 100835.\u00a0https://doi.org/10.1016/j.jslw.2021.100835",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.007965087890625
        },
        {
            "paperId": "9e9ed5b08218f24ca6e11afaf9a1dce823eba163",
            "corpusId": 256627276,
            "title": "Scalable Gaussian process regression enables accurate prediction of protein and small molecule properties with uncertainty quantitation",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 84,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.03294",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2302.03294?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2302.03294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2069052400",
                    "name": "J. Parkinson"
                },
                {
                    "authorId": "47825073",
                    "name": "Wen Wang"
                }
            ],
            "abstract": "Gaussian process (GP) is a Bayesian model which provides several advantages for regression tasks in machine learning such as reliable quantitation of uncertainty and improved interpretability. Their adoption has been precluded by their excessive computational cost and by the difficulty in adapting them for analyzing sequences (e.g. amino acid and nucleotide sequences) and graphs (e.g. ones representing small molecules). In this study, we develop efficient and scalable approaches for fitting GP models as well as fast convolution kernels which scale linearly with graph or sequence size. We implement these improvements by building an open-source Python library called xGPR. We compare the performance of xGPR with the reported performance of various deep learning models on 20 benchmarks, including small molecule, protein sequence and tabular data. We show that xGRP achieves highly competitive performance with much shorter training time. Furthermore, we also develop new kernels for sequence and graph data and show that xGPR generally outperforms convolutional neural networks on predicting key properties of proteins and small molecules. Importantly, xGPR provides uncertainty information not available from typical deep learning models. Additionally, xGPR provides a representation of the input data that can be used for clustering and data visualization. These results demonstrate that xGPR provides a powerful and generic tool that can be broadly useful in protein engineering and drug discovery. INTRODUCTION Identification of proteins and small molecules with desired properties is a task of crucial importance for the pharmaceutical and chemical industries1. In recent years, machine learning assisted approaches have become more popular2\u20138. Neural network/deep learning models are widely used owing to their flexibility, their ability to learn complex relationships from large datasets, and \u2013 owing to the development of libraries like Tensorflow and PyTorch \u2013 ease of implementation2. Despite deep learning\u2019s success, it suffers from several limitations. First, most deep learning architectures do not quantify their uncertainty on predictions, tend to be \u201cover-confident\u201d when extrapolating and are \u201cblack-box\u201d, so that it is very hard to determine why the model makes a particular prediction for a specific input9. A variety of approaches10\u201312 have been devised to estimate uncertainty on deep learning predictions but generally suffer from important limitations. Variational Bayesian neural networks for example provide approximate uncertainty but the quality of this approximation has been shown to be poor, resulting in unacceptable overconfidence in predictions for new data far from their training set13. Second, surprisingly, deep learning models have repeatedly been shown to lack robustness to so-called \u201cadversarial attacks\u201d, small perturbations to their input. Adding subtle noise not perceptible to the human eye, for example, could reliably cause deep learning models to misclassify a photo of a cat as guacamole, or a lionfish as eggnog14. In biology, protein structure prediction models have been shown to exhibit the same vulnerability to adversarial attacks15. Finally, deep learning often entails enormous computational cost. Many state of the art models use tens or hundreds of billions of learned parameters. Often a large fraction of their parameters can be removed without damaging performance, suggesting some more efficient approach is possible. Xu et al., for example, were able to prune 97% of the parameters of the BERT model in natural language processing and still achieve equivalent performance16. An alternative model architecture is Gaussian process (GP) regression17. A GP is a Bayesian model which defines a multivariate normal distribution over possible functions mapping the input x variable to an output y. In regions where there is little or no training data, the model expresses high uncertainty. In regions where there is substantial training data, the model predicts outcomes with more confidence. The type of functions in the distribution is determined by a kernel function that needs to be selected and its hyperparameters \u201ctuned\u201d (as with any other model) for a specific problem. The kernel function measures the similarity of any two x-inputs. GP models have at least four compelling advantages. First, like deep learning, if equipped with an appropriate kernel, a GP is able to approximate any relationship. Second, GP can calculate the marginal likelihood \u2013 the probability of the data averaged over all possible function values \u2013 in closed form, because the integral needed to calculate this value can be solved analytically. Consequently, kernel hyperparameters can be tuned by maximizing marginal likelihood rather than the likelihood, reducing the risk of overfitting and thus making the model more robust. Third, the model\u2019s predictions are generated using the similarity of new data points to those in the training set, where similarity is quantified by the kernel function. Unlike a deep learning model, a GP is not a black box, because one can determine how the model measures similarity between datapoints and generates predictions. This enables practitioners to incorporate \u201cprior knowledge\u201d about the relationship between inputs and outputs into the model architecture. Fourth, a GP reliably quantifies its uncertainty and assigns high uncertainty to datapoints very dissimilar from its training set. Uncertainty is important for protein engineering and drug design where experimentally evaluating model predictions is expensive. In these scenarios, one would rather use only model predictions that are more likely to be reliable. GP would detect \u201cdistribution shifts\u201d, where new data points are very different from the training set by assigning high uncertainty to its predictions, while a deep learning model may \u201cfail silently\u201d, generating low-accuracy predictions without providing any obvious sign of failure9,18,19. Distribution shifts are a critical problem in many applications19. DeGrave et al., for example, found many deep learning models trained on data from specific hospitals to diagnose COVID-19 from chest radiographs failed catastrophically when labeling data from different hospitals20. Images collected by different hospitals exhibited subtle differences, and this \u201cdistribution shift\u201d confounded the models, but because the deep learning models do not quantify their uncertainty, they provided no indication that a shift had occurred. This problem is among the issues described by Roberts et al. who found that, of 62 models for detecting COVID-19, none were clinically useful.21 The main challenge for GP is computational expense. A straightforward implementation scales with dataset size as 17, while modern implementations scale as 22, where N is the O(N) O(N) number of datapoints. Frequently in the literature deep learning is chosen over a GP because of scaling. Li et al., for example, in describing their deep learning model for traffic prediction, briefly consider using a GP and argue that this is not possible because \u201cGPs are hard to scale to a large dataset\u201d23. A second major challenge is the lack of efficient kernels for sequence and graph data. Many kernels for graphs have been described24, but typically exhibit quadratic or worse scaling in the size of the graph. Several approaches for approximating a GP to reduce cost have been described. Some \u201csparse\u201d methods use \u201cinducing points\u201d, a set of points in the input space that is considerably smaller than the number of datapoints, to construct an approximate GP. Performance however depends on the number and location of the inducing points. Since these methods exhibit rapid increase in cost with an increasing number of inducing points, they are limited to a number of inducing points in the low thousands. This is often insufficient to accurately model a complex relationship.25 One popular variant on this method is the stochastic variational Gaussian process (SVGP), which optimizes by maximizing a lower bound on the log marginal likelihood via stochastic gradients26. Both the hyperparameters of the kernel and the locations of the inducing points are optimized simultaneously. This creates a complicated optimization problem; it has been shown that the optimizer may become trapped in local minima during training of this type of model, causing subpar performance27. We show in the Supplementary Info (Section S2, Table S2) that in practice, the SVGP when equipped with the same kernel generally achieves both worse performance and significantly longer training time as compared with other approximations. Another prominent approach approximates the kernel function using Monte Carlo sampling, the random Fourier feature (RFF) approach of Rahimi and Recht28. Briefly, RFF methods approximate the kernel function via a random map corresponding to the kernel of interest, so that each input datapoint is converted into a \u201crandom feature\u201d representation; one is then able to approximate the Gaussian process using linear regression in the random feature space. As the size of the random map increases, so too does the accuracy of the approximation, but with diminishing returns. The size of the random map required to achieve a good approximation can be large; while this approach achieves linear scaling in the number of datapoints, it nonetheless remains expensive for many tasks. In this paper, we develop strategies for reducing the cost of fitting a GP and efficient kernels for graphs and sequences. We implement all these improvements in an open-source Python library, xGPR, and evaluate it on 20 benchmarks from the literature, including protein sequence, small molecule and tabular data.",
            "corpus_id": "256627276",
            "text": "Gaussian process (GP) is a Bayesian model which provides several advantages for regression tasks in machine learning such as reliable quantitation of uncertainty and improved interpretability. Their adoption has been precluded by their excessive computational cost and by the difficulty in adapting them for analyzing sequences (e.g. amino acid and nucleotide sequences) and graphs (e.g. ones representing small molecules). In this study, we develop efficient and scalable approaches for fitting GP models as well as fast convolution kernels which scale linearly with graph or sequence size. We implement these improvements by building an open-source Python library called xGPR. We compare the performance of xGPR with the reported performance of various deep learning models on 20 benchmarks, including small molecule, protein sequence and tabular data. We show that xGRP achieves highly competitive performance with much shorter training time. Furthermore, we also develop new kernels for sequence and graph data and show that xGPR generally outperforms convolutional neural networks on predicting key properties of proteins and small molecules. Importantly, xGPR provides uncertainty information not available from typical deep learning models. Additionally, xGPR provides a representation of the input data that can be used for clustering and data visualization. These results demonstrate that xGPR provides a powerful and generic tool that can be broadly useful in protein engineering and drug discovery. INTRODUCTION Identification of proteins and small molecules with desired properties is a task of crucial importance for the pharmaceutical and chemical industries1. In recent years, machine learning assisted approaches have become more popular2\u20138. Neural network/deep learning models are widely used owing to their flexibility, their ability to learn complex relationships from large datasets, and \u2013 owing to the development of libraries like Tensorflow and PyTorch \u2013 ease of implementation2. Despite deep learning\u2019s success, it suffers from several limitations. First, most deep learning architectures do not quantify their uncertainty on predictions, tend to be \u201cover-confident\u201d when extrapolating and are \u201cblack-box\u201d, so that it is very hard to determine why the model makes a particular prediction for a specific input9. A variety of approaches10\u201312 have been devised to estimate uncertainty on deep learning predictions but generally suffer from important limitations. Variational Bayesian neural networks for example provide approximate uncertainty but the quality of this approximation has been shown to be poor, resulting in unacceptable overconfidence in predictions for new data far from their training set13. Second, surprisingly, deep learning models have repeatedly been shown to lack robustness to so-called \u201cadversarial attacks\u201d, small perturbations to their input. Adding subtle noise not perceptible to the human eye, for example, could reliably cause deep learning models to misclassify a photo of a cat as guacamole, or a lionfish as eggnog14. In biology, protein structure prediction models have been shown to exhibit the same vulnerability to adversarial attacks15. Finally, deep learning often entails enormous computational cost. Many state of the art models use tens or hundreds of billions of learned parameters. Often a large fraction of their parameters can be removed without damaging performance, suggesting some more efficient approach is possible. Xu et al., for example, were able to prune 97% of the parameters of the BERT model in natural language processing and still achieve equivalent performance16. An alternative model architecture is Gaussian process (GP) regression17. A GP is a Bayesian model which defines a multivariate normal distribution over possible functions mapping the input x variable to an output y. In regions where there is little or no training data, the model expresses high uncertainty. In regions where there is substantial training data, the model predicts outcomes with more confidence. The type of functions in the distribution is determined by a kernel function that needs to be selected and its hyperparameters \u201ctuned\u201d (as with any other model) for a specific problem. The kernel function measures the similarity of any two x-inputs. GP models have at least four compelling advantages. First, like deep learning, if equipped with an appropriate kernel, a GP is able to approximate any relationship. Second, GP can calculate the marginal likelihood \u2013 the probability of the data averaged over all possible function values \u2013 in closed form, because the integral needed to calculate this value can be solved analytically. Consequently, kernel hyperparameters can be tuned by maximizing marginal likelihood rather than the likelihood, reducing the risk of overfitting and thus making the model more robust. Third, the model\u2019s predictions are generated using the similarity of new data points to those in the training set, where similarity is quantified by the kernel function. Unlike a deep learning model, a GP is not a black box, because one can determine how the model measures similarity between datapoints and generates predictions. This enables practitioners to incorporate \u201cprior knowledge\u201d about the relationship between inputs and outputs into the model architecture. Fourth, a GP reliably quantifies its uncertainty and assigns high uncertainty to datapoints very dissimilar from its training set. Uncertainty is important for protein engineering and drug design where experimentally evaluating model predictions is expensive. In these scenarios, one would rather use only model predictions that are more likely to be reliable. GP would detect \u201cdistribution shifts\u201d, where new data points are very different from the training set by assigning high uncertainty to its predictions, while a deep learning model may \u201cfail silently\u201d, generating low-accuracy predictions without providing any obvious sign of failure9,18,19. Distribution shifts are a critical problem in many applications19. DeGrave et al., for example, found many deep learning models trained on data from specific hospitals to diagnose COVID-19 from chest radiographs failed catastrophically when labeling data from different hospitals20. Images collected by different hospitals exhibited subtle differences, and this \u201cdistribution shift\u201d confounded the models, but because the deep learning models do not quantify their uncertainty, they provided no indication that a shift had occurred. This problem is among the issues described by Roberts et al. who found that, of 62 models for detecting COVID-19, none were clinically useful.21 The main challenge for GP is computational expense. A straightforward implementation scales with dataset size as 17, while modern implementations scale as 22, where N is the O(N) O(N) number of datapoints. Frequently in the literature deep learning is chosen over a GP because of scaling. Li et al., for example, in describing their deep learning model for traffic prediction, briefly consider using a GP and argue that this is not possible because \u201cGPs are hard to scale to a large dataset\u201d23. A second major challenge is the lack of efficient kernels for sequence and graph data. Many kernels for graphs have been described24, but typically exhibit quadratic or worse scaling in the size of the graph. Several approaches for approximating a GP to reduce cost have been described. Some \u201csparse\u201d methods use \u201cinducing points\u201d, a set of points in the input space that is considerably smaller than the number of datapoints, to construct an approximate GP. Performance however depends on the number and location of the inducing points. Since these methods exhibit rapid increase in cost with an increasing number of inducing points, they are limited to a number of inducing points in the low thousands. This is often insufficient to accurately model a complex relationship.25 One popular variant on this method is the stochastic variational Gaussian process (SVGP), which optimizes by maximizing a lower bound on the log marginal likelihood via stochastic gradients26. Both the hyperparameters of the kernel and the locations of the inducing points are optimized simultaneously. This creates a complicated optimization problem; it has been shown that the optimizer may become trapped in local minima during training of this type of model, causing subpar performance27. We show in the Supplementary Info (Section S2, Table S2) that in practice, the SVGP when equipped with the same kernel generally achieves both worse performance and significantly longer training time as compared with other approximations. Another prominent approach approximates the kernel function using Monte Carlo sampling, the random Fourier feature (RFF) approach of Rahimi and Recht28. Briefly, RFF methods approximate the kernel function via a random map corresponding to the kernel of interest, so that each input datapoint is converted into a \u201crandom feature\u201d representation; one is then able to approximate the Gaussian process using linear regression in the random feature space. As the size of the random map increases, so too does the accuracy of the approximation, but with diminishing returns. The size of the random map required to achieve a good approximation can be large; while this approach achieves linear scaling in the number of datapoints, it nonetheless remains expensive for many tasks. In this paper, we develop strategies for reducing the cost of fitting a GP and efficient kernels for graphs and sequences. We implement all these improvements in an open-source Python library, xGPR, and evaluate it on 20 benchmarks from the literature, including protein sequence, small molecule and tabular data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0343017578125
        },
        {
            "paperId": "8671aec4d79c5ecce38c298d57c99ae5c010df13",
            "corpusId": 263775936,
            "title": "When Clinical Prediction Is Steering the Ship, Beware the Drift of Its Wake",
            "venue": "Annals of Internal Medicine",
            "year": 2023,
            "referenceCount": 10,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.7326/M23-2345?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7326/M23-2345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239860786",
                    "name": "M. L. Robinson"
                },
                {
                    "authorId": "5696544",
                    "name": "B. Garibaldi"
                },
                {
                    "authorId": "2256510675",
                    "name": "Martin A. Lindquist"
                }
            ],
            "abstract": "Lofty expectations and billions of dollars have fueled unprecedented interest in the potential for artificial intelligence (AI) to improve health care delivery. Fears of inadequate safety monitoring, further entrenching harmful biases, and myriad other unintended consequences temper this enthusiasm. In this issue, Vaid and colleagues (1) raise concerns that successful implementation of predictive models to forecast and prevent adverse patient outcomes will reduce model performance over time and that model retraining may not be sufficient to rescue performance. For example, guided by a model that identifies patients at high risk for cardiovascular disease, clinicians may prescribe statins, control hypertension, and support lifestyle changes. If these interventions broadly reduce cardiovascular events, the model will predict more events than actually occur. Retraining the model in the setting of fewer observed events may downgrade cardiovascular risk, prompting fewer helpful interventions, and cause the revised model to underpredict events. Overestimation of cardiovascular disease risk in modern cohorts has been attributed in part to improvement in cardiovascular disease prevention since legacymodel creation (2). The authors test this concern by simulating 3 scenarios for the deployment of models to predict the risk for death or acute kidney injury in the first 5 days after admission to an intensive care unit (ICU). In the first scenario, only the mortality prediction model is deployed, in the second the mortality model is deployed sequentially after the acute kidney injury model, and in the third the 2models are deployed simultaneously. In each scenario, model performance in simulated populations that benefit from interventions afforded by model prediction is inferior to performance of the original model. Mitigation steps, including retraining models to include awareness of their predictions and censoring patients flagged for intervention, did not raise performance back to that of the original model. In the AI literature, the term model drift describes the phenomenon of model performance degradation over time. Various factors cause model drift, which may occur gradually or abruptly. To better understand, consider a set of input variables (X) and an outcome (Y) that we seek to predict. Their relationship can be modeled using their joint distribution (that is, P(X, Y)), which can be decomposed into 2 different distributions: the conditional distribution of the outcome given the input (that is, P(YjX)), and the marginal distribution of the outcome (that is, P(X)). This relationship can be written: P(X, Y)1\u20444 P(YjX) P(X). Model drift can occur if either of these distributions changes over time. Changes in P(X) are called data drift. For example, consider training a model on young adults but later applying it to elderly patients. Here, any agerelated differences in the distribution of the input variables could affect model performance. Changes instead to P(YjX) are described as concept drift. For example, consider that the area in which a person lives influences health outcomes. Outside interventions in a certain area (for example, removing lead pipes) could change the relationship between input and outcome. In general, concept drift is more likely than data drift to change the decision boundary in a predictive model and seems to capture the phenomenon described by Vaid and colleagues. Althoughwe share Vaid and colleagues' concerns about model drift, some limitations may constrain the generalizability of their findings. The effectiveness of predictive models to reduce the adverse outcomes that they predict may be overestimated. In their most conservative simulations, they assume that prospective identification of 90% of patients who die in the first 5 days after ICU admission will empower clinicians to unleash interventions that reduce mortality by 5%. But tools to stratify critical illness severity are not new\u2014the 40-year-old acute physiology and chronic health evaluation score serves a similar role (2). And measures that reduce ICU mortality are scarce\u2014a systematic review identified no such pharmacologic interventions (3). In addition, the results may be specific to the chosen variables and model type. Setting those concerns aside, we cannot discount that a future drug or quality improvement bundle might meet the model assumptions and by improving patient outcomes drive model drift. Current conversations on AI are incomplete without mention of large language models (LLMs). These enormous neural networks trained on massive data sources produce eerily humanlike responses. Those trained on clinical notes have already surpassed the performance of traditional machine learning to predict clinical outcomes (4). But LLMs are not immune to model drift and have new quirks\u2014simulations outside health care show that LLMs collapse if recursively trained on their own output (5). Health care applications, such as writing radiology reports or suggesting diagnoses, could suffer a similar fate. Future clinical LLMs may train on data sets that increasingly include AI-generated radiology reports or clinical notes influenced by AI-suggested diagnoses. Models may degrade as noise introduced by LLM \u201challucination\u201d and probabilistically chosen common diagnoses iteratively outcompete coherent entries and rare syndromes. Regulatory agencies provide guiding principles for machine learning, which emphasize monitoring postdeployment model performance (6). Monitoring must first consider wide variability in model performance across health systems (7). When models do drift, Vaid and colleagues show that refitting with the same predictors may not improve model performance. Diagnosis of the cause, starting with inspection for data drift, may help. Testing for concept drift is challenging because input variable permutations scale exponentially, but the relationship between",
            "corpus_id": "263775936",
            "text": "Lofty expectations and billions of dollars have fueled unprecedented interest in the potential for artificial intelligence (AI) to improve health care delivery. Fears of inadequate safety monitoring, further entrenching harmful biases, and myriad other unintended consequences temper this enthusiasm. In this issue, Vaid and colleagues (1) raise concerns that successful implementation of predictive models to forecast and prevent adverse patient outcomes will reduce model performance over time and that model retraining may not be sufficient to rescue performance. For example, guided by a model that identifies patients at high risk for cardiovascular disease, clinicians may prescribe statins, control hypertension, and support lifestyle changes. If these interventions broadly reduce cardiovascular events, the model will predict more events than actually occur. Retraining the model in the setting of fewer observed events may downgrade cardiovascular risk, prompting fewer helpful interventions, and cause the revised model to underpredict events. Overestimation of cardiovascular disease risk in modern cohorts has been attributed in part to improvement in cardiovascular disease prevention since legacymodel creation (2). The authors test this concern by simulating 3 scenarios for the deployment of models to predict the risk for death or acute kidney injury in the first 5 days after admission to an intensive care unit (ICU). In the first scenario, only the mortality prediction model is deployed, in the second the mortality model is deployed sequentially after the acute kidney injury model, and in the third the 2models are deployed simultaneously. In each scenario, model performance in simulated populations that benefit from interventions afforded by model prediction is inferior to performance of the original model. Mitigation steps, including retraining models to include awareness of their predictions and censoring patients flagged for intervention, did not raise performance back to that of the original model. In the AI literature, the term model drift describes the phenomenon of model performance degradation over time. Various factors cause model drift, which may occur gradually or abruptly. To better understand, consider a set of input variables (X) and an outcome (Y) that we seek to predict. Their relationship can be modeled using their joint distribution (that is, P(X, Y)), which can be decomposed into 2 different distributions: the conditional distribution of the outcome given the input (that is, P(YjX)), and the marginal distribution of the outcome (that is, P(X)). This relationship can be written: P(X, Y)1\u20444 P(YjX) P(X). Model drift can occur if either of these distributions changes over time. Changes in P(X) are called data drift. For example, consider training a model on young adults but later applying it to elderly patients. Here, any agerelated differences in the distribution of the input variables could affect model performance. Changes instead to P(YjX) are described as concept drift. For example, consider that the area in which a person lives influences health outcomes. Outside interventions in a certain area (for example, removing lead pipes) could change the relationship between input and outcome. In general, concept drift is more likely than data drift to change the decision boundary in a predictive model and seems to capture the phenomenon described by Vaid and colleagues. Althoughwe share Vaid and colleagues' concerns about model drift, some limitations may constrain the generalizability of their findings. The effectiveness of predictive models to reduce the adverse outcomes that they predict may be overestimated. In their most conservative simulations, they assume that prospective identification of 90% of patients who die in the first 5 days after ICU admission will empower clinicians to unleash interventions that reduce mortality by 5%. But tools to stratify critical illness severity are not new\u2014the 40-year-old acute physiology and chronic health evaluation score serves a similar role (2). And measures that reduce ICU mortality are scarce\u2014a systematic review identified no such pharmacologic interventions (3). In addition, the results may be specific to the chosen variables and model type. Setting those concerns aside, we cannot discount that a future drug or quality improvement bundle might meet the model assumptions and by improving patient outcomes drive model drift. Current conversations on AI are incomplete without mention of large language models (LLMs). These enormous neural networks trained on massive data sources produce eerily humanlike responses. Those trained on clinical notes have already surpassed the performance of traditional machine learning to predict clinical outcomes (4). But LLMs are not immune to model drift and have new quirks\u2014simulations outside health care show that LLMs collapse if recursively trained on their own output (5). Health care applications, such as writing radiology reports or suggesting diagnoses, could suffer a similar fate. Future clinical LLMs may train on data sets that increasingly include AI-generated radiology reports or clinical notes influenced by AI-suggested diagnoses. Models may degrade as noise introduced by LLM \u201challucination\u201d and probabilistically chosen common diagnoses iteratively outcompete coherent entries and rare syndromes. Regulatory agencies provide guiding principles for machine learning, which emphasize monitoring postdeployment model performance (6). Monitoring must first consider wide variability in model performance across health systems (7). When models do drift, Vaid and colleagues show that refitting with the same predictors may not improve model performance. Diagnosis of the cause, starting with inspection for data drift, may help. Testing for concept drift is challenging because input variable permutations scale exponentially, but the relationship between",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.017852783203125
        },
        {
            "paperId": "f8a064be57941ba9b0905fe728161c49c3763eff",
            "corpusId": 259083929,
            "title": "The Effect of Cod\u0131ng Educat\u0131on Des\u0131gned with D\u0131fferent V\u0131sual Programs on Academ\u0131c Success and Att\u0131tudes and Self-Eff\u0131c\u0131enc\u0131es of Secondary School Students",
            "venue": "Journal of teacher education and lifelong learning",
            "year": 2023,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dergipark.org.tr/en/download/article-file/3069451",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.51535/tell.1279547?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.51535/tell.1279547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2219594609",
                    "name": "Teslime Deni\u0307z"
                },
                {
                    "authorId": "2078960573",
                    "name": "A. T. Korucu"
                }
            ],
            "abstract": "This research was conducted with the aim of measuring students' attitudes towards computer and software course while giving coding education to students, determining students' self-efficacy perceptions towards computer and software course, measuring students' attitudes towards coding education, measuring students' academic success and observing the differences in educational outcomes resulting from these measurements. Has been carried out. Quantitative research methods were used in this study, which examines the effects of coding education designed with different visual programs on the academic achievement, attitudes and self-efficacy of secondary school students. In this study, the quantitative research method was adopted as the research model and the \"Pretest - Posttest Quasi-experimental Design Model with Control Group\" was used. The population of the research consists of 6th grade students, 17 girls and 28 boys, studying at Konya G\u00f6dene TOK\u0130 Secondary School in the 2020-2021 academic year. The applications were made for 10 weeks in accordance with the lesson plan period in the curriculum. In the study, an academic achievement test for the course was developed, the Attitude Scale towards Computer-Aided Education, the Self-Efficacy Perception Scale of Computer-Aided Education, the Attitude Scale towards Computer-Aided Coding Learning 3 different scales were used to determine. The data obtained as a result of the training, which lasted for 10 weeks in total, were analyzed in the IBM SPSS Statistics 22 program. The Cronbach Alpha reliability coefficients of the scales used were calculated as .870, for the Self-Efficacy Perception Scale for Computer Assisted Education, .81 for the Attitude Scale for Computer Assisted Education, and .92 for the Attitude Scale for Educational Computer Games Assisted Coding Learning. The difficulty indexes of the academic achievement scales used were calculated as .758 in the Scratch Academic Achievement Test and .726 in the Tospaa Academic Achievement Test. The scale developed according to the data obtained was found to be valid and reliable. According to the findings obtained at the end of the research, it was seen that the application of the Scratch program in coding education made a significant difference on the programming skills of the students and contributed to the coding education. The eta squared value was examined to determine the effect size of the computer coding environment on the total of Academic Achievement, Self-efficacy Perception Scale for Computer Aided Education, Attitude Scale towards Computer Assisted Education and Attitude Scale towards Educational Computer Games Supported Learning. It can be said that academic achievement has a \"large\" effect size on self-efficacy perceptions in computer-assisted education, attitudes towards computer-assisted education and educational computer-assisted coding education.",
            "corpus_id": "259083929",
            "text": "This research was conducted with the aim of measuring students' attitudes towards computer and software course while giving coding education to students, determining students' self-efficacy perceptions towards computer and software course, measuring students' attitudes towards coding education, measuring students' academic success and observing the differences in educational outcomes resulting from these measurements. Has been carried out. Quantitative research methods were used in this study, which examines the effects of coding education designed with different visual programs on the academic achievement, attitudes and self-efficacy of secondary school students. In this study, the quantitative research method was adopted as the research model and the \"Pretest - Posttest Quasi-experimental Design Model with Control Group\" was used. The population of the research consists of 6th grade students, 17 girls and 28 boys, studying at Konya G\u00f6dene TOK\u0130 Secondary School in the 2020-2021 academic year. The applications were made for 10 weeks in accordance with the lesson plan period in the curriculum. In the study, an academic achievement test for the course was developed, the Attitude Scale towards Computer-Aided Education, the Self-Efficacy Perception Scale of Computer-Aided Education, the Attitude Scale towards Computer-Aided Coding Learning 3 different scales were used to determine. The data obtained as a result of the training, which lasted for 10 weeks in total, were analyzed in the IBM SPSS Statistics 22 program. The Cronbach Alpha reliability coefficients of the scales used were calculated as .870, for the Self-Efficacy Perception Scale for Computer Assisted Education, .81 for the Attitude Scale for Computer Assisted Education, and .92 for the Attitude Scale for Educational Computer Games Assisted Coding Learning. The difficulty indexes of the academic achievement scales used were calculated as .758 in the Scratch Academic Achievement Test and .726 in the Tospaa Academic Achievement Test. The scale developed according to the data obtained was found to be valid and reliable. According to the findings obtained at the end of the research, it was seen that the application of the Scratch program in coding education made a significant difference on the programming skills of the students and contributed to the coding education. The eta squared value was examined to determine the effect size of the computer coding environment on the total of Academic Achievement, Self-efficacy Perception Scale for Computer Aided Education, Attitude Scale towards Computer Assisted Education and Attitude Scale towards Educational Computer Games Supported Learning. It can be said that academic achievement has a \"large\" effect size on self-efficacy perceptions in computer-assisted education, attitudes towards computer-assisted education and educational computer-assisted coding education.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00821685791015625
        }
    ],
    "quotes": {
        "cost": 0.223197,
        "quotes": [
            {
                "idx": 0,
                "key": "[220045816 | Xu et al. | 2020 | Citations: 206]",
                "snippets": "The key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks.\n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[4556964 | Popel et al. | 2018 | Citations: 311]": "Abstract This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra \u201cmore data and larger models\u201d, we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.",
                    "[51920640 | Guo et al. | 2018 | Citations: 345]": "We present a simple yet efficient approach capable of training deep neural networks on large-scale weakly-supervised web images, which are crawled raw from the Internet by using text queries, without any human annotation. We develop a principled learning strategy by leveraging curriculum learning, with the goal of handling a massive amount of noisy labels and data imbalance effectively. We design a new learning curriculum by measuring the complexity of data using its distribution density in a feature space, and rank the complexity in an unsupervised manner. This allows for an efficient implementation of curriculum learning on large-scale web images, resulting in a high-performance CNN the model, where the negative impact of noisy labels is reduced substantially. Importantly, we show by experiments that those images with highly noisy labels can surprisingly improve the generalization capability of model, by serving as a manner of regularization. Our approaches obtain state-of-the-art performance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101. With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on the WebVision challenge [18] for 1000-category classification. This result was the top performance by a wide margin, outperforming second place by a nearly 50% relative error rate. Code and models are available at: https://github.com/MalongTech/CurriculumNet."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 672,
                        "end": 1980,
                        "sentence_offsets": [
                            {
                                "start": 654,
                                "end": 855
                            },
                            {
                                "start": 858,
                                "end": 960
                            },
                            {
                                "start": 961,
                                "end": 1188
                            },
                            {
                                "start": 1189,
                                "end": 1363
                            },
                            {
                                "start": 1364,
                                "end": 1494
                            },
                            {
                                "start": 1495,
                                "end": 1619
                            },
                            {
                                "start": 1622,
                                "end": 1710
                            },
                            {
                                "start": 1711,
                                "end": 1823
                            },
                            {
                                "start": 1824,
                                "end": 1954
                            },
                            {
                                "start": 1955,
                                "end": 2167
                            }
                        ],
                        "ref_mentions": [
                            "4556964",
                            "51920640",
                            "873046"
                        ],
                        "quote": "The key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks.\n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[232233485 | Wei et al. | 2021 | Citations: 64]",
                "snippets": "Effective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect a greater improvement from curriculum learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Effective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect a greater improvement from curriculum learning.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[234338422 | Zhou et al. | 2021 | Citations: 17]",
                "snippets": "In the scenario of neural machine translation (NMT), empirical studies have shown that CL strategy contributes to convergence speed and model performance (Zhang et al., 2018;(Platanios et al., 2019)(Zhang et al., 2019)(Liu et al., 2020)(Zhan et al., 2021)(Ruiter et al., 2020). In these approaches, the learning difficulty of each training example is measured by different difficulty criteria. Early approaches depend on prior knowledge from various sources, including manually crafted features like sentence length and word rarity (Kocmi et al., 2017). The drawback lies in the fact that human understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples, to approximate the perspective of an NMT model. For example, (Platanios et al., 2019) turns discrete numerical difficulty scores into relative probabilities and then construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[155089817 | Zhang et al. | 2019 | Citations: 124]": "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.",
                    "[219260306 | Liu et al. | 2020 | Citations: 120]": "A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT\u201914 English-German and WMT\u201917 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).",
                    "[222176797 | Ruiter et al. | 2020 | Citations: 13]": "Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.",
                    "[232104951 | Zhan et al. | 2021 | Citations: 46]": "Meta-learning has been sufficiently validated to be beneficial for low-resource neural machine translation (NMT). However, we find that meta-trained NMT fails to improve the translation performance of the domain unseen at the meta-training stage. In this paper, we aim to alleviate this issue by proposing a novel meta-curriculum learning for domain adaptation in NMT. During meta-training, the NMT first learns the similar curricula from each domain to avoid falling into a bad local optimum early, and finally learns the curricula of individualities to improve the model robustness for learning domain-specific knowledge. Experimental results on 10 different low-resource domains show that meta-curriculum learning can improve the translation performance of both familiar and unfamiliar domains. All the codes and data are freely available at https://github.com/NLP2CT/Meta-Curriculum.",
                    "[26468344 | Kocmi et al. | 2017 | Citations: 141]": "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \u201ccurriculum learning\u201d). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \u201ccurricula\u201d achieve a small improvement over the baseline.",
                    "[85498775 | Platanios et al. | 2019 | Citations: 344]": "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1029,
                        "end": 2170,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "85498775",
                            "155089817",
                            "219260306",
                            "232104951",
                            "222176797",
                            "26468344",
                            "85498775"
                        ],
                        "quote": "In the scenario of neural machine translation (NMT), empirical studies have shown that CL strategy contributes to convergence speed and model performance (Zhang et al., 2018;(Platanios et al., 2019)(Zhang et al., 2019)(Liu et al., 2020)(Zhan et al., 2021)(Ruiter et al., 2020). In these approaches, the learning difficulty of each training example is measured by different difficulty criteria. Early approaches depend on prior knowledge from various sources, including manually crafted features like sentence length and word rarity (Kocmi et al., 2017). The drawback lies in the fact that human understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples, to approximate the perspective of an NMT model. For example, (Platanios et al., 2019) turns discrete numerical difficulty scores into relative probabilities and then construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[244908620 | Wang et al. | 2021 | Citations: 80]",
                "snippets": "The key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3.\n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37]53], and they should be more difficult to learn as verified in Section 2.3.\n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process.\n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy.\n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3.\n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37]53], and they should be more difficult to learn as verified in Section 2.3.\n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process.\n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy.\n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[246996522 | Kuznetsova et al. | 2022 | Citations: 3]",
                "snippets": "Most commonly two complexity strategies are employed: model competencebased (Wang et al., 2018)(Platanios et al., 2019)(Zhou et al., 2020) and data-driven. Complexity measures for data-driven learning include sentence/utterance length (Kim et al., 2017), language model score, n-gram size (Bengio et al., 2009)(Graves et al., 2017), word frequency ranking, and sentence norm [13]. Speech processing systems can rely on speech to noise ratio (SNR) as a measure of difficulty by gradually blending more and more noise into clean speech signals (Braun et al., 2016)(Higuchi et al., 2021)",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[11137059 | Graves et al. | 2017 | Citations: 530]": "We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",
                    "[14928979 | Braun et al. | 2016 | Citations: 84]": "The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called accordion annealing (ACCAN). It uses a multi-stage training schedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are first added and samples at increasing higher SNR values are gradually added up to an SNR value of 50dB. We also use a method called per-epoch noise mixing (PEM) that generates noisy training samples online during training and thus enables dynamically changing the SNR of our training data. Both the ACCAN and the PEM methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus. ACCAN decreases the average word error rate (WER) on the 20dB to \u221210dB SNR range by up to 31.4% when compared to a conventional multi-condition training method.",
                    "[20639213 | Wang et al. | 2018 | Citations: 28]": "Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.",
                    "[231979234 | Higuchi et al. | 2021 | Citations: 8]": "We propose dynamic curriculum learning via data parameters for noise robust keyword spotting. Data parameter learning has recently been introduced for image processing, where weight parameters, so-called data parameters, for target classes and instances are introduced and optimized along with model parameters. The data parameters scale logits and control importance over classes and instances during training, which enables automatic curriculum learning without additional annotations for training data. Similarly, in this paper, we propose using this curriculum learning approach for acoustic modeling, and train an acoustic model on clean and noisy utterances with the data parameters. The proposed approach automatically learns the difficulty of the classes and instances, e.g. due to low speech to noise ratio (SNR), in the gradient descent optimization and performs curriculum learning. This curriculum learning leads to overall improvement of the accuracy of the acoustic model. We evaluate the effectiveness of the proposed approach on a keyword spotting task. Experimental results show 7.7% relative reduction in false reject ratio with the data parameters compared to a base-line model which is simply trained on the multiconditioned dataset.",
                    "[33957080 | Kim et al. | 2017 | Citations: 44]": "Achieving high accuracy with end-to-end speech recognizers requires careful parameter initialization prior to training. Otherwise, the networks may fail to find a good local optimum. This is particularly true for online networks, such as unidirectional LSTMs. Currently, the best strategy to train such systems is to bootstrap the training from a tied-triphone system. However, this is time consuming, and more importantly, is impossible for languages without a high-quality pronunciation lexicon. In this work, we propose an initialization strategy that uses teacher-student learning to transfer knowledge from a large, well-trained, offline end-to-end speech recognition model to an online end-to-end model, eliminating the need for a lexicon or any other linguistic resources. We also explore curriculum learning and label smoothing and show how they can be combined with the proposed teacher-student learning for further improvements. We evaluate our methods on a Microsoft Cortana personal assistant task and show that the proposed method results in a 19 % relative improvement in word error rate compared to a randomly-initialized baseline system.",
                    "[85498775 | Platanios et al. | 2019 | Citations: 344]": "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 616,
                        "end": 1062,
                        "sentence_offsets": [
                            {
                                "start": 616,
                                "end": 718
                            },
                            {
                                "start": 719,
                                "end": 893
                            },
                            {
                                "start": 894,
                                "end": 1189
                            }
                        ],
                        "ref_mentions": [
                            "20639213",
                            "85498775",
                            "227275560",
                            "33957080",
                            "873046",
                            "11137059",
                            "14928979",
                            "231979234"
                        ],
                        "quote": "Most commonly two complexity strategies are employed: model competencebased (Wang et al., 2018)(Platanios et al., 2019)(Zhou et al., 2020) and data-driven. Complexity measures for data-driven learning include sentence/utterance length (Kim et al., 2017), language model score, n-gram size (Bengio et al., 2009)(Graves et al., 2017), word frequency ranking, and sentence norm [13]. Speech processing systems can rely on speech to noise ratio (SNR) as a measure of difficulty by gradually blending more and more noise into clean speech signals (Braun et al., 2016)(Higuchi et al., 2021)"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[247292373 | Varshney et al. | 2022 | Citations: 19]",
                "snippets": "We use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Method",
                        "pdf_hash": "",
                        "start": 491,
                        "end": 989,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[248810776 | Zhu et al. | 2022 | Citations: 6]",
                "snippets": "Numerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) (Han et al., 2018)(Xu et al., 2021), the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i (Lin et al., 2017) . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. (Huang et al., 2019) designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207995300 | Huang et al. | 2019 | Citations: 241]": "This paper proposes a novel noisy label detection approach, named O2U-net, for deep neural networks without human annotations. Different from prior work which requires specifically designed noise-robust loss functions or networks, O2U-net is easy to implement but effective. It only requires adjusting the hyper-parameters of the deep network to make its status transfer from overfitting to underfitting (O2U) cyclically. The losses of each sample are recorded during iterations. The higher the normalized average loss of a sample, the higher the probability of being noisy labels. O2U-net is naturally compatible with active learning and other human annotation approaches. This introduces extra flexibility for learning with noisy labels. We conduct sufficient experiments on multiple datasets in various settings. The experimental results prove the state-of-the-art of O2S-net.",
                    "[231807280 | Xu et al. | 2021 | Citations: 44]": "The recent paper by Byrd&Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.",
                    "[52902973 | Han et al. | 2018 | Citations: 9]": "It is challenging for stochastic optimization to handle large-scale sensitive data safely. Duchi et al. recently proposed a private sampling strategy to solve privacy leakage in stochastic optimization. However, this strategy leads to a degeneration in robustness, since this strategy is equal to noise injection on each gradient, which adversely affects updates of the primal variable. To address this challenge, we introduce a robust stochastic optimization under the framework of local privacy, which is called Privacy-pREserving StochasTIc Gradual lEarning (PRESTIGE). PRESTIGE bridges private updates of the primal variable (by private sampling) with gradual curriculum learning (CL). The noise injection leads to similar issue from label noise, but the robust learning process of CL can combat with label noise. Thus, PRESTIGE yields \u201cprivate but robust\u201d updates of the primal variable on the curriculum, that is, a reordered label sequence provided by CL. In theory, we reveal the convergence rate and maximum complexity of PRESTIGE. Empirical results on six datasets show that PRESTIGE achieves a good tradeoff between privacy preservation and robustness over baselines."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 571,
                        "end": 1761,
                        "sentence_offsets": [
                            {
                                "start": 571,
                                "end": 657
                            },
                            {
                                "start": 658,
                                "end": 818
                            },
                            {
                                "start": 819,
                                "end": 964
                            },
                            {
                                "start": 965,
                                "end": 1061
                            },
                            {
                                "start": 1062,
                                "end": 1168
                            },
                            {
                                "start": 1169,
                                "end": 1338
                            },
                            {
                                "start": 1339,
                                "end": 1467
                            },
                            {
                                "start": 1468,
                                "end": 1577
                            },
                            {
                                "start": 1578,
                                "end": 1695
                            },
                            {
                                "start": 1696,
                                "end": 1761
                            }
                        ],
                        "ref_mentions": [
                            "52902973",
                            "231807280",
                            "47252984",
                            "207995300"
                        ],
                        "quote": "Numerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) (Han et al., 2018)(Xu et al., 2021), the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i (Lin et al., 2017) . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. (Huang et al., 2019) designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[248887633 | Croitoru et al. | 2022 | Citations: 9]",
                "snippets": "The main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input (Pentina et al., 2014)(Jim\u00e9nez-S\u00e1nchez et al., 2019)(Wei et al., 2020) or metrics based on domain-specific heuristics. For instance, the text length (Kocmi et al., 2017)[44](Tay et al., 2019)(Zhang et al., 2021) and the word frequency [1,(Liu et al., 2018) have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works (Shi et al., 2016)[33]. Other solutions employed difficulty estimators (Ionescu et al., 2016) or even the confidence level of the predictions made by the neural network (Gong et al., 2016)(Hacohen et al., 2019) to approximate the complexity of the data samples. Other studies (Khan et al., 2024)(Khan et al., 2022)(Khan et al., 2023) used the error of a previously trained model to estimate the difficulty of each sample.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102350936 | Hacohen et al. | 2019 | Citations: 449]": "Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.",
                    "[166228313 | Tay et al. | 2019 | Citations: 110]": "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.",
                    "[204539326 | Jimenez-Sanchez et al. | 2019 | Citations: 48]": "Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning \"easy\" examples and move towards \"hard\", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons.",
                    "[221995570 | Wei et al. | 2020 | Citations: 49]": "Applying curriculum learning requires both a range of difficulty in data and a method for determining the difficulty of examples. In many tasks, however, satisfying these requirements can be a formidable challenge.In this paper, we contend that histopathology image classification is a compelling use case for curriculum learning. Based on the nature of histopathology images, a range of difficulty inherently exists among examples, and, since medical datasets are often labeled by multiple annotators, annotator agreement can be used as a natural proxy for the difficulty of a given example. Hence, we propose a simple curriculum learning method that trains on progressively-harder images as determined by annotator agreement.We evaluate our hypothesis on the challenging and clinically-important task of colorectal polyp classification. Whereas vanilla training achieves an AUC of 83.7% for this task, a model trained with our proposed curriculum learning approach achieves an AUC of 88.2%, an improvement of 4.5%. Our work aims to inspire researchers to think more creatively and rigorously when choosing contexts for applying curriculum learning.",
                    "[233433844 | Zhang et al. | 2021 | Citations: 19]": "BERT [1] is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia & BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time.",
                    "[254246401 | Khan et al. | 2022 | Citations: 5]": "Deep learning models require an enormous amount of data for training. However, recently there is a shift in machine learning from model-centric to data-centric approaches. In datacentric approaches, the focus is to refine and improve the quality of the data to improve the learning performance of the models rather than redesigning model architectures. In this paper, we propose CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two data-centric approaches i.e., curriculum learning and dataset pruning to improve the model learning accuracy and convergence speed. The proposed scheme applies loss-aware dataset pruning to iteratively remove the least significant samples and progressively reduces the size of the effective dataset in the curriculum learning training. Extensive experiments performed on crowd density estimation models validate the notion behind combining the two approaches by reducing the convergence time and improving generalization. To our knowledge, the idea of data pruning as an embedded process in curriculum learning is novel.",
                    "[256808576 | Khan et al. | 2023 | Citations: 28]": "Automatic crowd counting using density estimation has gained significant attention in computer vision research. As a result, a large number of crowd counting and density estimation models using convolution neural networks (CNN) have been published in the last few years. These models have achieved good accuracy over benchmark datasets. However, attempts to improve the accuracy often lead to higher complexity in these models. In real-time video surveillance applications using drones with limited computing resources, deep models incur intolerable higher inference delay. In this paper, we propose (i) a Lightweight Crowd Density estimation model (LCDnet) for real-time video surveillance, and (ii) an improved training method using curriculum learning (CL). LCDnet is trained using CL and evaluated over two benchmark datasets i.e., DroneRGBT and CARPK. Results are compared with existing crowd models. Our evaluation shows that the LCDnet achieves a reasonably good accuracy while significantly reducing the inference time and memory requirement and thus can be deployed over edge devices with very limited computing resources.",
                    "[26468344 | Kocmi et al. | 2017 | Citations: 141]": "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \u201ccurriculum learning\u201d). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \u201ccurricula\u201d achieve a small improvement over the baseline.",
                    "[266998701 | Khan et al. | 2024 | Citations: 1]": "Recent advances in deep learning techniques have achieved remarkable performance in several computer vision problems. A notably intuitive technique called Curriculum Learning (CL) has been introduced recently for training deep learning models. Surprisingly, curriculum learning achieves significantly improved results in some tasks but marginal or no improvement in others. Hence, there is still a debate about its adoption as a standard method to train supervised learning models. In this work, we investigate the impact of curriculum learning in crowd counting using the density estimation method. We performed detailed investigations by conducting 112 experiments using six different CL settings using eight different crowd models. Our experiments show that curriculum learning improves the model learning performance and shortens the convergence time.",
                    "[51606954 | Liu et al. | 2018 | Citations: 85]": "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively.",
                    "[6954583 | Shi et al. | 2016 | Citations: 81]": "We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques.",
                    "[8502955 | Pentina et al. | 2014 | Citations: 245]": "Sharing information between multiple tasks enables algorithms to achieve good generalization performance even from small amounts of training data. However, in a realistic scenario of multi-task learning not all tasks are equally related to each other, hence it could be advantageous to transfer information only between the most related tasks. In this work we propose an approach that processes multiple tasks in a sequence with sharing between subsequent tasks instead of solving all tasks jointly. Subsequently, we address the question of curriculum learning of tasks, i.e. finding the best order of tasks to be learned. Our approach is based on a generalization bound criterion for choosing the task order that optimizes the average expected classification performance over all tasks. Our experimental results show that learning multiple related tasks sequentially can be more effective than learning them jointly, the order in which tasks are being solved affects the overall performance, and that our model is able to automatically discover a favourable order of tasks.",
                    "[879067 | Ionescu et al. | 2016 | Citations: 142]": "We address the problem of estimating image difficulty defined as the human response time for solving a visual search task. We collect human annotations of image difficulty for the PASCAL VOC 2012 data set through a crowd-sourcing platform. We then analyze what human interpretable image properties can have an impact on visual search difficulty, and how accurate are those properties for predicting difficulty. Next, we build a regression model based on deep features learned with state of the art convolutional neural networks and show better results for predicting the ground-truth visual search difficulty scores produced by human annotators. Our model is able to correctly rank about 75% image pairs according to their difficulty score. We also show that our difficulty predictor generalizes well to new classes not seen during training. Finally, we demonstrate that our predicted difficulty scores are useful for weakly supervised object localization (8% improvement) and semi-supervised object classification (1% improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Curriculum Learning",
                        "pdf_hash": "",
                        "start": 972,
                        "end": 1829,
                        "sentence_offsets": [
                            {
                                "start": 972,
                                "end": 1157
                            },
                            {
                                "start": 1158,
                                "end": 1275
                            },
                            {
                                "start": 1276,
                                "end": 1403
                            },
                            {
                                "start": 1404,
                                "end": 1528
                            },
                            {
                                "start": 1529,
                                "end": 1714
                            },
                            {
                                "start": 1715,
                                "end": 1829
                            }
                        ],
                        "ref_mentions": [
                            "8502955",
                            "204539326",
                            "221995570",
                            "26468344",
                            "166228313",
                            "233433844",
                            "51606954",
                            "6954583",
                            "879067",
                            "10364203",
                            "102350936",
                            "266998701",
                            "254246401",
                            "256808576"
                        ],
                        "quote": "The main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input (Pentina et al., 2014)(Jim\u00e9nez-S\u00e1nchez et al., 2019)(Wei et al., 2020) or metrics based on domain-specific heuristics. For instance, the text length (Kocmi et al., 2017)[44](Tay et al., 2019)(Zhang et al., 2021) and the word frequency [1,(Liu et al., 2018) have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works (Shi et al., 2016)[33]. Other solutions employed difficulty estimators (Ionescu et al., 2016) or even the confidence level of the predictions made by the neural network (Gong et al., 2016)(Hacohen et al., 2019) to approximate the complexity of the data samples. Other studies (Khan et al., 2024)(Khan et al., 2022)(Khan et al., 2023) used the error of a previously trained model to estimate the difficulty of each sample."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[248965007 | Varshney et al. | 2022 | Citations: 8]",
                "snippets": "In addition to arranging the training instances into a learning curriculum, computing difficulty scores using model-based techniques has shown its benefits in several other areas, such as improving selective prediction ability (Varshney et al., 2020), under-standing training dynamics (Swayamdipta et al., 2020), and efficient evaluations (Varshney et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247292373 | Varshney et al. | 2022 | Citations: 19]": "Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students\u2019 potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5% instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2% higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.",
                    "[221856637 | Swayamdipta et al. | 2020 | Citations: 452]": "Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.",
                    "[247778884 | Varshney et al. | 2020 | Citations: 23]": "It\u2019s better to say \u201cI can\u2019t answer\u201d than to answer incorrectly. This selective prediction ability is crucial for NLP systems to be reliably deployed in real-world applications. Prior work has shown that existing selective prediction techniques fail to perform well, especially in the out-of-domain setting. In this work, we propose a method that improves probability estimates of models by calibrating them using prediction confidence and difficulty score of instances. Using these two signals, we first annotate held-out instances and then train a calibrator to predict the likelihood of correctness of the model\u2019s prediction. We instantiate our method with Natural Language Inference (NLI) and Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and Out-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the representations learned by our calibrator result in an improvement of (15.81%, 5.64%) and (6.19%, 13.9%) over \u2018MaxProb\u2019 -a selective prediction baseline- on NLI and DD tasks respectively."
                },
                "metadata": [
                    {
                        "section_title": "H Limitations of Computing Difficulty Scores using Model-based Techniques",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 365,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 365
                            }
                        ],
                        "ref_mentions": [
                            "247778884",
                            "221856637",
                            "247292373"
                        ],
                        "quote": "In addition to arranging the training instances into a learning curriculum, computing difficulty scores using model-based techniques has shown its benefits in several other areas, such as improving selective prediction ability (Varshney et al., 2020), under-standing training dynamics (Swayamdipta et al., 2020), and efficient evaluations (Varshney et al., 2022)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[251493126 | Karakasidis et al. | 2022 | Citations: 2]",
                "snippets": "To create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch (Hacohen et al., 2019). The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102350936 | Hacohen et al. | 2019 | Citations: 449]": "Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."
                },
                "metadata": [
                    {
                        "section_title": "Curriculum Learning",
                        "pdf_hash": "",
                        "start": 963,
                        "end": 2263,
                        "sentence_offsets": [
                            {
                                "start": 963,
                                "end": 1098
                            },
                            {
                                "start": 1099,
                                "end": 1205
                            },
                            {
                                "start": 1208,
                                "end": 1304
                            },
                            {
                                "start": 1305,
                                "end": 1373
                            },
                            {
                                "start": 1374,
                                "end": 1481
                            },
                            {
                                "start": 1484,
                                "end": 1636
                            },
                            {
                                "start": 1637,
                                "end": 1755
                            },
                            {
                                "start": 1756,
                                "end": 1895
                            },
                            {
                                "start": 1898,
                                "end": 2024
                            },
                            {
                                "start": 2025,
                                "end": 2162
                            },
                            {
                                "start": 2163,
                                "end": 2263
                            }
                        ],
                        "ref_mentions": [
                            "102350936"
                        ],
                        "quote": "To create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch (Hacohen et al., 2019). The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[253734400 | Jia et al. | 2022 | Citations: 4]",
                "snippets": "Previous work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018)(Kocmi et al., 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020)Kumar et al., 2019;(Shen et al., 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218470266 | Shen et al. | 2020 | Citations: 87]": "Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. CDL utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.",
                    "[220047761 | Zhou et al. | 2020 | Citations: 96]": "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",
                    "[26468344 | Kocmi et al. | 2017 | Citations: 141]": "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \u201ccurriculum learning\u201d). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \u201ccurricula\u201d achieve a small improvement over the baseline.",
                    "[51606954 | Liu et al. | 2018 | Citations: 85]": "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 444,
                        "end": 1020,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "51606954",
                            "26468344",
                            "220047761",
                            "218470266"
                        ],
                        "quote": "Previous work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018)(Kocmi et al., 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020)Kumar et al., 2019;(Shen et al., 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[254685579 | Lee et al. | 2022 | Citations: 12]",
                "snippets": "However, we argue that existing linguisticsinspired criteria, such as length, rarity, and masking ratio of a sequence, do not effectively reflect the nature of MLM, as verified empirically in Table 1.\n\nThe difficulty associated with MLM is significantly affected by the choice of tokens to be masked in the given sequence, rather than by the given sequence itself. For example, given \"The man is a Stanford <mask> student\", we can easily predict that the masked token would be University, whereas given \"The man is a <mask> University student\", it would be relatively difficult to predict the original token due to the insufficient clues in the context. Then, how can we measure the MLM difficulty? MLM can be viewed as predicting masked tokens based on other contextual tokens related to masked tokens. Therefore, if a word is related to many other words and phrases, it is likely that it has several clues in the context that make MLM easier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Methods",
                        "pdf_hash": "",
                        "start": 1035,
                        "end": 1979,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "However, we argue that existing linguisticsinspired criteria, such as length, rarity, and masking ratio of a sequence, do not effectively reflect the nature of MLM, as verified empirically in Table 1.\n\nThe difficulty associated with MLM is significantly affected by the choice of tokens to be masked in the given sequence, rather than by the given sequence itself. For example, given \"The man is a Stanford <mask> student\", we can easily predict that the masked token would be University, whereas given \"The man is a <mask> University student\", it would be relatively difficult to predict the original token due to the insufficient clues in the context. Then, how can we measure the MLM difficulty? MLM can be viewed as predicting masked tokens based on other contextual tokens related to masked tokens. Therefore, if a word is related to many other words and phrases, it is likely that it has several clues in the context that make MLM easier."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[259370648 | Elgaar et al. | 2023 | Citations: 0]",
                "snippets": "Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[259593617 | Zeng et al. | 2023 | Citations: 6]",
                "snippets": "Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[261076515 | Li et al. | 2023 | Citations: 211]",
                "snippets": "Our method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\".",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 502,
                        "end": 1534,
                        "sentence_offsets": [
                            {
                                "start": 502,
                                "end": 670
                            },
                            {
                                "start": 671,
                                "end": 829
                            },
                            {
                                "start": 830,
                                "end": 1020
                            },
                            {
                                "start": 1021,
                                "end": 1127
                            },
                            {
                                "start": 1128,
                                "end": 1341
                            },
                            {
                                "start": 1342,
                                "end": 1534
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\"."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[264819795 | Elgaar et al. | 2023 | Citations: 2]",
                "snippets": "We assume there exists a subset of linguistic complexity indices that are most influential to learning an NLP task by a particular model. To identify these indices for each model and NLP task, we derive a weight factor \u03c1 i \u2208 [\u22121, 1] for each linguistic index that quantifies how well the index estimates the true difficulty of data samples to the model, determined by model instantaneous loss against validation data. By learning these weight factors, we obtain precise estimations that shed light on the core linguistic complexity indices that each model needs at different stages of its training to learn an NLP task. In addition, these estimates can be readily used for linguistic curriculum development, e.g., by training models with linguistically easy samples (with respect to the model) and gradually introducing linguistically challenging samples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 640,
                        "end": 1495,
                        "sentence_offsets": [
                            {
                                "start": 640,
                                "end": 777
                            },
                            {
                                "start": 778,
                                "end": 1057
                            },
                            {
                                "start": 1058,
                                "end": 1259
                            },
                            {
                                "start": 1260,
                                "end": 1495
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We assume there exists a subset of linguistic complexity indices that are most influential to learning an NLP task by a particular model. To identify these indices for each model and NLP task, we derive a weight factor \u03c1 i \u2208 [\u22121, 1] for each linguistic index that quantifies how well the index estimates the true difficulty of data samples to the model, determined by model instantaneous loss against validation data. By learning these weight factors, we obtain precise estimations that shed light on the core linguistic complexity indices that each model needs at different stages of its training to learn an NLP task. In addition, these estimates can be readily used for linguistic curriculum development, e.g., by training models with linguistically easy samples (with respect to the model) and gradually introducing linguistically challenging samples."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[265456857 | Hajimolahoseini et al. | 2023 | Citations: 2]",
                "snippets": "Other metrics include the number of times a model misclassifies a sample and the high variance of gradients, indicating example difficulty (Dadalto et al., 2023). Prediction depth, the layer at which a k-NN classifier can successfully classify an example, can also serve as a measure of computational difficulty (Ma and Karaman, 2018).\n\nCurriculum learning methods revolve around identifying the optimal order of data samples to accelerate model convergence. Unlike dataset pruning, these methods do not remove data samples but emphasize the importance of the order in which data samples are presented during training. Curriculum learning methods define metrics to quantify the importance of each data sample, and the order of learning is determined based on these scores. These metrics can be crafted manually or generated automatically (Zhang and Pfister, 2021;de Mathelin et al., 2022). While handcrafted metrics may be task-specific, they often outperform automated metrics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 123,
                        "end": 1101,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 158
                            },
                            {
                                "start": 159,
                                "end": 321
                            },
                            {
                                "start": 322,
                                "end": 494
                            },
                            {
                                "start": 497,
                                "end": 618
                            },
                            {
                                "start": 619,
                                "end": 778
                            },
                            {
                                "start": 779,
                                "end": 932
                            },
                            {
                                "start": 933,
                                "end": 1049
                            },
                            {
                                "start": 1050,
                                "end": 1138
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Other metrics include the number of times a model misclassifies a sample and the high variance of gradients, indicating example difficulty (Dadalto et al., 2023). Prediction depth, the layer at which a k-NN classifier can successfully classify an example, can also serve as a measure of computational difficulty (Ma and Karaman, 2018).\n\nCurriculum learning methods revolve around identifying the optimal order of data samples to accelerate model convergence. Unlike dataset pruning, these methods do not remove data samples but emphasize the importance of the order in which data samples are presented during training. Curriculum learning methods define metrics to quantify the importance of each data sample, and the order of learning is determined based on these scores. These metrics can be crafted manually or generated automatically (Zhang and Pfister, 2021;de Mathelin et al., 2022). While handcrafted metrics may be task-specific, they often outperform automated metrics."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[265506572 | Chobey et al. | 2023 | Citations: 5]",
                "snippets": "There are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[232362223 | Wang et al. | 2021 | Citations: 611]": "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."
                },
                "metadata": [
                    {
                        "section_title": "Background",
                        "pdf_hash": "",
                        "start": 535,
                        "end": 824,
                        "sentence_offsets": [
                            {
                                "start": 535,
                                "end": 824
                            }
                        ],
                        "ref_mentions": [
                            "232362223"
                        ],
                        "quote": "There are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021)."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[266166227 | Zhou et al. | 2023 | Citations: 2]",
                "snippets": "Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Framework",
                        "pdf_hash": "",
                        "start": 887,
                        "end": 1008,
                        "sentence_offsets": [
                            {
                                "start": 887,
                                "end": 1008
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[266177072 | Sun et al. | 2023 | Citations: 2]",
                "snippets": "In this study, we introduce a novel scoring system called the Data Selection Curriculum (DSC) score, which serves as a measure of learning difficulty. The DSC score considers two essential factors: the difficulty of enhancing the ATS model through a specific instance and the expected performance of the model on that particular instance. Instances on which the current model already exhibits promising performance and stands to benefit significantly from slight adjustments are assigned as a lower learning difficulty. This is because these instances have the potential to contribute more to the final performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1732,
                        "end": 2347,
                        "sentence_offsets": [
                            {
                                "start": 1732,
                                "end": 1882
                            },
                            {
                                "start": 1883,
                                "end": 2070
                            },
                            {
                                "start": 2071,
                                "end": 2251
                            },
                            {
                                "start": 2252,
                                "end": 2347
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this study, we introduce a novel scoring system called the Data Selection Curriculum (DSC) score, which serves as a measure of learning difficulty. The DSC score considers two essential factors: the difficulty of enhancing the ATS model through a specific instance and the expected performance of the model on that particular instance. Instances on which the current model already exhibits promising performance and stands to benefit significantly from slight adjustments are assigned as a lower learning difficulty. This is because these instances have the potential to contribute more to the final performance."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[267740312 | Mekala et al. | 2024 | Citations: 21]",
                "snippets": "Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249060677 | Mekala et al. | 2022 | Citations: 20]": "Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."
                },
                "metadata": [
                    {
                        "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
                        "pdf_hash": "",
                        "start": 96,
                        "end": 539,
                        "sentence_offsets": [
                            {
                                "start": 96,
                                "end": 330
                            },
                            {
                                "start": 331,
                                "end": 429
                            },
                            {
                                "start": 430,
                                "end": 539
                            }
                        ],
                        "ref_mentions": [
                            "249060677"
                        ],
                        "quote": "Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[268696658 | Zhao et al. | 2024 | Citations: 5]",
                "snippets": "To address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Gradient-based Difficulty Measure",
                        "pdf_hash": "",
                        "start": 451,
                        "end": 1292,
                        "sentence_offsets": [
                            {
                                "start": 451,
                                "end": 660
                            },
                            {
                                "start": 661,
                                "end": 774
                            },
                            {
                                "start": 775,
                                "end": 853
                            },
                            {
                                "start": 854,
                                "end": 1020
                            },
                            {
                                "start": 1021,
                                "end": 1117
                            },
                            {
                                "start": 1118,
                                "end": 1292
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[269741199 | Lee et al. | 2024 | Citations: 2]",
                "snippets": "Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)...Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). (Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks...Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231709290 | Soviany et al. | 2021 | Citations: 359]": "Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.",
                    "[243766208 | Zhang et al. | 2021 | Citations: 14]": "With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages.",
                    "[265068175 | Ranaldi et al. | 2023 | Citations: 13]": "Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art.",
                    "[51606954 | Liu et al. | 2018 | Citations: 85]": "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."
                },
                "metadata": [
                    {
                        "section_title": "I. Related Work",
                        "pdf_hash": "",
                        "start": 567,
                        "end": 1139,
                        "sentence_offsets": [
                            {
                                "start": 567,
                                "end": 824
                            },
                            {
                                "start": 825,
                                "end": 1140
                            }
                        ],
                        "ref_mentions": [
                            "231709290",
                            "51606954",
                            "243766208"
                        ],
                        "quote": "Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)"
                    },
                    {
                        "section_title": "I. Related Work",
                        "pdf_hash": "",
                        "start": 1418,
                        "end": 2243,
                        "sentence_offsets": [
                            {
                                "start": 1418,
                                "end": 1628
                            },
                            {
                                "start": 1629,
                                "end": 1944
                            },
                            {
                                "start": 1945,
                                "end": 2243
                            }
                        ],
                        "ref_mentions": [
                            "265068175"
                        ],
                        "quote": "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). (Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks"
                    },
                    {
                        "section_title": "I. Related Work",
                        "pdf_hash": "",
                        "start": 2246,
                        "end": 2363,
                        "sentence_offsets": [
                            {
                                "start": 2246,
                                "end": 2362
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[269756933 | Kim et al. | 2024 | Citations: 10]",
                "snippets": "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[269762685 | Wang et al. | 2024 | Citations: 4]",
                "snippets": "In the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Approaches",
                        "pdf_hash": "",
                        "start": 661,
                        "end": 1422,
                        "sentence_offsets": [
                            {
                                "start": 661,
                                "end": 717
                            },
                            {
                                "start": 717,
                                "end": 822
                            },
                            {
                                "start": 822,
                                "end": 970
                            },
                            {
                                "start": 970,
                                "end": 1060
                            },
                            {
                                "start": 1060,
                                "end": 1155
                            },
                            {
                                "start": 1155,
                                "end": 1269
                            },
                            {
                                "start": 1269,
                                "end": 1422
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[270199394 | Ankner et al. | 2024 | Citations: 34]",
                "snippets": "Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1350,
                        "end": 1568,
                        "sentence_offsets": [
                            {
                                "start": 1350,
                                "end": 1568
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[270215134 | Kessler et al. | 2024 | Citations: 0]",
                "snippets": "It has also been shown that pruning data according to how easy they are to be forgotten over the course of training -as a measure of difficulty -results in training on less data while maintaining performance (Toneva et al., 2018).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "G.2.3 SUPERVISED PROTOTYPES",
                        "pdf_hash": "",
                        "start": 197,
                        "end": 427,
                        "sentence_offsets": [
                            {
                                "start": 197,
                                "end": 427
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It has also been shown that pruning data according to how easy they are to be forgotten over the course of training -as a measure of difficulty -results in training on less data while maintaining performance (Toneva et al., 2018)."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[271051051 | Jarca et al. | 2024 | Citations: 0]",
                "snippets": "The aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input (Jim\u00e9nez-S\u00e1nchez et al., 2019), which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204539326 | Jimenez-Sanchez et al. | 2019 | Citations: 48]": "Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning \"easy\" examples and move towards \"hard\", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1519,
                        "end": 2126,
                        "sentence_offsets": [
                            {
                                "start": 1519,
                                "end": 1588
                            },
                            {
                                "start": 1588,
                                "end": 1723
                            },
                            {
                                "start": 1723,
                                "end": 1845
                            },
                            {
                                "start": 1845,
                                "end": 1953
                            },
                            {
                                "start": 1953,
                                "end": 2126
                            }
                        ],
                        "ref_mentions": [
                            "204539326"
                        ],
                        "quote": "The aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input (Jim\u00e9nez-S\u00e1nchez et al., 2019), which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[271855275 | Yang et al. | 2024 | Citations: 0]",
                "snippets": "Policy-driven Difficulty Measurement: we begin by measuring the difficulty of data based on the model's real-time performance, transitioning from static, predefined metrics to more dynamic and adaptable ones.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Policy-driven Difficulty Measurement: we begin by measuring the difficulty of data based on the model's real-time performance, transitioning from static, predefined metrics to more dynamic and adaptable ones.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[274886046 | Jung et al. | 2024 | Citations: 0]",
                "snippets": "In recent years, data-driven and machine learning-based natural language processing (NLP) technologies have effectively addressed various challenges. To further enhance the performance of NLP models, it is crucial to understand the types of data that a model can handle well and those it struggles with. This study introduces a method to discern which types of data can be effectively processed by given neural network-based models and which pose difficulties. We define the criteria for hard-to-solve data, construct a pairwise easy-hard dataset, and propose a neural scoring model. This model ascertains the difficulty level of each data instance. To utilize the proposed difficulty level as an application, we employed curriculum learning. The experimental results show that our methodology can effectively distinguish between easy and hard data, and performance improves when applying the curriculum learning approach.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 922,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In recent years, data-driven and machine learning-based natural language processing (NLP) technologies have effectively addressed various challenges. To further enhance the performance of NLP models, it is crucial to understand the types of data that a model can handle well and those it struggles with. This study introduces a method to discern which types of data can be effectively processed by given neural network-based models and which pose difficulties. We define the criteria for hard-to-solve data, construct a pairwise easy-hard dataset, and propose a neural scoring model. This model ascertains the difficulty level of each data instance. To utilize the proposed difficulty level as an application, we employed curriculum learning. The experimental results show that our methodology can effectively distinguish between easy and hard data, and performance improves when applying the curriculum learning approach."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[277398866 | Zhang et al. | 2025 | Citations: 0]",
                "snippets": "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "1) ADDITIONAL TECHNIQUES AND APPLICATIONS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 796,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 239
                            },
                            {
                                "start": 240,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 623
                            },
                            {
                                "start": 624,
                                "end": 796
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[277596006 | Bae et al. | 2025 | Citations: 12]",
                "snippets": "Online filtering methods further explore this idea by dynamically adjusting the training data to match the current ability of the model (Cui et al., 2025). However, while previous work demonstrate the empirical effectiveness of such techniques, they often lack a detailed analysis of why or when certain difficulty distributions yield better learning outcomes. With G rollouts for each prompt x, we measure the pass rate p(x) as the average accuracy and filter them by predefined thresholds: e.g., 0.25 \u2264 p(x) \u2264 0.75 in this case.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1239,
                        "end": 1769,
                        "sentence_offsets": [
                            {
                                "start": 1239,
                                "end": 1394
                            },
                            {
                                "start": 1395,
                                "end": 1599
                            },
                            {
                                "start": 1600,
                                "end": 1769
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Online filtering methods further explore this idea by dynamically adjusting the training data to match the current ability of the model (Cui et al., 2025). However, while previous work demonstrate the empirical effectiveness of such techniques, they often lack a detailed analysis of why or when certain difficulty distributions yield better learning outcomes. With G rollouts for each prompt x, we measure the pass rate p(x) as the average accuracy and filter them by predefined thresholds: e.g., 0.25 \u2264 p(x) \u2264 0.75 in this case."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[277824032 | Yu et al. | 2025 | Citations: 3]",
                "snippets": "Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;(Lee et al., 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;(Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[272706931 | Lee et al. | 2024 | Citations: 6]": "By far, the most effective knowledge assessment in college education is to give students exam and grade their answers then assess their level of understanding. However, exam grading can be time-consuming, tedious, cumbersome, and sometimes the grading results are not consistent with the rubric. Here, we propose an AI based exam grader that can not only ease educators\u2019 burden but also produce accurate, consistent, and precise grading results. We have used GPT-3.5, GPT-4.0, and Gemini-pro, respectively, as our grading engine. To verify the correctness, precision, and accuracy of our proposed grader, the results were compared with the instructor\u2019s grading result and also with human grader such as teaching assistants. In our experiment, GPT-4.0 showed the most reliable and consistent results.",
                    "[275757809 | Yuan et al. | 2025 | Citations: 16]": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions based on correctness, Agent-R leverages MCTS to construct training data that recover correct trajectories from erroneous ones. A key challenge of agent reflection lies in the necessity for timely revision rather than waiting until the end of a rollout. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that Agent-R continuously improves the model's ability to recover from errors and enables timely error correction. Experiments on three interactive environments show that Agent-R effectively equips agents to correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%)."
                },
                "metadata": [
                    {
                        "section_title": "LLM-Adaptive Difficulty Grading",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1263,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 204
                            },
                            {
                                "start": 205,
                                "end": 411
                            },
                            {
                                "start": 412,
                                "end": 724
                            },
                            {
                                "start": 725,
                                "end": 890
                            },
                            {
                                "start": 893,
                                "end": 1105
                            },
                            {
                                "start": 1106,
                                "end": 1263
                            }
                        ],
                        "ref_mentions": [
                            "272706931",
                            "275757809"
                        ],
                        "quote": "Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;(Lee et al., 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;(Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[279243528 | Liu et al. | 2025 | Citations: 0]",
                "snippets": "To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of\"progressive overload\"(POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of\"progressive overload\"(POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.034638,
        "cot": "For the user's query about measuring the difficulty of datapoints for curriculum learning in language model training, I need to organize the quotes into logical dimensions that provide a comprehensive overview of measurement techniques and their impacts on training outcomes.\n\nFirst, I'll need a \"Background\" section to introduce curriculum learning and its importance in language model training. Although there's no specific quote for this section, it's essential to provide context for the reader to understand the subsequent sections.\n\nNext, I'll focus on \"Types of Difficulty Measurement Techniques\" which should be in synthesis format since we need to provide a coherent explanation of different approaches rather than just listing them. The quotes reveal several categories of measurement techniques including predefined/static measures, model-based/dynamic measures, and hybrid approaches. This synthesis will help organize the various approaches mentioned across quotes.\n\nFor \"Predefined/Static Difficulty Metrics\", a list format is appropriate as there are numerous specific metrics mentioned in the quotes that can be enumerated. These include metrics like sentence length, word rarity, and other task-specific features that can be calculated before model training.\n\nSimilarly, \"Model-Based/Dynamic Difficulty Metrics\" should be in list format to clearly present the various approaches that use model feedback to determine difficulty, such as loss-based methods, gradient-based methods, and performance-based metrics.\n\n\"Implementation Strategies for Curriculum Learning\" should be in synthesis format as it involves explaining how the difficulty metrics are used to construct and apply curricula during training, requiring a cohesive narrative rather than a simple list.\n\n\"Impact on Training Outcomes\" is critical to address the second part of the user's query. This should be in synthesis format to coherently discuss how different curriculum approaches affect model performance, convergence speed, and other training outcomes.\n\n\"Challenges and Limitations\" in synthesis format will provide a balanced view by discussing the difficulties in implementing effective curriculum learning strategies.\n\n\"Advanced and Recent Approaches\" in synthesis format will highlight cutting-edge techniques that address limitations of traditional approaches, focusing on newer and more sophisticated methods mentioned in the quotes.\n\nThis organization ensures a comprehensive coverage of the topic while maintaining a logical flow from basic concepts to advanced techniques and outcomes.",
        "plan": {
            "Background and Introduction to Curriculum Learning (synthesis)": [],
            "Types of Difficulty Measurement Techniques (synthesis)": [
                9,
                24,
                17
            ],
            "Predefined/Static Difficulty Metrics (list)": [
                0,
                2,
                4,
                7,
                10,
                11,
                13,
                15,
                22,
                23
            ],
            "Model-Based/Dynamic Difficulty Metrics (list)": [
                5,
                6,
                9,
                14,
                16,
                18,
                19,
                20,
                21,
                25,
                26,
                27,
                28,
                29,
                31
            ],
            "Implementation Strategies for Curriculum Learning (synthesis)": [
                1,
                3,
                9,
                12,
                24,
                33
            ],
            "Impact on Training Outcomes (synthesis)": [
                1,
                2,
                8,
                12,
                13,
                15,
                22,
                30,
                32
            ],
            "Challenges and Limitations (synthesis)": [
                0,
                10,
                23,
                27,
                32
            ],
            "Advanced and Recent Approaches (synthesis)": [
                3,
                12,
                14,
                20,
                21,
                32
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Background and Introduction to Curriculum Learning",
                "tldr": "Curriculum learning is an approach that organizes training examples from easy to difficult, mimicking human learning processes. This strategy aims to improve model convergence, generalization, and training efficiency by gradually exposing models to increasingly complex data. (LLM Memory)",
                "text": "\nCurriculum learning in machine learning, particularly for training large language models (LLMs), is inspired by the way humans learn complex tasks - starting with simple concepts before progressing to more difficult ones. This approach was formally introduced to machine learning by Bengio et al. in 2009, who demonstrated that organizing training examples in increasing order of difficulty could lead to better generalization and faster convergence. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nIn the context of LLMs, curriculum learning involves strategically ordering training data based on difficulty measures rather than the standard random sampling approach. The core principle is to initially expose the model to \"easy\" examples that establish foundational knowledge, then gradually introduce more complex examples that build upon this foundation. This progressive training regime helps the model develop more robust representations and avoid getting stuck in poor local optima during optimization. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nCurriculum learning offers several potential benefits for LLM training: improved final performance, faster convergence rates, enhanced generalization to unseen data, and better handling of noisy or adversarial examples. The effectiveness of a curriculum critically depends on how accurately we can measure the difficulty of training examples, which is precisely what makes this an active and challenging research area. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFor implementation, curriculum learning can follow either a discrete schedule with distinct phases of increasing difficulty, or a continuous pacing function that gradually introduces harder examples. Some approaches also incorporate competence-based progression, where the model only advances to harder examples after demonstrating mastery of simpler concepts. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types of Difficulty Measurement Techniques",
                "tldr": "Difficulty measurement techniques for curriculum learning can be broadly categorized into predefined/static approaches and dynamic/adaptive approaches. These techniques form the foundation of curriculum design, determining how training examples are ordered from easy to difficult. (5 sources)",
                "text": "\nCurriculum learning requires two fundamental components: a method to measure the difficulty of training examples and a strategy to schedule these examples during training <Paper corpusId=\"265506572\" paperTitle=\"(Chobey et al., 2023)\" isShortName></Paper> <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. The approaches for measuring difficulty can be classified into several categories, reflecting different methodologies for determining which examples are \"easy\" or \"hard\" for a model to learn.\n\nPredefined or static difficulty measures establish the curriculum before training begins and maintain this fixed ordering throughout the training process. These include metadata-based approaches, which leverage inherent characteristics of the data (such as utterance duration or signal-to-noise ratio) to estimate difficulty <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper>. Such predefined curricula were dominant in early curriculum learning research but lack flexibility for broad applications <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nAnother static approach is transfer learning-based difficulty measurement, which relies on an external, pre-trained \"teacher\" model to evaluate training examples. The assumption is that examples which cause more errors in the teacher model are likely more difficult <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper> <Paper corpusId=\"102350936\" paperTitle=\"(Hacohen et al., 2019)\" isShortName></Paper>. These teacher models provide curricula to guide the learning process of student models <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nIn contrast, dynamic or adaptive approaches adjust the curriculum during training based on feedback from the model itself. One notable example is self-paced learning (SPL), introduced by Kumar et al. in 2010, which automatically orders data according to training loss <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This approach recognizes that the perceived difficulty of examples changes as training progresses and dynamically adjusts the curriculum to match the model's current capabilities <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper>.\n\nMore recent innovations in adaptive curriculum learning include reinforcement learning methods, where teacher models adapt the curriculum based on student model feedback, as well as approaches based on Bayesian optimization, meta-learning, and adversarial learning <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. These automatic curriculum learning techniques aim to overcome the limitations of predefined approaches by creating more flexible and generalizable difficulty measurements.\n\nThe choice of difficulty measurement technique significantly influences how a curriculum is constructed and ultimately affects the training dynamics and performance of the resulting model. The increasing sophistication of these techniques reflects the growing understanding of how to effectively structure the learning process for large language models.",
                "citations": [
                    {
                        "id": "(Chobey et al., 2023)",
                        "snippets": [
                            "There are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 265506572,
                            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
                            "authors": [
                                {
                                    "authorId": "2268760204",
                                    "name": "Aryaman Chobey"
                                },
                                {
                                    "authorId": "2268760018",
                                    "name": "Oliver Smith"
                                },
                                {
                                    "authorId": "2268796061",
                                    "name": "Anzi Wang"
                                },
                                {
                                    "authorId": "2268760229",
                                    "name": "Grusha Prasad"
                                }
                            ],
                            "year": 2023,
                            "venue": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
                            "n_citations": 5
                        },
                        "score": 0.91162109375
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."
                        ],
                        "paper": {
                            "corpus_id": 232362223,
                            "title": "A Survey on Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2153687490",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "51310474",
                                    "name": "Yudong Chen"
                                },
                                {
                                    "authorId": "145583986",
                                    "name": "Wenwu Zhu"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 611
                        },
                        "score": 0
                    },
                    {
                        "id": "(Karakasidis et al., 2022)",
                        "snippets": [
                            "To create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch (Hacohen et al., 2019). The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model."
                        ],
                        "paper": {
                            "corpus_id": 251493126,
                            "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
                            "authors": [
                                {
                                    "authorId": "2181120338",
                                    "name": "Georgios Karakasidis"
                                },
                                {
                                    "authorId": "2066645527",
                                    "name": "Tam'as Gr'osz"
                                },
                                {
                                    "authorId": "1719346",
                                    "name": "M. Kurimo"
                                }
                            ],
                            "year": 2022,
                            "venue": "Interspeech",
                            "n_citations": 2
                        },
                        "score": 0.93603515625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "In the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning."
                        ],
                        "paper": {
                            "corpus_id": 269762685,
                            "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2256599610",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "2261888564",
                                    "name": "Yuwei Zhou"
                                },
                                {
                                    "authorId": "2191043236",
                                    "name": "Hong Chen"
                                },
                                {
                                    "authorId": "2156154955",
                                    "name": "Wenwu Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Web Conference",
                            "n_citations": 4
                        },
                        "score": 0.85546875
                    },
                    {
                        "id": "(Hacohen et al., 2019)",
                        "snippets": [
                            "Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."
                        ],
                        "paper": {
                            "corpus_id": 102350936,
                            "title": "On The Power of Curriculum Learning in Training Deep Networks",
                            "authors": [
                                {
                                    "authorId": "94064232",
                                    "name": "Guy Hacohen"
                                },
                                {
                                    "authorId": "1789171",
                                    "name": "D. Weinshall"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 449
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Predefined/Static Difficulty Metrics",
                "tldr": "Predefined difficulty metrics establish fixed criteria for measuring training example complexity before model training begins. These static metrics include text length, word rarity, linguistic features, domain-specific properties, and signal quality measures that enable systematic curriculum organization. (17 sources)",
                "text": "\nPredefined or static difficulty metrics establish a fixed curriculum ordering before training begins, providing a straightforward approach to implementing curriculum learning for large language models. These metrics can be categorized based on their underlying methodology:\n\n## Text-Based Features\n- **Length-based metrics**: Sentence or utterance length is one of the most widely used simple difficulty metrics, where shorter sentences are typically considered easier <Paper corpusId=\"26468344\" paperTitle=\"(Kocmi et al., 2017)\" isShortName></Paper> <Paper corpusId=\"166228313\" paperTitle=\"(Tay et al., 2019)\" isShortName></Paper> <Paper corpusId=\"233433844\" paperTitle=\"(Zhang et al., 2021)\" isShortName></Paper>. This approach is particularly common in machine translation tasks.\n- **Word frequency and rarity**: Examples containing more common words are considered easier than those with rare words <Paper corpusId=\"51606954\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper> <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This metric assumes that learning from common vocabulary first provides a better foundation.\n- **N-gram size**: Larger n-grams typically represent more complex linguistic patterns <Paper corpusId=\"11137059\" paperTitle=\"(Graves et al., 2017)\" isShortName></Paper>.\n\n## Linguistic Complexity\n- **Readability scores**: Standard readability indices measure text complexity based on sentence structure and vocabulary <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>.\n- **Grammatical error count**: The number of grammatical errors in a text can serve as a difficulty metric, with more errors indicating higher complexity <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>.\n- **Linguistic complexity indices**: Weighted combinations of various linguistic features can provide customized difficulty measurements tailored to specific models and tasks <Paper corpusId=\"264819795\" paperTitle=\"(Elgaar et al., 2023)\" isShortName></Paper>.\n\n## Domain-Specific Features\n- **Visual complexity**: In multimodal learning, examples with fewer and larger objects are considered easier <Paper corpusId=\"6954583\" paperTitle=\"(Shi et al., 2016)\" isShortName></Paper>.\n- **Signal quality**: For speech processing, signal-to-noise ratio (SNR) serves as a difficulty measure, where cleaner signals are easier to learn from <Paper corpusId=\"14928979\" paperTitle=\"(Braun et al., 2016)\" isShortName></Paper> <Paper corpusId=\"231979234\" paperTitle=\"(Higuchi et al., 2021)\" isShortName></Paper>.\n- **Content comprehensibility**: Metrics based on how easy it is to understand the content, considering factors beyond pure linguistic features <Paper corpusId=\"265068175\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper>.\n\n## Composite Approaches\n- **Multi-factor scoring**: Combining length, rarity, and comprehensibility into a single difficulty score can provide more nuanced curriculum organization <Paper corpusId=\"265068175\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper>.\n- **Cross-review methods**: Using multiple evaluation criteria to create a more robust difficulty assessment <Paper corpusId=\"243766208\" paperTitle=\"(Zhang et al._1, 2021)\" isShortName></Paper>.\n\nThe primary limitation of predefined metrics is their static nature and potential disconnect from how models actually perceive difficulty <Paper corpusId=\"220045816\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. These metrics are often highly task-specific and may not generalize well across different domains <Paper corpusId=\"253734400\" paperTitle=\"(Jia et al., 2022)\" isShortName></Paper>. Additionally, traditional linguistics-inspired metrics may not effectively reflect the actual challenges in specific learning objectives like masked language modeling <Paper corpusId=\"254685579\" paperTitle=\"(Lee et al., 2022)\" isShortName></Paper>.\n\nDespite these limitations, predefined metrics remain popular due to their simplicity and interpretability, requiring no model feedback during curriculum construction <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. They provide a straightforward starting point for implementing curriculum learning in large language model training.",
                "citations": [
                    {
                        "id": "(Kocmi et al., 2017)",
                        "snippets": [
                            "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \"curriculum learning\"). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \"curricula\" achieve a small improvement over the baseline."
                        ],
                        "paper": {
                            "corpus_id": 26468344,
                            "title": "Curriculum Learning and Minibatch Bucketing in Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "3452584",
                                    "name": "Tom Kocmi"
                                },
                                {
                                    "authorId": "143832874",
                                    "name": "Ondrej Bojar"
                                }
                            ],
                            "year": 2017,
                            "venue": "Recent Advances in Natural Language Processing",
                            "n_citations": 141
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tay et al., 2019)",
                        "snippets": [
                            "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components."
                        ],
                        "paper": {
                            "corpus_id": 166228313,
                            "title": "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "2992833",
                                    "name": "Shuohang Wang"
                                },
                                {
                                    "authorId": "26336902",
                                    "name": "Anh Tuan Luu"
                                },
                                {
                                    "authorId": "49252800",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "22209930",
                                    "name": "Minh C. Phan"
                                },
                                {
                                    "authorId": "2854297",
                                    "name": "Xingdi Yuan"
                                },
                                {
                                    "authorId": "30586030",
                                    "name": "J. Rao"
                                },
                                {
                                    "authorId": "144194328",
                                    "name": "S. Hui"
                                },
                                {
                                    "authorId": "2085709",
                                    "name": "Aston Zhang"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 110
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2021)",
                        "snippets": [
                            "BERT [1] is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia & BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time."
                        ],
                        "paper": {
                            "corpus_id": 233433844,
                            "title": "Reducing BERT Computation by Padding Removal and Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "144142217",
                                    "name": "Wei Zhang"
                                },
                                {
                                    "authorId": "2149192554",
                                    "name": "Wei Wei"
                                },
                                {
                                    "authorId": "2108908267",
                                    "name": "Wen Wang"
                                },
                                {
                                    "authorId": "2152164167",
                                    "name": "Lingling Jin"
                                },
                                {
                                    "authorId": "144332880",
                                    "name": "Zheng Cao"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Symposium on Performance Analysis of Systems and Software",
                            "n_citations": 19
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2018)",
                        "snippets": [
                            "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."
                        ],
                        "paper": {
                            "corpus_id": 51606954,
                            "title": "Curriculum Learning for Natural Answer Generation",
                            "authors": [
                                {
                                    "authorId": "49046540",
                                    "name": "Cao Liu"
                                },
                                {
                                    "authorId": "1954845",
                                    "name": "Shizhu He"
                                },
                                {
                                    "authorId": "2200096",
                                    "name": "Kang Liu"
                                },
                                {
                                    "authorId": "1390572170",
                                    "name": "Jun Zhao"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 85
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)",
                            "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). (Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks",
                            "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."
                        ],
                        "paper": {
                            "corpus_id": 269741199,
                            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
                            "authors": [
                                {
                                    "authorId": "2116659852",
                                    "name": "Joonho Lee"
                                },
                                {
                                    "authorId": "2301016207",
                                    "name": "Jae Oh Woo"
                                },
                                {
                                    "authorId": "2301014929",
                                    "name": "Juree Seok"
                                },
                                {
                                    "authorId": "2301015562",
                                    "name": "Parisa Hassanzadeh"
                                },
                                {
                                    "authorId": "2301015224",
                                    "name": "Wooseok Jang"
                                },
                                {
                                    "authorId": "2301016428",
                                    "name": "JuYoun Son"
                                },
                                {
                                    "authorId": "91748824",
                                    "name": "Sima Didari"
                                },
                                {
                                    "authorId": "2301014892",
                                    "name": "Baruch Gutow"
                                },
                                {
                                    "authorId": "2065513368",
                                    "name": "Heng Hao"
                                },
                                {
                                    "authorId": "2301015932",
                                    "name": "Hankyu Moon"
                                },
                                {
                                    "authorId": "2301166595",
                                    "name": "Wenjun Hu"
                                },
                                {
                                    "authorId": "2301413463",
                                    "name": "Yeong-Dae Kwon"
                                },
                                {
                                    "authorId": "2301133161",
                                    "name": "Taehee Lee"
                                },
                                {
                                    "authorId": "2301015935",
                                    "name": "Seungjai Min"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2
                        },
                        "score": 0.83544921875
                    },
                    {
                        "id": "(Graves et al., 2017)",
                        "snippets": [
                            "We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level."
                        ],
                        "paper": {
                            "corpus_id": 11137059,
                            "title": "Automated Curriculum Learning for Neural Networks",
                            "authors": [
                                {
                                    "authorId": "1753223",
                                    "name": "Alex Graves"
                                },
                                {
                                    "authorId": "1792298",
                                    "name": "Marc G. Bellemare"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "1708654",
                                    "name": "R. Munos"
                                },
                                {
                                    "authorId": "2645384",
                                    "name": "K. Kavukcuoglu"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 530
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zeng et al., 2023)",
                        "snippets": [
                            "Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms."
                        ],
                        "paper": {
                            "corpus_id": 259593617,
                            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
                            "authors": [
                                {
                                    "authorId": "8647899",
                                    "name": "Zijie Zeng"
                                },
                                {
                                    "authorId": "65953975",
                                    "name": "D. Ga\u0161evi\u0107"
                                },
                                {
                                    "authorId": "49901492",
                                    "name": "Guangliang Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 6
                        },
                        "score": 0.826171875
                    },
                    {
                        "id": "(Elgaar et al., 2023)",
                        "snippets": [
                            "We assume there exists a subset of linguistic complexity indices that are most influential to learning an NLP task by a particular model. To identify these indices for each model and NLP task, we derive a weight factor \u03c1 i \u2208 [\u22121, 1] for each linguistic index that quantifies how well the index estimates the true difficulty of data samples to the model, determined by model instantaneous loss against validation data. By learning these weight factors, we obtain precise estimations that shed light on the core linguistic complexity indices that each model needs at different stages of its training to learn an NLP task. In addition, these estimates can be readily used for linguistic curriculum development, e.g., by training models with linguistically easy samples (with respect to the model) and gradually introducing linguistically challenging samples."
                        ],
                        "paper": {
                            "corpus_id": 264819795,
                            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
                            "authors": [
                                {
                                    "authorId": "1659451954",
                                    "name": "Mohamed Elgaar"
                                },
                                {
                                    "authorId": "143656058",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.93115234375
                    },
                    {
                        "id": "(Shi et al., 2016)",
                        "snippets": [
                            "We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques."
                        ],
                        "paper": {
                            "corpus_id": 6954583,
                            "title": "Weakly Supervised Object Localization Using Size Estimates",
                            "authors": [
                                {
                                    "authorId": "8003001",
                                    "name": "Miaojing Shi"
                                },
                                {
                                    "authorId": "143865718",
                                    "name": "V. Ferrari"
                                }
                            ],
                            "year": 2016,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 81
                        },
                        "score": 0
                    },
                    {
                        "id": "(Braun et al., 2016)",
                        "snippets": [
                            "The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called accordion annealing (ACCAN). It uses a multi-stage training schedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are first added and samples at increasing higher SNR values are gradually added up to an SNR value of 50dB. We also use a method called per-epoch noise mixing (PEM) that generates noisy training samples online during training and thus enables dynamically changing the SNR of our training data. Both the ACCAN and the PEM methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus. ACCAN decreases the average word error rate (WER) on the 20dB to \u221210dB SNR range by up to 31.4% when compared to a conventional multi-condition training method."
                        ],
                        "paper": {
                            "corpus_id": 14928979,
                            "title": "A curriculum learning method for improved noise robustness in automatic speech recognition",
                            "authors": [
                                {
                                    "authorId": "143651538",
                                    "name": "Stefan Braun"
                                },
                                {
                                    "authorId": "145243593",
                                    "name": "Daniel Neil"
                                },
                                {
                                    "authorId": "1704961",
                                    "name": "Shih-Chii Liu"
                                }
                            ],
                            "year": 2016,
                            "venue": "European Signal Processing Conference",
                            "n_citations": 84
                        },
                        "score": 0
                    },
                    {
                        "id": "(Higuchi et al., 2021)",
                        "snippets": [
                            "We propose dynamic curriculum learning via data parameters for noise robust keyword spotting. Data parameter learning has recently been introduced for image processing, where weight parameters, so-called data parameters, for target classes and instances are introduced and optimized along with model parameters. The data parameters scale logits and control importance over classes and instances during training, which enables automatic curriculum learning without additional annotations for training data. Similarly, in this paper, we propose using this curriculum learning approach for acoustic modeling, and train an acoustic model on clean and noisy utterances with the data parameters. The proposed approach automatically learns the difficulty of the classes and instances, e.g. due to low speech to noise ratio (SNR), in the gradient descent optimization and performs curriculum learning. This curriculum learning leads to overall improvement of the accuracy of the acoustic model. We evaluate the effectiveness of the proposed approach on a keyword spotting task. Experimental results show 7.7% relative reduction in false reject ratio with the data parameters compared to a base-line model which is simply trained on the multiconditioned dataset."
                        ],
                        "paper": {
                            "corpus_id": 231979234,
                            "title": "Dynamic Curriculum Learning via Data Parameters for Noise Robust Keyword Spotting",
                            "authors": [
                                {
                                    "authorId": "47159622",
                                    "name": "T. Higuchi"
                                },
                                {
                                    "authorId": "46708564",
                                    "name": "S. Saxena"
                                },
                                {
                                    "authorId": "1786479",
                                    "name": "M. Souden"
                                },
                                {
                                    "authorId": "2428613",
                                    "name": "Tien Dung Tran"
                                },
                                {
                                    "authorId": "8749296",
                                    "name": "Masood Delfarah"
                                },
                                {
                                    "authorId": "1839173",
                                    "name": "C. Dhir"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                            "n_citations": 8
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ranaldi et al., 2023)",
                        "snippets": [
                            "Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art."
                        ],
                        "paper": {
                            "corpus_id": 265068175,
                            "title": "Modeling Easiness for Training Transformers with Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2008183566",
                                    "name": "Leonardo Ranaldi"
                                },
                                {
                                    "authorId": "2199247500",
                                    "name": "Giulia Pucci"
                                },
                                {
                                    "authorId": "103839825",
                                    "name": "F. M. Zanzotto"
                                }
                            ],
                            "year": 2023,
                            "venue": "Recent Advances in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al._1, 2021)",
                        "snippets": [
                            "With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages."
                        ],
                        "paper": {
                            "corpus_id": 243766208,
                            "title": "Review and Arrange: Curriculum Learning for Natural Language Understanding",
                            "authors": [
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "1855978",
                                    "name": "Zhendong Mao"
                                },
                                {
                                    "authorId": "1754285124",
                                    "name": "Benfeng Xu"
                                },
                                {
                                    "authorId": "143906199",
                                    "name": "Quan Wang"
                                },
                                {
                                    "authorId": "1699819",
                                    "name": "Yongdong Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                            "n_citations": 14
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2020)",
                        "snippets": [
                            "The key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks.\n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model."
                        ],
                        "paper": {
                            "corpus_id": 220045816,
                            "title": "Curriculum Learning for Natural Language Understanding",
                            "authors": [
                                {
                                    "authorId": "1754285124",
                                    "name": "Benfeng Xu"
                                },
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "1855978",
                                    "name": "Zhendong Mao"
                                },
                                {
                                    "authorId": "143906199",
                                    "name": "Quan Wang"
                                },
                                {
                                    "authorId": "143994657",
                                    "name": "Hongtao Xie"
                                },
                                {
                                    "authorId": "1699819",
                                    "name": "Yongdong Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 206
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Jia et al., 2022)",
                        "snippets": [
                            "Previous work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018)(Kocmi et al., 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020)Kumar et al., 2019;(Shen et al., 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks."
                        ],
                        "paper": {
                            "corpus_id": 253734400,
                            "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
                            "authors": [
                                {
                                    "authorId": "2056108122",
                                    "name": "Qi Jia"
                                },
                                {
                                    "authorId": "5826956",
                                    "name": "Yizhu Liu"
                                },
                                {
                                    "authorId": "2112389755",
                                    "name": "Haifeng Tang"
                                },
                                {
                                    "authorId": "1796651",
                                    "name": "Kenny Q. Zhu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 4
                        },
                        "score": 0.892578125
                    },
                    {
                        "id": "(Lee et al., 2022)",
                        "snippets": [
                            "However, we argue that existing linguisticsinspired criteria, such as length, rarity, and masking ratio of a sequence, do not effectively reflect the nature of MLM, as verified empirically in Table 1.\n\nThe difficulty associated with MLM is significantly affected by the choice of tokens to be masked in the given sequence, rather than by the given sequence itself. For example, given \"The man is a Stanford <mask> student\", we can easily predict that the masked token would be University, whereas given \"The man is a <mask> University student\", it would be relatively difficult to predict the original token due to the insufficient clues in the context. Then, how can we measure the MLM difficulty? MLM can be viewed as predicting masked tokens based on other contextual tokens related to masked tokens. Therefore, if a word is related to many other words and phrases, it is likely that it has several clues in the context that make MLM easier."
                        ],
                        "paper": {
                            "corpus_id": 254685579,
                            "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
                            "authors": [
                                {
                                    "authorId": "2165377996",
                                    "name": "Mingyu Lee"
                                },
                                {
                                    "authorId": "8289290",
                                    "name": "Jun-Hyung Park"
                                },
                                {
                                    "authorId": "2145466424",
                                    "name": "Junho Kim"
                                },
                                {
                                    "authorId": "14694269",
                                    "name": "Kang-Min Kim"
                                },
                                {
                                    "authorId": "66593705",
                                    "name": "SangKeun Lee"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 12
                        },
                        "score": 0.83056640625
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.962890625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Model-Based/Dynamic Difficulty Metrics",
                "tldr": "Dynamic difficulty metrics evolve during training by measuring how the model itself interacts with training examples. These approaches include loss-based metrics, gradient-based measurements, and adaptive techniques that continuously adjust difficulty assessments based on model performance. (18 sources)",
                "text": "\nModel-based or dynamic difficulty metrics represent a more sophisticated approach to curriculum learning, adapting the difficulty assessment as training progresses. Unlike static metrics, these techniques evolve based on the model's changing capabilities and performance. The main categories include:\n\n## Loss-Based Metrics\n- **Training loss**: Self-paced learning (SPL) uses the current training loss to determine example difficulty, with lower loss examples considered easier <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52902973\" paperTitle=\"(Han et al., 2018)\" isShortName></Paper>.\n- **Perplexity scores**: The perplexity of samples under a reference language model can effectively identify difficult examples for pruning or curriculum ordering <Paper corpusId=\"270199394\" paperTitle=\"(Ankner et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266166227\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n- **Instruction-Following Difficulty (IFD)**: This compares model loss with and without instructional context to measure how much an instruction helps in generating responses <Paper corpusId=\"261076515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\n## Gradient-Based Metrics\n- **Gradient magnitude**: The magnitude of loss gradients provides a more informative measure of example difficulty than loss alone, especially in plateau regions of the loss landscape <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n- **Gradient variance**: High variance in gradients across training iterations can indicate challenging examples <Paper corpusId=\"265456857\" paperTitle=\"(Hajimolahoseini et al., 2023)\" isShortName></Paper>.\n\n## Model Behavior Metrics\n- **Misclassification frequency**: The number of times a model incorrectly classifies an example during training can indicate difficulty <Paper corpusId=\"265456857\" paperTitle=\"(Hajimolahoseini et al., 2023)\" isShortName></Paper>.\n- **Forgettability**: Examples that are frequently forgotten over the course of training are considered more difficult <Paper corpusId=\"270215134\" paperTitle=\"(Kessler et al., 2024)\" isShortName></Paper>.\n- **Learning percentage/order**: Examples that show more learning in earlier epochs are considered easier, while those requiring more training time are deemed harder <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>.\n\n## Adaptive Curriculum Approaches\n- **Teacher-student feedback**: External \"teacher\" models evaluate training examples and create curricula for \"student\" models <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper> <Paper corpusId=\"102350936\" paperTitle=\"(Hacohen et al., 2019)\" isShortName></Paper>.\n- **Data Selection Curriculum (DSC) score**: This considers both the difficulty of enhancing model performance through a specific instance and the expected performance on that instance <Paper corpusId=\"266177072\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n- **Pass rate filtering**: Using multiple model evaluations to measure the pass rate on examples and filtering based on predefined thresholds <Paper corpusId=\"277596006\" paperTitle=\"(Bae et al., 2025)\" isShortName></Paper>.\n- **Policy-driven measurements**: These assess difficulty based on the model's real-time performance rather than static metrics <Paper corpusId=\"271855275\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n## Specialized Model-Based Metrics\n- **Representation distance**: Measuring the distance between example representations in the model's embedding space can indicate difficulty <Paper corpusId=\"266166227\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n- **Prediction depth**: The layer at which a k-NN classifier can successfully classify an example serves as a measure of computational difficulty <Paper corpusId=\"265456857\" paperTitle=\"(Hajimolahoseini et al., 2023)\" isShortName></Paper>.\n- **Neural scoring models**: Specialized models trained to ascertain the difficulty level of data instances based on pairwise easy-hard datasets <Paper corpusId=\"274886046\" paperTitle=\"(Jung et al., 2024)\" isShortName></Paper>.\n\nDynamic difficulty metrics offer several advantages over static approaches, particularly their ability to adapt to the model's evolving capabilities during training <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper>. As models improve, what constitutes a \"difficult\" example changes, and these adaptive approaches can account for such shifts <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper>. However, implementing these metrics typically requires additional computational resources for monitoring model performance throughout training <Paper corpusId=\"247292373\" paperTitle=\"(Varshney et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhu et al., 2022)",
                        "snippets": [
                            "Numerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) (Han et al., 2018)(Xu et al., 2021), the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i (Lin et al., 2017) . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. (Huang et al., 2019) designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty."
                        ],
                        "paper": {
                            "corpus_id": 248810776,
                            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
                            "authors": [
                                {
                                    "authorId": "2152350103",
                                    "name": "Weiyao Zhu"
                                },
                                {
                                    "authorId": "2061463125",
                                    "name": "Ou Wu"
                                },
                                {
                                    "authorId": "2165378509",
                                    "name": "Fengguang Su"
                                },
                                {
                                    "authorId": "2165450358",
                                    "name": "Yingjun Deng"
                                }
                            ],
                            "year": 2022,
                            "venue": "ACM Transactions on Knowledge Discovery from Data",
                            "n_citations": 6
                        },
                        "score": 0.8095703125
                    },
                    {
                        "id": "(Han et al., 2018)",
                        "snippets": [
                            "It is challenging for stochastic optimization to handle large-scale sensitive data safely. Duchi et al. recently proposed a private sampling strategy to solve privacy leakage in stochastic optimization. However, this strategy leads to a degeneration in robustness, since this strategy is equal to noise injection on each gradient, which adversely affects updates of the primal variable. To address this challenge, we introduce a robust stochastic optimization under the framework of local privacy, which is called Privacy-pREserving StochasTIc Gradual lEarning (PRESTIGE). PRESTIGE bridges private updates of the primal variable (by private sampling) with gradual curriculum learning (CL). The noise injection leads to similar issue from label noise, but the robust learning process of CL can combat with label noise. Thus, PRESTIGE yields \"private but robust\" updates of the primal variable on the curriculum, that is, a reordered label sequence provided by CL. In theory, we reveal the convergence rate and maximum complexity of PRESTIGE. Empirical results on six datasets show that PRESTIGE achieves a good tradeoff between privacy preservation and robustness over baselines."
                        ],
                        "paper": {
                            "corpus_id": 52902973,
                            "title": "Privacy-Preserving Stochastic Gradual Learning",
                            "authors": [
                                {
                                    "authorId": "2087238859",
                                    "name": "Bo Han"
                                },
                                {
                                    "authorId": "1807998",
                                    "name": "I. Tsang"
                                },
                                {
                                    "authorId": "33285410",
                                    "name": "Xiaokui Xiao"
                                },
                                {
                                    "authorId": "2119322767",
                                    "name": "Ling Chen"
                                },
                                {
                                    "authorId": "35148540",
                                    "name": "S. Fung"
                                },
                                {
                                    "authorId": "23157864",
                                    "name": "C. Yu"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Transactions on Knowledge and Data Engineering",
                            "n_citations": 9
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ankner et al., 2024)",
                        "snippets": [
                            "Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric."
                        ],
                        "paper": {
                            "corpus_id": 270199394,
                            "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
                            "authors": [
                                {
                                    "authorId": "2172356226",
                                    "name": "Zachary Ankner"
                                },
                                {
                                    "authorId": "73066558",
                                    "name": "Cody Blakeney"
                                },
                                {
                                    "authorId": "34824875",
                                    "name": "Kartik K. Sreenivasan"
                                },
                                {
                                    "authorId": "2304322161",
                                    "name": "Max Marion"
                                },
                                {
                                    "authorId": "2028252288",
                                    "name": "Matthew L. Leavitt"
                                },
                                {
                                    "authorId": "1690452",
                                    "name": "Mansheej Paul"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 34
                        },
                        "score": 0.90283203125
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty."
                        ],
                        "paper": {
                            "corpus_id": 266166227,
                            "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
                            "authors": [
                                {
                                    "authorId": "102489044",
                                    "name": "Jianing Zhou"
                                },
                                {
                                    "authorId": "41048608",
                                    "name": "Ziheng Zeng"
                                },
                                {
                                    "authorId": "2008458",
                                    "name": "Hongyu Gong"
                                },
                                {
                                    "authorId": "2263637139",
                                    "name": "Suma Bhat"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Our method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\"."
                        ],
                        "paper": {
                            "corpus_id": 261076515,
                            "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2150655891",
                                    "name": "Ming Li"
                                },
                                {
                                    "authorId": "2144289768",
                                    "name": "Yong Zhang"
                                },
                                {
                                    "authorId": "2111336489",
                                    "name": "Zhitao Li"
                                },
                                {
                                    "authorId": "1391200710",
                                    "name": "Jiuhai Chen"
                                },
                                {
                                    "authorId": "2108451006",
                                    "name": "Lichang Chen"
                                },
                                {
                                    "authorId": "145292435",
                                    "name": "Ning Cheng"
                                },
                                {
                                    "authorId": "66063851",
                                    "name": "Jianzong Wang"
                                },
                                {
                                    "authorId": "2213956781",
                                    "name": "Tianyi Zhou"
                                },
                                {
                                    "authorId": "91353860",
                                    "name": "Jing Xiao"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 211
                        },
                        "score": 0.8603515625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "To address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."
                        ],
                        "paper": {
                            "corpus_id": 268696658,
                            "title": "Symmetric Self-Paced Learning for Domain Generalization",
                            "authors": [
                                {
                                    "authorId": "2110600983",
                                    "name": "Di Zhao"
                                },
                                {
                                    "authorId": "34930533",
                                    "name": "Yun Sing Koh"
                                },
                                {
                                    "authorId": "2276344048",
                                    "name": "Gillian Dobbie"
                                },
                                {
                                    "authorId": "2293565669",
                                    "name": "Hongsheng Hu"
                                },
                                {
                                    "authorId": "2293453267",
                                    "name": "Philippe Fournier-Viger"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 5
                        },
                        "score": 0.89013671875
                    },
                    {
                        "id": "(Hajimolahoseini et al., 2023)",
                        "snippets": [
                            "Other metrics include the number of times a model misclassifies a sample and the high variance of gradients, indicating example difficulty (Dadalto et al., 2023). Prediction depth, the layer at which a k-NN classifier can successfully classify an example, can also serve as a measure of computational difficulty (Ma and Karaman, 2018).\n\nCurriculum learning methods revolve around identifying the optimal order of data samples to accelerate model convergence. Unlike dataset pruning, these methods do not remove data samples but emphasize the importance of the order in which data samples are presented during training. Curriculum learning methods define metrics to quantify the importance of each data sample, and the order of learning is determined based on these scores. These metrics can be crafted manually or generated automatically (Zhang and Pfister, 2021;de Mathelin et al., 2022). While handcrafted metrics may be task-specific, they often outperform automated metrics."
                        ],
                        "paper": {
                            "corpus_id": 265456857,
                            "title": "SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling",
                            "authors": [
                                {
                                    "authorId": "3401413",
                                    "name": "Habib Hajimolahoseini"
                                },
                                {
                                    "authorId": "152432835",
                                    "name": "Omar Mohamed Awad"
                                },
                                {
                                    "authorId": "2238208401",
                                    "name": "Walid Ahmed"
                                },
                                {
                                    "authorId": "2244623536",
                                    "name": "Austin Wen"
                                },
                                {
                                    "authorId": "2265581307",
                                    "name": "Saina Asani"
                                },
                                {
                                    "authorId": "2265582207",
                                    "name": "Mohammad Hassanpour"
                                },
                                {
                                    "authorId": "2265581967",
                                    "name": "Farnoosh Javadi"
                                },
                                {
                                    "authorId": "2268405981",
                                    "name": "Mehdi Ahmadi"
                                },
                                {
                                    "authorId": "71106411",
                                    "name": "Foozhan Ataiefard"
                                },
                                {
                                    "authorId": "2265599969",
                                    "name": "Kangling Liu"
                                },
                                {
                                    "authorId": "2238404323",
                                    "name": "Yang Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.7900390625
                    },
                    {
                        "id": "(Kessler et al., 2024)",
                        "snippets": [
                            "It has also been shown that pruning data according to how easy they are to be forgotten over the course of training -as a measure of difficulty -results in training on less data while maintaining performance (Toneva et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 270215134,
                            "title": "SAVA: Scalable Learning-Agnostic Data Valuation",
                            "authors": [
                                {
                                    "authorId": "2304474212",
                                    "name": "Samuel Kessler"
                                },
                                {
                                    "authorId": "2304718669",
                                    "name": "Tam Le"
                                },
                                {
                                    "authorId": "2304715378",
                                    "name": "Vu Nguyen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 0
                        },
                        "score": 0.80615234375
                    },
                    {
                        "id": "(Mekala et al., 2024)",
                        "snippets": [
                            "Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."
                        ],
                        "paper": {
                            "corpus_id": 267740312,
                            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2284673632",
                                    "name": "Alex Nguyen"
                                },
                                {
                                    "authorId": "2284595153",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 21
                        },
                        "score": 0.9521484375
                    },
                    {
                        "id": "(Mekala et al., 2022)",
                        "snippets": [
                            "Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."
                        ],
                        "paper": {
                            "corpus_id": 249060677,
                            "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2113540861",
                                    "name": "Chengyu Dong"
                                },
                                {
                                    "authorId": "2884976",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 20
                        },
                        "score": 0
                    },
                    {
                        "id": "(Karakasidis et al., 2022)",
                        "snippets": [
                            "To create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch (Hacohen et al., 2019). The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model."
                        ],
                        "paper": {
                            "corpus_id": 251493126,
                            "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
                            "authors": [
                                {
                                    "authorId": "2181120338",
                                    "name": "Georgios Karakasidis"
                                },
                                {
                                    "authorId": "2066645527",
                                    "name": "Tam'as Gr'osz"
                                },
                                {
                                    "authorId": "1719346",
                                    "name": "M. Kurimo"
                                }
                            ],
                            "year": 2022,
                            "venue": "Interspeech",
                            "n_citations": 2
                        },
                        "score": 0.93603515625
                    },
                    {
                        "id": "(Hacohen et al., 2019)",
                        "snippets": [
                            "Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."
                        ],
                        "paper": {
                            "corpus_id": 102350936,
                            "title": "On The Power of Curriculum Learning in Training Deep Networks",
                            "authors": [
                                {
                                    "authorId": "94064232",
                                    "name": "Guy Hacohen"
                                },
                                {
                                    "authorId": "1789171",
                                    "name": "D. Weinshall"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 449
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "In this study, we introduce a novel scoring system called the Data Selection Curriculum (DSC) score, which serves as a measure of learning difficulty. The DSC score considers two essential factors: the difficulty of enhancing the ATS model through a specific instance and the expected performance of the model on that particular instance. Instances on which the current model already exhibits promising performance and stands to benefit significantly from slight adjustments are assigned as a lower learning difficulty. This is because these instances have the potential to contribute more to the final performance."
                        ],
                        "paper": {
                            "corpus_id": 266177072,
                            "title": "Data Selection Curriculum for Abstractive Text Summarization",
                            "authors": [
                                {
                                    "authorId": "48904791",
                                    "name": "Shichao Sun"
                                },
                                {
                                    "authorId": "2273661632",
                                    "name": "Ruifeng Yuan"
                                },
                                {
                                    "authorId": "2237429230",
                                    "name": "Jianfei He"
                                },
                                {
                                    "authorId": "2314396",
                                    "name": "Ziqiang Cao"
                                },
                                {
                                    "authorId": "2237591981",
                                    "name": "Wenjie Li"
                                },
                                {
                                    "authorId": "2237752880",
                                    "name": "Xiaohua Jia"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.7890625
                    },
                    {
                        "id": "(Bae et al., 2025)",
                        "snippets": [
                            "Online filtering methods further explore this idea by dynamically adjusting the training data to match the current ability of the model (Cui et al., 2025). However, while previous work demonstrate the empirical effectiveness of such techniques, they often lack a detailed analysis of why or when certain difficulty distributions yield better learning outcomes. With G rollouts for each prompt x, we measure the pass rate p(x) as the average accuracy and filter them by predefined thresholds: e.g., 0.25 \u2264 p(x) \u2264 0.75 in this case."
                        ],
                        "paper": {
                            "corpus_id": 277596006,
                            "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
                            "authors": [
                                {
                                    "authorId": "152846184",
                                    "name": "Sanghwan Bae"
                                },
                                {
                                    "authorId": "2354048562",
                                    "name": "Jiwoo Hong"
                                },
                                {
                                    "authorId": "2294848361",
                                    "name": "M. Lee"
                                },
                                {
                                    "authorId": "2354326988",
                                    "name": "Hanbyul Kim"
                                },
                                {
                                    "authorId": "2353956752",
                                    "name": "JeongYeon Nam"
                                },
                                {
                                    "authorId": "10469987",
                                    "name": "Donghyun Kwak"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.921875
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Policy-driven Difficulty Measurement: we begin by measuring the difficulty of data based on the model's real-time performance, transitioning from static, predefined metrics to more dynamic and adaptable ones."
                        ],
                        "paper": {
                            "corpus_id": 271855275,
                            "title": "P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for Optimizing LLM Training",
                            "authors": [
                                {
                                    "authorId": "2224854311",
                                    "name": "Yingxuan Yang"
                                },
                                {
                                    "authorId": "2315948077",
                                    "name": "Huayi Wang"
                                },
                                {
                                    "authorId": "2111875607",
                                    "name": "Muning Wen"
                                },
                                {
                                    "authorId": "2315940339",
                                    "name": "Weinan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94580078125
                    },
                    {
                        "id": "(Jung et al., 2024)",
                        "snippets": [
                            "In recent years, data-driven and machine learning-based natural language processing (NLP) technologies have effectively addressed various challenges. To further enhance the performance of NLP models, it is crucial to understand the types of data that a model can handle well and those it struggles with. This study introduces a method to discern which types of data can be effectively processed by given neural network-based models and which pose difficulties. We define the criteria for hard-to-solve data, construct a pairwise easy-hard dataset, and propose a neural scoring model. This model ascertains the difficulty level of each data instance. To utilize the proposed difficulty level as an application, we employed curriculum learning. The experimental results show that our methodology can effectively distinguish between easy and hard data, and performance improves when applying the curriculum learning approach."
                        ],
                        "paper": {
                            "corpus_id": 274886046,
                            "title": "Assessing and Scoring Difficulty of Hard-to-Solve Data in Summarization Tasks",
                            "authors": [
                                {
                                    "authorId": "2159741574",
                                    "name": "Jeesu Jung"
                                },
                                {
                                    "authorId": "2065447556",
                                    "name": "H. Seo"
                                },
                                {
                                    "authorId": "2210707252",
                                    "name": "Hyuk Namgoong"
                                },
                                {
                                    "authorId": "2313048573",
                                    "name": "Sangkeun Jung"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Jarca et al., 2024)",
                        "snippets": [
                            "The aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input (Jim\u00e9nez-S\u00e1nchez et al., 2019), which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model."
                        ],
                        "paper": {
                            "corpus_id": 271051051,
                            "title": "CBM: Curriculum by Masking",
                            "authors": [
                                {
                                    "authorId": "2310341007",
                                    "name": "Andrei Jarca"
                                },
                                {
                                    "authorId": "2154573729",
                                    "name": "Florinel-Alin Croitoru"
                                },
                                {
                                    "authorId": "2249763264",
                                    "name": "R. Ionescu"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.83203125
                    },
                    {
                        "id": "(Varshney et al., 2022)",
                        "snippets": [
                            "We use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1)."
                        ],
                        "paper": {
                            "corpus_id": 247292373,
                            "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
                            "authors": [
                                {
                                    "authorId": "2067056655",
                                    "name": "Neeraj Varshney"
                                },
                                {
                                    "authorId": "1817207",
                                    "name": "Swaroop Mishra"
                                },
                                {
                                    "authorId": "2064619864",
                                    "name": "Chitta Baral"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 19
                        },
                        "score": 0.8203125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Implementation Strategies for Curriculum Learning",
                "tldr": "Implementing curriculum learning requires choosing appropriate difficulty metrics and designing effective scheduling mechanisms to gradually expose models to more complex data. Various approaches range from static predefined curricula to dynamic adaptive systems that adjust training patterns based on model feedback. (6 sources)",
                "text": "\nCurriculum learning implementation can be structured in several ways, depending on how training examples are scheduled and presented to the model. The fundamental components of any curriculum learning implementation include a difficulty measurement technique and a strategy for scheduling examples during training.\n\n## Curriculum Scheduling Approaches\n\n### Static Scheduling\nStatic or predefined curricula establish a fixed ordering of training examples before training begins. These include:\n\n- **Staged training**: Training proceeds through distinct phases of increasing difficulty, with each phase containing examples within a specific difficulty range <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper>.\n- **Augmentation-based scheduling**: Starting with original data before gradually increasing the proportion of augmented (and presumably more difficult) examples <Paper corpusId=\"244908620\" paperTitle=\"(Wang et al._1, 2021)\" isShortName></Paper>.\n- **Class-based scheduling**: Organizing training by class difficulty, allowing the model to learn easier classes before progressing to more difficult ones <Paper corpusId=\"244908620\" paperTitle=\"(Wang et al._1, 2021)\" isShortName></Paper>.\n\n### Dynamic Scheduling\nDynamic curricula adjust as training progresses, based on model feedback:\n\n- **Self-paced learning (SPL)**: Automatically orders data according to training loss, enabling examples to be introduced based on the model's current capabilities <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n- **Teacher-student feedback**: Uses teacher models to provide curricula guidance to student models, with adjustments based on student performance <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n- **Progressive overload curriculum learning (POCL)**: Incrementally introduces subsets of training samples from easy to hard at fixed intervals, applying loss functions with progressively rising temperatures <Paper corpusId=\"279243528\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\n## Pacing Functions\n\nThe rate at which more difficult examples are introduced is controlled by pacing functions:\n\n- **Linear pacing**: Gradually increases the difficulty threshold in a linear fashion.\n- **Exponential pacing**: Accelerates the introduction of difficult examples as training progresses.\n- **Step-wise pacing**: Increases difficulty in discrete jumps at predetermined intervals.\n\nRecent research has challenged conventional wisdom about curriculum pacing, suggesting that the optimal curricula may not always follow monotonic patterns. Non-monotonic curricula, which intermix examples of varying difficulty rather than strictly progressing from easy to hard, have shown superior performance in some cases <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>.\n\n## Practical Considerations\n\nFor effective curriculum learning implementation, several factors should be considered:\n\n- **Distribution of difficulty**: A wider range of difficulty in training data tends to yield greater improvements from curriculum learning <Paper corpusId=\"232233485\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper>.\n- **Computational overhead**: Dynamic curricula typically require additional computational resources to monitor model performance and adjust the curriculum accordingly.\n- **Generalization**: Curricula discovered for smaller datasets and models have shown promising performance when transferred to larger datasets and models <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>.\n- **Temperature parameters**: Parameters that control the distribution of example difficulty can significantly impact curriculum effectiveness <Paper corpusId=\"232233485\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper>.\n\nThe evolution of curriculum learning implementations has moved from rigid predefined approaches toward more flexible and general automatic methods. These include reinforcement learning-based techniques, Bayesian optimization approaches, meta-learning strategies, and adversarial learning methods <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This shift reflects the growing understanding that effective curricula should adapt to the specific learning dynamics of the model being trained.",
                "citations": [
                    {
                        "id": "(Karakasidis et al., 2022)",
                        "snippets": [
                            "To create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch (Hacohen et al., 2019). The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model."
                        ],
                        "paper": {
                            "corpus_id": 251493126,
                            "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
                            "authors": [
                                {
                                    "authorId": "2181120338",
                                    "name": "Georgios Karakasidis"
                                },
                                {
                                    "authorId": "2066645527",
                                    "name": "Tam'as Gr'osz"
                                },
                                {
                                    "authorId": "1719346",
                                    "name": "M. Kurimo"
                                }
                            ],
                            "year": 2022,
                            "venue": "Interspeech",
                            "n_citations": 2
                        },
                        "score": 0.93603515625
                    },
                    {
                        "id": "(Wang et al._1, 2021)",
                        "snippets": [
                            "The key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3.\n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37]53], and they should be more difficult to learn as verified in Section 2.3.\n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process.\n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy.\n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks."
                        ],
                        "paper": {
                            "corpus_id": 244908620,
                            "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding",
                            "authors": [
                                {
                                    "authorId": "1455165108",
                                    "name": "Deze Wang"
                                },
                                {
                                    "authorId": "10414205",
                                    "name": "Zhouyang Jia"
                                },
                                {
                                    "authorId": "50342128",
                                    "name": "Shanshan Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Yue Yu"
                                },
                                {
                                    "authorId": "2153657836",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2114050396",
                                    "name": "Wei Dong"
                                },
                                {
                                    "authorId": "144078016",
                                    "name": "Xiangke Liao"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Software Engineering",
                            "n_citations": 80
                        },
                        "score": 0.8408203125
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "In the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning."
                        ],
                        "paper": {
                            "corpus_id": 269762685,
                            "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2256599610",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "2261888564",
                                    "name": "Yuwei Zhou"
                                },
                                {
                                    "authorId": "2191043236",
                                    "name": "Hong Chen"
                                },
                                {
                                    "authorId": "2156154955",
                                    "name": "Wenwu Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Web Conference",
                            "n_citations": 4
                        },
                        "score": 0.85546875
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of\"progressive overload\"(POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures."
                        ],
                        "paper": {
                            "corpus_id": 279243528,
                            "title": "Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework",
                            "authors": [
                                {
                                    "authorId": "2366142077",
                                    "name": "Lingyuan Liu"
                                },
                                {
                                    "authorId": "2366090070",
                                    "name": "Mengxiang Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Elgaar et al._1, 2023)",
                        "snippets": [
                            "Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively."
                        ],
                        "paper": {
                            "corpus_id": 259370648,
                            "title": "HuCurl: Human-induced Curriculum Discovery",
                            "authors": [
                                {
                                    "authorId": "1659451954",
                                    "name": "Mohamed Elgaar"
                                },
                                {
                                    "authorId": "143656058",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.8955078125
                    },
                    {
                        "id": "(Wei et al., 2021)",
                        "snippets": [
                            "Effective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect a greater improvement from curriculum learning."
                        ],
                        "paper": {
                            "corpus_id": 232233485,
                            "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "144026731",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "2150608874",
                                    "name": "Chengyu Huang"
                                },
                                {
                                    "authorId": "1918441",
                                    "name": "Soroush Vosoughi"
                                },
                                {
                                    "authorId": "2153510147",
                                    "name": "Yu Cheng"
                                },
                                {
                                    "authorId": "1845277073",
                                    "name": "Shiqi Xu"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 64
                        },
                        "score": 0.8037109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Impact on Training Outcomes",
                "tldr": "Curriculum learning strategies can significantly improve convergence speed, final model performance, and training efficiency across various NLP tasks. The specific benefits depend on the chosen difficulty metrics, the distribution of difficulty in training data, and the implementation approach. (13 sources)",
                "text": "\nCurriculum learning (CL) has demonstrated substantial positive impacts on training outcomes across a range of natural language processing tasks. One of the most consistently reported benefits is improved convergence speed, with studies showing that well-designed curricula can reduce training time by up to 70% <Paper corpusId=\"85498775\" paperTitle=\"(Platanios et al., 2019)\" isShortName></Paper>. This acceleration occurs because curriculum approaches help models avoid getting stuck in poor local optima early in training, leading to more efficient learning trajectories.\n\nBeyond faster convergence, curriculum learning has been shown to improve model performance on various benchmarks. For machine translation tasks, multiple studies have demonstrated that curriculum strategies contribute to both convergence speed and final model performance <Paper corpusId=\"234338422\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"155089817\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper> <Paper corpusId=\"219260306\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>. Some implementations have achieved performance improvements of up to 2.2 BLEU points <Paper corpusId=\"85498775\" paperTitle=\"(Platanios et al., 2019)\" isShortName></Paper>, while others have reported improvements of 1.17 to 1.56 BLEU points along with training speedups of 2.22x to 3.33x <Paper corpusId=\"219260306\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>.\n\nThe magnitude of performance improvement from curriculum learning appears to be directly related to the distribution of difficulty within the training data. When training data exhibits a wider range of difficulty, the potential benefits of curriculum learning increase substantially <Paper corpusId=\"232233485\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper>. This is especially relevant for augmentation-based approaches, where temperature parameters control the perturbation level of examples and thereby affect the distribution of difficulty.\n\nNotably, recent research has challenged conventional wisdom about the optimal structure of curricula. While traditional approaches have favored monotonic progressions (either easy-to-hard or hard-to-easy), studies have found that non-monotonic curricula\u2014which intermix examples of varying difficulty\u2014often outperform strictly monotonic approaches <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. This suggests that the relationship between curriculum structure and training outcomes is more complex than initially theorized.\n\nThe specific difficulty measurement technique used can significantly influence training outcomes. In automated text scoring systems, curriculum learning has shown improvements of up to 4.5%, with difficulty measures based on grammatical error counts often outperforming other metrics <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>. Additionally, curricula discovered for smaller datasets and models have demonstrated good transferability to larger datasets and models <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>, suggesting that effective curriculum strategies may be somewhat generalizable across scale.\n\nBeyond direct performance improvements, curriculum learning approaches have shown beneficial effects as regularization methods. For example, implementing a curriculum based on sequence length has been shown to reduce gradient variance, enabling training of auto-regressive models with significantly larger batch sizes and learning rates without introducing training instability <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.\n\nMore sophisticated adaptive approaches, such as those used in knowledge distillation, can further enhance training outcomes by dynamically adjusting the emphasis on different samples based on the model's current performance <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. These methods ensure that the training process remains responsive to the evolving capabilities of the model, leading to more efficient and effective outcomes.\n\nThe benefits of curriculum learning extend beyond standard training scenarios. Research has shown that model-based difficulty metrics used in curriculum learning can also improve selective prediction ability (helping models determine when not to answer), enhance understanding of training dynamics, and enable more efficient evaluations <Paper corpusId=\"248965007\" paperTitle=\"(Varshney et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"247292373\" paperTitle=\"(Varshney et al., 2022)\" isShortName></Paper> <Paper corpusId=\"221856637\" paperTitle=\"(Swayamdipta et al., 2020)\" isShortName></Paper>.\n\nDespite these benefits, traditional curriculum approaches based on coarse-grained categories (such as question length) or simplistic LLM-based scoring methods may not capture the nuanced reasoning demands of complex tasks <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>. This highlights the need for more sophisticated, fine-grained adaptive frameworks that can effectively bridge the gap between data difficulty and model capability as training progresses.",
                "citations": [
                    {
                        "id": "(Platanios et al., 2019)",
                        "snippets": [
                            "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU."
                        ],
                        "paper": {
                            "corpus_id": 85498775,
                            "title": "Competence-based Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "144888672",
                                    "name": "Emmanouil Antonios Platanios"
                                },
                                {
                                    "authorId": "3397269",
                                    "name": "Otilia Stretcu"
                                },
                                {
                                    "authorId": "1700325",
                                    "name": "Graham Neubig"
                                },
                                {
                                    "authorId": "1719347",
                                    "name": "B. P\u00f3czos"
                                },
                                {
                                    "authorId": "40975594",
                                    "name": "Tom Michael Mitchell"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 344
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "In the scenario of neural machine translation (NMT), empirical studies have shown that CL strategy contributes to convergence speed and model performance (Zhang et al., 2018;(Platanios et al., 2019)(Zhang et al., 2019)(Liu et al., 2020)(Zhan et al., 2021)(Ruiter et al., 2020). In these approaches, the learning difficulty of each training example is measured by different difficulty criteria. Early approaches depend on prior knowledge from various sources, including manually crafted features like sentence length and word rarity (Kocmi et al., 2017). The drawback lies in the fact that human understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples, to approximate the perspective of an NMT model. For example, (Platanios et al., 2019) turns discrete numerical difficulty scores into relative probabilities and then construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b)."
                        ],
                        "paper": {
                            "corpus_id": 234338422,
                            "title": "Self-Guided Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2144764522",
                                    "name": "Lei Zhou"
                                },
                                {
                                    "authorId": "46573238",
                                    "name": "Liang Ding"
                                },
                                {
                                    "authorId": "1800354",
                                    "name": "Kevin Duh"
                                },
                                {
                                    "authorId": "2293543",
                                    "name": "Ryohei Sasano"
                                },
                                {
                                    "authorId": "2874038",
                                    "name": "Koichi Takeda"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Workshop on Spoken Language Translation",
                            "n_citations": 17
                        },
                        "score": 0.83203125
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs."
                        ],
                        "paper": {
                            "corpus_id": 155089817,
                            "title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "49469742",
                                    "name": "Xuan Zhang"
                                },
                                {
                                    "authorId": "38581144",
                                    "name": "Pamela Shapiro"
                                },
                                {
                                    "authorId": "48387892",
                                    "name": "Manish Kumar"
                                },
                                {
                                    "authorId": "145324163",
                                    "name": "Paul McNamee"
                                },
                                {
                                    "authorId": "2954727",
                                    "name": "Marine Carpuat"
                                },
                                {
                                    "authorId": "1800354",
                                    "name": "Kevin Duh"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 124
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2020)",
                        "snippets": [
                            "A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT\u201914 English-German and WMT\u201917 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x)."
                        ],
                        "paper": {
                            "corpus_id": 219260306,
                            "title": "Norm-Based Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "1390611971",
                                    "name": "Xuebo Liu"
                                },
                                {
                                    "authorId": "1736553736",
                                    "name": "Houtim Lai"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 120
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wei et al., 2021)",
                        "snippets": [
                            "Effective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect a greater improvement from curriculum learning."
                        ],
                        "paper": {
                            "corpus_id": 232233485,
                            "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "144026731",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "2150608874",
                                    "name": "Chengyu Huang"
                                },
                                {
                                    "authorId": "1918441",
                                    "name": "Soroush Vosoughi"
                                },
                                {
                                    "authorId": "2153510147",
                                    "name": "Yu Cheng"
                                },
                                {
                                    "authorId": "1845277073",
                                    "name": "Shiqi Xu"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 64
                        },
                        "score": 0.8037109375
                    },
                    {
                        "id": "(Elgaar et al._1, 2023)",
                        "snippets": [
                            "Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively."
                        ],
                        "paper": {
                            "corpus_id": 259370648,
                            "title": "HuCurl: Human-induced Curriculum Discovery",
                            "authors": [
                                {
                                    "authorId": "1659451954",
                                    "name": "Mohamed Elgaar"
                                },
                                {
                                    "authorId": "143656058",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.8955078125
                    },
                    {
                        "id": "(Zeng et al., 2023)",
                        "snippets": [
                            "Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms."
                        ],
                        "paper": {
                            "corpus_id": 259593617,
                            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
                            "authors": [
                                {
                                    "authorId": "8647899",
                                    "name": "Zijie Zeng"
                                },
                                {
                                    "authorId": "65953975",
                                    "name": "D. Ga\u0161evi\u0107"
                                },
                                {
                                    "authorId": "49901492",
                                    "name": "Guangliang Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 6
                        },
                        "score": 0.826171875
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)",
                            "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). (Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks",
                            "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."
                        ],
                        "paper": {
                            "corpus_id": 269741199,
                            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
                            "authors": [
                                {
                                    "authorId": "2116659852",
                                    "name": "Joonho Lee"
                                },
                                {
                                    "authorId": "2301016207",
                                    "name": "Jae Oh Woo"
                                },
                                {
                                    "authorId": "2301014929",
                                    "name": "Juree Seok"
                                },
                                {
                                    "authorId": "2301015562",
                                    "name": "Parisa Hassanzadeh"
                                },
                                {
                                    "authorId": "2301015224",
                                    "name": "Wooseok Jang"
                                },
                                {
                                    "authorId": "2301016428",
                                    "name": "JuYoun Son"
                                },
                                {
                                    "authorId": "91748824",
                                    "name": "Sima Didari"
                                },
                                {
                                    "authorId": "2301014892",
                                    "name": "Baruch Gutow"
                                },
                                {
                                    "authorId": "2065513368",
                                    "name": "Heng Hao"
                                },
                                {
                                    "authorId": "2301015932",
                                    "name": "Hankyu Moon"
                                },
                                {
                                    "authorId": "2301166595",
                                    "name": "Wenjun Hu"
                                },
                                {
                                    "authorId": "2301413463",
                                    "name": "Yeong-Dae Kwon"
                                },
                                {
                                    "authorId": "2301133161",
                                    "name": "Taehee Lee"
                                },
                                {
                                    "authorId": "2301015935",
                                    "name": "Seungjai Min"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2
                        },
                        "score": 0.83544921875
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes."
                        ],
                        "paper": {
                            "corpus_id": 277398866,
                            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
                            "authors": [
                                {
                                    "authorId": "2326763710",
                                    "name": "Dingzong Zhang"
                                },
                                {
                                    "authorId": "2345374431",
                                    "name": "Devi Listiyani"
                                },
                                {
                                    "authorId": "2302811064",
                                    "name": "Priyanka Singh"
                                },
                                {
                                    "authorId": "2309099465",
                                    "name": "Manoranjan Mohanty"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.9208984375
                    },
                    {
                        "id": "(Varshney et al._1, 2022)",
                        "snippets": [
                            "In addition to arranging the training instances into a learning curriculum, computing difficulty scores using model-based techniques has shown its benefits in several other areas, such as improving selective prediction ability (Varshney et al., 2020), under-standing training dynamics (Swayamdipta et al., 2020), and efficient evaluations (Varshney et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 248965007,
                            "title": "Let the Model Decide its Curriculum for Multitask Learning",
                            "authors": [
                                {
                                    "authorId": "2067056655",
                                    "name": "Neeraj Varshney"
                                },
                                {
                                    "authorId": "1817207",
                                    "name": "Swaroop Mishra"
                                },
                                {
                                    "authorId": "2064619864",
                                    "name": "Chitta Baral"
                                }
                            ],
                            "year": 2022,
                            "venue": "Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing",
                            "n_citations": 8
                        },
                        "score": 0.79638671875
                    },
                    {
                        "id": "(Varshney et al., 2022)",
                        "snippets": [
                            "We use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1)."
                        ],
                        "paper": {
                            "corpus_id": 247292373,
                            "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
                            "authors": [
                                {
                                    "authorId": "2067056655",
                                    "name": "Neeraj Varshney"
                                },
                                {
                                    "authorId": "1817207",
                                    "name": "Swaroop Mishra"
                                },
                                {
                                    "authorId": "2064619864",
                                    "name": "Chitta Baral"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 19
                        },
                        "score": 0.8203125
                    },
                    {
                        "id": "(Swayamdipta et al., 2020)",
                        "snippets": [
                            "Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization."
                        ],
                        "paper": {
                            "corpus_id": 221856637,
                            "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics",
                            "authors": [
                                {
                                    "authorId": "2705113",
                                    "name": "Swabha Swayamdipta"
                                },
                                {
                                    "authorId": "2279023325",
                                    "name": "Roy Schwartz"
                                },
                                {
                                    "authorId": "35219984",
                                    "name": "Nicholas Lourie"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 452
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2025)",
                        "snippets": [
                            "Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;(Lee et al., 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;(Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability."
                        ],
                        "paper": {
                            "corpus_id": 277824032,
                            "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
                            "authors": [
                                {
                                    "authorId": "2356617563",
                                    "name": "Qianjin Yu"
                                },
                                {
                                    "authorId": "2355802308",
                                    "name": "Keyu Wu"
                                },
                                {
                                    "authorId": "2355823766",
                                    "name": "Zihan Chen"
                                },
                                {
                                    "authorId": "2355786652",
                                    "name": "Chushu Zhang"
                                },
                                {
                                    "authorId": "2355643664",
                                    "name": "Manlin Mei"
                                },
                                {
                                    "authorId": "2356799678",
                                    "name": "Lingjun Huang"
                                },
                                {
                                    "authorId": "2355645282",
                                    "name": "Fang Tan"
                                },
                                {
                                    "authorId": "2355785017",
                                    "name": "Yongsheng Du"
                                },
                                {
                                    "authorId": "2356331597",
                                    "name": "Kunlin Liu"
                                },
                                {
                                    "authorId": "2355782385",
                                    "name": "Yurui Zhu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.8232421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges and Limitations",
                "tldr": "Despite its effectiveness, curriculum learning faces significant challenges including difficulty defining accurate metrics, reliance on human expertise, and implementing dynamic approaches that adapt to evolving model capabilities. (8 sources)",
                "text": "\nImplementing effective curriculum learning for large language models presents several notable challenges and limitations. One of the fundamental difficulties lies in defining appropriate metrics for measuring example difficulty. Traditional approaches often rely on predefined heuristics such as text length or word rarity, which may not accurately reflect the true complexity of training examples from the model's perspective <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. These simplistic metrics frequently fail to capture the nuanced reasoning demands of complex tasks, limiting their effectiveness across different domains <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nMany curriculum learning approaches suffer from poor generalizability across different tasks and domains. Strategies that utilize human-crafted rules based on linguistic features or task-specific observations are highly dependent on the target dataset and often fail to transfer effectively to different applications <Paper corpusId=\"253734400\" paperTitle=\"(Jia et al., 2022)\" isShortName></Paper> <Paper corpusId=\"51606954\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper>. This task-specificity creates significant barriers to developing universal curriculum learning approaches.\n\nAnother critical limitation is the dependence on human expert input for designing effective curricula. Traditional curriculum strategies, while proven effective, often lack practicality due to their reliance on domain expertise that may not always be available <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper> <Paper corpusId=\"204539326\" paperTitle=\"(Jimenez-Sanchez et al., 2019)\" isShortName></Paper>. This dependency on human judgment introduces scalability challenges, especially when applying curriculum learning to novel domains or extremely large datasets.\n\nStatic curricula present another significant limitation, as they remain fixed throughout the training process without adapting to the model's evolving capabilities. Predefined curricula cannot account for the continuously changing perception of difficulty as training progresses <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper>. This inflexibility may result in suboptimal training schedules that fail to align with the model's actual learning trajectory.\n\nCoarse-grained curriculum designs often oversimplify difficulty levels, such as categorizing questions merely by length, while LLM-based scoring methods struggle to capture the nuanced demands of complex reasoning tasks <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>. These approaches lack the granularity needed to effectively bridge the gap between data difficulty and model capability as training progresses.\n\nEven model-based curriculum approaches face challenges in accurately measuring difficulty. Models trained on previous epochs or external data may not perfectly reflect the current model's perception of difficulty <Paper corpusId=\"253734400\" paperTitle=\"(Jia et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>. The dynamic nature of perceived difficulty as the model learns makes it challenging to design curricula that remain optimal throughout the training process.\n\nFor real-world applications, the computational overhead of dynamic curriculum approaches can be prohibitive. Self-paced learning and other adaptive methods require continuous assessment of model performance, which adds significant computational cost, especially for large-scale language models <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper> .\n\nCross-domain applicability represents another persistent challenge. While some researchers have proposed more generalizable approaches such as the Cross Review method for evaluating difficulty, many curriculum learning strategies remain highly specialized to particular tasks or domains <Paper corpusId=\"220045816\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. This specialization makes it difficult to transfer successful curriculum strategies between different applications without significant modifications.\n\nThese limitations highlight the need for more sophisticated, fine-grained adaptive frameworks that can effectively measure example difficulty from the model's perspective while maintaining practical implementation requirements. Future research directions may focus on developing more generalizable difficulty metrics and curriculum strategies that can adapt dynamically to the model's evolving capabilities across diverse tasks and domains.",
                "citations": [
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Yu et al., 2025)",
                        "snippets": [
                            "Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;(Lee et al., 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;(Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability."
                        ],
                        "paper": {
                            "corpus_id": 277824032,
                            "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
                            "authors": [
                                {
                                    "authorId": "2356617563",
                                    "name": "Qianjin Yu"
                                },
                                {
                                    "authorId": "2355802308",
                                    "name": "Keyu Wu"
                                },
                                {
                                    "authorId": "2355823766",
                                    "name": "Zihan Chen"
                                },
                                {
                                    "authorId": "2355786652",
                                    "name": "Chushu Zhang"
                                },
                                {
                                    "authorId": "2355643664",
                                    "name": "Manlin Mei"
                                },
                                {
                                    "authorId": "2356799678",
                                    "name": "Lingjun Huang"
                                },
                                {
                                    "authorId": "2355645282",
                                    "name": "Fang Tan"
                                },
                                {
                                    "authorId": "2355785017",
                                    "name": "Yongsheng Du"
                                },
                                {
                                    "authorId": "2356331597",
                                    "name": "Kunlin Liu"
                                },
                                {
                                    "authorId": "2355782385",
                                    "name": "Yurui Zhu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.8232421875
                    },
                    {
                        "id": "(Jia et al., 2022)",
                        "snippets": [
                            "Previous work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018)(Kocmi et al., 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020)Kumar et al., 2019;(Shen et al., 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks."
                        ],
                        "paper": {
                            "corpus_id": 253734400,
                            "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
                            "authors": [
                                {
                                    "authorId": "2056108122",
                                    "name": "Qi Jia"
                                },
                                {
                                    "authorId": "5826956",
                                    "name": "Yizhu Liu"
                                },
                                {
                                    "authorId": "2112389755",
                                    "name": "Haifeng Tang"
                                },
                                {
                                    "authorId": "1796651",
                                    "name": "Kenny Q. Zhu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 4
                        },
                        "score": 0.892578125
                    },
                    {
                        "id": "(Liu et al., 2018)",
                        "snippets": [
                            "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."
                        ],
                        "paper": {
                            "corpus_id": 51606954,
                            "title": "Curriculum Learning for Natural Answer Generation",
                            "authors": [
                                {
                                    "authorId": "49046540",
                                    "name": "Cao Liu"
                                },
                                {
                                    "authorId": "1954845",
                                    "name": "Shizhu He"
                                },
                                {
                                    "authorId": "2200096",
                                    "name": "Kang Liu"
                                },
                                {
                                    "authorId": "1390572170",
                                    "name": "Jun Zhao"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 85
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jarca et al., 2024)",
                        "snippets": [
                            "The aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input (Jim\u00e9nez-S\u00e1nchez et al., 2019), which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model."
                        ],
                        "paper": {
                            "corpus_id": 271051051,
                            "title": "CBM: Curriculum by Masking",
                            "authors": [
                                {
                                    "authorId": "2310341007",
                                    "name": "Andrei Jarca"
                                },
                                {
                                    "authorId": "2154573729",
                                    "name": "Florinel-Alin Croitoru"
                                },
                                {
                                    "authorId": "2249763264",
                                    "name": "R. Ionescu"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.83203125
                    },
                    {
                        "id": "(Jimenez-Sanchez et al., 2019)",
                        "snippets": [
                            "Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning \"easy\" examples and move towards \"hard\", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons."
                        ],
                        "paper": {
                            "corpus_id": 204539326,
                            "title": "Medical-based Deep Curriculum Learning for Improved Fracture Classification",
                            "authors": [
                                {
                                    "authorId": "1404288037",
                                    "name": "Amelia Jim\u00e9nez-S\u00e1nchez"
                                },
                                {
                                    "authorId": "1704770",
                                    "name": "D. Mateus"
                                },
                                {
                                    "authorId": "79884102",
                                    "name": "S. Kirchhoff"
                                },
                                {
                                    "authorId": "4858301",
                                    "name": "C. Kirchhoff"
                                },
                                {
                                    "authorId": "5140452",
                                    "name": "P. Biberthaler"
                                },
                                {
                                    "authorId": "145587209",
                                    "name": "N. Navab"
                                },
                                {
                                    "authorId": "49463953",
                                    "name": "M. Ballester"
                                },
                                {
                                    "authorId": "50186507",
                                    "name": "G. Piella"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                            "n_citations": 48
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2020)",
                        "snippets": [
                            "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                        ],
                        "paper": {
                            "corpus_id": 220047761,
                            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2110349143",
                                    "name": "Yikai Zhou"
                                },
                                {
                                    "authorId": "21299583",
                                    "name": "Baosong Yang"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "153379180",
                                    "name": "Yu Wan"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2020)",
                        "snippets": [
                            "The key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks.\n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model."
                        ],
                        "paper": {
                            "corpus_id": 220045816,
                            "title": "Curriculum Learning for Natural Language Understanding",
                            "authors": [
                                {
                                    "authorId": "1754285124",
                                    "name": "Benfeng Xu"
                                },
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "1855978",
                                    "name": "Zhendong Mao"
                                },
                                {
                                    "authorId": "143906199",
                                    "name": "Quan Wang"
                                },
                                {
                                    "authorId": "143994657",
                                    "name": "Hongtao Xie"
                                },
                                {
                                    "authorId": "1699819",
                                    "name": "Yongdong Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 206
                        },
                        "score": 0.9140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Advanced and Recent Approaches",
                "tldr": "Recent innovations in curriculum learning focus on non-monotonic training sequences, gradient-based difficulty assessments, and adaptive frameworks that respond to model evolution. These approaches overcome limitations of traditional methods by providing more fine-grained, dynamic difficulty measurements that better align with actual learning trajectories. (7 sources)",
                "text": "\nRecent research in curriculum learning has challenged conventional wisdom and introduced several innovative approaches that address key limitations of traditional methods. One significant advancement is the discovery that optimal curricula may not always follow the strictly monotonic easy-to-hard or hard-to-easy progression that dominated earlier work. Studies have shown that non-monotonic curricula\u2014which strategically intermix examples of varying difficulty\u2014often outperform traditional monotonic approaches <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. This finding suggests that the relationship between curriculum structure and training effectiveness is more complex than previously understood.\n\nThe transferability of curriculum strategies has also seen important breakthroughs. Research has demonstrated that curricula discovered for smaller datasets and models can perform well when applied to larger datasets and models, suggesting that effective curriculum strategies may be somewhat generalizable across scale <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. This scalability has significant implications for efficiently implementing curriculum learning in increasingly large language models.\n\nNovel difficulty measurement techniques have emerged to address the limitations of static metrics. The Gradient-based Difficulty Measure (GDM) evaluates example difficulty through dynamic measurement of gradient magnitude with respect to the example itself <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Unlike training loss, which can be misleading in plateau regions of the loss landscape, gradient magnitude provides more fine-grained insights into how examples influence model parameter updates, resulting in more accurate difficulty assessments <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nAnother innovative approach is the Instruction-Following Difficulty (IFD) score, which evaluates how much an instruction helps in generating corresponding responses by comparing model loss with and without instructional context <Paper corpusId=\"261076515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This method forms part of a self-guided process that begins by familiarizing the model with a small dataset subset before evaluating and selecting more challenging examples for focused training <Paper corpusId=\"261076515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nLearning-based difficulty metrics represent another advancement, with methods that utilize learning percentage as a difficulty metric for self-ranking training data <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper>. This approach considers examples that show more learning in earlier epochs as easier, while those requiring more training time are deemed harder, allowing models to prioritize more challenging examples <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>.\n\nClass-based and augmentation-based curriculum strategies have also shown promise. Class-based approaches calculate average difficulty scores for each class and schedule training to progress from easier classes to more difficult ones <Paper corpusId=\"244908620\" paperTitle=\"(Wang et al._1, 2021)\" isShortName></Paper>. Augmentation-based strategies initially train on original data before gradually increasing the proportion of augmented data, which is typically more challenging for models to learn <Paper corpusId=\"244908620\" paperTitle=\"(Wang et al._1, 2021)\" isShortName></Paper>.\n\nDespite these advances, existing approaches still face limitations in accurately accounting for continuously evolving model capabilities. Traditional methods often rely on static difficulty labels or heuristic rules that inadequately track model development <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>. Recent work has highlighted the shortcomings of coarse-grained curriculum designs that oversimplify difficulty levels (e.g., categorizing questions merely by length) and LLM-based scoring methods that struggle to capture nuanced reasoning demands <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nThe latest research emphasizes the need for more sophisticated, fine-grained adaptive frameworks that can bridge the gap between data difficulty and model capability as training progresses <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>. These approaches draw inspiration from adaptive assessment techniques in educational settings to automatically calibrate training data to align with the model's current competence, optimizing learning efficiency <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Elgaar et al._1, 2023)",
                        "snippets": [
                            "Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively."
                        ],
                        "paper": {
                            "corpus_id": 259370648,
                            "title": "HuCurl: Human-induced Curriculum Discovery",
                            "authors": [
                                {
                                    "authorId": "1659451954",
                                    "name": "Mohamed Elgaar"
                                },
                                {
                                    "authorId": "143656058",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.8955078125
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "To address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."
                        ],
                        "paper": {
                            "corpus_id": 268696658,
                            "title": "Symmetric Self-Paced Learning for Domain Generalization",
                            "authors": [
                                {
                                    "authorId": "2110600983",
                                    "name": "Di Zhao"
                                },
                                {
                                    "authorId": "34930533",
                                    "name": "Yun Sing Koh"
                                },
                                {
                                    "authorId": "2276344048",
                                    "name": "Gillian Dobbie"
                                },
                                {
                                    "authorId": "2293565669",
                                    "name": "Hongsheng Hu"
                                },
                                {
                                    "authorId": "2293453267",
                                    "name": "Philippe Fournier-Viger"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 5
                        },
                        "score": 0.89013671875
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Our method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\"."
                        ],
                        "paper": {
                            "corpus_id": 261076515,
                            "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2150655891",
                                    "name": "Ming Li"
                                },
                                {
                                    "authorId": "2144289768",
                                    "name": "Yong Zhang"
                                },
                                {
                                    "authorId": "2111336489",
                                    "name": "Zhitao Li"
                                },
                                {
                                    "authorId": "1391200710",
                                    "name": "Jiuhai Chen"
                                },
                                {
                                    "authorId": "2108451006",
                                    "name": "Lichang Chen"
                                },
                                {
                                    "authorId": "145292435",
                                    "name": "Ning Cheng"
                                },
                                {
                                    "authorId": "66063851",
                                    "name": "Jianzong Wang"
                                },
                                {
                                    "authorId": "2213956781",
                                    "name": "Tianyi Zhou"
                                },
                                {
                                    "authorId": "91353860",
                                    "name": "Jing Xiao"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 211
                        },
                        "score": 0.8603515625
                    },
                    {
                        "id": "(Mekala et al., 2024)",
                        "snippets": [
                            "Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."
                        ],
                        "paper": {
                            "corpus_id": 267740312,
                            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2284673632",
                                    "name": "Alex Nguyen"
                                },
                                {
                                    "authorId": "2284595153",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 21
                        },
                        "score": 0.9521484375
                    },
                    {
                        "id": "(Mekala et al., 2022)",
                        "snippets": [
                            "Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."
                        ],
                        "paper": {
                            "corpus_id": 249060677,
                            "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2113540861",
                                    "name": "Chengyu Dong"
                                },
                                {
                                    "authorId": "2884976",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 20
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al._1, 2021)",
                        "snippets": [
                            "The key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3.\n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37]53], and they should be more difficult to learn as verified in Section 2.3.\n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process.\n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy.\n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks."
                        ],
                        "paper": {
                            "corpus_id": 244908620,
                            "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding",
                            "authors": [
                                {
                                    "authorId": "1455165108",
                                    "name": "Deze Wang"
                                },
                                {
                                    "authorId": "10414205",
                                    "name": "Zhouyang Jia"
                                },
                                {
                                    "authorId": "50342128",
                                    "name": "Shanshan Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Yue Yu"
                                },
                                {
                                    "authorId": "2153657836",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2114050396",
                                    "name": "Wei Dong"
                                },
                                {
                                    "authorId": "144078016",
                                    "name": "Xiangke Liao"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Software Engineering",
                            "n_citations": 80
                        },
                        "score": 0.8408203125
                    },
                    {
                        "id": "(Yu et al., 2025)",
                        "snippets": [
                            "Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;(Lee et al., 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;(Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability."
                        ],
                        "paper": {
                            "corpus_id": 277824032,
                            "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
                            "authors": [
                                {
                                    "authorId": "2356617563",
                                    "name": "Qianjin Yu"
                                },
                                {
                                    "authorId": "2355802308",
                                    "name": "Keyu Wu"
                                },
                                {
                                    "authorId": "2355823766",
                                    "name": "Zihan Chen"
                                },
                                {
                                    "authorId": "2355786652",
                                    "name": "Chushu Zhang"
                                },
                                {
                                    "authorId": "2355643664",
                                    "name": "Manlin Mei"
                                },
                                {
                                    "authorId": "2356799678",
                                    "name": "Lingjun Huang"
                                },
                                {
                                    "authorId": "2355645282",
                                    "name": "Fang Tan"
                                },
                                {
                                    "authorId": "2355785017",
                                    "name": "Yongsheng Du"
                                },
                                {
                                    "authorId": "2356331597",
                                    "name": "Kunlin Liu"
                                },
                                {
                                    "authorId": "2355782385",
                                    "name": "Yurui Zhu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.8232421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.269382
    }
}