{
    "query": "What empirical studies systematically document cases of measurable degradation in general language generation, creativity, or reasoning abilities in LLMs as a result of fine-tuning for narrow or specific tasks?",
    "user_id": "lib_user",
    "task_id": "b3a7e227-21fa-4017-af88-b00756265419",
    "timestamp": "2025-06-23T21:23:05.745031",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 8,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.148221,
    "decomposed_query": {
        "rewritten_query": "Empirical studies documenting measurable degradation in general language generation, creativity, or reasoning abilities in LLMs as a result of fine-tuning for narrow or specific tasks.",
        "keyword_query": "empirical studies measurable degradation language generation creativity reasoning abilities LLMs fine-tuning narrow specific tasks",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010254,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.15382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332098876",
                    "name": "Elita Lobo"
                },
                {
                    "authorId": "40228633",
                    "name": "Chirag Agarwal"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ],
            "abstract": "Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",
            "corpus_id": 274234789,
            "sentences": [
                {
                    "corpus_id": "274234789",
                    "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                    "text": "Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",
                    "score": 0.6037427349648203,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82080078125
                },
                {
                    "corpus_id": "274234789",
                    "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                    "text": "Notably, prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024), ii) deactivation of safety filters initially embedded in LLMs (Kumar et al., 2024), making the models vulnerable to generating harmful content, and iii) increased risk of privacy breaches due to the higher extraction rate of finetuning data (Singh et al., 2024;Zeng et al., 2024). Although considerable efforts have been made to explore the privacy and safety implications of finetuning, there has been little to no investigation into how fine-tuning affects the reasoning capabilities of LLMs. If fine-tuning LLMs diminishes their reasoning abilities, LLMs may lose their core appeal to users (Brown et al., 2020;Wei et al., 2022b) A key method for eliciting reasoning in LLMs is the Chain-of-Thought reasoning approach Wei et al. (2022b). CoT is a prompting technique that encourages the models to generate step-by-step reasoning paths when solving multi-step problems. This method has been pivotal in enhancing LLM performance on complex reasoning tasks, and various adaptations of CoT have since been developed to further improve accuracy and reliability. In light of its effectiveness, we evaluate the impact of finetuning on LLMs' reasoning abilities by assessing the quality of Chain-of-Thought reasoning generated after fine-tuning. Our work. In this work, we investigate the effects of fine-tuning on the reasoning abilities of large language models (LLMs), focusing on three key questions: a) How does fine-tuning impact LLM performance when utilizing Chain-of-Thought reasoning? b) Does fine-tuning affect the faithfulness of CoT reasoning? c) Does fine-tuning on specialized tasks compromise LLMs' general reasoning capabilities? Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate.",
                    "score": 0.6305731854135186,
                    "section_title": "Introduction",
                    "char_start_offset": 1590,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1054
                        },
                        {
                            "start": 1055,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1433
                        },
                        {
                            "start": 1434,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1824
                        },
                        {
                            "start": 1825,
                            "end": 2019
                        },
                        {
                            "start": 2020,
                            "end": 2201
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 424,
                            "end": 444,
                            "matchedPaperCorpusId": "267149066"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7958984375
                },
                {
                    "corpus_id": "274234789",
                    "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                    "text": "First, we outline our experimental setup to evaluate the impact of fine-tuning on the reasoning abilities of LLMs and then present our results. Datasets. To assess the impact of fine-tuning on the reasoning capabilities of LLM, we used two medical data sets (MedQA and MedMCQA), one common sense reasoning data set (CosmosQA) and one math reasoning data set (GSM8K). We use one dataset from each of these categories for finetuning the LLM and we use all the datasets to evaluate the effect of fine-tuning on the given dataset. With slight abuse of notation, we will denote any test dataset as in-distribution (IID) if the dataset belongs to the same category as the fine-tuning dataset and out-of-distribution (OOD) otherwise. Next, we describe each of the four datasets. i) MedQA (Jin et al., 2021): Multiple choice question answers from the United States Medical License Exams (USMLE), ii) MedMCQA (Pal et al., 2022): Multiple choice question answers from the All India Institute of Medical Sciences (AIIMS) and National Eligibility cum Entrance Exam (NEET), iii) CosmosQA (Huang et al., 2019): Multiple choice questions formulated from commonsense-based reading comprehensions, and iv) GSM8K (Cobbe et al., 2021): Math word problems from diverse grades. \n\nModels. We work with three LLMs: a 4bit quantized Llama-3-8b-Instruct model (Abhimanyu Dubey, 2024) provided by Unsloth library, GPT-3.5-0125, and GPT-4 known for their exceptional reasoning capabilities on various tasks. \n\nImplementation details. The primary goal of this paper is to explore the effects of vanilla finetuning, i.e., fine-tuning without incorporating reasoning steps in the responses, on the CoT reasoning capabilities of the model. However, achieving accuracy improvements on the GSM8K dataset proved challenging when fine-tuning the model using only question-answer pairs from the GSM8K training set.",
                    "score": 0.44252589792614583,
                    "section_title": "Experimental Setup",
                    "char_start_offset": 12744,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 1256
                        },
                        {
                            "start": 1259,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1480
                        },
                        {
                            "start": 1483,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1708
                        },
                        {
                            "start": 1709,
                            "end": 1878
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 781,
                            "end": 799,
                            "matchedPaperCorpusId": "221970190"
                        },
                        {
                            "start": 900,
                            "end": 918,
                            "matchedPaperCorpusId": "247763070"
                        },
                        {
                            "start": 1075,
                            "end": 1095,
                            "matchedPaperCorpusId": "202540590"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.167236328125
                }
            ],
            "relevance_judgement": 0.82080078125,
            "relevance_judgment_input_expanded": "# Title: On the Impact of Fine-Tuning on Chain-of-Thought Reasoning\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Elita Lobo, Chirag Agarwal, Himabindu Lakkaraju\n## Abstract\nLarge language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.\n## Introduction\nNotably, prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024), ii) deactivation of safety filters initially embedded in LLMs (Kumar et al., 2024), making the models vulnerable to generating harmful content, and iii) increased risk of privacy breaches due to the higher extraction rate of finetuning data (Singh et al., 2024;Zeng et al., 2024). Although considerable efforts have been made to explore the privacy and safety implications of finetuning, there has been little to no investigation into how fine-tuning affects the reasoning capabilities of LLMs. If fine-tuning LLMs diminishes their reasoning abilities, LLMs may lose their core appeal to users (Brown et al., 2020;Wei et al., 2022b) A key method for eliciting reasoning in LLMs is the Chain-of-Thought reasoning approach Wei et al. (2022b). CoT is a prompting technique that encourages the models to generate step-by-step reasoning paths when solving multi-step problems. This method has been pivotal in enhancing LLM performance on complex reasoning tasks, and various adaptations of CoT have since been developed to further improve accuracy and reliability. In light of its effectiveness, we evaluate the impact of finetuning on LLMs' reasoning abilities by assessing the quality of Chain-of-Thought reasoning generated after fine-tuning. Our work. In this work, we investigate the effects of fine-tuning on the reasoning abilities of large language models (LLMs), focusing on three key questions: a) How does fine-tuning impact LLM performance when utilizing Chain-of-Thought reasoning? b) Does fine-tuning affect the faithfulness of CoT reasoning? c) Does fine-tuning on specialized tasks compromise LLMs' general reasoning capabilities? Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate.\n\n## Experimental Setup\nFirst, we outline our experimental setup to evaluate the impact of fine-tuning on the reasoning abilities of LLMs and then present our results. Datasets. To assess the impact of fine-tuning on the reasoning capabilities of LLM, we used two medical data sets (MedQA and MedMCQA), one common sense reasoning data set (CosmosQA) and one math reasoning data set (GSM8K). We use one dataset from each of these categories for finetuning the LLM and we use all the datasets to evaluate the effect of fine-tuning on the given dataset. With slight abuse of notation, we will denote any test dataset as in-distribution (IID) if the dataset belongs to the same category as the fine-tuning dataset and out-of-distribution (OOD) otherwise. Next, we describe each of the four datasets. i) MedQA (Jin et al., 2021): Multiple choice question answers from the United States Medical License Exams (USMLE), ii) MedMCQA (Pal et al., 2022): Multiple choice question answers from the All India Institute of Medical Sciences (AIIMS) and National Eligibility cum Entrance Exam (NEET), iii) CosmosQA (Huang et al., 2019): Multiple choice questions formulated from commonsense-based reading comprehensions, and iv) GSM8K (Cobbe et al., 2021): Math word problems from diverse grades. \n\nModels. We work with three LLMs: a 4bit quantized Llama-3-8b-Instruct model (Abhimanyu Dubey, 2024) provided by Unsloth library, GPT-3.5-0125, and GPT-4 known for their exceptional reasoning capabilities on various tasks. \n\nImplementation details. The primary goal of this paper is to explore the effects of vanilla finetuning, i.e., fine-tuning without incorporating reasoning steps in the responses, on the CoT reasoning capabilities of the model. However, achieving accuracy improvements on the GSM8K dataset proved challenging when fine-tuning the model using only question-answer pairs from the GSM8K training set.",
            "reference_string": "[274234789 | Lobo et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Enhancing AI Safety Through the Fusion of Low Rank Adapters",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.06208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2199011429",
                    "name": "Satya Swaroop Gudipudi"
                },
                {
                    "authorId": "2339778876",
                    "name": "Sreeram Vipparla"
                },
                {
                    "authorId": "2340190055",
                    "name": "Harpreet Singh"
                },
                {
                    "authorId": "9343338",
                    "name": "Shashwat Goel"
                },
                {
                    "authorId": "1734731",
                    "name": "P. Kumaraguru"
                }
            ],
            "abstract": "Instruction fine-tuning of large language models (LLMs) is a powerful method for improving task-specific performance, but it can inadvertently lead to a phenomenon where models generate harmful responses when faced with malicious prompts. In this paper, we explore Low-Rank Adapter Fusion (LoRA) as a means to mitigate these risks while preserving the model's ability to handle diverse instructions effectively. Through an extensive comparative analysis against established baselines using recognized benchmark datasets, we demonstrate a 42\\% reduction in the harmfulness rate by leveraging LoRA fusion between a task adapter and a safety adapter, the latter of which is specifically trained on our safety dataset. However, we also observe exaggerated safety behaviour, where the model rejects safe prompts that closely resemble unsafe ones",
            "corpus_id": 275471407,
            "sentences": [
                {
                    "corpus_id": "275471407",
                    "title": "Enhancing AI Safety Through the Fusion of Low Rank Adapters",
                    "text": "Large Language Models (LLMs) have demonstrated remarkable proficiency, exhibiting advanced linguistic and reasoning capabilities [1] that make them increasingly favored choices for conversational agents. With each new iteration, these models are released to the public with enhanced functionalities designed to assist in a myriad of user tasks, ranging from simple queries to complex problem-solving scenarios. \n\nThese LLMs excel at performing general tasks and can also be adapted for specific activities through In-context Learning (ICL) [2], where the model leverages existing parameters without the  need for updates. However, when more profound task-specific performance tuning is required, finetuning becomes necessary. Here, parameters of the base model are modified to align with the demands of downstream tasks. In this realm, Parameter-Efficient Fine-Tuning (PEFT) [3] has emerged as a popular strategy, particularly within large-scale models. Techniques like Low-Rank Adaptation (LoRA) [4] stand out due to their practicality in selectively updating a small subset of parameters, thereby maintaining the vast pre-trained knowledge base while optimizing the model towards specific tasks. \n\nDespite the advantages, fine-tuning can inadvertently lead to jailbreaking of the model, where the LLM deviates from safety constraints previously set by the base configuration. This issue has been noted in several studies [6,7,8,9], which highlight the challenges of maintaining safety alignment when adapting models through fine-tuning. \n\nLLMs have increasingly incorporated advanced safety mechanisms, such as Reinforcement Learning from Human Feedback (RLHF) [5], to simultaneously optimize for both helpfulness and harm reduction. These safety alignment techniques are crucial for aligning model outputs with ethical guidelines and user expectations. However, while fine-tuning LLMs on downstream tasks can significantly enhance their helpfulness and task-specific performance, this fine-tuning process often inadvertently compromises the models' inherent safety protocols. This degradation in safety measures during fine-tuning raises critical concerns, as it may lead to the generation of outputs that, although high-performing, could be potentially harmful or biased. \n\nTo address the challenge of maintaining safety standards while enhancing task performance during fine-tuning, our contribution is twofold:",
                    "score": 0.45399869762200623,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 410
                        },
                        {
                            "start": 413,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 953
                        },
                        {
                            "start": 954,
                            "end": 1197
                        },
                        {
                            "start": 1200,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1538
                        },
                        {
                            "start": 1541,
                            "end": 1735
                        },
                        {
                            "start": 1736,
                            "end": 1855
                        },
                        {
                            "start": 1856,
                            "end": 2078
                        },
                        {
                            "start": 2079,
                            "end": 2275
                        },
                        {
                            "start": 2278,
                            "end": 2416
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1423,
                            "end": 1426,
                            "matchedPaperCorpusId": "265308865"
                        },
                        {
                            "start": 1426,
                            "end": 1428,
                            "matchedPaperCorpusId": "263671523"
                        },
                        {
                            "start": 1428,
                            "end": 1430,
                            "matchedPaperCorpusId": "265067269"
                        },
                        {
                            "start": 1663,
                            "end": 1666,
                            "matchedPaperCorpusId": "4787508"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67626953125
                }
            ],
            "relevance_judgement": 0.67626953125,
            "relevance_judgment_input_expanded": "# Title: Enhancing AI Safety Through the Fusion of Low Rank Adapters\n# Venue: arXiv.org\n# Authors: Satya Swaroop Gudipudi, Sreeram Vipparla, Harpreet Singh, Shashwat Goel, P. Kumaraguru\n## Abstract\nInstruction fine-tuning of large language models (LLMs) is a powerful method for improving task-specific performance, but it can inadvertently lead to a phenomenon where models generate harmful responses when faced with malicious prompts. In this paper, we explore Low-Rank Adapter Fusion (LoRA) as a means to mitigate these risks while preserving the model's ability to handle diverse instructions effectively. Through an extensive comparative analysis against established baselines using recognized benchmark datasets, we demonstrate a 42\\% reduction in the harmfulness rate by leveraging LoRA fusion between a task adapter and a safety adapter, the latter of which is specifically trained on our safety dataset. However, we also observe exaggerated safety behaviour, where the model rejects safe prompts that closely resemble unsafe ones\n## Introduction\nLarge Language Models (LLMs) have demonstrated remarkable proficiency, exhibiting advanced linguistic and reasoning capabilities [1] that make them increasingly favored choices for conversational agents. With each new iteration, these models are released to the public with enhanced functionalities designed to assist in a myriad of user tasks, ranging from simple queries to complex problem-solving scenarios. \n\nThese LLMs excel at performing general tasks and can also be adapted for specific activities through In-context Learning (ICL) [2], where the model leverages existing parameters without the  need for updates. However, when more profound task-specific performance tuning is required, finetuning becomes necessary. Here, parameters of the base model are modified to align with the demands of downstream tasks. In this realm, Parameter-Efficient Fine-Tuning (PEFT) [3] has emerged as a popular strategy, particularly within large-scale models. Techniques like Low-Rank Adaptation (LoRA) [4] stand out due to their practicality in selectively updating a small subset of parameters, thereby maintaining the vast pre-trained knowledge base while optimizing the model towards specific tasks. \n\nDespite the advantages, fine-tuning can inadvertently lead to jailbreaking of the model, where the LLM deviates from safety constraints previously set by the base configuration. This issue has been noted in several studies [6,7,8,9], which highlight the challenges of maintaining safety alignment when adapting models through fine-tuning. \n\nLLMs have increasingly incorporated advanced safety mechanisms, such as Reinforcement Learning from Human Feedback (RLHF) [5], to simultaneously optimize for both helpfulness and harm reduction. These safety alignment techniques are crucial for aligning model outputs with ethical guidelines and user expectations. However, while fine-tuning LLMs on downstream tasks can significantly enhance their helpfulness and task-specific performance, this fine-tuning process often inadvertently compromises the models' inherent safety protocols. This degradation in safety measures during fine-tuning raises critical concerns, as it may lead to the generation of outputs that, although high-performing, could be potentially harmful or biased. \n\nTo address the challenge of maintaining safety standards while enhancing task performance during fine-tuning, our contribution is twofold:",
            "reference_string": "[275471407 | Gudipudi et al. | 2024 | Citations: 3]"
        },
        {
            "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 64,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20089, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292435932",
                    "name": "David Stap"
                },
                {
                    "authorId": "2269141224",
                    "name": "Eva Hasler"
                },
                {
                    "authorId": "2296993932",
                    "name": "Bill Byrne"
                },
                {
                    "authorId": "2062908179",
                    "name": "C. Monz"
                },
                {
                    "authorId": "2303845498",
                    "name": "Ke Tran"
                }
            ],
            "abstract": "Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters. Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.",
            "corpus_id": 270123515,
            "sentences": [
                {
                    "corpus_id": "270123515",
                    "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities",
                    "text": "We investigated how fine-tuning on parallel data affects the qualitative advantages of LLMs for machine translation. While previous research predominantly focused on summary quality metrics like COMET, our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings. To prevent these degradations, we develop a fine-tuning method tailored for machine translation, that combines monolingual and parallel data. We show that this approach mitigates the degradation of LLMs' qualitative advantages, thereby preserving their capabilities while improving general translation quality.",
                    "score": 0.47850351343200115,
                    "section_title": "Conclusion",
                    "char_start_offset": 21885,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 288
                        },
                        {
                            "start": 289,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 707
                        },
                        {
                            "start": 708,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1508
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65576171875
                }
            ],
            "relevance_judgement": 0.65576171875,
            "relevance_judgment_input_expanded": "# Title: The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: David Stap, Eva Hasler, Bill Byrne, C. Monz, Ke Tran\n## Abstract\nFine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters. Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.\n## Conclusion\nWe investigated how fine-tuning on parallel data affects the qualitative advantages of LLMs for machine translation. While previous research predominantly focused on summary quality metrics like COMET, our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings. To prevent these degradations, we develop a fine-tuning method tailored for machine translation, that combines monolingual and parallel data. We show that this approach mitigates the degradation of LLMs' qualitative advantages, thereby preserving their capabilities while improving general translation quality.",
            "reference_string": "[270123515 | Stap et al. | 2024 | Citations: 12]"
        },
        {
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 41,
            "citation_count": 318,
            "influential_citation_count": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.08747",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.08747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2181146702",
                    "name": "Yun Luo"
                },
                {
                    "authorId": "2149231975",
                    "name": "Zhen Yang"
                },
                {
                    "authorId": "33427918",
                    "name": "Fandong Meng"
                },
                {
                    "authorId": "2110450452",
                    "name": "Yafu Li"
                },
                {
                    "authorId": "48128428",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2167740183",
                    "name": "Yue Zhang"
                }
            ],
            "abstract": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving a satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs' knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.",
            "corpus_id": 261031244,
            "sentences": [
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "In this study, we conducted an empirical investigation into the catastrophic forgetting (CF) phenomenon experienced by large language models (LLMs) during continual instruction tuning. Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills. Furthermore, our comparative analysis showed that the decoder-only model, BLOOMZ, demonstrates a superior ability to retain knowledge and skills during continual fine-tuning when compared to the encoder-decoder model, mT0. Additionally, we discovered that employing general instruction tuning techniques may help alleviate the CF problem in LLMs. Our empirical study suggests that exploring more effective methods to mitigate CF in LLMs during continual fine-tuning is a promising research direction. When applying LLMs, practitioners should remain vigilant and pay close attention to the issue of knowledge forgetting that may occur after instruction tuning. Addressing this challenge is crucial to ensure the reliable and consistent performance of LLMs in real-world applications.",
                    "score": 0.44726060330824413,
                    "section_title": "Conclusion",
                    "char_start_offset": 22052,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 185,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1242
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64892578125
                }
            ],
            "relevance_judgement": 0.64892578125,
            "relevance_judgment_input_expanded": "# Title: An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning\n# Venue: arXiv.org\n# Authors: Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yue Zhang\n## Abstract\nCatastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving a satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs' knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.\n## Conclusion\nIn this study, we conducted an empirical investigation into the catastrophic forgetting (CF) phenomenon experienced by large language models (LLMs) during continual instruction tuning. Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills. Furthermore, our comparative analysis showed that the decoder-only model, BLOOMZ, demonstrates a superior ability to retain knowledge and skills during continual fine-tuning when compared to the encoder-decoder model, mT0. Additionally, we discovered that employing general instruction tuning techniques may help alleviate the CF problem in LLMs. Our empirical study suggests that exploring more effective methods to mitigate CF in LLMs during continual fine-tuning is a promising research direction. When applying LLMs, practitioners should remain vigilant and pay close attention to the issue of knowledge forgetting that may occur after instruction tuning. Addressing this challenge is crucial to ensure the reliable and consistent performance of LLMs in real-world applications.",
            "reference_string": "[261031244 | Luo et al. | 2023 | Citations: 318]"
        },
        {
            "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 55,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.04700, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3028818",
                    "name": "Jia-Chen Gu"
                },
                {
                    "authorId": "2269760395",
                    "name": "Haoyang Xu"
                },
                {
                    "authorId": "2152612230",
                    "name": "Jun-Yu Ma"
                },
                {
                    "authorId": "2887562",
                    "name": "Pan Lu"
                },
                {
                    "authorId": "2072392338",
                    "name": "Zhen-Hua Ling"
                },
                {
                    "authorId": "2257127887",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2256996328",
                    "name": "Nanyun Peng"
                }
            ],
            "abstract": "Model editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model\u2019s behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing\u2019s improvements on factuality may come at the cost of a significant degradation of the model\u2019s general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT is proposed to regularize the edit update weights by imposing constraints on their complexity based on the RElative Change in weighT. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.",
            "corpus_id": 266899568,
            "sentences": [
                {
                    "corpus_id": "266899568",
                    "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
                    "text": "Model editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model\u2019s behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing\u2019s improvements on factuality may come at the cost of a significant degradation of the model\u2019s general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT is proposed to regularize the edit update weights by imposing constraints on their complexity based on the RElative Change in weighT. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.",
                    "score": 0.4413500039906212,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59912109375
                },
                {
                    "corpus_id": "266899568",
                    "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
                    "text": "In light of the above issues, we systematically study if model editing hurts the general abilities of LLMs. This work studies model editing in the singleversus sequential-editing and instanceversus batch-editing settings. The edited models are evaluated on a variety of downstream tasks to see if there are any side effects on performance before and after editing. Extensive empirical experiments are conducted on four popular editing methods: KN (Dai et al., 2022), MEND (Mitchell et al., 2022a), ROME (Meng et al., 2022), and MEMIT (Meng et al., 2023) applied to three representative LLMs: GPT-2 XL (1.5B) (Radford et al., 2019), LLaMA-1 (7B) (Touvron et al., 2023a), and LLaMA-2 (7B) (Touvron et al., 2023b). Eight representative tasks including reasoning (Cobbe et al., 2021), natural language inference (Dagan et al., 2005), open-domain QA (Kwiatkowski et al., 2019), closed-domain QA (Clark et al., 2019), dialogue (Cui et al., 2020), summarization (Gliwa et al., 2019), named entity recognition (Sang and Meulder, 2003), and sentiment analysis (Socher et al., 2013) are employed to understand the impact of model editing on the general abilities of LLMs. \n\nExperimental results show that existing LLMs are not robust to weight perturbations, and editing even a few parameters can significantly affect their general abilities. Strikingly, with a single pass of editing involving less than 1% parameters, LLaMA-1 (7B) exhibited a drastic performance degradation to nearly 0 on all the tasks we tried. These results demonstrate that current editing algorithms struggle to work effectively in tandem with LLMs to simultaneously improve model factuality and maintain general abilities. \n\nFurthermore, our analysis of the causes of side effects reveals that current model editing methods change the original model weights too much, resulting in overfitting to new editing facts.",
                    "score": 0.47730747587650135,
                    "section_title": "Introduction",
                    "char_start_offset": 2023,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 1161
                        },
                        {
                            "start": 1164,
                            "end": 1332
                        },
                        {
                            "start": 1333,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1687
                        },
                        {
                            "start": 1690,
                            "end": 1879
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 447,
                            "end": 465,
                            "matchedPaperCorpusId": "233296761"
                        },
                        {
                            "start": 472,
                            "end": 496,
                            "matchedPaperCorpusId": "239050360"
                        },
                        {
                            "start": 503,
                            "end": 522,
                            "matchedPaperCorpusId": "255825985"
                        },
                        {
                            "start": 608,
                            "end": 630,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 808,
                            "end": 828,
                            "matchedPaperCorpusId": "8587959"
                        },
                        {
                            "start": 845,
                            "end": 871,
                            "matchedPaperCorpusId": "86611921"
                        },
                        {
                            "start": 890,
                            "end": 910,
                            "matchedPaperCorpusId": "165163607"
                        },
                        {
                            "start": 955,
                            "end": 975,
                            "matchedPaperCorpusId": "208010268"
                        },
                        {
                            "start": 1002,
                            "end": 1026,
                            "matchedPaperCorpusId": "2470716"
                        },
                        {
                            "start": 1051,
                            "end": 1072,
                            "matchedPaperCorpusId": "990233"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.4814453125
                }
            ],
            "relevance_judgement": 0.59912109375,
            "relevance_judgment_input_expanded": "# Title: Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jia-Chen Gu, Haoyang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, Nanyun Peng\n## Abstract\nModel editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model\u2019s behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing\u2019s improvements on factuality may come at the cost of a significant degradation of the model\u2019s general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT is proposed to regularize the edit update weights by imposing constraints on their complexity based on the RElative Change in weighT. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.\n## Introduction\nIn light of the above issues, we systematically study if model editing hurts the general abilities of LLMs. This work studies model editing in the singleversus sequential-editing and instanceversus batch-editing settings. The edited models are evaluated on a variety of downstream tasks to see if there are any side effects on performance before and after editing. Extensive empirical experiments are conducted on four popular editing methods: KN (Dai et al., 2022), MEND (Mitchell et al., 2022a), ROME (Meng et al., 2022), and MEMIT (Meng et al., 2023) applied to three representative LLMs: GPT-2 XL (1.5B) (Radford et al., 2019), LLaMA-1 (7B) (Touvron et al., 2023a), and LLaMA-2 (7B) (Touvron et al., 2023b). Eight representative tasks including reasoning (Cobbe et al., 2021), natural language inference (Dagan et al., 2005), open-domain QA (Kwiatkowski et al., 2019), closed-domain QA (Clark et al., 2019), dialogue (Cui et al., 2020), summarization (Gliwa et al., 2019), named entity recognition (Sang and Meulder, 2003), and sentiment analysis (Socher et al., 2013) are employed to understand the impact of model editing on the general abilities of LLMs. \n\nExperimental results show that existing LLMs are not robust to weight perturbations, and editing even a few parameters can significantly affect their general abilities. Strikingly, with a single pass of editing involving less than 1% parameters, LLaMA-1 (7B) exhibited a drastic performance degradation to nearly 0 on all the tasks we tried. These results demonstrate that current editing algorithms struggle to work effectively in tandem with LLMs to simultaneously improve model factuality and maintain general abilities. \n\nFurthermore, our analysis of the causes of side effects reveals that current model editing methods change the original model weights too much, resulting in overfitting to new editing facts.",
            "reference_string": "[266899568 | Gu et al. | 2024 | Citations: 55]"
        },
        {
            "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
            "venue": "IR-RAG@SIGIR",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303471987",
                    "name": "Emile Contal"
                },
                {
                    "authorId": "2283934772",
                    "name": "Garrin McGoldrick"
                }
            ],
            "abstract": "Large Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs. Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance. We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores. Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.",
            "corpus_id": 270068369,
            "sentences": [
                {
                    "corpus_id": "270068369",
                    "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
                    "text": "Large Language Models (LLMs) have emerged as a powerful tool for natural language processing, demonstrating remarkable abilities in areas like text completion, summarization, and question answering [1]. One of their most intriguing capabilities is their potential to learn \"common sense\" -general knowledge about the world that allows them to reason and make inferences beyond the literal meaning of text. This has fueled excitement about the possibility of achieving zero-shot learning, where LLMs can solve unseen problems without any prior training on specific tasks [2]. \n\nHowever, a crucial distinction exists between generic public knowledge and the specific private knowledge required for most real-world use cases. While LLMs excel at generic text completion or chat-like interactions, practical applications often demand solving specific and repeatable downstream tasks within a particular domain [3]. This typically necessitates knowledge specific to a business or organization, such as understanding internal processes, up-to-date product details, or customer behavior. \n\nFine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]. As a final consideration, keeping LLMs constantly up-to-date using fine-tuning can be infeasible, especially for domains with frequently changing information like e-commerce product inventory, whereas it is easy to update a database in real-time from which information is retrieved. \n\nAs an alternative to fine-tuning, In-Context Learning (ICL) offers a promising approach for leveraging LLMs in scenarios with limited data. This approach exploits the demonstrated ability of LLMs for \"meta-learning\" -essentially, learning how to learn.",
                    "score": 0.4434542787712594,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 574
                        },
                        {
                            "start": 577,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1080
                        },
                        {
                            "start": 1083,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1769
                        },
                        {
                            "start": 1770,
                            "end": 2052
                        },
                        {
                            "start": 2055,
                            "end": 2194
                        },
                        {
                            "start": 2195,
                            "end": 2307
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 198,
                            "end": 201,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 906,
                            "end": 909,
                            "matchedPaperCorpusId": "235458009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52685546875
                }
            ],
            "relevance_judgement": 0.52685546875,
            "relevance_judgment_input_expanded": "# Title: RAGSys: Item-Cold-Start Recommender as RAG System\n# Venue: IR-RAG@SIGIR\n# Authors: Emile Contal, Garrin McGoldrick\n## Abstract\nLarge Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs. Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance. We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores. Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.\n## Introduction\nLarge Language Models (LLMs) have emerged as a powerful tool for natural language processing, demonstrating remarkable abilities in areas like text completion, summarization, and question answering [1]. One of their most intriguing capabilities is their potential to learn \"common sense\" -general knowledge about the world that allows them to reason and make inferences beyond the literal meaning of text. This has fueled excitement about the possibility of achieving zero-shot learning, where LLMs can solve unseen problems without any prior training on specific tasks [2]. \n\nHowever, a crucial distinction exists between generic public knowledge and the specific private knowledge required for most real-world use cases. While LLMs excel at generic text completion or chat-like interactions, practical applications often demand solving specific and repeatable downstream tasks within a particular domain [3]. This typically necessitates knowledge specific to a business or organization, such as understanding internal processes, up-to-date product details, or customer behavior. \n\nFine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]. As a final consideration, keeping LLMs constantly up-to-date using fine-tuning can be infeasible, especially for domains with frequently changing information like e-commerce product inventory, whereas it is easy to update a database in real-time from which information is retrieved. \n\nAs an alternative to fine-tuning, In-Context Learning (ICL) offers a promising approach for leveraging LLMs in scenarios with limited data. This approach exploits the demonstrated ability of LLMs for \"meta-learning\" -essentially, learning how to learn.",
            "reference_string": "[270068369 | Contal et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 31,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.02442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2028219138",
                    "name": "Zhi Rui Tam"
                },
                {
                    "authorId": "2217944277",
                    "name": "Cheng-Kuang Wu"
                },
                {
                    "authorId": "2314965749",
                    "name": "Yi-Lin Tsai"
                },
                {
                    "authorId": "2306137538",
                    "name": "Chieh-Yen Lin"
                },
                {
                    "authorId": "2278588523",
                    "name": "Hung-yi Lee"
                },
                {
                    "authorId": "2306102701",
                    "name": "Yun-Nung Chen"
                }
            ],
            "abstract": "Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).This study investigates whether such constraints on generation space impact LLMs\u2019 abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs\u2019 performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs\u2019 reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.",
            "corpus_id": 271709856,
            "sentences": [
                {
                    "corpus_id": "271709856",
                    "title": "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.",
                    "text": "Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).This study investigates whether such constraints on generation space impact LLMs\u2019 abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs\u2019 performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs\u2019 reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.",
                    "score": 0.4813983162582596,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51318359375
                }
            ],
            "relevance_judgement": 0.51318359375,
            "relevance_judgment_input_expanded": "# Title: Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, Yun-Nung Chen\n## Abstract\nStructured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).This study investigates whether such constraints on generation space impact LLMs\u2019 abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs\u2019 performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs\u2019 reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.\n",
            "reference_string": "[271709856 | Tam et al. | 2024 | Citations: 31]"
        },
        {
            "title": "CRANE: Reasoning with constrained LLM generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 41,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.09061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2069960232",
                    "name": "Debangshu Banerjee"
                },
                {
                    "authorId": "2218724103",
                    "name": "Tarun Suresh"
                },
                {
                    "authorId": "1413931779",
                    "name": "Shubham Ugare"
                },
                {
                    "authorId": "1704478",
                    "name": "Sasa Misailovic"
                },
                {
                    "authorId": "2301556017",
                    "name": "Gagandeep Singh"
                }
            ],
            "abstract": "Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.",
            "corpus_id": 276317504,
            "sentences": [
                {
                    "corpus_id": "276317504",
                    "title": "CRANE: Reasoning with constrained LLM generation",
                    "text": "Recent works such as Tam et al. (2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. Tam et al. (2024) attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding. This observation raises the following open questions: \n\n\u2022 RQ1: Do LLMs truly lose reasoning capabilities under constrained decoding? \u2022 RQ2: How can we leverage the benefits of constrained decoding in reducing syntax errors while preserving the unconstrained reasoning capabilities of LLMs? \n\nKey Challenges: First, we need to formally identify the root cause of the reduction in functional accuracy of endto-end systems when a pre-trained LLM operates under constrained generation. Unlike the empirical observations in (Tam et al., 2024), we seek a formal justification for this reduction that is not limited to specific LLMs used in experiments but extends to any LLM, including more powerful ones developed in the future. \n\nSecond, we must design cost-efficient decoding strategies that address the shortcomings of existing constrained decoding methods while improving functional accuracy. In this work, we do not consider task-specific fine-tuning of LLMs, as fine-tuning for each task is compute-intensive. Unlike constrained decoding, fine-tuning does not guarantee that the LLM output adheres to formal constraints.",
                    "score": 0.4509049439840014,
                    "section_title": "Introduction",
                    "char_start_offset": 1722,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 373
                        },
                        {
                            "start": 376,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 609
                        },
                        {
                            "start": 612,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 1043
                        },
                        {
                            "start": 1046,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1441
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 21,
                            "end": 38,
                            "matchedPaperCorpusId": "271709856"
                        },
                        {
                            "start": 176,
                            "end": 193,
                            "matchedPaperCorpusId": "271709856"
                        },
                        {
                            "start": 839,
                            "end": 857,
                            "matchedPaperCorpusId": "271709856"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50048828125
                }
            ],
            "relevance_judgement": 0.50048828125,
            "relevance_judgment_input_expanded": "# Title: CRANE: Reasoning with constrained LLM generation\n# Venue: arXiv.org\n# Authors: Debangshu Banerjee, Tarun Suresh, Shubham Ugare, Sasa Misailovic, Gagandeep Singh\n## Abstract\nCode generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.\n## Introduction\nRecent works such as Tam et al. (2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. Tam et al. (2024) attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding. This observation raises the following open questions: \n\n\u2022 RQ1: Do LLMs truly lose reasoning capabilities under constrained decoding? \u2022 RQ2: How can we leverage the benefits of constrained decoding in reducing syntax errors while preserving the unconstrained reasoning capabilities of LLMs? \n\nKey Challenges: First, we need to formally identify the root cause of the reduction in functional accuracy of endto-end systems when a pre-trained LLM operates under constrained generation. Unlike the empirical observations in (Tam et al., 2024), we seek a formal justification for this reduction that is not limited to specific LLMs used in experiments but extends to any LLM, including more powerful ones developed in the future. \n\nSecond, we must design cost-efficient decoding strategies that address the shortcomings of existing constrained decoding methods while improving functional accuracy. In this work, we do not consider task-specific fine-tuning of LLMs, as fine-tuning for each task is compute-intensive. Unlike constrained decoding, fine-tuning does not guarantee that the LLM output adheres to formal constraints.",
            "reference_string": "[276317504 | Banerjee et al. | 2025 | Citations: 7]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "274234789",
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "text": "Notably, prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024), ii) deactivation of safety filters initially embedded in LLMs (Kumar et al., 2024), making the models vulnerable to generating harmful content, and iii) increased risk of privacy breaches due to the higher extraction rate of finetuning data (Singh et al., 2024;Zeng et al., 2024). Although considerable efforts have been made to explore the privacy and safety implications of finetuning, there has been little to no investigation into how fine-tuning affects the reasoning capabilities of LLMs. If fine-tuning LLMs diminishes their reasoning abilities, LLMs may lose their core appeal to users (Brown et al., 2020;Wei et al., 2022b) A key method for eliciting reasoning in LLMs is the Chain-of-Thought reasoning approach Wei et al. (2022b). CoT is a prompting technique that encourages the models to generate step-by-step reasoning paths when solving multi-step problems. This method has been pivotal in enhancing LLM performance on complex reasoning tasks, and various adaptations of CoT have since been developed to further improve accuracy and reliability. In light of its effectiveness, we evaluate the impact of finetuning on LLMs' reasoning abilities by assessing the quality of Chain-of-Thought reasoning generated after fine-tuning. Our work. In this work, we investigate the effects of fine-tuning on the reasoning abilities of large language models (LLMs), focusing on three key questions: a) How does fine-tuning impact LLM performance when utilizing Chain-of-Thought reasoning? b) Does fine-tuning affect the faithfulness of CoT reasoning? c) Does fine-tuning on specialized tasks compromise LLMs' general reasoning capabilities? Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate.",
            "score": 0.6305731854135186,
            "section_title": "Introduction",
            "char_start_offset": 1590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 444,
                    "matchedPaperCorpusId": "267149066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "274234789",
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "text": "Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",
            "score": 0.6037427349648203,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "267636822",
            "title": "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets",
            "text": "The introduction of open-source LLMs like LLAMA-2 (Touvron et al., 2023) enabled the creation of several language models that focus on specific applications. This application gives more capabilities for these LLMs by teaching them to use tools (Schick et al., 2023), write code (Roziere et al., 2023), understand videos (Zhang et al., 2023a), or work for different languages (Cui et al., 2023). To achieve remarkable understanding and generation abilities, LLMs require large training data and huge compute resources (Hoffmann et al., 2022). The work by Dong et al. (2023) explores how LLMs' generation, natural language understanding, and problem-solving abilities relate to the data they are trained on and its composition. This work suggests that the amount of composition data is more important for these abilities to show in a lowresource scenario. \n\nUsing self-instructed fine-tuning, the work by Wei et al. (2022); Taori et al. (2023); Cui et al. (2023) showed a new approach to align the generation outputs of the generative models through the application of NLP tasks. These tasks are structured around natural language instruction templates, providing a novel means to guide the model's generation process toward better adherence to task-specific requirements. LLAMA-Adapter (Zhang et al., 2023b) also shows that it is possible to reduce the fine-tuning time of LLAMA-7B by introducing lightweight adapters on top of the model. \n\nAcquiring and preparing a dataset for instruction fine-tuning presents a significant challenge due to the extensive labor and resources required. There are several ways of acquiring instruction data, including manual dataset creation, using generative models (Wang et al., 2022;Taori et al., 2023), or using machine translation instruction data for training LLMs for specific languages (Cui et al., 2023). \n\nFine-tuning LLMs such as LLAMA-2 for specific tasks is an area of exploration as well.",
            "score": 0.5771145345198634,
            "section_title": "Related Work",
            "char_start_offset": 2827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05059814453125
        },
        {
            "corpus_id": "259252359",
            "title": "Math Word Problem Solving by Generating Linguistic Variants of Problem Statements",
            "text": "Although LLMs excel at natural language understanding and have serendipitous emergent reasoning abilities (Yang et al., 2023), they are still lackluster in complex reasoning tasks (Huang and Chang, 2022). Numerous studies on complex reasoning tasks have empirically demonstrated that the approach of fine-tuning smaller models is more effective (Ho et al., 2022) than adopting LLM prompting techniques like Chain of Thought (CoT) prompting (Wei et al., 2022).",
            "score": 0.565866179488699,
            "section_title": "Deep Learning-based Methods",
            "char_start_offset": 9380,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 459
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1490478515625
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "Conclusions: LLM post-training is a complex endeavor that involves improvements to instruction following, stylistic formatting, reasoning abilities, and general alignment to human preferences. LLMs can imitate the required style with \"superficial\" finetuning using a handful of examples, leading to the Superficial Alignment Hypothesis. However, a solely stylistic evaluation fails to characterize the many aspects of reasoning and task-specific capabilities that are key goals of finetuning. In fact, taskspecific skills & reasoning significantly improve after post-training with more examples compared to the pre-trained model. These improvements closely follow a power law in our experiments with the number of finetuning examples across multiple model families and sizes. We also see that these improvements are driven by the model's reasoning ability during generation, and are not limited to the model's alignment to formatting or style. In addition, we see that the win rate against other models can be a misleading metric to measure tasks that require complex reasoning, signaling the need for holistic evaluation programs leveraging standardized, objective benchmarks, in addition to measurement of alignment to human preferences. \n\nWe also observe that good post-training can help LLMs overcome problems associated with knowledge cutoff, by enabling them to better utilize knowledge from beyond the pre-training corpus either via further finetuning or RAG. These results put together highlight the qualitative and quantitative characteristics of post-training, and the role of data scaling in this.",
            "score": 0.5652572920771209,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 21311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09136962890625
        },
        {
            "corpus_id": "267500209",
            "title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
            "text": "Reasoning ability, as a basic ability of LLMs, has received great attention recently due to its great importance. Despite the notable improvements made by CoT (Wei et al., 2022), LLMs are still struggling with the tasks that require complex or high-order multi-step reasoning, such as logical reasoning and mathematic proof. Therefore, intensive research efforts have been dedicated to addressing the aforementioned issues. Generally, they can be categorized as follows. \n\nFine-tuning-based methods. These methods aim to improve the reasoning ability of LLMs through supervised fine-tuning. Usually, LLMs are fine-tuned by the samples which require manual labeling of reasoning processes, such as (Ouyang et al., 2022;Wang et al., 2022b). However, it can be labor-intensive due to the costly labeling of complex reasoning processes. The works of (Shridhar et al., 2022;Zelikman et al., 2022) first used LLMs to generate reasoning processes, but only the samples with correct results are selected for fine-tuning LLMs to reduce the labeling cost. Additionally, fine-tuned LLMs on specific tasks can suffer from the problem of \"catastrophic forgetting\", which means that the original knowledge inherited by the pre-trained LLMs will be lost and thus the ability to generalize to downstream tasks will be weakened. To this end, Cheng et al. (2023) trained a prompt retriever using the output scores of LLMs. When fine-tuning, LLMs are frozen just as a data labeler which effectively reduces the impact on LLMs. \n\nTool-based methods. Tool-based methods propose to utilize external tools to augment the capabilities of LLMs in accomplishing complex tasks (Qin et al., 2023;Schick et al., 2024). More-over, Jin et al. (2024); Yang et al. (2023) augment LLMs with external real-time knowledge or domain-specific information through specific tools. Additionally, Retrieval-Augmented Generation (RAG) related methods (Gao et al., 2023;Ma et al., 2023;Peng et al., 2024) have received a lot of attention recently, and these methods improve the reasoning ability of LLMs by incorporating external knowledge.",
            "score": 0.5649976400127099,
            "section_title": "Related Work",
            "char_start_offset": 25306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 177,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 697,
                    "end": 718,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 718,
                    "end": 737,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 869,
                    "end": 891,
                    "matchedPaperCorpusId": "247762790"
                },
                {
                    "start": 1325,
                    "end": 1344,
                    "matchedPaperCorpusId": "257532394"
                },
                {
                    "start": 1668,
                    "end": 1688,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 1701,
                    "end": 1718,
                    "matchedPaperCorpusId": "258298113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328369140625
        },
        {
            "corpus_id": "261696920",
            "title": "Evaluating the Deductive Competence of Large Language Models",
            "text": "1 Introduction \n\nThe development and availability of highly fluent large language models (LLMs) (i.e., (Brown et al., 2020;Devlin et al., 2019;Ouyang et al., 2022;Zhang et al., 2022)) has increased interest in assessing their reasoning and problem solving abilities (Bhargava and Ng, 2022;Geva et al., 2020;Jumelet et al., 2019;Mitchell, 2021;Trinh and Le, 2019;Webb et al., 2022). Despite considerable performance improvements on benchmark tasks, LLMs exhibit mixed results on reasoning tasks. Some research has suggested that LLMs may have emergent reasoning abilities that enable better performance than those of human subjects (Webb et al., 2022). Other research has suggested that LLM reasoning performance is inconsistent and task dependent. Such research has suggested that some tasks, such as four term analogy problems (Mikolov et al., 2013) and different natural language inference tasks (Williams et al., 2018), are simply easier to solve. Other types of reasoning tasks such as analogy generation (Bhavya et al., 2022) and deductive competence (Dasgupta et al., 2022) are more challenging. (Dasgupta et al., 2022) has investigated deductive competence in LLMs with characteristically mixed results. They demonstrated that one LLM, Chinchilla (Hoffmann et al., 2022), showed content effects on reasoning similar to human behavior documented in the cognitive science literature. For zero-shot performance, they found 50% accuracy for what they call realistic problems but chance accuracy for unrealistic problems. A 5-shot presentation resulted in some performance improvement for realistic problems, but performance on unrealistic problems remained low. \n\nIn this paper, we extend the previous research in several ways. First, we investigate the extent to which limited performance may be due to how the task was formatted. Prior research has demonstrated that overall performance can vary according to how a particular task is formatted (Gao et al., 2021;Jiang et al., 2021;Li and Liang, 2021;Shin et al., 2020).",
            "score": 0.5578149274127581,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 17,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 143,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 266,
                    "end": 289,
                    "matchedPaperCorpusId": "246430982"
                },
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "215548225"
                },
                {
                    "start": 307,
                    "end": 328,
                    "matchedPaperCorpusId": "202676782"
                },
                {
                    "start": 828,
                    "end": 850,
                    "matchedPaperCorpusId": "7478738"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1007080078125
        },
        {
            "corpus_id": "255096269",
            "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "text": "Recent work Kojima et al., 2022) has illustrated improvements in the performance of LLMs on reasoning tasks, when prompted to generate a reasoning chain in natural language before generating the answer. Based on these findings, we attempt to explicitly fine-tune LLMs to perform reasoning by compiling a set of 14 reasoning datasets (see Appendix A.1 for a list of  Figure 5: Effect of fine-tuning using reasoning datasets on each generalization level for OPT-IML 30B in a 5-shot setting, aggregated by task category. We experiment with adding 1%, 2% and 4% reasoning datasets by proportion. Note that the baseline for this experiment is based on a different proportion than other experiments.\n\nthese datasets), where the output includes a rationale before the answer and by including these datasets during instruction-tuning. This set includes the 9 datasets used by Chung et al. (2022b) in their CoT category as well as some additional datasets. Each dataset has a single prompt that uses an instruction, that explicitly asks the model to generate a reasoning chain (Kojima et al., 2022), followed by examples in the few-shot setting that illustrate how the reasoning chain should be produced before the answer. We show an example with such a prompt in Table 6. Using benchmark proportions of \"2/1/27/40/27/1/2\" as a baseline (see Section 4.3), we experiment with adding 1%, 2%, and 4% proportions of reasoning data (by reducing the proportion of the highest proportion benchmark i.e. SuperNatInst), and present results for the 5-shot setting in Figure 5 (full 0 and 5-shot results in Appendix Table 20) by generalization level and task category. We see a substantial performance improvement on the 2/14 held-out validation reasoning tasks (Rouge-L from 12.2% to 31.6%) when we instruction-tune with reasoning datasets, but alongside, we also see improvements on other held-out task categories such as Cause-Effect, Stereotype Detection, Toxicity Detection, and Word Analogy. Furthermore, adding 1% reasoning data results in the largest gains overall, beyond which, the gains start to reduce on MMLU",
            "score": 0.543557548118873,
            "section_title": "Effects of Adding Reasoning Datasets",
            "char_start_offset": 34247,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "266362350",
            "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions",
            "text": "Typically, there are three types of well-studied emergent abilities: in-context learning (ICL), instruction following, and step-by-step reasoning. In-context learning assumes that the language model has been provided with natural language instructions and/or several task demonstrations. LLMs can generate the expected output for test instances by completing the word sequence of input text without requiring additional training or gradient update, which is first introduced by GPT-3 [17]. Recent ICL research focuses on reducing inductive bias [116,201]. The instruction following ability means that LLMs are shown to perform well on unseen tasks that are also described in the form of instructions after fine-tuning with a mixture of multi-task datasets formatted via natural language descriptions, known as instruction tuning. Instruction tuning improves the generalization ability of LLMs. The LLMs are better aligned with human intentions [238]. Recent instruction tuning studies focus on how to align LLMs with tasks and user preferences effectively [161] and efficiently [290]. Step-by-step reasoning means that LLMs can solve complex tasks that require multi-step reasoning. Chain-of-Thought (CoT) [241] introduces intermediate steps of reasoning steps in prompt design. Least-to-most [291] breaks down reasoning steps into simpler problems. Self-consistency [234] prompting further enhances LLMs reasoning by ensembling diverse CoT reasoning paths. Tree-of-Thought (ToT) [257] and Graph-of-Thought (GoT) [12,258] enable LLMs to explore the thought processes in tree and graph structure, respectively. Moreover, preliminary explorations show that LLMs can use external tools [195], be parametric knowledge bases [163], have theory-of-mind [108], act as agents [230,247], have graph understanding abilities [226], and can serve as optimizers [251]. \n\nApart from using LLMs with frozen parameters, another line of work focuses on efficiently fine-tuning parameters in LLMs, namely parameter efficient fine-tuning, which helps LLMs efficiently adapt to specific tasks, datasets, and domain-specific understanding. Prefix tuning [126] keeps the language model parameters frozen and optimizes a small continuous task-specific vector called the prefix.",
            "score": 0.5427677857108536,
            "section_title": "Large Language Model",
            "char_start_offset": 19319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1056,
                    "end": 1061,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1206,
                    "end": 1211,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "To examine the impact of different fine-tuning methods on the inherent capabilities of LLMs, we evaluate the performance of models finetuned by LoRA and SPRING on several other (non-RAG) tasks.These tasks are commonly used to evaluate LLMs' reasoning, mathematical abilities, and world knowledge, including BoolQ (Clark et al., 2019), CommonsenseQA (Talmor et al., 2019), GSM8K (Cobbe et al., 2021), andMMLU (Hendrycks et al., 2021).The experimental results are shown in Table 2. 4 From the results, we can observe: (1) Thanks to the plugand-play design of our method, SPRING can revert to to the original LLMs by not using virtual tokens.Therefore, it successfully preserves the original capabilities of the LLMs.In contrast, LoRA, which adjusts the model's parameters for RAG tasks, inevitably compromises the model's performance on other tasks.(2) A noticeable decline is observed in the few-shot evaluation, reflecting a decrease in the in-context learning abilities of LLMs.This decline may stem from the fact that RAG fine-tuning does not incorporate in-context learning capabilities.Besides, fine-tuning for RAG tasks may lead the model to overfit to specific task formats, thereby impairing its general generation abilities (more empirical studies are detailed in Appendix G).",
            "score": 0.5419383751775001,
            "section_title": "Performance on Other Tasks",
            "char_start_offset": 20033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 847
                },
                {
                    "start": 847,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1284
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 333,
                    "matchedPaperCorpusId": "165163607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4990234375
        },
        {
            "corpus_id": "273638438",
            "title": "O1 Replication Journey: A Strategic Progress Report - Part 1",
            "text": "Self-improvement in LLM Self-improvement methods for LLMs aim to enhance model performance by enabling them to learn from their own outputs with minimal human intervention. These approaches typically involve supervised fine-tuning (SFT) on high-quality outputs generated by the models ( ), highlighting their potential for driving further advancements in LLM capabilities. However, recent findings suggest that LLM-generated texts often exhibit truncated \"tails\", meaning that the distribution of generated outputs lacks the variability found in human-generated content, particularly in the less common, outlier responses (or \"tails\" of the distribution) (Shumailov et al., 2024;Dohmatob et al., 2024). This reduced variability can lead to a phenomenon known as model collapse, where the model converges toward a narrower range of behaviors, ultimately harming performance (Shumailov et al., 2024). This issue has been observed in tasks like language modeling (Shumailov et al., 2024) and iterative preference optimization for mathematical reasoning (Wu et al., 2024b). To mitigate the risk of model collapse, researchers recommend maintaining a balanced mix of clean, human-authored data alongside LLM-generated content during training (Shumailov et al., 2024;Dohmatob et al., 2024;Gerstgrasser et al., 2024). This approach helps preserve diversity and prevents the model from degrading in performance over time.",
            "score": 0.5371326463306274,
            "section_title": "Internal Thought",
            "char_start_offset": 19726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33056640625
        },
        {
            "corpus_id": "272366724",
            "title": "Enhancing Event Reasoning in Large Language Models through Instruction Fine-Tuning with Semantic Causal Graphs",
            "text": "Improving LLM performance on specific tasks faces challenges when pretraining hasn't provided the necessary skills or domain-specific knowledge. Various approaches have been proposed to address this issue, each targeting different aspects of model capabilities. Few-shot learning (Brown et al. 2020) provides the LLM with task examples, leveraging the model's ability to adapt to new tasks with minimal guidance. Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) augments the model with external knowledge, enabling access to information beyond its training data. Chain-of-Thought (CoT) prompting (Wei et al. 2022) guides LLMs to break down complex problems into intermediate steps, enhancing their reasoning abilities. \n\nMore fundamental model training techniques have also been developed to align LLMs with specific tasks or desired behaviors. These include Instruction Tuning (Ouyang et al. 2022), which fine-tunes models on diverse task instructions, Reinforcement Learning with Human Feedback (RLHF) (Stiennon et al. 2020), which optimizes model outputs based on human preferences, and Direct Preference Optimization (DPO) (Rafailov et al. 2024), which efficiently adapts model parameters using positive and negative examples. Despite these advancements, there remains a significant gap in explicitly enhancing LLMs' understanding of causal relationships, which could be crucial for tasks requiring such capabilities.",
            "score": 0.5366521082567931,
            "section_title": "Enhancing LLM Task Performance",
            "char_start_offset": 5932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 298,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 450,
                    "end": 468,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 886,
                    "end": 906,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1012,
                    "end": 1034,
                    "matchedPaperCorpusId": "268417191"
                },
                {
                    "start": 1135,
                    "end": 1157,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.100341796875
        },
        {
            "corpus_id": "265609599",
            "title": "ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions",
            "text": "Recently, LLMs have shown remarkable zero-shot and few-shot abilities in various language generation contexts [2,25,39].However, they still face challenges in more complex tasks like mathematical reasoning [9,11], often requiring expensive computational resources for fine-tuning.To address this, researchers have been exploring novel prompting methods to instruct LLMs in these tasks, including chain-of-thought (CoT) prompting [40].This enables LLMs to perform intermediate reasoning steps, significantly enhances LLMs' reasoning abilities, especially for complex mathematical and decision-making tasks.",
            "score": 0.5346248430435061,
            "section_title": "RELATED WORK 2.1 Large Language Models & Prompting",
            "char_start_offset": 3530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 120,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 605
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060089111328125
        },
        {
            "corpus_id": "265609639",
            "title": "Data Management For Training Large Language Models: A Survey",
            "text": "Since LLMs have shown surprisingly emergent abilities in handling various NLP tasks, multitask fine-tuning appears to be promising to improve LLMs' generalization performance on unseen tasks. The benefits of increasing the number of tasks in SFT have been experimentally proven on models with different sizes ranging from 3B to 540B parameters (Wang et al., 2022;Sanh et al., 2022;Wei et al., 2021;Chung et al., 2022). With the scaling of tasks, the mixture ratio of data targeting different tasks is also found to be critical and usually decided by experiments and intuitions (Iyer et al., 2022;Longpre et al., 2023a). To enable LLMs to solve targeted tasks with specific skills, representation similarity (Ivison et al., 2023;Lee et al., 2024) and gradient similarity (Xia et al., 2024) is proposed to select relevant multitask subsets. \n\nHowever, conflicts might exist among the many tasks. Dong et al. (2023) focus on task composition among mathematical reasoning, code generation, and general human-aligning abilities. They find that model abilities are improved when the mixed data amount is small but decreased otherwise. The negative impact of large amount mixing data might lie in the similarity degree of data format and data distribution among different SFT tasks. Wang et al. (2023b) also experimentally show that different instruction datasets may correspond to different specific abilities. And winning across all evaluations using a single dataset or combination seems to be challenging. \n\nDivergent from compositing multiple tasks, some works claim that integration of LLMs tuned on single task data can outperform one LLM tuned on multiple tasks (Jang et al., 2023;Chen et al., 2023b). But fine-tuning more task-specific LLMs also means more resource consumption. How to efficiently equip LLMs with the ability to solve multiple tasks still demands more exploration.",
            "score": 0.5344219485978667,
            "section_title": "Task Composition",
            "char_start_offset": 17280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 344,
                    "end": 363,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 381,
                    "end": 398,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 596,
                    "end": 618,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 1663,
                    "end": 1682,
                    "matchedPaperCorpusId": "256627673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.091552734375
        },
        {
            "corpus_id": "270562085",
            "title": "GUARD-D-LLM: An LLM-Based Risk Assessment Engine for the Downstream uses of LLMs",
            "text": "The emergence of Large Language Models (LLMs) has revolutionized the field of natural language processing, leading to significant advancements in various downstream tasks.Notable LLMs such as GPT4 [30], LLama2 [46], Falcon [34], and Mistral [17] have garnered attention due to their adaptability and effectiveness across diverse applications.Research focusing on open-sourced LLMs [7] has contributed to democratizing advanced language models, enabling widespread access and utilization.\n\nThese LLMs have been instrumental in driving innovations in several key areas, including adapting LLMs, prompt engineering, emergent abilities, and optimization strategies.In the area of adaption of LLMs, advanced techniques such as leveraging large context length [9] [18] [37], and fine-tuning methods like instruction tuning [54], alignment tuning, & memory-efficient model adaptation [1] [20] [25] have emerged.These approaches allow LLMs to be fine-tuned for specific tasks and domains, enhancing their performance and applicability to targeted applications.\n\nAdditionally, prompt engineering techniques such as in-context learning [22], chain-of-thought [47] [48], tree-of-thought [52], and planning have facilitated the development of more sophisticated and context-aware language models [23] [51] [55].These techniques enable LLMs to understand and generate language in a more nuanced and coherent manner, enhancing their practical utility through efficient adaptation to various contexts.\n\nMoreover, emergent abilities such as in-context learning, instruction following, and step-by-step reasoning (CoT) have expanded the capabilities of LLMs, enabling them to comprehend and reason through complex language inputs effectively.Fine-tuning methods [9] [31] [36] [53] play a crucial role in tailoring these emergent abilities to specific tasks, ensuring optimal performance in diverse scenarios.Furthermore, optimization strategies, including prompt optimization techniques like vLLM [20] or LLMLingua , and inference optimization strategies, have enhanced the efficiency and performance of LLMs in processing and generating language.Efficient approaches to fine-tuning contribute significantly to the optimization of these models, ensuring that they can adapt quickly and effectively to changing requirements.",
            "score": 0.533931650014714,
            "section_title": "Emergence of LLMs",
            "char_start_offset": 1459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 342
                },
                {
                    "start": 342,
                    "end": 487
                },
                {
                    "start": 489,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1052
                },
                {
                    "start": 1054,
                    "end": 1299
                },
                {
                    "start": 1299,
                    "end": 1486
                },
                {
                    "start": 1488,
                    "end": 1725
                },
                {
                    "start": 1725,
                    "end": 1891
                },
                {
                    "start": 1891,
                    "end": 2130
                },
                {
                    "start": 2130,
                    "end": 2306
                }
            ],
            "ref_mentions": [
                {
                    "start": 763,
                    "end": 767,
                    "matchedPaperCorpusId": "258686160"
                },
                {
                    "start": 886,
                    "end": 890,
                    "matchedPaperCorpusId": "266551872"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0755615234375
        },
        {
            "corpus_id": "259924436",
            "title": "A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models",
            "text": "To improve the efficacy of language models, researchers have dedicated to scaling up language models [10,82], by increasing the number of model parameters, expanding the size of training datasets, and enhancing computational resources. These lead to the emergence of large language models, namely transformer-based language models with billions of parameters and pre-trained with large amounts of text data. LLMs usually have the same neural network understanding, generation, and reasoning compared to smaller PLMs. They exhibit significantly enhanced abilities in handling complex language tasks. Moreover, researchers have found that LLMs have the emergent ability (the ability not presented in small PLMs but LLMs [118]) of in-context learning, namely the capability to perform unseen tasks without any specific training data or with only a few training samples, by following natural language instructions. \n\nBased on representative LLMs such as GPT-3 [96], recent efforts have been explored in the following directions to improve the capability of LLMs: \n\nManuscript submitted to ACM \n\n\u2022 Instruction Tuning To enhance the generalization and in-context learning capabilities of LLMs, researchers such as T0 [100] and FLAN [117], further explored fine-tuning LLMs with diverse tasks expressed by natural language instruction, namely instruction tuning. The instruction tuning, makes LLMs to better understand and respond to instructions, enabling them to generalize more effectively to new tasks. During the instruction tuning process, researchers [15] have found certain factors including scaling the number of tasks, and model size and using chain-of-thought tuning data, that contribute to improving LLMs' generalization ability and reasoning ability on complex tasks. \u2022 Human Alignment In addition to the capacity to follow instructions, it's crucial to ensure that large language models (LLMs) are guided by human preferences, avoiding generating unfaithful and toxic information [106]. \n\nTo achieve this, studies such as InstructGPT [82], have explored fine-tuning LLMs with reinforcement learning from human feedback (RLHF) [14], where the human preferences are used as the reward signal to fine-tune LLMs. RLHF has demonstrated effectiveness in improving the truthfulness and toxicity of LLMs.",
            "score": 0.5306150026591628,
            "section_title": "Large Language Models",
            "char_start_offset": 19293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1211,
                    "end": 1216,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1226,
                    "end": 1231,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1988,
                    "end": 1993,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 2042,
                    "end": 2046,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0634765625
        },
        {
            "corpus_id": "276812856",
            "title": "Uncovering inequalities in new knowledge learning by large language models across different languages",
            "text": "This presents a significant disadvantage for users relying on low-resource languages when new knowledge is introduced in other languages. Finally, the response accuracy in the in-context learning setting is higher than that in the fine-tuning setting, which aligns with existing research findings (30,31). For example, one study evaluated the performance of LLMs using fine-tuning and in-context learning in few-shot computational social science tasks and found that models using in-context learning generally outperformed those that were fine-tuned (30). A possible explanation is that, in in-context learning, LLMs can leverage their pre-trained knowledge and general reasoning abilities to quickly comprehend and adapt to specific tasks. In contrast, fine-tuning may sometimes diminish their reasoning capabilities (31,32).",
            "score": 0.5287659141592225,
            "section_title": "Equally Transferable?",
            "char_start_offset": 14005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 826
                }
            ],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "268248396"
                },
                {
                    "start": 818,
                    "end": 822,
                    "matchedPaperCorpusId": "268248396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41015625
        },
        {
            "corpus_id": "269921932",
            "title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning",
            "text": "Despite the impressive performance of LLMs with in-context learning, certain scenarios still necessitate fine-tuning, which can be broadly categorized into three types.The first type, instruction tuning, aims to better align LLMs with end tasks and user preferences, without significantly enhancing the knowledge and capabilities of LLMs (Zhou et al., 2024).This approach simplifies the process of dealing with varied tasks and understanding complex instructions.The second type involves complex reasoning tasks such as mathematical problemsolving (Collins et al., 2023;Imani et al., 2023;Yu et al., 2023), where general instruction tuning often falls short in handling complex, symbolic, multistep reasoning tasks.To improve the reasoning abilities of LLMs, the majority of research focuses on creating corresponding training datasets, either by leveraging larger teacher models like GPT-4 (Fu et al., 2023), or by rephrasing questions along a reasoning path (Yu et al., 2023).The third type, continual pretraining (Cheng et al., 2023;Chen et al., 2023;Han et al., 2023;Liu et al., 2023), aims to enhance the domain-specific capabilities of LLMs.Unlike instruction tuning, it necessitates fine-tuning to augment the corresponding domainspecific knowledge and capabilities.However, most variants of LoRA (Kopiczko et al., 2023;Lialin et al., 2023;Dettmers et al., 2024;Zhu et al., 2024) predominantly employ instruction tuning or text classification tasks from GLUE (Wang et al., 2018) to validate their efficacy on LLMs.Given that instruction tuning requires the least capacity for fine-tuning compared to other types, it may not accurately reflect the effectiveness of LoRA variants.To better evaluate their methods, recent works (Meng et al., 2024;Liu et al., 2024;Shi et al., 2024;Renduchintala et al., 2023) have employed reasoning tasks to test their methods.But the training sets used are often too small for LLMs to effectively learn reasoning.",
            "score": 0.5233525225561289,
            "section_title": "Fine-Tuning with LLMs",
            "char_start_offset": 5765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1521
                },
                {
                    "start": 1521,
                    "end": 1685
                },
                {
                    "start": 1685,
                    "end": 1865
                },
                {
                    "start": 1865,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 891,
                    "end": 908,
                    "matchedPaperCorpusId": "256390607"
                },
                {
                    "start": 1071,
                    "end": 1088,
                    "matchedPaperCorpusId": "264833257"
                },
                {
                    "start": 1347,
                    "end": 1369,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 1751,
                    "end": 1768,
                    "matchedPaperCorpusId": "264833257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08587646484375
        },
        {
            "corpus_id": "269982787",
            "title": "Can LLMs Solve longer Math Word Problems Better?",
            "text": "To deeply understand the underlying factors contributing to the improvements of our methods across various LLMs, we conduct a more fine-grained analysis on semantic understanding and math reasoning abilities of LLMs in solving long MWPs. We leverage informativeness and missing step metrics derived from (Golovneva et al., 2023) to capture these two facets, respectively (see also Section 2.3). The results are presented in Figure 4. \n\nFrom Figure 4, we observe a performance decline in both informativeness and missing step values across all LLMs from one round to the next, indicating that both their problem understanding and mathematical reasoning abilities are negatively impacted by longer contexts, with a more pronounced drop in understanding, collectively leading to the accuracy degradation in solving longer MWPs. Furthermore, both CoRe and extension enhance the math reasoning ability (shown by the missing step value) of proprietary and open-source LLMs, respectively. The improved math reasoning ability accounts for the improvement of problem solving accuracy in Table 2, which underscores the validity of our motivations and the effectiveness of our methods. \n\nIn contrast, we observe that the semantic understanding of both proprietary and open-source LLMs remains generally unchanged after applying CoRe or extension, suggesting that neither prompting techniques nor supervised fine-tuning significantly impact the semantic understanding capabilities of LLMs. Based on this observation, we hypothesize that the language understanding ability of LLMs is predominantly established during the pre-training stage and is minimally influenced by post-training adjustments or prompts, which differs from reasoning skills, which can be optimized by employing specific reasoning patterns in prompts or through fine-tuning with additional data.",
            "score": 0.5229640116418706,
            "section_title": "FINE-GRAINED ANALYSIS ON SEMANTIC UNDERSTANDING AND MATH REASONING",
            "char_start_offset": 20111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 328,
                    "matchedPaperCorpusId": "254685985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22802734375
        },
        {
            "corpus_id": "258840866",
            "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning",
            "text": "In recent years, LLMs have revolutionized the landscape of natural language processing, showcasing remarkable capabilities across various tasks (Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022;Touvron et al., 2023). These models require vast quantities of data and extensive pre-training. However, scaling up the size of the model and data incurs a resource cost (Kaplan et al., 2020;Sorscher et al., 2022), and brings challenges for deployment in real-world applications. Few organizations have sufficient computational devices to fine-tune or re-train LLMs for specific domains. \n\nDistilling LLMs to specific domains may provide a resource-friendly and effective research pathway. This approach allows us to reduce the size of LLMs, yielding smaller, domain-specific models with comparable performance. \n\nIn this paper, we focus on how to distill the reasoning ability of LLMs into smaller models while maintaining good performance. Prior works utilized LLMs to synthesize data and then fine-tune smaller models, or aligned predicted distribution to distill LLMs (Ho et al., 2022;Fu et al., 2023;Hsieh et al., 2023;Wang et al., 2023;Kang et al., 2024). The data synthesis paradigm is inspired by chainof-thought (CoT, Wei et al. (2022)) prompting in LLMs. CoT prompting elicits LLMs to generate intermediate steps, which significantly improves reasoning performance. Then, data synthesis entails LLMs generating CoT that are collated into downstream fine-tuning datasets. These CoT data are used to fine-tune smaller models, thereby transferring the reasoning ability. However, as illustrated in Figure 1, LLMs frequently produce faulty reasoning, i.e., they may provide the correct final answer but incorrect intermediate reasoning steps (d'Avila Garcez and Lamb, 2020;Frieder et al., 2023). Such faulty reasoning in datasets confuses the small models in fine-tuning and hinders the learning of reasoning abilities. Additionally, off-the-shelf powerful LLMs are black-box (e.g., ChatGPT) and inaccessible for prediction distribution.",
            "score": 0.5224658643828921,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 164,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 397,
                    "end": 419,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1111,
                    "end": 1130,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 1233,
                    "end": 1250,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09417724609375
        },
        {
            "corpus_id": "260202767",
            "title": "ArcGPT: A Large Language Model Tailored for Real-world Archival Applications",
            "text": "While these benchmarks have made significant progress in evaluating NLU tasks, their primary focus has been on assessing language skills. As a result, they have become less commonly used as benchmarks for LLMs, as many of these models are now capable of generating fluent and plausible language (Li et al., 2023a). Meanwhile, various benchmarks have been proposed to evaluate LLMs' performance in different aspects, including question answering (Rajpurkar et al., 2018;Kwiatkowski et al., 2019;Li et al., 2022), knowledge reasoning (Clark et al., 2018;Talmor et al., 2019;Sawada et al., 2023), and code generation (Chen et al., 2021;Austin et al., 2021). While general-purpose evaluation bench-marks have been instrumental in evaluating the overall language capabilities of LLMs, they may not be able to capture the nuances and complexities of specific domains. As a result, these benchmarks may have limitations when it comes to assessing the performance of LLMs in specific domains.",
            "score": 0.5155868596097216,
            "section_title": "General LLMs and General-purpose Evaluation Benchmarks",
            "char_start_offset": 9053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 984
                }
            ],
            "ref_mentions": [
                {
                    "start": 445,
                    "end": 469,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 469,
                    "end": 494,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0482177734375
        },
        {
            "corpus_id": "276903421",
            "title": "Emergent Abilities in Large Language Models: A Survey",
            "text": "Recent research has introduced various prompting strategies to enhance language model capabilities. These include tailored prompts, such as chain-of-thought (CoT) prompting, which guides multi-step reasoning, and advanced fine-tuning approaches, such as instruction following, which adapts models to specific tasks. Wei et al. [87] found that certain techniques led to sudden jumps in performance, particularly in large models. For instance, CoT prompting significantly improved performance in math word problems because these problems require step-by-step reasoning, which is exactly the type of thinking CoT induces. \n\nNot just prompting but also fine-tuning strategies have shown emergent effects. Wei et al. [87] further demonstrated that instruction tuning [61] [86], where tasks are framed as instructions, and scratchpad reasoning [58], which predicts intermediate steps, yield substantial performance boosts, but only in large-scale models (100B+ parameters). Lu et al. [55] explored this phenomenon further, disentangling the effects of few-shot prompting, instruction tuning, and CoT prompting to assess emergent abilities in isolation. They questioned whether instruction-tuned LLMs genuinely develop reasoning abilities or simply perform better due to learned heuristics. \n\nTheir experiments, conducted on four model families (GPT-3, T5, LLaMA, and Falcon) across 22 tasks, revealed that without few-shot prompting, these models showed no emergent abilities, performing only marginally better than random guessing, except in two cases: Hindu Knowledge (which relies on memory) and Nonsense Word Grammar (which tests formal linguistic abilities rather than functional reasoning). They concluded that in-context learning (i.e., few-shot prompting) is essential for emergent functional abilities, and while instruction tuning improves general performance, it does not lead to genuine reasoning. \n\nWhile [55] provided valuable insights at the time, it is important to recognize that the field of LLMs has advanced rapidly since their research. More recent models, such as OpenAI o3-mini, Claude 3.5, Gemini 2.0, and DeepSeek-R1, have achieved remarkable advances, calling into question the relevance of its findings. Emerging abilities studies have consistently shown that larger, better-trained models can exhibit fundamentally different and often unpredictable behaviors.",
            "score": 0.5152966757350639,
            "section_title": "B. Relationship between Emergent Abilities and Prompt Strategies",
            "char_start_offset": 22351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203369140625
        },
        {
            "corpus_id": "271916117",
            "title": "CIPHER: Cybersecurity Intelligent Penetration-Testing Helper for Ethical Researcher",
            "text": "Techniques like instruction fine-tuning (FLAN [20]) and model mimicry (Orca [21]) further improve reasoning capabilities. Despite advancements, challenges in LLM reasoning persist, motivating ongoing research [7,22]. \n\nLLM-Based Chatbots: LLM-based chatbots like ChatGPT excel in customer support, education, and complex problem-solving by synthesizing large volumes of information into detailed responses. However, they lack the specialized knowledge required for offensive penetration testing [23]. \n\nSupervised Fine-Tuning: Supervised fine-tuning enhances model performance on domain-specific datasets, particularly in areas like penetration testing, ensuring accurate application of specialized language. Unlike Retrieval-Augmented Generation (RAG), finetuning improves the model's domain comprehension [24]. \n\nLLM Alignment: Efforts to align language models with human values focus on ensuring these models exhibit traits such as helpfulness and truthfulness. Reinforcement Learning with Human Feedback (RLHF) fine-tunes large language models (LLMs) to achieve this alignment. This process involves training a reward model to predict human preferences and then using algorithms like Proximal Policy Optimization (PPO) for fine-tuning. Although PPO generally yields better results, Direct Preference Optimization (DPO) simplifies the process by fine-tuning directly with human-labeled preferences [25,26]. \n\nIncorporating Domain-Specific Knowledge: Domain-specific knowledge enhances LLM accuracy in specialized fields like medicine [27] and cybersecurity. Techniques such as Domain-Adaptive Pretraining (DAPT) and adaptive fine-tuning (AdaptLLM) are crucial for developing specialized models tailored for specific tasks, leveraging domain-specific datasets for improved insights [24,28].",
            "score": 0.5133851146807499,
            "section_title": "Background and Related Works",
            "char_start_offset": 7866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1405,
                    "end": 1408,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10540771484375
        },
        {
            "corpus_id": "269484653",
            "title": "Is Temperature the Creativity Parameter of Large Language Models?",
            "text": "The inherent complexity of creativity means it is highly unlikely that there exists such an easy solution as a single parameter that enables creativity in LLMs.Of course, careful fine-tuning for specific tasks or prompting might produce the desired outcomes, however, fine-tuning comes at the cost of generality, and prompting is unpredictable and inconsistent across different models.From this work, we suggest future work that could be useful for progressing LLM creativity.\n\nBenchmarks for Creativity In general, the creative abilities of LLMs are mostly evaluated on tests from psychology (e.g.Torrance Test of Creativity Thinking, Alternative Uses Test, Divergent Association Test), or inferred from phenomenological observations in benchmarks of other tasks, such as mathematical reasoning (Bubeck et al. 2023).To our knowledge, there are no strong LLM creativity benchmarks that go further.We only presented a minimal case and reliably scaling it up is challenging, and this is partly due to the complexity of evaluating creativity, but it is important to investigate what such benchmarks should be, to make more robust claims about the creativity of LLMs.\n\nDecoding Strategies More advanced decoding strategies could be interesting if designed for specific purposes.Decoding strategies are vital for the LLM to produce fluent natural language, and might be similarly helpful for producing creative writing or communication.Besides the two well-known parameters, top-k and nucleus sampling (Holtzman et al. 2020), there are more complex decoding strategies, namely mirostat (Basu et al. 2021) and locally typical sampling (Meister et al. 2023).A decoding strategy designed with an information-theoretic notion of creativity (Mondol and Brown 2021) in mind might be fruitful to enable more creative behaviours regardless of model and prompt.\n\nImplicit Information LLMs appear to capture lots of implicit information.An important direction of research here is how information is preserved as we increase the complexity of the prompt for specific tasks.For example, by asking a specific questions following a taxonomy, we could observe how much information is implicit in the model.This in turn might inform how to design the prompt, and condition the LLM to maximise the quality of the desired output.",
            "score": 0.5129835493312637,
            "section_title": "Towards More Creativity in LLMs",
            "char_start_offset": 37505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 476
                },
                {
                    "start": 478,
                    "end": 598
                },
                {
                    "start": 598,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1163
                },
                {
                    "start": 1165,
                    "end": 1274
                },
                {
                    "start": 1274,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1651
                },
                {
                    "start": 1651,
                    "end": 1847
                },
                {
                    "start": 1849,
                    "end": 1922
                },
                {
                    "start": 1922,
                    "end": 2057
                },
                {
                    "start": 2057,
                    "end": 2186
                },
                {
                    "start": 2186,
                    "end": 2306
                }
            ],
            "ref_mentions": [
                {
                    "start": 1581,
                    "end": 1598,
                    "matchedPaperCorpusId": "231636243"
                },
                {
                    "start": 1629,
                    "end": 1650,
                    "matchedPaperCorpusId": "252918666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2015380859375
        },
        {
            "corpus_id": "268385476",
            "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
            "text": "While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs\u2019 generalization ability are not fully understood.This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets.Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model\u2019s generalization ability.Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.",
            "score": 0.5125993307532861,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32958984375
        },
        {
            "corpus_id": "268666984",
            "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach",
            "text": "Our study presents findings that challenge certain established conclusions regarding the evaluation of LLMs in previous research.First, we question the purported superiority of instruction-tuning over fine-tuning.While previous studies (Wei et al., 2021;Liang et al., 2022;Zhao et al., 2023) indicate that instruction-tuned models generally outperform base models, our data does not support this assertion.According to our ANOVA and Tukey tests, no significant differences were observed between instruction-tune, fine-tune, and RL-tune across six evaluation datasets.Second, regarding the performance of small-sized, open-source models in mathematical reasoning, Zhao et al. (2023) reported their underperformance.However, referring back to Fig. 3, if we define the parameter range from the first to the third quartile as small-sized, these models exhibit comparable performance in mathematical reasoning tasks (e.g., GMS8K) to their larger-scaled counterparts.Third, Zhao et al. (2023) claim that the 'Llama' model outperforms others is not corroborated by our analysis.We found that 'Llama', along with other models like 'GPT2', 'Minstral', and 'Falcon', show equivalent proficiency in complex reasoning tasks (GMS8K and ARC).Fourth, while Zhao et al. (2023) suggested that scaling up open-source models consistently enhances performance, our findings indicate that this may be task-dependent.In the 'TruthfulQA' task, increasing model parameters led to diminished performance.Moreover, as parameter sizes grow much larger, their effects become unpredictable.This suggests that scaling up models within a certain range can consistently improve performance, but beyond that range, the outcomes become uncertain.\n\nNext, we discuss emergent abilities.The emergence of some advanced capabilities in LLMs might be attributed to their training, as inferred from comparisons with smaller-sized language models.However, the presence of certain abilities in the majority of LLMs does not necessarily imply that these abilities are intrinsic characteristics.As illustrated in Fig. 3, these abilities manifest even with minimal parameters (as low as 0.01 billion).",
            "score": 0.5124103432400329,
            "section_title": "Discussion",
            "char_start_offset": 29729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 213
                },
                {
                    "start": 213,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1071
                },
                {
                    "start": 1071,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1479
                },
                {
                    "start": 1479,
                    "end": 1561
                },
                {
                    "start": 1561,
                    "end": 1712
                },
                {
                    "start": 1714,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1905
                },
                {
                    "start": 1905,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2155
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0638427734375
        },
        {
            "corpus_id": "268248940",
            "title": "PHAnToM: Persona-Based Prompting Has an Effect on Theory-of-Mind Reasoning in Large Language Models",
            "text": "Multiple research studies have shown the brittleness of LLMs to the input prompts. Zero shot Chain-of-Thought (incorporating one-line in prompts, like \"First,\" or \"Let's think step by step\") (Kojima et al. 2022;Bsharat, Myrzakhan, and Shen 2023) has empirically allowed LLMs to become stronger reasoners, especially for arithmetic tasks. In other works, strategies like role-play (including a description of someone the LLM should embody) (Kong et al. 2023) or threats (reminding the LLM they would be penalized if they answer wrongly, or that the users' life matters gravely on this answer) (Bsharat, Myrzakhan, and Shen 2023) have also demonstrated effectiveness in improving LLM performance. Sclar et al. (2023) find that small prompt variations often yield large performance differences. Wu et al. (2023) showed that with Instruction Fine-tuning, LLMs can distinguish instruction with context and focus more on instructions. They further show that instruction fine-tuning encourages self-attention heads to encode more word-word relations related to instruction verbs. Gupta et al. (2024) found that LLM's reasoning abilities can be affected by persona prompts across different socio-demographic groups (race, gender, religion, disability, and political affiliation). Encouraged by these findings, we were inspired to examine the sensitivities of LLMs to personality role-play via prompting on socio-cognitive reasoning in LLMs.",
            "score": 0.5108438786531606,
            "section_title": "Sensitivity of LLMs to Prompts",
            "char_start_offset": 4831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1432
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 211,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 695,
                    "end": 714,
                    "matchedPaperCorpusId": "264172710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08868408203125
        },
        {
            "corpus_id": "265609811",
            "title": "Jellyfish: A Large Language Model for Data Preprocessing",
            "text": "With advancements in the field of natural language processing (NLP), LLMs have become one of the hottest topics in the AI research community. Representative LLMs include OpenAI's GPT series (in particular, GPT-3, 3.5, and 4), Anthropic's Claude, Google's Gemini, Mistral AI's Mistral [36], Meta's Llama [93] series, as well as their variants that can be found at Hugging Face [34]. Due to their superb ability to process natural language, LLMs have not only been used in NLP applications (e.g., ChatGPT and Claude), but also catalyzed the rise of LLM-powered autonomous agents [97] as AI assistants (e.g., by GPTs) or tools for engineering [77,30] or simulation [105,104] purposes. Another popular LLM-centric research direction is retrieval-augmented generation (RAG) [50,51], which gives LLMs access to external information to improve generation performance. We refer readers to [115] for a survey on LLMs. Some LLMs are open-source (e.g., Llama and Llama 2), and they can be fine-tuned with additional tasks to improve their abilities in logical reasoning, question answering, and so on. Among these fine-tuning approaches, instruction tuning [111] has become a prevalent one which further trains LLMs on a dataset consisting of (instruction, output) pairs in a supervised fashion, hence bridging the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. For efficiency of fine-tuning, parameter-efficient fine-tuning (PEFT) approaches enable adaptation of LLMs to downstream applications without fine-tuning all the parameters. Notable methods are adapter tuning [31], prefix-tuning [54], and low-rank adaptation (LoRA) [32]. In particular, LoRA achieves significantly fewer trainable parameters and no additional inference latency, and has become a prevalent PEFT approach.",
            "score": 0.5102380714413122,
            "section_title": "Large Language Models",
            "char_start_offset": 10031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1849
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1638,
                    "end": 1642,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03802490234375
        },
        {
            "corpus_id": "276079902",
            "title": "We're Different, We're the Same: Creative Homogeneity Across LLMs",
            "text": "Large language models (LLMs) have moved out of research labs and into our everyday lives. Given their advanced abilities to generate text and respond to prompts, LLMs are often marketed as creativity support tools that allow users to write drafts, edit documents, and generate novel ideas with ease [2, 4,20,21]. Consumers have responded eagerly to these suggestions. According to a 2024 survey by Adobe, over half of Americans have used generative AI tools like LLMs as creative partners for brainstorming, drafting written content, creating images, or writing code. An overwhelming majority of LLM users surveyed believe these models will help them be more creative [39]. \n\nWhile appealing, outsourcing our creative thinking to LLMs could have unintended consequences and demands further scrutiny. For example, recent work has unearthed complications around the use of LLMs as creativity support tools. Researchers found that LLM-aided creative outputs look individually creative but are often quite similar to other LLM-aided outputs. Such \"homogeneity\" in LLM-aided creative outputs has been observed in a variety of settings, from creative writing to online survey responses to research idea generation and beyond [7,16,37,43,53]. \n\nWhile concerning, these works typically only look at a single LLM and it's effect on downstream creative content. \n\nIn a prototypical example, Doshi and Hauser [16] compared the individual and collective creativity of two groups of writers-humans alone and humans aided by ChatGPT-and found that stories produced by the ChatGPT-aided group were more homogeneous. Related work from Moon, Green, and Kushlev [37] compared college essays written by humans and GPT models and found that LLM-authored essays contributed fewer new ideas and were more homogeneous than human-authored essays. However, such work begs the question: does the observed homogeneity occur because only a single type of LLM (GPT variants) is studied? It could be reasonably argued that a single LLM must have a limited range of outputs, causing the homogeneity. Perhaps if writers all used different LLMs, creativity would be restored. \n\nRecent work studying feature space alignment in LLMs suggests otherwise.",
            "score": 0.5102248474191756,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2142
                },
                {
                    "start": 2145,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 1219,
                    "end": 1222,
                    "matchedPaperCorpusId": "267406608"
                },
                {
                    "start": 1222,
                    "end": 1225,
                    "matchedPaperCorpusId": "271119565"
                },
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "271119565"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1600341796875
        },
        {
            "corpus_id": "262054014",
            "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
            "text": "We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",
            "score": 0.5085084129956303,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.286376953125
        },
        {
            "corpus_id": "276885187",
            "title": "Cross-linguistic disagreement as a conflict of semantic alignment norms in multilingual AI~Linguistic Diversity as a Problem for Philosophy, Cognitive Science, and AI~",
            "text": "When the responses of such LLMs are LLM-consistent but neither CL-nor Folkconsistent, however, that pattern still deserves respect, providing insight based on large empirical data and playing a normative role (since that is presumably not a product of fine-tuning, given that AI developers do not share motivations with other developers to align their LLMs in a particular pattern). \n\nOn the other hand, if LLM-consistency does not hold, only those responses with some specific patterns are plausibly regarded as results of fine-tuning there (considering that AI developers do not have motivations to fine-tune their LLMs to an arbitrary pattern of response). In particular, a conversational AI whose responses are CL-or Folk-consistent, is likely a product of fine-tuning (since, given the responses of other LLMs, such patterns do not naturally emerge). Thus, once the failure of LLM-consistency is recognized, the interest of the study shifts from the possible normative answers provided by LLMs to questions concerning how LLMs are trained and the ideals of their developers. \n\nToday, there are numerous benchmark tests for LLMs to evaluate their performance (e.g., the ARC Benchmark for reasoning capabilities, HellaSwag for commonsense reasoning, MMLU to measure knowledge and language understanding, Parsing Fact From Fiction to assess LLM accuracy with TruthfulQA). As long as researchers and developers build and train their LLMs to achieve high scores in such benchmark tests, however, there are performances LLMs are expected to do. In particular, LLMs are often expected to give correct answers to questions. Such tests are, therefore, by nature normative, assuming that an LLM with a higher score is a better LLM, and importantly, many such tests (or important pieces of them) are benchmarked to human judgments. This is also true for the task of translation. The standard benchmarks, such as BLEU (Bilingual Evaluation Understudy), are based on human judgments (even AI-based benchmark tools like COMET are also based on human translations). If so, it is LLMs that should mimic human performance rather than vice versa, which is also true in the case of moral judgments (cf.",
            "score": 0.5080587094017516,
            "section_title": "LLM-Consistency: Consistency across different SOAT LLMs",
            "char_start_offset": 26197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.091064453125
        },
        {
            "corpus_id": "268733095",
            "title": "Learning From Correctness Without Prompting Makes LLM Efficient Reasoner",
            "text": "Recent effort (Gonen et al., 2023) show that an LLM's familiarity with a prompt's language predicts its effectiveness, with lower perplexity prompts leading to better performance.Unlike existing efforts, LECO focuses on learning from one's correct reasoning steps, without the need for feedback mechanisms including human intervention, external tools, or tailored prompts.\n\nReasoning without Prompting Recent studies have been focusing on improving the reasoning abilities of LLMs through various methodologies, primarily centered around the enhancement of prompting techniques.These works include few-shot prompting with intermediate steps augmented demonstrations (Wei et al., 2022;Fu et al., 2023;Yao et al., 2023;Wang et al., 2023) or zero-shot prompting with specific instructions (Kojima et al., 2022;Yasunaga et al., 2023).Although these methods have shown promising results, their effectiveness is often constrained by their task-specific nature and the labor-intensive process of designing prompts, leading to inconsistent outcomes across different tasks (Ye & Durrett, 2022;Zhou et al., 2023).\n\nAnother strategy to facilitate reasoning involves instruction tuning, which leverages a significant volume of chain-of-thought (CoT) data (Chung et al., 2022;Mukherjee et al., 2023;Gunasekar et al., 2023;Luo et al., 2023).Recently, Liu et al. (2024) proposed to tune LLMs by comparing the logit differences between a pair of tuned and untuned smaller models, showcasing improvements in reasoning without CoT distillation.In contrast to these methods, our LECO introduces an intrinsic self-correct reasoning mechanism that does not depend on fine-tuning or auxiliary models.\n\nAdditionally, there has been an interest in refining decoding algorithms specifically for reasoning.Notably, contrastive decoding (Li et al., 2023) has been developed to enhance a model's generation quality by adjusting the logits from smaller models, with recent research indicating its potential to boost reasoning performance (O 'Brien & Lewis, 2023).",
            "score": 0.5058636466130639,
            "section_title": "Related Work",
            "char_start_offset": 6526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 372
                },
                {
                    "start": 374,
                    "end": 578
                },
                {
                    "start": 578,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 1103
                },
                {
                    "start": 1105,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1678
                },
                {
                    "start": 1680,
                    "end": 1780
                },
                {
                    "start": 1780,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 34,
                    "matchedPaperCorpusId": "254408772"
                },
                {
                    "start": 666,
                    "end": 684,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 684,
                    "end": 700,
                    "matchedPaperCorpusId": "252683303"
                },
                {
                    "start": 700,
                    "end": 717,
                    "matchedPaperCorpusId": "252683303"
                },
                {
                    "start": 717,
                    "end": 735,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 786,
                    "end": 807,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1064,
                    "end": 1084,
                    "matchedPaperCorpusId": "252873674"
                },
                {
                    "start": 1084,
                    "end": 1102,
                    "matchedPaperCorpusId": "253265328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059417724609375
        },
        {
            "corpus_id": "268819621",
            "title": "A Theory for Length Generalization in Learning to Reason",
            "text": "Here we review the related empirical work, which includes the evaluation of LLMs in reasoning, chainof-thoughts for reasoning, dealing with length generalization (LG) in learning to reason, and dealing with\n\nLG in text generation.\n\nEvaluations and limitations of LLMs in reasoning.Continuing with the discussion about evaluations of the reasoning capabilities of LLMs in Sec. 1, we present a more extensive literature survey here.In general, evaluations conducted on several latest LLMs showed that they struggled with many reasoning tasks [GBWD23, TZL + 23].\n\nIn Sec. 1, we discussed empirical works about LG [AWA + 22, DLS + 23, ZBB + 22].In these papers, the authors also tried to mitigate the problem through improved training and CoT [AWA + 22], improved prompting and fine-tuning of LLMs [ZBB + 22], and curriculum learning [ABLR23].An evaluation of the deductive reasoning capability of LLMs was also conducted in [PG23], which shows that CoT helps improve the results, but does not achieve perfect accuracy.None of them studied the LG problem theoretically as we do.Below, we focus on surveying other empirical works.Many of them identified limitations of LLMs in solving different reasoning problems, but few have characterized the limitations in a formal manner to facilitate theoretical investigation.\n\n[MVTF23] created a dataset specifically for mathematical reasoning that can be perturbed.They showed that perturbations of the tasks heavily affect the results, reducing F1 score from 97% to 17%, which suggests that inference is likely to be dominated by surface-level patterns unrelated to the deeper understanding of the mathematical operators.However, this evaluation was done using only BERT [DCLT18] based models, but not on more recent LLMs like ChatGPT and GPT4.[WQR + 23] used \"counterfactual\" tasks that deviate from the standard reasoning tasks to evaluate LLMs.It was found that the performance degrades substantially compared to the default conditions, which again suggests that while LLMs can perform reasoning to some extent, they often rely on narrow, non-transferable procedures or surface patterns for tasksolving.",
            "score": 0.5053297069936353,
            "section_title": "F Related Empirical Work",
            "char_start_offset": 46361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 208,
                    "end": 230
                },
                {
                    "start": 232,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 559
                },
                {
                    "start": 561,
                    "end": 641
                },
                {
                    "start": 641,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1312
                },
                {
                    "start": 1314,
                    "end": 1403
                },
                {
                    "start": 1403,
                    "end": 1660
                },
                {
                    "start": 1660,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1886
                },
                {
                    "start": 1886,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 921,
                    "end": 927,
                    "matchedPaperCorpusId": "258048648"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349853515625
        },
        {
            "corpus_id": "260887090",
            "title": "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models",
            "text": "Previous Solutions Despite the tremendous success of LLMs in almost all NLP tasks, persistent challenges such as knowledge cutoff and biased/toxic outputs remain. To counter these challenges, two approaches are generally employed: \n\n1) FINE-TUNING: Traditional fine-tuning techniques, along with delta tuning (Ding et al., 2022) and LoRA tuning (Hu et al., 2021) utilize domainspecific datasets to update the model's internal parametric knowledge. However, these methods face two notable challenges: First, they consume considerable resources. Second, they risk the potential of catastrophic forgetting (Ramasesh et al., 2022). \n\n2) PROMPT-AUGMENTATION: Given a sufficient number of demonstrations or retrieved contexts, LLMs can learn to enhance reasoning (Yu et al., 2022) and generation through external knowledge (Borgeaud et al., 2022;Guu et al., 2020;Lewis et al., 2020). However, the performance may be sensitive to factors such as the prompting template, the selection of in-context examples (Zhao et al., 2021), or retrieved contexts (Ren et al., 2023). These approaches also encounter the issue of context length limitation (Liu et al., 2023a). \n\nKnowledge Storage Mechanism Within the NLP literature, numerous studies have delved into understanding the location of different types of knowledge in language models (Petroni et al., 2019;Roberts et al., 2020;Jiang et al., 2020). LLMs can be conceptualized as knowledge banks, and the transformer MLP layers function as key-value memories according to observations from Geva et al. (2021). This configuration promotes efficient knowledge adjustments by precisely localizing knowledge within the MLP layers (denoted as knowledge editing). \n\nKnowledge editing enables nimble alterations to the LLMs' behavior through one data point. Another promising attribute of knowledge editing is its ability to ensure the locality of editing, meaning that modifications are contained within specific contexts. Additionally, the knowledge editing technique can mitigate harmful language generation (Geva et al., 2022). In this paper, we present EASYEDIT, an easy-to-use knowledge editing framework for LLMs.",
            "score": 0.5036405494318256,
            "section_title": "Background",
            "char_start_offset": 4631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 757,
                    "end": 774,
                    "matchedPaperCorpusId": "253098034"
                },
                {
                    "start": 817,
                    "end": 840,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 840,
                    "end": 857,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 857,
                    "end": 876,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1000,
                    "end": 1019,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1324,
                    "end": 1346,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1346,
                    "end": 1367,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 1528,
                    "end": 1546,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 2042,
                    "end": 2061,
                    "matchedPaperCorpusId": "247762385"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0938720703125
        },
        {
            "corpus_id": "271050386",
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "text": "Beyond the pre-training objective, recent studies have aimed to model human preferences to better align LLMs with human expectations.For example, the well-known InstructGPT (Ouyang et al., 2022) introduces reinforcement learning from human feedback (RLHF), which uses preference rewards as an additional training objective.Although RLHF is effective at making LLMs more helpful to users, it inevitably incurs an alignment tax, which refers to performance degradation after RLHF.Recent research has explored various techniques to mitigate alignment tax issues (Lin et al., 2023;Lu et al., 2024b;Fu et al., 2024b).Emergent Abilities The fundamental capability of language models is text generation, where tokens are auto-regressively generated based on preceding tokens using greedy search or nucleus sampling (Holtzman et al., 2020a):\n\nInterestingly, LLMs can not only generate realistic text but also perform specific tasks when provided with task-specific prompts, without requiring fine-tuning on particular downstream tasks (Brown et al., 2020).This phenomenon is one of the most important differences between LLMs and previous PLMs.Wei et al. (2022b) define the emergent ability as \"an ability that is not present in smaller models but is present in larger models.\"Among these emergent abilities, in-context learning (ICL) (Brown et al., 2020;Dong et al., 2022) and instruction following are commonly used and significantly enhance the ability of LLMs to process various tasks.\n\nICL helps LLMs understand tasks by using several task examples as demonstrations.When provide these demonstrations as prompts, LLMs can automatically generate reasonable output for the given test example, which can be formalized as:\n\nInstruction following ability are typically emerge in LLMs that have been fine-tuned on examples formatted with instructions on multiple tasks.The generation process can be formalized as:\n\nwhere I refers to the given instruction for current example x.",
            "score": 0.5032317251509866,
            "section_title": "Large Language Models",
            "char_start_offset": 5848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 833
                },
                {
                    "start": 835,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1136
                },
                {
                    "start": 1136,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1481
                },
                {
                    "start": 1483,
                    "end": 1564
                },
                {
                    "start": 1564,
                    "end": 1715
                },
                {
                    "start": 1717,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 1904
                },
                {
                    "start": 1906,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 194,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 808,
                    "end": 832,
                    "matchedPaperCorpusId": "127986954"
                },
                {
                    "start": 1027,
                    "end": 1047,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1136,
                    "end": 1154,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1327,
                    "end": 1347,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12890625
        },
        {
            "corpus_id": "267617275",
            "title": "V-STaR: Training Verifiers for Self-Taught Reasoners",
            "text": "Given a pretrained language model G and the original training data of a task D SFT = {(x 1 , y 1 ), (x 2 , y 2 ), \u2022 \u2022 \u2022 (x N , y N )}, where x is typically a description of a problem and y is the solution, such as chain-of-thought rationale or generated code. The de facto approach for such tasks with causal language models is supervised fine-tuning (SFT) with the negative log-likelihood objective on the training data: \n\nwhere G is also referred to as generator in reasoning tasks. LLMs can be used to generate high quality chain-of-thought rationales or solutions for a range of tasks. This observation \n\nhas motivated using correct generations from the model itself to bootstrap problem-solving and reasoning abilities (Zelikman et al., 2022;Singh et al., 2023;Yuan et al., 2023).",
            "score": 0.5021407770008883,
            "section_title": "Preliminaries",
            "char_start_offset": 3821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 785
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03985595703125
        },
        {
            "corpus_id": "270526238",
            "title": "Large language model to multimodal large language model: A journey to shape the biological macromolecules to biological sciences and medicine",
            "text": "LLMs, such as GPT models (GPT-3.5 and GPT-4), have achieved remarkable success in various NLP tasks. However, they also come with certain significant limitations (Table 3). LLMs can generate coherent and contextually relevant text, but they often lack deep understanding of the world and common sense reasoning. They can generate nonsensical or incorrect responses in specific contexts. 148 LMs can inadvertently preserve biases present in the training data, which can result in biased or unfair outputs, especially when dealing with sensitive topics such as gender, race, or religion. Mitigating biases in LLMs remains a significant challenge. \n\nMoreover, while LLMs excel in understanding and generating text based on context, they may struggle with long-term dependencies or maintaining coherence over extended passages. 6 This can lead to inconsistencies or inaccuracies in generated text, especially in complex or significant scenarios. The LLMs typically require vast amounts of data for pre-training, which can be expensive and resource-intensive. Furthermore, they may stumble with generalizing to out-ofdomain or low-resource domains where training data are limited. \n\nWhile fine-tuning LLMs on specific tasks can improve performance, it often requires careful selection of hyperparameters, task-specific data, and fine-tuning strategies. 149  addition, fine-tuning may only sometimes lead to optimal performance, especially for tasks with unique requirements or constraints. \n\nIn the case of safety and ethical concerns, the LLMs have the potential to generate harmful or malicious content, including misinformation, hate speech, or inappropriate material. 150 Ensuring LLMs' safe and ethical use poses significant challenges for researchers and practitioners. Training and deploying LLMs can be computationally expensive and resource-intensive, requiring powerful hardware and substantial infrastructure. It can limit accessibility to LLMs for researchers and organizations with limited resources. 151 Addressing these limitations requires ongoing research and development efforts in bias mitigation, robustness testing, model interpretability, and ethical AI frameworks. In addition, interdisciplinary collaboration involving experts from diverse fields, such as linguistics, psychology, and ethics, is essential to foster the responsible development and deployment of LLMs.",
            "score": 0.5016133191182287,
            "section_title": "Limitations of LLMs",
            "char_start_offset": 48956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "259136071"
                },
                {
                    "start": 824,
                    "end": 825,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 1667,
                    "end": 1670,
                    "matchedPaperCorpusId": "263209831"
                },
                {
                    "start": 2009,
                    "end": 2012,
                    "matchedPaperCorpusId": "263827375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.166259765625
        },
        {
            "corpus_id": "265329916",
            "title": "Large Language Models and Information Retrieval",
            "text": "LLMs possess the remarkable ability to reformulate a wide array of natural language processing tasks into a text-to-text format. This format involves translating the task into a language-based question or statement. For instance, in machine translation, a sentence in English could be reformulated as a question in the form of \"Translate this sentence into French.\" By framing tasks in a text-to-text format, LLMs can effectively address an extensive spectrum of NLP tasks. This approach provides a more unified and adaptable way to handle diverse tasks. For example, the same underlying LLM model that translates can be repurposed for summarization by reformulating the task as \"Summarize this article.\" The process of fine-tuning includes modifying the model's architecture for the specific task, defining a task-specific objective function, and adjusting hyperparameters. Fine-tuning ensures that the model optimizes its understanding and performance for the designated task. It democratizes NLP tasks, making it easier for researchers and practitioners to achieve state-of-the-art results with less data and computational resources. \n\nIn essence, this two-step methodology, pre-training followed by fine-tuning, has revolutionized the field of natural language processing. It empowers the development of highly adaptable and proficient language models, allowing them to excel in various real-world applications, from sentiment analysis to language translation, and much more.",
            "score": 0.5004010772790218,
            "section_title": "Task Reformulation through Text-to-Text Format",
            "char_start_offset": 19571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1479
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06463623046875
        },
        {
            "corpus_id": "271903190",
            "title": "CogLM: Tracking Cognitive Development of Large Language Models",
            "text": "LLM Evaluation Due to the importance of LLMs, their abilities have been thoroughly evaluated on a wide range of problems. Large-scale efforts have been invested in constructing large benchmarks itegrated with numerous LM evaluations across a number of fields (Srivastava et al., 2022;Liang et al., 2022;Hendrycks et al., 2020;Biderman et al., 2023). Due to the superior performance of LLMs in a number of traditional NLP tasks, recently challenging tasks have been proposed to test the upper bound performance of LLMs (Hendrycks et al., 2021;Valmeekam et al., 2022;Gendron et al., 2023). Some benchmarks include evaluation of specific cognitive abilities, such as common sense reasoning (Ismayilzada et al., 2023), planning (Xie et al., 2024), and deductive reasoning (Saparov and He, 2022). While previous benchmarks focus on measuring either a type or a category of advanced ability in LLMs, few studies explore the development relationship between different abilities, which is crucial for understanding the emergence of LLMs' abilities. \n\nCognitive psychology survey on LLMs Several works introduce tools from cognitive psychology to study LLMs. Such as understanding the behavior in LLMs (Ritter et al., 2017;Kosoy et al., 2022;Hagendorff et al., 2022;Portelance et al., 2023), exploring the human-like abilities in LLMs (Han et al., 2022;Kosinski, 2023;Aher et al., 2023;Pan and Zeng, 2023), and improving LLMs' performance on certain task (Betz et al., 2021). \n\nOur work is most similar to present work on using cognitive psychology to explore whether LMs \"learn and think like people\" by Binz and Schulz (2023), which suggests that LLMs struggle to reason causally due to the differences in how humans and LLMs learn about the world. The key difference in our approaches is that Binz and Schulz (2023) aims to study GPT-3 by assessing its advanced abilities (e.g.",
            "score": 0.4985664533943744,
            "section_title": "Related Work",
            "char_start_offset": 18328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 326,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 326,
                    "end": 348,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 687,
                    "end": 713,
                    "matchedPaperCorpusId": "264438931"
                },
                {
                    "start": 768,
                    "end": 790,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 1193,
                    "end": 1214,
                    "matchedPaperCorpusId": "22689427"
                },
                {
                    "start": 1257,
                    "end": 1281,
                    "matchedPaperCorpusId": "261696384"
                },
                {
                    "start": 1326,
                    "end": 1344,
                    "matchedPaperCorpusId": "269447366"
                },
                {
                    "start": 1359,
                    "end": 1377,
                    "matchedPaperCorpusId": "251719353"
                },
                {
                    "start": 1596,
                    "end": 1618,
                    "matchedPaperCorpusId": "250113371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05059814453125
        },
        {
            "corpus_id": "266374853",
            "title": "LPR: Large Language Models-Aided Program Reduction",
            "text": "Large Language Models (LLMs) have proved their remarkable capability of undertaking multiple text-processing tasks, including source code-related works. Recent works focus on applying LLMs to facilitate software engineering tasks, or assessing the effectiveness, potential and limitations of LLMs on software development and maintenance. Some research [13,[45][46][47] focus on empirically applying LLMs on automatic program repair (APR). Huang et al. [13] performed an empirical study on improvement brought by model fine-tuning in APR. Xia et al. [46] thoroughly evaluated 9 state-ofthe-art LLMs across multiple datasets and programming languages, and demonstrated that directly applying LLMs has already significantly outperformed all existing APR techniques. Additionally, some works focus on LLMs' performance w.r.t. code completion, generation and fuzzing [6,21,35,53], by leveraging the code analysis and generation ability of LLMs. \n\nSimilar to these studies, our approach LPR leverages LLMs for a software engineering task, i.e., program reduction. LPR harnesses the comprehension and generation capabilities of LLMs to refine the results of program reduction. However, our work distinguishes itself in the nature of the programs processed by LLMs. In related research, programs are typically logical and goal-oriented, often designed to fulfill a specific purpose. In contrast, the programs involved in our program reduction task are random, chaotic, and lack a clear objective. Consequently, our research sheds light on the performance of LLMs when dealing with programs that do not have an easily discernible purpose.",
            "score": 0.49615318846749246,
            "section_title": "LLMs for Software Engineering",
            "char_start_offset": 42200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1629
                }
            ],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "265054960"
                },
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "265054960"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "255340904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056640625
        },
        {
            "corpus_id": "272827681",
            "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions",
            "text": "Bias in Large Language Models The increasing adoption of LLMs has raised ethical concerns about their tendency to perpetuate negative stereotypes and inappropriate content (Nissim et al., 2020;Hutchinson et al., 2020;Esiobu et al., 2023). LLMs have been shown to disproportionately impact individuals of specific social demographics, such as religion, sex, race, age, educational institution, nationality, and disability (Abid et al., 2021;Gonen and Goldberg, 2019;Wan et al., 2023;Sap et al., 2021;Kamruzzaman et al., 2024;Venkit et al., 2022). This bias is often revealed in natural language generation tasks (Sheng et al., 2019), code generation (Huang et al., 2024), and persists across various languages (Zhou et al., 2019). \n\nImplicit bias evaluation Existing metrics quantify bias in LLMs through various approaches, such as question-answering (QA) prompts (Shin et al., 2024;Nangia et al., 2020;Nadeem et al., 2021;Parrish et al., 2022) and sentence completion tasks or counterfactual evaluations (Gehman et al., 2020;Dhamala et al., 2021;Huang et al., 2020). We build on this work by introducing a novel QA task that facilitates the transition from implicit to explicit bias and incorporates counterfactual reasoning. \n\nHuman-model alignment Training models on human feedback has been explored to improve summarization quality (Stiennon et al., 2020), assess the trustworthiness of LLMs (Li et al., 2024), and align human and model judgments in casual and moral reasoning tasks (Nie et al., 2023). Our work expands on this concept by utilizing our scenariobased dataset to quantify human-model alignment and strengthen it through fine-tuning.",
            "score": 0.49576817431188464,
            "section_title": "Related Work",
            "char_start_offset": 23103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1651
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 193,
                    "matchedPaperCorpusId": "165163511"
                },
                {
                    "start": 193,
                    "end": 217,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 421,
                    "end": 440,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 440,
                    "end": 465,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 465,
                    "end": 482,
                    "matchedPaperCorpusId": "264128125"
                },
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "252819117"
                },
                {
                    "start": 611,
                    "end": 631,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 709,
                    "end": 728,
                    "matchedPaperCorpusId": "202537733"
                },
                {
                    "start": 883,
                    "end": 903,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 903,
                    "end": 923,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 923,
                    "end": 944,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1047,
                    "end": 1066,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 1336,
                    "end": 1359,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 1487,
                    "end": 1505,
                    "matchedPaperCorpusId": "264802129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034942626953125
        },
        {
            "corpus_id": "258291494",
            "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
            "text": "At the end of this section, we also want to provide more analyses about the impacts of the fine-tuning on LLM's language generation abilities. In Table 8, we illustrate some examples about the generation results by the LLM in Graph-ToolFormer before and after the fine-tuning with the graph reasoning prompt data. Specifically, we select the inputs from two different sources, i.e., two instances from the Pile testing set (Pile was the data used for GPT-J pre-training) and two instances from the recent news articles on the web. \n\nBy comparing the generation results, we can observe very large (negative) impacts of the fine-tuning with the prompt datasets on LLM's language generation ability. For these four input payloads, after the fine-tuning, the outputs generated by the LLM in Graph-ToolFormer are either some random tokens or contains the unexpected API calls, and only for the third input, the output by the LLM in Graph-ToolFormer is till closely related to the inputs. \n\nTherefore, if we plan to make Graph-ToolFormer a very general language interface that can not only handle graph reasoning tasks but also still possess the language generation ability for the inputs not related to graph reasoning, some new continual learning techniques will be needed in model fine-tuning, so the LLM in Graph-ToolFormer will not suffer from the catastrophic forgetting problem after fine-tuning.",
            "score": 0.49466601007263844,
            "section_title": "Language Generation Ability Revisit",
            "char_start_offset": 118374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1673583984375
        },
        {
            "corpus_id": "264398751",
            "title": "Language models and psychological sciences",
            "text": "However, this observation, which may be problematic for AI researchers, is actually a positive feature of LLMs as model of cognition. In fact, humans show the same pattern of differential performance to the same problem with different linguistic variations. In other terms, minor linguistic variations in the problems are affecting problemsolving accuracy (Jitendra and Xin, 1997). In their research, children were presented with the following word problem: \"There are 8 birds and 3 worms. a) How many more birds are there than worms? b) How many birds will not get a worm?. \" Alternative a) and b) have the same meaning but the first leads to 17% accuracy while the second to 83% accuracy. Similar results, which indicate a change in accuracy due to minor changes in wording, has been reported by Hickendorff (2021). This fragment of debate is clearly indicating the different objectives that AI researchers and cognitive psychologists have. What is a weakness for the first may be a strength for the seconds. As cognitive psychologists, we have always to evaluate the data empirically and not rely on the intuition of a Ph.D. level evaluator as AI researchers are keen in doing. \n\nFinally, another factor that may overestimate the reasoning abilities of LLM include \"data contamination\" which refers to the situation where the LLM has been exposed to test data during its training process. This gives the LLM an unfair advantage on tests and benchmarks, as it may have already seen the questions and answers before. For example, GPT-4 performs better on problems published before 2021 (GPT-4 training cut-off) with respect to those published after 2021.",
            "score": 0.4941698754319191,
            "section_title": "The problem with \"shortcut\" learning and other confounding factors",
            "char_start_offset": 46099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1655
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 380,
                    "matchedPaperCorpusId": "145673445"
                },
                {
                    "start": 798,
                    "end": 816,
                    "matchedPaperCorpusId": "239012535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06903076171875
        },
        {
            "corpus_id": "267617105",
            "title": "The Generative AI Paradox in Evaluation: \u201cWhat It Can Solve, It May Not Evaluate\u201d",
            "text": "Reassessing the capabilities of LLMs Recent studies have raised questions about the inferred capabilities of LLMs based on their task performance. Dziri et al. (2023) suggest that LLMs do not necessarily develop systematic problem-solving abili- ties to address multi-step compositional reasoning tasks. Echoing this, Wu et al. (2023) observe that while current language models demonstrate certain abstract reasoning abilities, their dependence on specific, non-generalizable procedures for problemsolving calls for a more discerning assessment of their capabilities. This observation extends beyond tasks that require advanced intelligence, such as reasoning. In a similar vein, West et al. (2023) posit that impressive generation abilities in generative models, in contrast to humans, may not necessarily be based on an equivalent level of understanding capabilities. \n\nLarge Language Model as an evaluator Recent studies propose directly using LLMs as referencefree evaluators for Natural Language Generation tasks (Fu et al., 2023;Wang et al., 2023b). Zheng et al. (2023) propose to use LLMs as a judge to evaluate a chatbot's multi-turn conversational and instruction-following ability. Similar to our study, Wang et al. (2023a) use LLM as an evaluator for Open-QA task, but provide golden set to the evaluator model. Meanwhile, Hu and Levy (2023) analyze the validity of prompting LLMs to evaluate linguistic knowledge and show that the results from such method cannot be taken as conclusive, comparing the results with the direct method of computing probabilities of tokens based on the models' internal logits.",
            "score": 0.4931451846855347,
            "section_title": "Related Work",
            "char_start_offset": 2487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1618
                }
            ],
            "ref_mentions": [
                {
                    "start": 1035,
                    "end": 1054,
                    "matchedPaperCorpusId": "257378627"
                },
                {
                    "start": 1056,
                    "end": 1075,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1334,
                    "end": 1352,
                    "matchedPaperCorpusId": "258833033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0863037109375
        },
        {
            "corpus_id": "270559332",
            "title": "Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning",
            "text": "Recent studies [31] indicate that Large Language Models (LLMs) can handle intricate tasks using the Chain of Thought (COT) mechanism [32].LLMs have gained significance in solving math word problems (MWPs) [21], with MathPrompter [17] showcasing excellent results, not only generating correct answers but also complex reasoning steps.Various approaches aim to enhance LLMs' mathematical capabilities and address challenges [28].[36] investigates factors like pre-training loss, supervised data, and augmented data, proposing rejection sampling fine-tuning (RFT) to improve mathematical reasoning.WizardMath [22] introduces a reinforced Evol-Instruct Feedback (RLEIF) method to enhance reasoning abilities through supervised fine-tuning and PPO training [27].MAmmoTH [37] combines Chain of Thought (CoT) and Program-of-Thought [8] rationales to teach LLMs to use external tools like Python interpreters for mathematical problem-solving.\n\nTo assess the correctness of reasoning steps, most existing work [23,35] evaluates the quality by directly comparing the final answer.However, some early studies explore reasoning step quality differently.[26] measures reasoning step quality by comparing the similarity between generated and reference reasoning.[12] treats powerful LLMs as verifiers, asking them to generate judgments for the reasoning steps.[33] introduces a new methodology employing validity and redundancy to characterize reasoning quality, along with accompanying LLMs to assess them automatically.\n\nVarious methods extend LLMs as verifiers and demonstrate their usage for self-correction [40].[41] shows that models like GPT-4 align with human preferences, indicating their potential as tools for accessing LLM-generated responses.[24] finds that LLMs struggle to find their own reasoning errors in code generation but can correct them with adequate feedback.However, there's still a lack of clarity in math reasoning and using LLMs for mistake detection and rectification in foreign reasoning steps, not just their own self-generated reasoning steps.Our work focuses on LLMs' ability to correct MWPs reasoning steps and rectify them to reach the correct answer, as well as whether LLMs generalize to newer and complex datasets.",
            "score": 0.4925795567649027,
            "section_title": "Related Work",
            "char_start_offset": 24981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 427
                },
                {
                    "start": 427,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 934
                },
                {
                    "start": 936,
                    "end": 1070
                },
                {
                    "start": 1070,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1346
                },
                {
                    "start": 1346,
                    "end": 1507
                },
                {
                    "start": 1509,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1741
                },
                {
                    "start": 1741,
                    "end": 1869
                },
                {
                    "start": 1869,
                    "end": 2061
                },
                {
                    "start": 2061,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "253801709"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07476806640625
        },
        {
            "corpus_id": "260704607",
            "title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation",
            "text": "LLMs have revolutionized Natural Language Processing (NLP) and beyond. They demonstrate great potential in few-shot learning (Brown et al., 2020), code generation (Nijkamp et al., 2023), reasoning (Yao et al., 2023) and other tasks. Furthermore, LLM powered autonomous agents (Weng, 2023) are widely applied in solving complex problems, like multimodal generation (Shen et al., 2023), software developing (Qian et al., 2023) and social simulating (Park et al., 2023). \n\nAlthough LLMs have reformed the paradigm of NLP, the problem of evaluation keeps haunting this field. Old benchmarks become out-ofdate. Since LLMs achieve human-level Natural Language Understanding (NLU) and Natural Language Generation (NLG) abilities (OpenAI, 2023). To address the pressing need for novel benchmarks, the NLP community has introduced an array of fresh evaluation tasks and datasets, encompassing a diverse spectrum of abilities, including close-book question-answering (QA) based knowledge testing (Hendrycks et al., 2020;Huang et al., 2023), human-centric standardized exams (Zhong et al., 2023), multi-turn dialogue (Lin and Chen, 2023), reasoning (Liu et al., 2023a;bench authors, 2023) and safety assessment (Sun et al., 2023). \n\nHowever, there are still many problems with these new benchmarks. 1) Evaluated abilities are limited by the task formats. Since a majority of these tasks adopt a single-turn QA format, they are insufficient to comprehensively evaluate various aspects of LLMs' capabilities. For instance, they fail to assess the models' proficiency in adhering to instructions in dialogue or mimicking human-like social interactions. 2) Benchmarks can be easily hacked. Avoiding the leakage of test set is of paramount importance when evaluate a model's ability. Nonetheless, considering the amount of pretrained knowledge of LLM, it has become more and more inevitable to inadvertently mix test cases into the training set. (Gunasekar et al., 2023).",
            "score": 0.49023589663072087,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 1219
                },
                {
                    "start": 1222,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 185,
                    "matchedPaperCorpusId": "252668917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052032470703125
        },
        {
            "corpus_id": "261064777",
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "text": "Other research mainly investigates and analyzes the shortcomings of LLMs in their applications. For instance, Meade et al. (Meade et al., 2022) survey and summarize the bias mitigation techniques in pre-trained language models. The paper empirically studies the five currently popular bias mitigation techniques and proposes three intrinsic bias benchmarks to quantify the effectiveness of each technique. Huang et al. (Huang and Chang, 2023) comprehensively describes the reasoning capabilities of LLMs, including technologies to improve and induce reasoning capabilities in these models, methods and benchmarks for assessing reasoning capabilities, and suggestions for future directions. As LLMs sometimes provide unrealistic yet seemingly plausible predictions (termed as hallucinations, see (Welleck et al., 2020)), Mialon et al. (Mialon et al., 2023) reviews two methods to enhance the abilities of LLMs, namely, by leveraging reasoning skills and invoking external tools, aiming to improve context and curb hallucinations. Xu et al. (Xu and McAuley, 2023) pays special attention to the inference stage during the construction of LLMs and reviews the current state of model compression and acceleration techniques, including benchmarks, metrics, and methods. \n\nZan et al. (Zan et al., 2023) investigates the performance of 27 large models in the field of generating code from a natural language description (or NL2Code). The main contribution of the paper is an intuitive comparison of the NL2Code capabilities of LLMs on the HumanEval benchmark. However, its research is limited to the software engineering task of code generation. It does not summarize the applications of other LLMs in code generation, nor does it investigate other software engineering tasks, such as code conversion. \n\nWong et al. (Wong et al., 2023) introduces some popular Language Modelbased Learning (LLM) approaches and their applications in downstream tasks related to AI-assisted programming. The tasks covered in the article include code generation, code completion, code translation, code refinement, code summarization, defect detection, and clone detection. However, it is worth noting that the AI-assisted methods discussed in this article are not limited to LLM but also encompass various other AI techniques.",
            "score": 0.49011751694046357,
            "section_title": "Other works on reviewing LLM",
            "char_start_offset": 55173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1793
                },
                {
                    "start": 1796,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 1029,
                    "end": 1061,
                    "matchedPaperCorpusId": "254069544"
                },
                {
                    "start": 1808,
                    "end": 1827,
                    "matchedPaperCorpusId": "258217984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16015625
        },
        {
            "corpus_id": "268247756",
            "title": "Eliciting Better Multilingual Structured Reasoning from LLMs through Code",
            "text": "Recall that our fine-tuning recipe aims to improve reasoning of an LLM, while maintaining its natural language understanding (NLU) abilities. We show this is the case by reporting results on 3 multilingual tasks: \n\n\u2022 XNLI: natural language inference \u2022 XStoryCloze: given 4 sentences from a short story, choose between 2 possible completions \u2022 XQUAD: extractive question answering To query LLMs, we follow the specific prompting guidelines for each task from Ahuja et al. (2023). Table 2 shows that for all 3 tasks, the differences between BLOOMZ and BLOOMZ-TCC are statistically insignificant. Therefore, the mitigation strategies we used, LoRA and training data replay, have proved effective.",
            "score": 0.4898272609076185,
            "section_title": "Non-Complex Reasoning Task Results",
            "char_start_offset": 21077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 212
                },
                {
                    "start": 215,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 693
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053009033203125
        },
        {
            "corpus_id": "277451521",
            "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs",
            "text": "LLMs have untapped potential for code compilation and optimization tasks. \n\nLLM The study by Hosseini et al. [142] delves into the gradeschool math problem-solving abilities of LLMs by examining their performance on paired problems-where the solution to the first is essential for resolving the second-to uncover what the authors term a \"reasoning gap.\" While many LLMs perform adequately on isolated math questions, their accuracy drops significantly when tasked with chained, multi-hop problems, revealing that the contextual overload and additional reasoning steps introduce challenges not apparent in standard benchmarks. Notably, this gap is more evident in smaller, cost-efficient, and math-specialized models, suggesting that even targeted fine-tuning can lead to overfitting and hinder generalization to composite tasks. The paper also highlights that improvements from instruction-tuning and code generation vary with model size, and that the reasoning shortfall is attributable not to data leakage but to inherent distractions and insufficient second-hop reasoning capabilities. Overall, the findings prompt a reevaluation of how LLMs are assessed for complex, multi-step problem solving, urging the development of more nuanced benchmarks that capture true reasoning proficiency. \n\nGou et al. [127] presents ToRA, a series of Tool-integrated Reasoning Agents that enhance large language models' ability to tackle complex mathematical problems by integrating natural language reasoning with external computational tools such as symbolic solvers and computation libraries. Recognizing that traditional LLMs struggle with intricate mathematical tasks due to limitations in pure language processing, the authors propose a novel training regimen that involves curating interactive tool-use trajectories from mathematical datasets, leveraging imitation learning on annotated data, and applying output space shaping to fine-tune reasoning behavior. Empirical evaluations demonstrate that ToRA models outperform existing open-source alternatives on ten mathematical reasoning benchmarks, yielding average absolute improvements between 13% and 19%.",
            "score": 0.4893471515580634,
            "section_title": "Leveraging LLMs for Code Compilation",
            "char_start_offset": 168452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2149
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340087890625
        },
        {
            "corpus_id": "267094860",
            "title": "Assessing and Understanding Creativity in Large Language Models",
            "text": "In recent years, the realm of artificial intelligence (AI) has witnessed a meteoric rise in the development and sophistication of Large Language Models (LLMs) [1,2]. LLMs have significantly advanced in their capabilities in addressing a variety of conventional natural language processing tasks, such as reasoning and natural language understanding [3,4,5,6]. Moreover, LLMs have also demonstrated significant value in widespread applications. From transforming rudimentary text into compelling narratives [7,8], unlocking a new realm of storytelling, to solving complex algorithmic problems [9], these models have shown a semblance of what could be interpreted as creativity. The practical manifestations of this creativity have penetrated various sectors, including science research, where they assist in idea generation and suggestion [10]; education, by providing personalized learning experiences [11]; and in the entertainment industry, creating music and art [12,13]. In many of their applications, LLMs seem to exhibit the ability to generate original text, aiding tasks related to imagination and creativity, suggesting that they may indeed possess elements of creativity. \n\nFrom the broad capabilities demonstrated by LLMs, the creativity they exhibit is a key reason they are considered powerful. However, behind the impressive abilities of LLMs lies a significant question that warrants careful examination: do these models actually possess real creativity, or is their apparent smartness merely an illusion-a complex imitation of human thinking created by their training paradigm? This question touches on the very nature of LLM intelligence, which may not be easily explained. Since LLMs have shown considerable creativity, understanding the extent and characteristics of this creativity is essential. Gaining deeper insight into the creativity of LLMs can not only guide us in further improving their performance but also in enhancing our understanding of the nature of their creativity. This, in turn, informs our daily use and application of these models, underscoring the need for an effective method to measure and assess their creativity. \n\nCreativity, as a term, traditionally refers to the natural ability to think innovatively, to make unconventional connections, and to devise solutions that are both novel and effective [14]. Assessing the creativity of LLMs is fraught with challenges. Firstly, the question of creativity does not have clear answers to refer to.",
            "score": 0.4892725989815109,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2158
                },
                {
                    "start": 2161,
                    "end": 2350
                },
                {
                    "start": 2351,
                    "end": 2411
                },
                {
                    "start": 2412,
                    "end": 2488
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "249063032"
                },
                {
                    "start": 354,
                    "end": 356,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 356,
                    "end": 358,
                    "matchedPaperCorpusId": "249872774"
                },
                {
                    "start": 509,
                    "end": 511,
                    "matchedPaperCorpusId": "259189720"
                },
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "249872774"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 966,
                    "end": 970,
                    "matchedPaperCorpusId": "247627867"
                },
                {
                    "start": 970,
                    "end": 973,
                    "matchedPaperCorpusId": "248326443"
                },
                {
                    "start": 2345,
                    "end": 2349,
                    "matchedPaperCorpusId": "146772784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06573486328125
        },
        {
            "corpus_id": "270063606",
            "title": "Perturbation-Restrained Sequential Model Editing",
            "text": "To explore the side effects of sequential model editing on the general abilities of LLMs, four representative tasks with corresponding datasets were adopted for assessment following previous work (Gu et al., 2024;Gupta et al., 2024b;Lin et al., 2024;Zhang et al., 2024), including: \n\nReasoning on the GSM8K (Cobbe et al., 2021), and the results were measured by solve rate. \n\nSummarization on the SAMSum (Gliwa et al., 2019), and the results were measured by the average of ROUGE-1, ROUGE-2 and ROUGE-L following Lin (2004). \n\nOpen-domain QA on the Natural Question (Kwiatkowski et al., 2019), and the results were measured by exact match (EM) with the reference answer after minor normalization as in Chen et al. (2017) and Lee et al. (2019). \n\nNatural language inference (NLI) on the RTE (Dagan et al., 2005), and the results were measured by accuracy of two-way classification. \n\nFor each dataset, some examples were randomly sampled for evaluation. Details of prompts for each task were shown in Appendix B.4. \n\nFigure 3: The downstream task performance (%) of models edited by three editing methods with LLaMA-2 (7B) on the ZSRE dataset. The dashed lines refer to the results of the unrestrained editing methods. The solid lines refer to the results of the editing methods coupled with the proposed PRUNE framework. Statistical significance tests were performed to demonstrate that the improvement in PRUNE compared to baseline was statistically significant (t-test with p-value <0.05).",
            "score": 0.48873539494512475,
            "section_title": "DOWNSTREAM TASKS, DATASETS AND METRICS",
            "char_start_offset": 22280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1491
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 213,
                    "matchedPaperCorpusId": "266899568"
                },
                {
                    "start": 233,
                    "end": 250,
                    "matchedPaperCorpusId": "266550989"
                },
                {
                    "start": 250,
                    "end": 269,
                    "matchedPaperCorpusId": "268358212"
                },
                {
                    "start": 404,
                    "end": 424,
                    "matchedPaperCorpusId": "208010268"
                },
                {
                    "start": 513,
                    "end": 523,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 566,
                    "end": 592,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 702,
                    "end": 720,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 725,
                    "end": 742,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 790,
                    "end": 810,
                    "matchedPaperCorpusId": "8587959"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1724853515625
        },
        {
            "corpus_id": "265212753",
            "title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation",
            "text": "Driven by advances in deep learning and NLP, LLMs have demonstrated outstanding proficiency in various generation and understanding tasks (Ope-nAI, 2023;Anil et al., 2023). However, existing benchmarks (Hendrycks et al., 2020;Zhong et al., 2023;Zheng et al., 2023a) for evaluating LLMs mainly focus on NLP tasks, such as common sense reasoning, academic examination, and authenticity verification. Existing evaluation methods are significantly insufficient in terms of evaluating completeness and comprehensiveness for code understanding and generation capabilities of LLMs. Firstly, many code LLMs, such as CodeT5+ (Wang et al., 2023b), WizardCoder (Luo et al., 2023), and Code LLaMA (Rozi\u00e8re et al., 2023), employ their own specific single-task evaluation datasets, making it infeasible to comprehensively compare the performance of various LLMs on code understanding and generation tasks on a unified standard. \n\nSecondly, existing datasets mostly evaluate LLMs on code tasks (Chen et al., 2021;Austin et al., 2021) for a narrow range of popular programming languages, with a focus on Python and single program synthesis tasks. However, software development often involves multiple programming languages, each following different programming paradigms such as object-oriented, functional, and procedural. Evaluating LLMs within a multilingual framework can reveal their ability to generalize across various languages and paradigms. Moreover, the complementarity between multiple tasks facilitates a comprehensive evaluation of the overall performance of LLMs, ensuring that an LLM is not over-optimized for a specific task and can maintain strong performance across diverse tasks. Importantly, multitask settings more accurately simulate the various requirements and challenges faced in real-world software development practices and hence better test the generalizability of LLMs.",
            "score": 0.48840998190889306,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 685,
                    "end": 707,
                    "matchedPaperCorpusId": "261100919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045867919921875
        },
        {
            "corpus_id": "262053463",
            "title": "OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch",
            "text": "Instruction tuning, which involves the method of fine-tuning LLMs on an instruction dataset in a supervised manner, has played a crucial role in the significant advancements of LLMs in recent years (Zhang et al., 2023b). Starting from the T5 model (Raffel et al., 2020), which pioneers the concept of consolidating diverse NLP tasks as generative tasks. By employing task-specific prompts to guide the model, this method streamlines the process of applying LLMs to an extensive array of applications, laying the foundation for subsequent instruction tuning models such as FLAN (Wei et al., 2021;Chung et al., 2022) and T0 (Sanh et al., 2021), which further improve performance across diverse tasks by incorporating more task-specific instructions during the pre-training phase. An approach related to instruction tuning is chain-of-thought (CoT) prompting (Nye et al., 2021;Wei et al., 2022), which enhances instructions with descriptions of intermediate reasoning steps, thereby boosting LLM performance (Wang et al., 2022;Zelikman et al., 2022;Wu et al., 2023b;Xu et al., 2023). At present, the open-source community offers a multitude of instruction datasets, such as Alpaca (Taori et al., 2023) and Dolly (Conover et al., 2023a). These instructions aim to enhance specific professional abilities of LLMs, such as code generation ability (Chaudhary, 2023), or the general capabilities like commonsense reasoning skills (Zhang et al., 2023c). However, the wide variety and inconsistent quality of these datasets pose challenges, with each dataset typically comprising a relatively small amount of data and focusing on a single language. In this work, we construct the BiFlan dataset, the first Bilingual Flan dataset built upon the cleansed Flan data (Longpre et al., 2023), containing various instruction types and tasks in English and Chinese language.",
            "score": 0.48831382125027956,
            "section_title": "Instruction Tuning",
            "char_start_offset": 7960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 269,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1024,
                    "end": 1046,
                    "matchedPaperCorpusId": "247762790"
                },
                {
                    "start": 1422,
                    "end": 1443,
                    "matchedPaperCorpusId": "252762275"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03521728515625
        },
        {
            "corpus_id": "261530723",
            "title": "Data-Juicer: A One-Stop Data Processing System for Large Language Models",
            "text": "After tokenization and pre-training, LLMs have succeeded in stimulating a wide range of advanced capabilities. The LLM pre-training data generally includes various types derived from the web crawlers [26,71], dialogues or social media [107], book-length formal texts [36,110], rigorous encyclopedias and academic texts [31,100], structured coding texts [18,57], and more texts from financial, medical and legal domains [58,91,104]. A challenge is nonetheless posed in the careful processing and formulation of pre-training data to filter noise, redundancy, irrelevance, and potentially toxic [33,62]. \n\nFine-tuning Data. Numerous studies have underscored that fine-tuning -the process of refining pre-trained LLMs using a smaller, task-specific dataset -can further enhance or unlock additional capabilities of LLMs [40,53,97,98]. Crucially, this process also paves the way for better aligning the behavior of these advanced models with human values and preferences [60,68]. \n\nIn this phase, though the data volume decreases exponentially compared to the pre-training phase, the format of fine-tuning data is quite different [73]. Typically, given a textual dataset {( 1 ,  1 ,  1 ), ..., (  ,   ,   ), ..., (  ,   ,   )}, the goal of fine-tuning is to adjust the pre-trained LLM  0 to find  * that maximizes the likelihood of the task-oriented response   for the user query   : \n\nHere   stands for task-specific instructions, such as \"summarize the following text: \", optionally accompanied by a few demonstrative samples for in-context learning [9]. The fine-tuning data can be broadly categorized into two types: Instruct Fine-Tuning (IFT) datasets to enhance the instruction-following abilities of LLMs and are usually adapted from existing NLP benchmarks [4,61]; and Chat Fine-Tuning (CFT) datasets for enhanced dialog ability and human value alignment [70,92]. There are preliminary explorations emphasizing the importance of data diversity over volume for fine-tuning data [20,95].",
            "score": 0.4882136600199294,
            "section_title": "BACKGROUND AND RELATED WORKS 2.1 Large Language Model (LLM) Data",
            "char_start_offset": 9174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1378
                },
                {
                    "start": 1381,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 596,
                    "end": 599,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 1547,
                    "end": 1550,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1760,
                    "end": 1763,
                    "matchedPaperCorpusId": "246485605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04681396484375
        },
        {
            "corpus_id": "266933337",
            "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
            "text": "Training Pipeline. LLMs undergo a series of exquisite development steps to implement high-quality text generation. The typical process of LLM development contains three stepspre-training, supervised fine-tuning, and learning from human feedback [11], [24], [33]- [40]. In what follows, we will briefly review the core steps for training LLMs to help readers understand the preliminary knowledge of LLM construction. \n\n\u2022 Pre-Training. The initial LLM is pre-trained on a largescale corpora to obtain extensive general knowledge. The pretraining corpora is a mixture of datasets from diverse sources, including web pages, books, and user dialog data. Moreover, specialized data, such as code, multilingual data, and scientific data, is incorporated to enhance LLMs's reasoning and task-solving abilities [41]- [44]. For the collected raw data, data pre-processing [2]- [5] is required to remove noise and redundancy. After that, tokenization [45] is used to transform textual data into token sequences for language modeling. By maximizing the likelihood of token sequences, the pre-trained model is empowered with impressive language understanding and generation ability. \n\n\u2022 Supervised Fine-Tuning (SFT). Different from the pretraining process which requires a huge demand for computational resources, SFT usually trains the model on a smaller scale but well-designed high-quality instances to unlock LLMs' ability to deal with prompts of multiple downstream tasks [46]. Among recent LLM fine-tuning methods, instruction tuning [11] has become the most popular one, in which the input prompts follow the instruction format. \n\n\u2022 Learning from Human Feedback. Reinforcement learning from human feedback (RLHF) is a typical method for aligning LLMs' responses with human preference [11], [47], [48] and enhancing the safety of LLMs [4], [47]. In RLHF, a reward model is trained with human feedback to score the quality of LLMs' output content, where the human preference is expressed as the ranking of multiple LLM outputs about a certain input prompt. Particularly, the architecture of a reward model can also be a language model.",
            "score": 0.4875451038247871,
            "section_title": "II. BACKGROUND",
            "char_start_offset": 7405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "258947756"
                },
                {
                    "start": 808,
                    "end": 812,
                    "matchedPaperCorpusId": "252668917"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "219683473"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07861328125
        },
        {
            "corpus_id": "276928587",
            "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews",
            "text": "Through chain-of-thought prompting (Wei et al., 2022), LLMs are guided with a step-by-step reasoning process, enabling them to break down complex problems into a series of incremental reasoning steps (Chu et al., 2024). This decomposition enhances LLMs' performance in reasoning tasks, advancing structured prompting techniques (Qiao et al., 2024;Cesista, 2025). Recent studies highlight that the systematization and structuring of reasoning significantly affect LLMs' performance, particularly their ability to perform multi-stage reasoning and identify key tasks at each stage. This can be achieved through independent language reasoning systems (Zhong et al., 2024) or supervised fine-tuning (Xu et al., 2025). In this paper, we introduce structured reasoning to automated peer review, aligning LLM behavior with human review practices by dividing the review process into a threestage structured procedure, improving review generation outcomes. \n\n3 Dataset and Benchmark",
            "score": 0.4873410217696536,
            "section_title": "Chain-of-thought Reasoning in Large Language Models",
            "char_start_offset": 6944,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 973
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 53,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 200,
                    "end": 218,
                    "matchedPaperCorpusId": "263153015"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02655029296875
        },
        {
            "corpus_id": "273403563",
            "title": "Think Thrice Before You Act: Progressive Thought Refinement in Large Language Models",
            "text": "Our experimental results show that LLMs trained with PTR can improve the quality of their previous answers across ten tasks, including knowledge reasoning, code generation, mathematical reasoning, comprehension, summarizing, and text generation. The average performance across these tasks improved from 49.6% to 53.5%, with a significant improvement on the MMLU task, where accuracy increased from 57.1% to 64.1% for Qwen2-8B. Notably, these improvements occur without task-specific fine-tuning, demonstrating that our method activates the model to learn progressive refinement from the PTR dataset. Moreover, in more open-ended tasks, LLMs have also demonstrated further improvements in answer quality and formatting beyond correctness. \n\nOur contributions are threefold: \n\n\u2022 We propose the PTR method to stimulate models' progressive refinement abilities and enhance generalization across various tasks without additional task-specific fine-tuning. \u2022 We design an efficient weak-strong model collaborative selection strategy to construct high-quality PTR datasets without extra feedback. \u2022 We introduce a novel weighted thought-mask fine-tuning method to instill general progressive refinement capabilities in LLMs.",
            "score": 0.48727187656935256,
            "section_title": "-Confucius",
            "char_start_offset": 4013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1217
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093505859375
        },
        {
            "corpus_id": "269757441",
            "title": "Levels of AI Agents: from Rules to Large Language Models",
            "text": "LLMs [4] are the category of Transformer-based language models that are characterized by having an enormous number of parameters, typically numbering in the hundreds of billions or even more. These models are trained on massive text datasets, enabling them to understand natural language and perform a wide range of complex tasks, primarily through text generation and comprehension. Some well-known examples of LLMs include GPT-3/4, PaLM, OPT, and LLaMA1/2. \n\nExtensive research has shown that scaling can largely improve the model capacity of LLMs. Thus, it is useful to establish a quantitative approach to characterizing the scaling effect. There are two representative scaling laws for Transformer language models: one from OpenAI [7], another from Google DeepMind [8]. \n\nThe \"pre-train+fine-tune\" procedure is replaced by another procedure called \"pre-train+prompt+predict\" [9]. In this paradigm, instead of adapting pre-trained LMs to downstream tasks via objective engineering, downstream tasks are reformulated to look more like those solved during the original LM training with the help of a textual prompt. \n\nFig. 2 The new paradigm as pre-training and fine-tuning in the foundation model In this way, by selecting the appropriate prompts, the model behavior can be manipulated so that the pre-trained LM itself can be used to predict the desired output, sometimes even without any additional task-specific training. Prompt engineering [10] works by finding the most appropriate prompt to allow a LM to solve the task at hand. The emergent abilities of LLMs are one of the most significant characteristics that distinguish them from smaller language models. Specifically, in-context learning (ICL) [11], instruction following [12] and reasoning with chain-of-thought (CoT) [13] are three typical emergent abilities for LLMs. \n\nParameter-efficient fine tuning (PEFT) [14] is a crucial technique used to adapt pre-trained language models (LLMs) to specialized downstream applications. PEFT can be divided into addition-based, selection/specification-based or reparameterization-based. It only needs fine-tuning a small subset of parameters, making it convenient for edge devices, and it can effectively mitigate the catastrophic forgetting problem.",
            "score": 0.4871182840261617,
            "section_title": "LLMs",
            "char_start_offset": 2729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1835
                },
                {
                    "start": 1838,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "236493269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08477783203125
        },
        {
            "corpus_id": "266977273",
            "title": "INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning",
            "text": "Large Language Models for Information Retrieval LLMs possess a remarkable capacity for language understanding, enabling them to be highly valuable in comprehending user queries and documents. Therefore, many researchers have explored applying LLMs to IR tasks (Zhu et al., 2023). Existing studies can be roughly categorized into two groups. The first group of studies treats LLMs as search agents to accomplish search tasks (Nakano et al., 2021;Qin et al., 2023a;Liu et al., 2023). A typical method is WebGPT (Nakano et al., 2021), which employs imitation learning to teach an LLM (i.e.,  to use search engines and answer questions like a human. The other group of studies mainly focuses on applying LLMs to specific IR tasks, such as query reformulation (Wang et al., 2023a;Srinivasan et al., 2022;Tang et al., 2023;Mao et al., 2023) and document ranking (Sun et al., 2023;Zhang et al., 2023b;Ma et al., 2023;Zhuang et al., 2023). Most of these studies rely on prompting LLMs in a zero-shot or few-shot manner. However, due to the inherent complexity of the IR task and the relative scarcity of IR-related concepts in natural language texts, LLMs often cannot achieve superior performance to fine-tuned smaller models in IR tasks (Sun et al., 2023;Gao et al., 2023). \n\nDifferent from existing studies, our research focuses on using instruction tuning to improve the overall performance of LLMs on various search tasks. This involves enhancing the models' abilities to interpret and respond to search-related instructions more effectively, thereby improving their utility in complex IR scenarios. \n\nInstruction Tuning for LLMs Instruction tuning (IT) aims at fine-tuning pre-trained LLMs on a collection of formatted instances in the form of natural language (Wei et al., 2022;Mishra et al., 2022;Wang et al., 2022Wang et al., , 2023b)). After IT, LLMs can better follow instructions and perform human tasks.",
            "score": 0.48708447472960076,
            "section_title": "Related Work",
            "char_start_offset": 3582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 894,
                    "end": 910,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1249,
                    "end": 1266,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1759,
                    "end": 1777,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1797,
                    "end": 1814,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 1814,
                    "end": 1836,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04559326171875
        },
        {
            "corpus_id": "259203671",
            "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling",
            "text": "Ensuring specific text structures, such as the logical order of concepts throughout the entire text, can be difficult. This difficulty is amplified for tasks requiring formal rules or grammar. This is because LLMs mainly focus on the local context information of words and sentences during pre-training, while ignoring global syntactic and structural knowledge. A proposal for addressing this problem is to adopt an iterative prompting approach in generating text [59], mimicking the process of human writing. In contrast, KGs offer a structured summary and emphasize the correlation of relevant concepts when complex events involving the same entity extend across multiple sentences [60], thus enhancing the process of structured text generation. \n\nHallucination. When generating factual or knowledgegrounded texts, LLMs may produce content that contradicts existing sources or lack supporting evidence. This challenge widely occurs in existing LLMs and is known as the problem of hallucination, which results in a drop in their performance and poses risks when deploying them for real-world applications. The cause of this issue is related to LLMs' limited ability to utilize correct internal and external knowledge during task-solving. To alleviate this problem, existing studies have resorted to alignment tuning strategies, which incorporate human feedback to fine-tune LLMs. KGs provide structured and explicit representations of knowledge, which can be dynamically incorporated to augment LLMs, resulting in more factual rationales and reduced hallucination in generation [61]. \n\nInconsistency. With the help of the chain-of-thought strat-egy, LLMs are capable of solving some complex reasoning tasks based on step-by-step reasoning. Despite their superior performance, LLMs may at times arrive at the desired answer based on an invalid reasoning path or produce an incorrect answer despite following a correct reasoning process. As a result, inconsistency arises between the derived answer and the underlying reasoning process. Additionally, research [62] has revealed that LLMs' abilities to forecast facts and answer queries are highly influenced by specific prompt templates and related entities. This is because that LLMs rely largely on simple heuristics to make predictions, their generations are correlated with co-occurrence frequencies between the target word and words in the prompt.",
            "score": 0.4867279685325203,
            "section_title": "D. Pros and Cons of LLMs",
            "char_start_offset": 21392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1584
                },
                {
                    "start": 1587,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2401
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "252873593"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "222272210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13525390625
        },
        {
            "corpus_id": "259203671",
            "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling",
            "text": "Instruction tuning is a unique finetuning approach that fine-tunes LLMs on a collection of natural language formatted instances. With this approach, LLMs are enabled to perform well on previously unseen tasks described through natural language instructions without relying on explicit examples [49]. For example, Wei et al. [57] fine-tuned a 137B parameter LLM on over 60 datasets based on instruction tuning and tested it on unseen task types. The experimental results demonstrated that the instruction-tuned model significantly outperformed its unmodified counterpart and zero-shot GPT-3. \n\nHuman Alignment. LLMs can be trained to generate highquality, harmless responses that align with human values through the technique of RLHF, which involves incorporating humans into the training loop using carefully designed labeling strategies. RLHF comprises three steps: 1) collecting a labeled dataset consisting of input prompts and target outputs to finetune LLMs in a supervised way; 2) training a reward model on the assembled data, and 3) optimizing LLMs by formulating its optimization as a reinforcement learning problem. With this approach, LLMs are enabled to generate appropriate outputs that adhere to human expectations. \n\nTools Manipulation. Traditional PLMs are trained on plain text data, which limits their ability to solve non-textual tasks. Besides, their abilities are limited by the pre-training corpus, and cannot effectively solve tasks requiring real-time knowledge. In response to these limitations, recent LLMs are developed with the ability to manipulate external tools such as search engine, calculator, and compiler to enhance their performance in specialized domains [58]. More recently, the plugin mechanism has been supported in LLMs, providing an avenue for implementing novel functions. This mechanism has significantly broadened the range of capacities for LLMs, making them more flexible and adaptable to diverse tasks. \n\nAlthough LLMs have made significant progress in natural language understanding and human-like content generation, they still have the following limitations and challenges [49]. \n\nUnstructured Generation. LLMs commonly rely on natural language prompts or instructions to generate text under specific conditions. This mechanism presents challenges for precisely constraining the generated outputs according to finegrained or structural criteria. Ensuring specific text structures, such as the logical order of concepts throughout the entire text, can be difficult.",
            "score": 0.4861478208025325,
            "section_title": "D. Pros and Cons of LLMs",
            "char_start_offset": 18994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2130
                },
                {
                    "start": 2133,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2397
                },
                {
                    "start": 2398,
                    "end": 2516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11968994140625
        },
        {
            "corpus_id": "266999728",
            "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
            "text": "Since reasoning abilities are essential for solving complex tasks, a wealth of research has focused on improving reasoning abilities in LLMs. Using supervised data to pretrain or fine-tune LLMs is a straightforward method to enhance their reasoning capabilities. For instance, during the pretraining stage, Lewkowycz et al. [102] incorporate a variety of mathematical corpora into their training set to improve quantitative reasoning abilities. In the fine-tuning stage, Rajani et al. [103] utilize a small instruction-tuning dataset to fine-tune the pretrained GPT model. Beyond supervised training, in-context learning [104,105] and prompt engineering [106] have also shown great potential for improving reasoning abilities. such as the Chain-of-Thought approach [104] and its variants [107,108]. Another line of research [109,110,111] has proposed interacting LLMs with external tools, such as a Python code interpreter [111], to enhance the accuracy of each reasoning step. Within the domain of MLLMs, these methods, namely instruction-tuning, prompt engineering, and tool usage have also been explored for multimodal reasoning, which we will later elaborate. \n\n3 Multimodal Large Language Models V-L Interaction Module: As shown in Figure 1, some models incorporate visual tokens as if they were a foreign language, directly injecting them into the input layer. For instance, LLaVA [13] directly inputs the visual signal into the input layer of the large language model. In contrast, other models, such as Flamingo [12], employ cross-attention layers to facilitate interactions between visual and language features within transformer blocks. Connector Architecture: The specifics of the connector design also play a pivotal role in determining the capabilities of MLLMs. A key differentiating factor among these models is their choice of visual-language connectors. Models such as BLIP-2 [22], Flamingo [12], and QWen-VL [112] utilize query-based connectors like Q-former/perceiver resampler. Conversely, LLaVA [13] and MiniGPT4-v2 [113] employ a multilayer perceptron (MLP) as the connector.",
            "score": 0.4854916698996574,
            "section_title": "Improving Reasoning Abilities for LLMs",
            "char_start_offset": 21707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 329,
                    "matchedPaperCorpusId": "250144408"
                },
                {
                    "start": 621,
                    "end": 626,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 654,
                    "end": 659,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 765,
                    "end": 770,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 824,
                    "end": 829,
                    "matchedPaperCorpusId": "253708270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0535888671875
        },
        {
            "corpus_id": "268248855",
            "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "text": "The examples provided discuss the use of Large Language Models (LLMs) as proxies for human behaviors and the systematic study of LLMs' personalities based on established personality trait theories. Here's a summary of the key points from the examples: 1. LLMs Mimicking Human Behaviors: -Recent studies have shown that LLMs can mimic human behaviors, including reasoning and cognitive abilities, as well as participate in social science experiments. \n\n-These studies are mostly empirical and based on case-by-case observations. 2. Systematic Study of LLMs' Personalities: -The authors propose a systematic and quantitative approach to study LLMs' behaviors by evaluating and inducing personality traits in LLMs. \n\n-This approach does not require supervised fine-tuning or human evaluation of generated utterances.",
            "score": 0.4848498895831562,
            "section_title": "GPT-4.0 with few shot",
            "char_start_offset": 52953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 813
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0251312255859375
        },
        {
            "corpus_id": "272397970",
            "title": "Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges",
            "text": "Delving deeper into specific cognitive abilities, Srinivasan et al. [18] proposed novel methods based on cognitive science principles to test LLMs' common sense reasoning abilities through prototype analysis and proverb understanding. These methods offer new ways to assess LLMs' cognitive capabilities in more nuanced and context-dependent tasks. Binz and Schulz [19] used tools from cognitive psychology to study GPT-3, assessing its decision-making, information search, deliberation, and causal reasoning abilities. Their approach demonstrates the potential of cognitive psychology in studying AI and demystifying how LLMs solve tasks. \n\nIn summary, Large Language Models exhibit remarkable parallels with human cognitive processes, particularly in language and sensory tasks, yet they fall short in several critical areas, such as reasoning under novel conditions and functional linguistic competence. The diverse methodologies employed to evaluate LLMs' cognitive abilities highlight both their potential and limitations as models of human cognition. As LLMs continue to evolve, they provide a valuable tool for exploring the nature of human intelligence, but their differences from human cognitive processes must be carefully considered. Future research should aim to refine these models further, improving their alignment with human cognition and addressing the gaps that currently exist. Understanding the complex interplay between LLMs and human cognitive processes will advance both AI and cognitive science, bridging the divide between machine and human intelligence.",
            "score": 0.48322656353527277,
            "section_title": "B. Methods for evaluating LLMs cognitive abilities",
            "char_start_offset": 9165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1578
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "261392381"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "250113371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1134033203125
        },
        {
            "corpus_id": "266693275",
            "title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
            "text": "In-context learning (ICL) is a paradigm that enables LLMs to learn tasks with examples in the form of demonstrations (Brown et al., 2020). It leverages task instructions and a few demonstration examples to convey the task semantics, which are then combined with query questions to create inputs for the language model to make predictions. ICL has demonstrated impressive performance in various natural language processing and code intelligence. However, the performance of ICL is known to rely on high-quality demonstrations (Gao et al., 2023b) strongly. To fully unleash the potential of ICL, LogicAsker utilizes the weak skills of each LLM to construct both correct and incorrect examples with expected answers and explanations as demonstrations to facilitate the reasoning of LLMs. The generation process follows a similar approach to the test case generation described in \u00a7 3.2, with the difference being that we append a brief explanation and the correct answer at the end of each case. We show an instance of the demonstration example in Appendix D. Fine-tuning is another widely used technique to enhance model performance on specific tasks (Moslem et al., 2023;Wei et al., 2021). This process involves taking a pre-trained model and further training it on a smaller, task-specific dataset. The rationale behind fine-tuning is to leverage the learned features and knowledge of the pre-trained model, adapting it to particular nuances and characteristics of a targeted domain or task. In this paper, we directly utilize the data generated by LogicAsker to fine-tune LLMs to improve their reasoning ability.",
            "score": 0.48304553533629957,
            "section_title": "Improving LLMs",
            "char_start_offset": 15498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1612
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07080078125
        },
        {
            "corpus_id": "266166902",
            "title": "Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters",
            "text": "As mechanistic understanding and control of LLMs remains complex, researchers have increasingly adopted methods from human behavioral sciences for characterizing LLMs' behavior: in the same way that the human brain is largely a black box that must be probed using experimental methods and constructs, LLMs may be studied in the same way. \n\nIn addition to studies that have used the methods of cognitive psychology to understand LLMs' reasoning and grammatical abilities (e.g., Linzen et al., 2016;Futrell et al., 2019;Cai et al., 2023), researchers have increasingly adapted methods from psychometrics (Miotto et al., 2022;Bodroza et al., 2023;Abramski et al., 2023), which seek to characterize LLMs in terms of personality variables such as agreeableness and conscientiousness, and methods from behavioral economics (Cartwright, 2018;Phelps and Russell, 2023;Horton, 2023), which characterize LLMs' decision-making in terms of preferences for risk and reward. \n\nPrior research on prompting techniques (Wei et al., 2022;Wang et al., 2023) has shown that subtle modifications in input prompts can lead to varied outcomes in reasoning tasks (Cobbe et al., 2021). Srivastava et al. (2022) revealed that Large Language Models (LLMs) are notably susceptible to the precise wording of natural language questions, especially when presented in a multiple-choice setting. In a recent study, Ouyang et al. (2023) emphasized the influence of temperature adjustments on LLM's performance in code generation tasks. Unlike previous studies, our research delves into the sensitivity of LLMs concerning economic decisionmaking abilities. \n\nOur work is a focused followup on Binz and Schulz (2023), investigating the sensitivity of one of their results to changes in prompt and hyperparameters. Binz and Schulz (2023) evaluated on decision-making, information search, deliberation, and causal reasoning in text-davinci-002 (Brown et al., 2020) by presenting it with prompts such as the one shown in Figure 1.",
            "score": 0.48289537851595205,
            "section_title": "Background and Related Work",
            "char_start_offset": 1935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1991
                }
            ],
            "ref_mentions": [
                {
                    "start": 477,
                    "end": 497,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 497,
                    "end": 518,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 1658,
                    "end": 1680,
                    "matchedPaperCorpusId": "250113371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07501220703125
        },
        {
            "corpus_id": "277151246",
            "title": "Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them",
            "text": "In this section, we discuss the implications and limitations of our results and further connect them to existing studies. Do LLMs reason or recite? Recent studies suggested that LLMs can tackle certain hard tasks that are conceived to involve sophisticated reasoning [8,35], yet fail on easy problems at the same time [5,34]. This sparks a scientific debate on whether they genuinely master human-like reasoning or merely recite answers to similar problems in their training set. By empirically showing that LLMs are able to extrapolate the never-before-seen rules learned in finetuning to novel domains and modalities, our results add another piece of evidence for the former argument, since a model that merely recites should not exhibit such generalizability. That being said, we do not argue that LLMs do not recite at all: as shown in Fig. 3, Fig. 4, and Fig. 5, there still exists a gap between the performance on the fine-tuning domain and other domains, implying a non-negligible fraction of memorization in the problem-solving process of LLMs. \n\nIs reasoning an emergent ability? A notable feature of LLMs is that some of their abilities are emergent [33,48], i.e., only existing after sufficient scaling. Our results in Fig. 4 suggest that the reasoning ability of LLMs may be also emergent: for complex operator overloading, an 8B Llama model exhibits considerably smaller performance gaps between the fine-tuning domain and the test domain, compared to a 3B model from the same model fam-ily. For example, overloading a{+}b = a 2 + b results in an accuracy gap of 20% for Llama-3.1-8B, while inducing a much larger gap of 62% for Llama-3.2-3B. This implies the existence of a phase change in how LLMs solve math reasoning tasks as the model size grows. \n\nFine-tuning or in-context learning? Technically, our proposed MisFT differs from many existing LLM evaluation pipelines in that it requires an additional fine-tuning phase while existing approaches typically use manuallyengineered prompts to steer LLMs for evaluation, such as providing counterfactual examples or rules [27,39].",
            "score": 0.4825649036731119,
            "section_title": "Discussion",
            "char_start_offset": 27730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1764
                },
                {
                    "start": 1767,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "262083829"
                },
                {
                    "start": 1164,
                    "end": 1167,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061767578125
        },
        {
            "corpus_id": "267412707",
            "title": "Integration of cognitive tasks into artificial general intelligence test for large models",
            "text": "LLMs have demonstrated remarkable proficiency in accomplishing both \"pretext tasks\" and \"downstream tasks\".In the pretext task, LLMs such as GPT 1,14 and BERT 36 can learn language representations from large-scale texts without the need for manual annotation through self-supervised learning methods.The foundational language knowledge acquired by LLMs demonstrated zero-shot generalization capabilities.This facilitates their broad applicability across various downstream tasks, 37 such as understanding tasks, [38][39][40] generation tasks, 41,42 and reasoning tasks, 43,44 as shown in Table 1.\n\nFurthermore, LLMs have extended their language ability to encompass cognitive capabilities, such as few-shot learning, 14 in-context learning, 45 problem solving. 46,479][50][51][52] Interestingly, the emergence of \"advanced intelligence\" is not a result of deliberate training on specific tasks but rather a natural consequence of the pre-training process using extensive amounts of textual data.\n\nInitiating training for LLMs from scratch requires significant time and computational resources.To circumvent potentially expensive and ineffective training, it is conventional to periodically evaluate the capabilities of LLMs during the training process, thereby enabling timely adaptations of training strategies.However, relying solely on language tasks fails to provide a comprehensive evaluation of the capabilities of LLMs.A lower loss in language tasks does not necessarily indicate a higher level of intelligence.There is a need to bridge the gap between language tasks testing and general intelligence evaluation, transitioning from language tests to cognitive tests, and ultimately to AGI test, as shown in  provide valuable insights into comprehending and evaluating the cognitive abilities of LLMs.For instance, a commonly employed approach in the assessment of \"working memory\" is the n-back task.\n\nThis task is like a memory game where participants judge whether a new stimulus matches one from the previous n stimuli. 53Certain cognitive tasks that were traditionally employed to assess human or animal cognition are now being utilized to evaluate the cognitive capabilities of LLMs. 54,55Some pioneering work reports that LLMs have demonstrated human-like performance. 54,56,57For instance, Theory of mind (ToM) has been applied to assess large models, revealing that GPT-4 exhibits ToM capabilities similar to human inference patterns. 48,56,58",
            "score": 0.4822746495478645,
            "section_title": "From language tests to cognitive tests",
            "char_start_offset": 5766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 596
                },
                {
                    "start": 598,
                    "end": 766
                },
                {
                    "start": 766,
                    "end": 995
                },
                {
                    "start": 997,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1312
                },
                {
                    "start": 1312,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1518
                },
                {
                    "start": 1518,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 1907
                },
                {
                    "start": 1909,
                    "end": 2032
                },
                {
                    "start": 2032,
                    "end": 2201
                },
                {
                    "start": 2201,
                    "end": 2290
                },
                {
                    "start": 2290,
                    "end": 2458
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 149,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 159,
                    "end": 161,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 480,
                    "end": 482,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "8310135"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 546,
                    "end": 548,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 570,
                    "end": 573,
                    "matchedPaperCorpusId": "19240019"
                },
                {
                    "start": 573,
                    "end": 575,
                    "matchedPaperCorpusId": "85504763"
                },
                {
                    "start": 717,
                    "end": 719,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 761,
                    "end": 764,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 764,
                    "end": 766,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "252668917"
                },
                {
                    "start": 772,
                    "end": 776,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 2030,
                    "end": 2032,
                    "matchedPaperCorpusId": "14432275"
                },
                {
                    "start": 2199,
                    "end": 2201,
                    "matchedPaperCorpusId": "257257414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047515869140625
        },
        {
            "corpus_id": "271709856",
            "title": "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.",
            "text": "Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).This study investigates whether such constraints on generation space impact LLMs\u2019 abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs\u2019 performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs\u2019 reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.",
            "score": 0.4813983162582596,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "The KM scaling law [74] focuses on the benefits of increasing model size and dataset size, while the Chinchilla scaling law [56] highlights the importance of balancing model size with the appropriate amount of data for optimal performance.Additional research also indicates that scaling can significantly enhance the capacity of LLMs [15,25].This improvement occurs because larger models can capture more complex patterns and relationships within the data.Additionally, increasing the amount of training data exposes the model to a broader range of information, further improving its generalization abilities and enabling it to handle diverse and challenging tasks more effectively.\n\nLLMs primarily possess three basic abilities: language generation, knowledge utilization, and complex reasoning.Current mainstream LLMs perform language generation by proposing the next token based on previous tokens [10].LLMs can also generate specialized languages, such as programming code, via code synthesis [50].The ability of knowledge utilization refers to LLMs that can accomplish knowledge-intensive tasks through knowledge provided during pre-training and within prompts.This ability is primarily evaluated through question-answering (QA) tasks [71] and knowledge graph completion tasks [139].Complex reasoning refers to the ability to understand and use supporting evidence or logic to derive conclusions or make decisions [59].This can be assessed through tasks, such as knowledge reasoning [125] and symbolic reasoning [154].\n\nLLMs also have some advanced abilities, such as interacting with external environments or user tools.Some studies have enabled LLMs to perform specific tasks like autonomous driving through external interfaces [22] or control characters in games to achieve specific goals [151].When solving complex problems, LLMs can use external tools if deemed necessary, such as a search engine [102], image generation models [11], and compilers [43].Such tools are used to enhance the performance of LLMs in various applications.These capabilities stem from LLMs' proficiency in understanding context, generating relevant output, and interacting with other systems through well-defined interfaces, thereby enhancing their performance in various applications.\n\nEmergent abilities manifest primarily in three ways: in-context learning [33], instruction following [127], and step-by-step reasoning [113].",
            "score": 0.48079158033560637,
            "section_title": "Large Language Models",
            "char_start_offset": 6774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 342
                },
                {
                    "start": 342,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 682
                },
                {
                    "start": 684,
                    "end": 796
                },
                {
                    "start": 796,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 1002
                },
                {
                    "start": 1002,
                    "end": 1166
                },
                {
                    "start": 1166,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1523
                },
                {
                    "start": 1525,
                    "end": 1626
                },
                {
                    "start": 1626,
                    "end": 1803
                },
                {
                    "start": 1803,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2042
                },
                {
                    "start": 2042,
                    "end": 2271
                },
                {
                    "start": 2273,
                    "end": 2414
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 338,
                    "end": 341,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 901,
                    "end": 905,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 1282,
                    "end": 1287,
                    "matchedPaperCorpusId": "5378837"
                },
                {
                    "start": 1419,
                    "end": 1423,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 1488,
                    "end": 1493,
                    "matchedPaperCorpusId": "250729995"
                },
                {
                    "start": 1517,
                    "end": 1522,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1797,
                    "end": 1802,
                    "matchedPaperCorpusId": "268042457"
                },
                {
                    "start": 1958,
                    "end": 1962,
                    "matchedPaperCorpusId": "253708270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058563232421875
        },
        {
            "corpus_id": "276902977",
            "title": "MMGenBench: Fully Automatically Evaluating LMMs from the Text-to-Image Generation Perspective",
            "text": "The rapid advancements in LLMs have prompted the development of diverse benchmarks to evaluate their performance. Early efforts, such as LMExamQA [7], introduced the \"Language-Model-as-an-Examiner\" framework to provide a scalable and comprehensive evaluation for LLMs. However, there remains a significant gap in the evaluation of the reasoning capabilities of LLMs, especially in dynamic contexts. To address this deficiency, recent frameworks like DYVAL [83] and DYVAL2 [84] have focused on dynamic reasoning tasks. DYVAL concentrates on reasoning abilities, while DYVAL2 extends the evaluation to a broader psychometric approach, thereby providing deeper insights into cognitive evaluations. Additionally, Auto-Bencher [38] has automated the generation of challenging and novel datasets, specifically designed for the evaluation of LLM. Platforms such as UNIGEN [69] and Task Me Anything [80] aim to further enhance evaluation by developing domain-specific benchmarks tailored to the unique capabilities of LLMs.",
            "score": 0.48043351721292304,
            "section_title": "A.1. Automatic Benchmarks",
            "char_start_offset": 26091,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1015
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "259095491"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "263310319"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "267897463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06634521484375
        },
        {
            "corpus_id": "273098893",
            "title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement",
            "text": "Finally, the correct reasoning paths, along with the instructions and answers, are used to fine-tune , enhancing its reasoning capability. that they struggle to improve the given LLMs on out-of-domain (OOD) tasks that are different from the fine-tuning data that they generate reasoning paths and further train the model on. Specifically, we evaluate them on six OOD tasks including math, logic, common sense, and natural language inference. Our results indicate that further post-training using data produced by these methods leads to a 4.6% decline on the given LLM's performance on such OOD tasks. In other words, existing methods don't generalize well on OOD tasks and cannot make given LLMs reasoning generalists. \n\nTo fill this gap, we explore how to self-synthesize reasoning paths as post training data to make LLMs generalize well across various OOD reasoning tasks. We hypothesize that existing methods fail to generalize to OOD tasks because their synthesized reasoning paths are overly tailored to specific training tasks and lack diversity. These methods predominantly utilize a chain-of-thought approach, which leads to uniform and narrow reasoning patterns, rather than employing a range of cognitive strategies that foster diverse thinking steps. To resolve these shortcomings, we introduce ReGenesis, a reasoning generalist framework designed to generate general and diverse reasoning paths guided by various reasoning principles. ReGenesis adopts a methodological approach that transitions from abstract and general reasoning guidelines to concrete and specific reasoning solutions, as shown in Figure 1. More specifically, we first create multiple general task-agnostic reasoning guidelines in the beginning of the reasoning path generation. Then, given each general reasoning guideline, we prompt the LLM to augment such general guideline into a task-specific reasoning guideline. Note that in this step we fill in task-specific details into general guidelines while still maintaining their task-agnostic reasoning structure, as shown in Figure 1. Then we feed each task-specific reasoning guideline into the given LLM to generate a reasoning structure without directly solving the tasks. Each reasoning structure is subsequently used to create one candidate reasoning path.",
            "score": 0.48023073078149614,
            "section_title": "Final Reasoning Solution:",
            "char_start_offset": 5260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2294
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29150390625
        },
        {
            "corpus_id": "267027689",
            "title": "Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges",
            "text": "LLMs to enhance their reasoning capabilities. This approach aligns model outputs closely with labeled datasets, allowing the models to produce highly accurate predictions within specific domains. One such study by [109] demonstrated the efficacy of fine-tuning a pre-trained GPT model, which generated rationales for predictions on the CoS-E dataset [121]. The results revealed that models trained with explanations exhibited improved performance in commonsense question-answering tasks. However, the effectiveness of fine-tuning methods heavily relies on the availability of a specific dataset that includes explicit reasoning steps. Acquiring such a dataset can prove to be challenging. Moreover, the scope of inference from fine-tuned models is restricted to the dataset's domain, hinging largely on the data's inferential quality. This constraint highlights the benefits and limitations of fully supervised fine-tuning, as it narrows the model's reasoning abilities to the dataset's specific domain. Consequently, it underscores the need to explore methods that harness LLMs' intrinsic reasoning capabilities, potentially providing broader relevance and deeper insights beyond the limitations of domain-specific datasets. \n\n3.4.2 Prompt Engineering for Reasoning. Efforts have been made in recent research to tackle the constraints inherent in the fine-tuning process of LLMs. These fine-tuning methods tend to overfit specific dataset distributions, reducing their effectiveness on more diverse datasets. In response to this issue, a variety of strategies have been proposed. These strategies are designed to draw upon the robust reasoning abilities inherent in LLMs by leveraging their extensively pretrained parameters. One approach involves guiding LLMs to generate inference and reasoning through demonstrations or prompts. For example, Wei et al. [136] introduced the \"Chain of Thought\" (CoT) method, which utilized natural language reasoning steps as prompts for the model. By integrating CoT within a few-shot prompting framework, the model leveraged its extensive parameters to produce analogous chains of reasoning. Consequently, this approach empowered the model to adeptly navigate complex reasoning tasks across diverse domains, obviating additional training or fine-tuning. This innovation underscored the model's inherent capability to generate deductive pathways, significantly enhancing its applicability and versatility in problem-solving scenarios without extensive domain-specific adaptations.",
            "score": 0.4796061595864082,
            "section_title": "Supervised Fine-tuning for Reasoning. Previous studies have primarily focused on fully supervised fine-tuning",
            "char_start_offset": 47560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2291
                },
                {
                    "start": 2292,
                    "end": 2517
                }
            ],
            "ref_mentions": [
                {
                    "start": 1857,
                    "end": 1862,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.218017578125
        },
        {
            "corpus_id": "260735852",
            "title": "Shepherd: A Critic for Language Model Generation",
            "text": "LLMs performance has explored an iterative natural language feedback and refinement approach. Self-Improve (Huang et al., 2022) uses highconfidence rationale-augmented answers generated by a 540B-parameter LLM for fine-tuning it and shows improvements on general reasoning abilities of the model. Saunders et al. (2022) finetune a LLM to write natural language critiques for both model generated and human written text and argue that larger models can write more helpful critiques and can use these critiques to self-refine. Self-Refine (Madaan et al., 2023) uses a single LLM for generation, providing feedback, and refinement. Critic (Gou et al., 2023) proposes interacting with tools for evaluating certain aspects of text, and using the feedback for refinement. SelFee (Ye et al., 2023) collects generations, feedback and revised generations from ChatGPT and finetunes LLaMA models to build a critique model. Self-Correct (Welleck et al., 2023) decouples the generator from the corrector and shows generations can be improved even when the corrector is much smaller than the base generator.",
            "score": 0.4787415338962655,
            "section_title": "Critique models. Recent work on improving",
            "char_start_offset": 28707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1094
                }
            ],
            "ref_mentions": [
                {
                    "start": 926,
                    "end": 948,
                    "matchedPaperCorpusId": "253244506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0615234375
        },
        {
            "corpus_id": "270123515",
            "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities",
            "text": "We investigated how fine-tuning on parallel data affects the qualitative advantages of LLMs for machine translation. While previous research predominantly focused on summary quality metrics like COMET, our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings. To prevent these degradations, we develop a fine-tuning method tailored for machine translation, that combines monolingual and parallel data. We show that this approach mitigates the degradation of LLMs' qualitative advantages, thereby preserving their capabilities while improving general translation quality.",
            "score": 0.47850351343200115,
            "section_title": "Conclusion",
            "char_start_offset": 21885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1508
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "270923714",
            "title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model",
            "text": "The evolution of large language models (LLMs), including advancements seen in the generative pre-trained Transformer (GPT) series by OpenAI, marks a significant milestone in the field of natural language processing (NLP).A pivotal strategy in their development has been instruction-tuning, which leverages human-curated prompts, feedback, and benchmark datasets to tailor LLMs' adaptability to specific domains, such as complex reasoning (Wei et al., 2022;Chai et al., 2024;Li et al., 2024) and coding (Rozi\u00e8re et al., 2023;Li et al., 2023a).Instruction tuning (Wang et al., 2023) emerges as an innovative approach, creating new tasks with bespoke instructions, thus enhancing model performance and cost-effectiveness.The diversity and scope of instruction data are critical for the model's ability to generalize and excel in previously unseen tasks, underpinning the continuous advancement and specialization of LLMs in various fields.Instruction tuning is a groundbreaking technique designed to fine-tune these powerful LLMs for better task-specific performance without intensive retraining on massive datasets, which delicately recalibrates the response of LLMs by giving them instructions or prompts that are carefully crafted to elicit more accurate, contextually appropriate, or nuanced outputs.Many recent works (Sun et al., 2023;Luo et al., 2023) try to synthesize instructions, input, and output samples from an LLM and then filter invalid or similar ones.However, these methods focus on generating diverse single-turn dialogues based on the query or response.In Figure 1, continuing pre-training with raw documents and instruction tuning with human-annotated SFT data can inject domain-specific knowledge, but the process requires two-stage training and large-scale data.Therefore, How to produce reasonable multiturn dialogues for instruction turning to inject the knowledge of raw documents into LLMs only with instruction tuning.",
            "score": 0.4782105600289649,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1781
                },
                {
                    "start": 1781,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 456,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 561,
                    "end": 580,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06878662109375
        },
        {
            "corpus_id": "277621450",
            "title": "An Explicit Syllogistic Legal Reasoning Framework for Large Language Models",
            "text": "Existing work has explored various strategies to enhance the reasoning capabilities of large language models (LLMs). One such strategy is the Chain-of-Thought (CoT) prompting technique [29,42], which encourages the model to generate a series of intermediate reasoning steps through specific prompts. For example, the simple prompt \"Let's think step by step\" [13] has been shown to improve the reasoning ability of LLMs. Further, In-Context Learning (ICL) [31] involves providing examples containing intermediate reasoning steps to teach the model how to perform reasoning. In the retrievalaugmented generation scenarios, enhancing the LLM's reasoning ability can be achieved by improving the prompts input to the LLM using its feedback, including compressing the retrieved documents [12,18] and iteratively retrieval to augment generation [28]. Additionally, supervised fine-tuning using task-related data [7], enhancing supervised fine-tuning with retrieved document [41], and incorporating CoT data into fine-tuning [22] can all strengthen the reasoning ability of LLMs by adjusting their parameters. Recently, the release of OpenAI's o1 has inspired many works [38,44] to explore reasoning techniques similar to o1, emphasizing the importance of scaling CoT and reinforcement learning to strengthen LLM reasoning abilities. However, all of the aforementioned works primarily focus on open-domain problems such as mathematics, code, and commonsense reasoning, without addressing explicit reasoning in the legal domain. This gap is the focus of our paper, which aims to equip LLMs with the ability to perform explicit syllogistic legal reasoning.",
            "score": 0.4778629339209492,
            "section_title": "Reasoning in LLM",
            "char_start_offset": 9643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1647
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 189,
                    "matchedPaperCorpusId": "257205763"
                },
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 787,
                    "end": 790,
                    "matchedPaperCorpusId": "263830231"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0726318359375
        },
        {
            "corpus_id": "268385476",
            "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
            "text": "For each task, we designate a specific dataset as the training set.The remaining selected datasets are subsequently divided into two groups: in-domain datasets, closely aligned with the training set in terms of content and structure, and out-of-domain datasets, which possess significant differences.With these datasets, our research investigates two critical questions: i) the ability of fine-tuned LLMs to adapt to both in-domain and out-of-domain test sets, and ii) the impact of finetuning on the ICL ability of LLMs across different types of tasks.\n\nWe find that models fine-tuned on text generation and classification tasks exhibit different behaviors when evaluated on test sets.Specifically, we observe that models fine-tuned for classification tasks tend to exhibit positive transfer when applied to out-of-domain datasets of the same fine-tuning/test task type.In contrast, models fine-tuned on generation tasks frequently experience negative transfer under similar conditions.Interestingly, while fine-tuning the LLMs on generation tasks generally does not detrimentally affect their performance on classification tasks, the reverse is not true; models fine-tuned on classification tasks typically fail to work on generation tasks.Moreover, we experimentally observe that integrating the ICL strategy during fine-tuning on generation tasks can enhance an LLM's generalization ability.We also investigate other factors, such as training data size and the number of in-context examples.We hope this study offers comprehensive insights into finetuning strategies for LLMs, not only in enhancing task-specific performance but also in fostering broader generalization abilities.",
            "score": 0.47732460722892817,
            "section_title": "Introduction",
            "char_start_offset": 1781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 67,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 553
                },
                {
                    "start": 555,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1495
                },
                {
                    "start": 1495,
                    "end": 1684
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1993408203125
        },
        {
            "corpus_id": "266899568",
            "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
            "text": "In light of the above issues, we systematically study if model editing hurts the general abilities of LLMs. This work studies model editing in the singleversus sequential-editing and instanceversus batch-editing settings. The edited models are evaluated on a variety of downstream tasks to see if there are any side effects on performance before and after editing. Extensive empirical experiments are conducted on four popular editing methods: KN (Dai et al., 2022), MEND (Mitchell et al., 2022a), ROME (Meng et al., 2022), and MEMIT (Meng et al., 2023) applied to three representative LLMs: GPT-2 XL (1.5B) (Radford et al., 2019), LLaMA-1 (7B) (Touvron et al., 2023a), and LLaMA-2 (7B) (Touvron et al., 2023b). Eight representative tasks including reasoning (Cobbe et al., 2021), natural language inference (Dagan et al., 2005), open-domain QA (Kwiatkowski et al., 2019), closed-domain QA (Clark et al., 2019), dialogue (Cui et al., 2020), summarization (Gliwa et al., 2019), named entity recognition (Sang and Meulder, 2003), and sentiment analysis (Socher et al., 2013) are employed to understand the impact of model editing on the general abilities of LLMs. \n\nExperimental results show that existing LLMs are not robust to weight perturbations, and editing even a few parameters can significantly affect their general abilities. Strikingly, with a single pass of editing involving less than 1% parameters, LLaMA-1 (7B) exhibited a drastic performance degradation to nearly 0 on all the tasks we tried. These results demonstrate that current editing algorithms struggle to work effectively in tandem with LLMs to simultaneously improve model factuality and maintain general abilities. \n\nFurthermore, our analysis of the causes of side effects reveals that current model editing methods change the original model weights too much, resulting in overfitting to new editing facts.",
            "score": 0.47730747587650135,
            "section_title": "Introduction",
            "char_start_offset": 2023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 465,
                    "matchedPaperCorpusId": "233296761"
                },
                {
                    "start": 472,
                    "end": 496,
                    "matchedPaperCorpusId": "239050360"
                },
                {
                    "start": 503,
                    "end": 522,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 608,
                    "end": 630,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 808,
                    "end": 828,
                    "matchedPaperCorpusId": "8587959"
                },
                {
                    "start": 845,
                    "end": 871,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 890,
                    "end": 910,
                    "matchedPaperCorpusId": "165163607"
                },
                {
                    "start": 955,
                    "end": 975,
                    "matchedPaperCorpusId": "208010268"
                },
                {
                    "start": 1002,
                    "end": 1026,
                    "matchedPaperCorpusId": "2470716"
                },
                {
                    "start": 1051,
                    "end": 1072,
                    "matchedPaperCorpusId": "990233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4814453125
        },
        {
            "corpus_id": "261705916",
            "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models",
            "text": "Though numerous technical solutions have been proposed in the survey for hallucinations in LLMs, there exist some potential directions: \n\n\u2022 Data Construction Management. As previously discussed, the style, and knowledge of LLMs is basically learned during model pre-training. High quality data present promising opportunities for facilitating the reduction of hallucinations in LLMs (Kirstain et al., 2022). Inspired by the basic rule of machine learning models: \"Garbage input, garbage output\", Zhou et al. (2023) proposes the superficial alignment hypothesis, which views alignment as learning to interact with the user. The results of simple fine-tuning on a few high-quality samples demonstrate that data quality and diversity outweigh the importance of fine-tuning large-scale instructions (Mishra et al., 2021;Wei et al., 2022a;Sanh et al., 2022) andRLHF (Bai et al., 2022;Ouyang et al., 2022). To perform efficiently in knowledge-intensive verticals, we argue that construction of entity-centred fine-tuned instructions (Bao et al., 2023;Gui et al., 2023;Wei Zhu and Wang, 2023) is a promising direction that it can combine the structured knowledge and semantic relevance of knowledge graphs to enhance the factual-ity of generated entity information. Another feasible proposal is to incorporate a self-curation phase (Li et al., 2023g) in the instruction construction process to rate the quality of candidate pairs. During the iteration process, quality evaluation (Chen et al., 2023c) based on manual or automated rule constraints could provide self-correction capacity. \n\n\u2022 Downstream Task Alignment. Generic LLMs have a certain degree of natural language problem comprehension in a variety of open environments. However, the main problem still remains in the deviation from the application requirements, which leads to emergence of diverse hallucinations. Thus, downstream task alignment especially built on vertical domain cognition necessitates expanded symbolic reasoning, decomposition and planning of complex tasks, and faithful external knowledge injection. Specifically, while expert in language processing, LLMs struggle to make breakthroughs in mathematical abilities, a deficiency attributable to the textual training objective.",
            "score": 0.4769925540445029,
            "section_title": "Future Directions",
            "char_start_offset": 30965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 138,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 406,
                    "matchedPaperCorpusId": "238583118"
                },
                {
                    "start": 816,
                    "end": 834,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 834,
                    "end": 856,
                    "matchedPaperCorpusId": "239009562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044525146484375
        },
        {
            "corpus_id": "268553782",
            "title": "Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases",
            "text": "Numerous efforts have been made toward the generation of creative language forms, with a range of findings regarding whether or not popular LLMs truly exhibit human-level creativity in these tasks. Chakrabarty et al. (2023) present work applying the Torrance Tests of Creative Thinking (TTCT) to objectively analyze the outputs of LLMs and human authors on a narrative writing task, finding that LLM generations perform measurably worse than humans, passing 3-10x fewer criteria outlined by the TTCT. However, it is important to note that this was in comparison to professional authors, who represent a very niche subset of the best human writers available. \n\nOn the other hand, G\u00f3mez-Rodr\u00edguez and Williams (2023) compare human and LLM-authored narratives and find that LLMs are able to match or surpass human performance on several of the evaluation criteria they present. However, in this case, the \"creativity\" of the task was somewhat diminished by having prescribed rules about the topic, characters, and writing style, where the task may be more construed as emulating the writing of an existing work. However, similarly, both Franceschelli and Musolesi (2023) and Clark et al. (2021) observe that humans are infrequently able to distinguish creative works written by other humans from those authored by LLMs, with the latter often achieving very high-quality outputs. Overall, there is clear potential and room for improvement in the field of automatically generating creative language forms. A popular trend is to investigate the extent to which models can be trained to generate language forms where training data is scarce. W\u00f6ckener et al. (2021) investigate this for the generation of poetry using \u223c16k and \u223c67k quatrains of English and German poetry respectively, and notice difficulties in GPT-2 learning sub-lexical phenomena including rhyme from this number of training examples alone. However, poetry presents a highly restrictive form of literary language where many forms contain formal constraints regarding length, syllable count, and metrical patterns. Additionally, Van de Cruys (2020) presents work on the generation of Shakespearean sonnets, another literary niche that contains an even more limited number of available training samples.",
            "score": 0.4761384315659126,
            "section_title": "Creative Language Generation",
            "char_start_offset": 11373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 1172,
                    "end": 1191,
                    "matchedPaperCorpusId": "235694265"
                },
                {
                    "start": 1635,
                    "end": 1657,
                    "matchedPaperCorpusId": "241583252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06890869140625
        },
        {
            "corpus_id": "264306101",
            "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
            "text": "LLM-as-Agent Before the rise of LLMs (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;Zeng et al., 2022), agent tasks primarily relied on reinforcement learning or encoder models like BERT. With the advent of LLMs, research shifted towards LLM agents. Notably, Re-Act (Yao et al., 2023) innovatively combined CoT reasoning with agent actions. Several studies also applied language models to specific agent tasks, such as online shopping (Yao et al., 2022), web browsing (Deng et al., 2023), and household exploration (Shridhar et al., 2020). Recently, with ChatGPT showcasing advanced planning and reasoning skills, research like ReWOO (Xu et al., 2023) and RCI (Kim et al., 2023) has delved into prompting strategies and frameworks to boost language model efficiency in agent tasks without the need for fine-tuning. \n\nInstruction Tuning Instruction tuning aims at aligning the language models to follow human instructions and produce outputs that better fit human preferences. Instruction tuning mainly focus on training language models to follow human instructions among multiple general tasks. For instance, FLAN (Wei et al., 2022a) and T0 (Sanh et al., 2022) demonstrates the strong zero-shot generalization ability of language models fine-tuned on multiple task datasets. Further, FLAN-V2 (Longpre et al., 2023) explores the performance of instruction tuning across multiple scales of models and datasets. With the impressive alignment capability demonstrated by commercial LLMs, many recent works (Chiang et al., 2023;Wang et al., 2023a) propose methods to distill instruction tuning dataset from close-sourced model to enhance the alignment of open-source models.",
            "score": 0.4758506779384333,
            "section_title": "RELATED WORK",
            "char_start_offset": 22930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 284,
                    "end": 302,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "250264533"
                },
                {
                    "start": 533,
                    "end": 556,
                    "matchedPaperCorpusId": "222208810"
                },
                {
                    "start": 1132,
                    "end": 1151,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1159,
                    "end": 1178,
                    "matchedPaperCorpusId": "239009562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0482177734375
        },
        {
            "corpus_id": "270258104",
            "title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
            "text": "Large language models (LLMs) [1,2,3] have demonstrated significant potential in reasoning capabilities across a variety of reasoning benchmarks [4,5,6,7,8,9,10,11], broadening their potential applications in fields such as psychology, education, and social sciences [12,13,14].The widespread use of LLMs accentuates the necessity of rigorously evaluating their reasoning abilities, particularly in context-rich scenarios that mirror real-world complexities.\n\nWhile assessments on abstract logical problems [15,9] showcase LLMs' theoretical reasoning capacities, they do not entirely capture their practical utility in real-life applications where context drastically affects outcomes.Conversely, focusing exclusively on context-specific tasks may conceal the fundamental mechanisms that empower LLMs to process and reason with information.Thus, exploring the balance between contextualized and abstract reasoning is vital for responsibly advancing LLM technology and ensuring its effectiveness across various domains.\n\nTo this end, we introduce ContextHub -a pioneering benchmark designed to meticulously disentangle and evaluate the core reasoning capabilities of LLMs from the influences of contextual information.By leveraging a dual-assessment framework, ContextHub compares LLMs' performance on identical logical constructs within both abstract and richly contextualized settings.This approach not only highlights the differential impacts of context on reasoning but also provides a scalable and flexible methodology that can be adapted across various domains and LLM architectures.Our approach aims to address two main questions:\n\n1. Evaluation disentanglement: how accurate and robust is it to evaluate LLMs' reasoning abilities using abstract logic problems or various contextualized logic problems?By comparing the performance of LLMs on abstract and contextualized logical problems, we can gain a better understanding of the role of context in LLMs' reasoning abilities.\n\n2. Fine-tuning disentanglement: how does model generalization differ when fine-tuning LLMs using abstract logic problems or contextualized logic problems?By comparing the performance of LLMs on unseen abstract and contextualized logic problems, we can gain insights into the types of data that are most effective for improving LLMs' reasoning abilities while maintaining consistent performance across different domains.\n\nWe first utilize DyVal [9] to generate 4 different difficulty levels of formal logic templates.",
            "score": 0.47556693622250057,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 457
                },
                {
                    "start": 459,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 1017
                },
                {
                    "start": 1019,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1587
                },
                {
                    "start": 1587,
                    "end": 1635
                },
                {
                    "start": 1637,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 1980
                },
                {
                    "start": 1982,
                    "end": 2136
                },
                {
                    "start": 2136,
                    "end": 2401
                },
                {
                    "start": 2403,
                    "end": 2498
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 32,
                    "end": 34,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 151,
                    "end": 153,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 153,
                    "end": 155,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "259262573"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "258291730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2215576171875
        },
        {
            "corpus_id": "273026376",
            "title": "Mixing It Up: The Cocktail Effect of Multi-Task Fine-Tuning on LLM Performance - A Case Study in Finance",
            "text": "Recently, the application of large language models (LLMs) in domain-specific contexts has seen rapid growth, particularly in fields such as medicine (Singhal et al., 2023;Wu et al., 2024), law (Huang et al., 2023), and finance (Cheng et al., 2023;Wu et al., 2023). As LLMs are increasingly adopted across various domains, accurate evaluation of their domain-specific capabilities has become more necessary. While many benchmarks exist to evaluate LLM performance, they are typically designed for general purposes and not specifically for domainspecific evaluations. \n\nA common method for assessing LLM performance within a domain is through downstream tasks (Yang et al., 2024;Gu et al., 2021;Xie et al., 2024b). Such benchmarks emphasize well-defined, highly specific tasks that seek to reflect real-world applications within the target domain. These tasks are frequently framed as standard natural language processing (NLP) problems, such as text classification, summarization, causal reasoning, arithmetic reasoning, and more. While each test individually provides limited insight into domain-specific capabilities, when combined, they offer a broader representation, facilitating a more comprehensive evaluation. \n\nLLMs possess zero-shot capabilities (Kojima et al., 2022), allowing them to perform downstream tasks without prior task-specific training. However, they sometimes struggle with these tasks due to issues such as formatting, problem understanding, or reasoning failures. A common approach to improve their performance is to fine-tune the models directly on the downstream task, improving performance on it directly (Zhou et al., 2023). Consequently, many benchmarks provide both training and test splits to facilitate fine-tuning and evaluation. Still, fine-tuning on a single task may not fully optimize the model's performance. \n\nIn this work, we investigated the impact of multitask fine-tuning. Instead of fine-tuning the model solely on the target downstream task, we fine-tune it on multiple related downstream tasks simultaneously. We conduct a massive ablation study to explore the interactions between various financial tasks and datasets. In total, we conduct 220 train- ing experiments to provide an in-depth evaluation of different financial benchmarks and LLMs.",
            "score": 0.4754807056082403,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 171,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 171,
                    "end": 187,
                    "matchedPaperCorpusId": "269136910"
                },
                {
                    "start": 227,
                    "end": 247,
                    "matchedPaperCorpusId": "271745635"
                },
                {
                    "start": 658,
                    "end": 677,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 693,
                    "end": 711,
                    "matchedPaperCorpusId": "268042106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2005615234375
        },
        {
            "corpus_id": "269214525",
            "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing",
            "text": "Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.",
            "score": 0.47539508597052577,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11181640625
        },
        {
            "corpus_id": "276937618",
            "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
            "text": "Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human's learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask,\"can prompting help us teach LLMs how to learn\". In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model's interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.",
            "score": 0.47515708303653437,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08251953125
        },
        {
            "corpus_id": "273821122",
            "title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks, including argument classification, deal completeness classification, and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model's accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.",
            "score": 0.4743635797901305,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1778564453125
        },
        {
            "corpus_id": "253080328",
            "title": "Large Language Models Can Self-Improve",
            "text": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate\"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",
            "score": 0.47334464058050446,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0531005859375
        },
        {
            "corpus_id": "259203671",
            "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling",
            "text": "A proliferation of benchmarks and tasks has been leveraged to evaluate the effectiveness and superiority of LLMs. Results from corresponding experiments demonstrate that LLMs achieve much better performance than previous deep learning models and smaller PLMs on a variety of NLP tasks. Besides, LLMs exhibit some emergent abilities and are capable of solving some complex tasks that traditional models and smaller PLMs cannot address. In summary, LLMs have the following superior characteristics. \n\nZero-shot Learning. LLMs outperform other models with zero-shot learning on most tasks and even perform better than fine-tuned models on some tasks. An empirical study [15] has shown that ChatGPT outperforms previous models with zero-shot learning on 9 of 13 datasets and even outperforms fully fine-tuned task-specific models on 4 tasks. This superior performance is attributed to the rich and diverse input data as well as the large parameter scale of LLMs, which allow them to capture the underlying patterns of natural language with high fidelity, leading to more robust and accurate inferences. \n\nIn-context Learning. In-context learning (ICL) is a paradigm that allows LLMs to learn tasks from only a few instances in the form of demonstration [56]. ICL was exhibited for the first time by GPT-3, which has become a common approach to use LLMs. ICL employs a formatted natural language prompt, which includes a description of the task and a handful of examples to illustrate the way to accomplish it. The ICL ability also benefits from the strong sequence processing ability and the rich knowledge reserve of LLMs. \n\nStep-by-step Reasoning. By utilizing the chain-of-thought prompting strategy, LLMs can successfully complete some complex tasks, including arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Such tasks are typically beyond the capability of smaller PLMs. The chain-of-thought is an improved prompting strategy, which integrates intermediate reasoning steps into the prompts to boost the performance of LLMs on complex reasoning tasks. Besides, the step-bystep reasoning ability is believed to be potentially acquired through training LLMs on well-structured code data [54]. \n\nInstruction Following. Instruction tuning is a unique finetuning approach that fine-tunes LLMs on a collection of natural language formatted instances.",
            "score": 0.47328976929974353,
            "section_title": "D. Pros and Cons of LLMs",
            "char_start_offset": 16756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2212
                },
                {
                    "start": 2215,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0889892578125
        },
        {
            "corpus_id": "263134950",
            "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
            "text": "Large Language Models Large language models (LLMs) trained on massive amounts of data have shown impressive abilities in generating high-quality, coherent text and following zero-shot or few-shot instructions in a diverse set of tasks [44; 45; 11; 61; 62]. These models are usually trained following three steps: pre-training, supervised fine-tuning (SFT) and alignment with human or AI feedback [12; 52; 67; 46; 33; 6] etc. As most public available LLMs focus on training on English corpora, many efforts have been devoted to extending LLMs to Chinese. These works either pre-training a new LLM from scratch on Chinese-centric corpora [56; 77; 17; 57; 60], or expanding the vocabulary of an existing English-centric LLM then performing SFT on Chinese instruction data [16; 29; 76]. There have also been studies that are dedicated to adapting LLMs to the legal domain by fine-tuning on legal specific corpora [14; 25; 75]. However, a comprehensive evaluation to compare the existing LLMs regarding their legal knowledge is still lacking. Our focus is primarily on models that can complete corresponding tasks based on instructions, so we exclude pre-trained small language models such as Lawformer [69] and LegalBERT [8], which require task-specific fine-tuning to perform competiviely. \n\nExisting Benchmarks As the rapid development of LLMs, conventional approaches of evaluating a model's performance on a single task through fine-tuning [71; 65; 40; 79; 49] is no longer adequate for evaluating LLMs. A growing body of research works have recently focused on developing more comprehensive and systematic benchmarks to evaluate various capabilities of LLMs. Examples Figure 2: Three cognitive dimensions for evaluating large language models in LawBench. In order to specialize in legal tasks, LLMs must be able to (1) memorize necessary legal concepts, terminologies, articles and facts; (2) understand entities, events and relationships in legal text; and finally (3) simulate law professionals to apply legal knowledge and necessary reasoning in solving realistic tasks.",
            "score": 0.4732111656485147,
            "section_title": "Related Work",
            "char_start_offset": 5900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "234342706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055206298828125
        },
        {
            "corpus_id": "265043634",
            "title": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation",
            "text": "Human decision-making often results from extended chains of thought. It has been demonstrated that prompting with explicit intermediate reasoning can enhance the performance of language models on complex tasks. (Zelikman et al., 2022) investigate how these reasoning chains, also known as rationales, can be utilized in a feedback loop to improve the performance of LLMs. The project's goal is to enhance the quality of the rationales generated by LLMs, thereby improving the model's accuracy on problems that require reasoning. They observe that improving rationale generation can be achieved by fine-tuning a set of rationales; however, manually creating such a dataset can be extremely labor-intensive. Their approach is to leverage the pre-existing reasoning abilities of LLMs to iteratively bootstrap a model's capacity to generate high-quality rationales. Their bootstrapping protocol unfolds in three steps. First, they prompt an LLM with a few examples to \"selfgenerate\" rationales. Next, they refine the model's ability to produce better rationales by fine-tuning it with those rationales that lead to correct answers. Finally, they repeat the process with the improved model until no further performance enhancements are observed. \n\nThey note that their bootstrapping routine enhances performance on familiar problems but falls short in solving new ones because the model does not receive feedback for incorrectly answered problems. To address this, for every problem the model fails to solve, they manually create a rationalization that includes the correct answer and incorpo-rate both the problem and its rationalization into the fine-tuning training set. This method improves performance on previously unseen problems. \n\nTo evaluate their system, they use three data sets. The first is a generated data set that synthesizes multi-digit integer addition problems. Each example includes an input, an answer, and a \"scratchpad\" that breaks down the individual steps required to solve the problem correctly. An example of the multi-digit addition problem is depicted in Figure 5.",
            "score": 0.4730843002989614,
            "section_title": "Chain-of-thought reasoning",
            "char_start_offset": 9070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06878662109375
        },
        {
            "corpus_id": "269449779",
            "title": "Evaluating and Mitigating Linguistic Discrimination in Large Language Models",
            "text": "LLMs are pre-trained language models with tens or even thousands of billions [35], such as LLaMA [29] (a collection of models ranging from 7B to 65B parameters) and GPT-3 [4](with 175B parameters).Compared to smaller language models such as GPT-2 [23], LLMs not only achieve substantial performance improvements on traditions tasks [6], but also exhibit emergent abilities previously unseen in smaller models.There are three typical kinds of emergent abilities exhibited by LLMs, i.e., in-context learning, instruction following, and step-by-step reasoning [12].In-context learning refers to that LLMs can complete a new task without additional training when provided with several samples in the prompt [8].Instruction following, achieved through instruction tuning with various tasks, allows LLMs to follow instructions in the prompt without explicit examples [33].Step-by-step reasoning involves LLMs solving complex tasks incrementally using the chain-of-thought (CoT) prompting strategy [12,30] Thanks to the remarkable abilities of LLMs in handling various tasks described in natural language and their multilingual capabilities, numerous LLM-based applications and services have been released, widely embraced, and frequently used by individuals from diverse cultural backgrounds.For instance, ChatGPT [18], a LLMbased chatbot, has become integral to many people's daily lives, serving as a platform for searching answers to various questions, seeking advice, and even completing simple coding tasks.However, due to the uneven distribution of training text corpus across different languages and limited efforts on fine-tuning, LLMs inherently exhibit varying performance when faced with different languages, resulting in linguistic discrimination.",
            "score": 0.47287216862116555,
            "section_title": "Large language models",
            "char_start_offset": 8915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 409
                },
                {
                    "start": 409,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1506
                },
                {
                    "start": 1506,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 247,
                    "end": 251,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044677734375
        },
        {
            "corpus_id": "259360395",
            "title": "A Survey on Evaluation of Large Language Models",
            "text": "In contrast, more complex tasks have become the mainstream benchmarks for assessing the capabilities of LLMs. These include tasks such as mathematical reasoning [226,237,244] and structured data inference [86,151]. Overall, LLMs show great potential in reasoning and show a continuous improvement trend, but still face many challenges and limitations, requiring more in-depth research and optimization. \n\n3.1.3 Natural language generation. NLG evaluates the capabilities of LLMs in generating specific texts, which consists of several tasks, including summarization, dialogue generation, machine translation, question answering, and other open-ended generation tasks. \n\nSummarization is a generation task that aims to learn a concise abstract for the given sentence. In this evaluation, Liang et al. [114] found that TNLG v2 (530B) [179] achieved the highest score in both scenarios, followed by OPT (175B) [247] in second place. The fine-tuned Bart [106] is still better than zero-shot ChatGPT. Specifically, ChatGPT demonstrates comparable zero-shot performance to the text-davinci-002 [6], but performs worse than GPT-3.5 [159]. These findings indicate that LLMs, particularly ChatGPT, have a general performance in summarization tasks. \n\nEvaluating the performance of LLMs on dialogue tasks is crucial to the development of dialogue systems and improving human-computer interaction. Through such evaluation, the natural language processing ability, context understanding ability and generation ability of the model can be improved, so as to realize a more intelligent and more natural dialogue system. Both Claude and ChatGPT generally achieve better performance across all dimensions when compared to GPT-3.5 [121,159]. When comparing the Claude and ChatGPT models, both models demonstrate competitive performance across different evaluation dimensions, with Claude slightly outperforming ChatGPT in specific configurations. Research by Bang et al. [6] underscores that fully fine-tuned models tailored for specific tasks surpass ChatGPT in both task-oriented and knowledge-based dialogue contexts. Additionally, Zheng et al. [259] have curated a comprehensive LLMs conversation dataset, LMSYS-Chat-1M, encompassing up to one million samples.",
            "score": 0.4725319880752158,
            "section_title": "Natural Language Processing Tasks",
            "char_start_offset": 21336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 166,
                    "matchedPaperCorpusId": "249063032"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11614990234375
        },
        {
            "corpus_id": "268531869",
            "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus",
            "text": "In Section 3.1, we assessed the logical coherence of LLMs by solving 100 ARC tasks using three different prompting techniques.Our results, which showed an accuracy range from 4% to 12%, reveal variability in the reasoning performance depending on the prompting approach employed.While the LLMs displayed a rudimentary level of logical ability on simpler tasks, a deeper qualitative examination exposed underlying inconsistencies.Our analysis of problem-solving demonstrated that LLMs could navigate certain tasks with logical precision.Yet, further scrutiny of correctly solved tasks with flawed processes unveiled gaps in their reasoning, suggesting that the LLMs' logic could be superficial.Moreover, the observation of LLMs faltering on tasks similar to ones they had previously solved indicates a lack of robust logical structure in their reasoning.\n\nHowever, it is crucial to acknowledge that this study focused on assessing logical capabilities only through varying prompting techniques.Alternative strategies such as domain-specific model fine-tuning or exploring diverse LLM architectures might yield different insights into their logical abilities and coherence.Thus, additional research and experimentation are warranted to fully understand the scope and limitations of LLMs' inferential skills.",
            "score": 0.47218831856714083,
            "section_title": "Conclusion.",
            "char_start_offset": 25811,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 279
                },
                {
                    "start": 279,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 853
                },
                {
                    "start": 855,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1171
                },
                {
                    "start": 1171,
                    "end": 1305
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1358642578125
        },
        {
            "corpus_id": "266844311",
            "title": "InFoBench: Evaluating Instruction Following Ability in Large Language Models",
            "text": "The adaptation of LLMs for enhanced usability has garnered significant attention. Two prevalent approaches for this adaptation are instruction tuning and alignment tuning (Zhao et al., 2023). Instruction tuning (Wang et al., 2022;Lou et al., 2023) aims to unlock the capabilities of LLMs by finetuning them on annotated \"instructional\" data. This involves datasets with natural language instructions and their corresponding desired outcomes. Several LLM adaptations have embraced this method for effective refinement (Taori et al., 2023;Wei et al., 2021;Chung et al., 2022;Iyer et al., 2022;Ouyang et al., 2022). Alignment tuning (Ziegler et al., 2019;Christiano et al., 2017;Ouyang et al., 2022;Bai et al., 2022) differs from instruction tuning by em-phasizing human feedback to ensure LLMs align with human values and preferences. This approach addresses LLMs' tendencies to exhibit unintended behaviors like fabricating information or producing biased expressions (Ouyang et al., 2022;Kenton et al., 2021), steering them more in line with human expectations. \n\nA natural extension of this line of inquiry is assessing LLMs' ability to accurately follow natural language instructions and align with human preferences. In terms of evaluation, using LLMs for Natural Language Generation (NLG) evaluation has shown promise in numerous tasks (Zheng et al., 2023;Liu et al., 2023;Wang et al., 2023a;Fu et al., 2023;Kocmi and Federmann, 2023;Xu et al., 2023;Chiang et al., 2023;Eldan and Li, 2023). Given this context, in our proposed work, we employ LLMs as evaluators to assess their aptitude for instructionfollowing. Two concurrent studies align with our efforts in establishing benchmarks for evaluating LLMs' instruction-following abilities. In contrast to our method of decomposing a single instruction into multiple constraints, Jiang et al. (2023) adopts a distinct approach by sequentially adding fine-grained constraints to construct multi-level instructions.",
            "score": 0.47178316508922946,
            "section_title": "Related Work",
            "char_start_offset": 24329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 1474,
                    "end": 1493,
                    "matchedPaperCorpusId": "258686446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "268510197",
            "title": "RAFT: Adapting Language Model to Domain Specific RAG",
            "text": "Trained on vast quantities of public data, Large Language Models LLMs have achieved significant advances in a wide range of general knowledge reasoning tasks Brown et al. (2020); Wei et al. (2022).However, increasingly LLMs are being employed in specialized domains to support tasks ranging from code completion for specific software frameworks to question answering on specific document collections (e.g., legal or medical documents).In these settings, general knowledge reasoning is less critical and instead the primary goal is to maximize accuracy based on a given set of documents.Indeed, adapting LLMs to the specialized domains (e.g., recent news, enterprise private documents, or program resources constructed after the training cutoff) is essential to many emerging applications (Vu et al., 2023;Lazaridou et al., 2022) and is the focus of this work.This paper studies the following question -How do we adapt pre-trained LLMs for Retrieval Augmented Generation (RAG) in specialized domains?\n\nWhen it comes to adapting LLMs to specialized domains, we consider the following two candidates: in-context learning through Retrieval-Augmented Generation (RAG) and supervised fine-tuning.RAG based methods allow the LLM to reference the documents when  answering questions.However, RAG based in-context learning methods fail to leverage the learning opportunity afforded by the fixed domain setting and early access to the test documents.Alternatively, supervised fine-tuning offers the opportunity to learn more general patterns in the documents and better align to end tasks and user preferences Zhou et al. (2023).However, existing fine-tuning based approaches either fail to leverage the documents at test time (don't incorporate RAG) or fail to account for the imperfections in retrieval process during training.\n\nWe can draw an analogy to an open-book exam.Existing in-context retrieval methods are equivalent to taking an open-book exam without studying.Alternatively, existing finetuning based approaches implement \"studying\" by either directly \"memorizing\" Xiong et al. (2023) the input documents or answering practice questions Wang et al. (2022) without referencing the documents.While these approaches leverage in-domain learning they fail to prepare for the open-book nature of the test setting.",
            "score": 0.47093928978267147,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 999
                },
                {
                    "start": 1001,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1440
                },
                {
                    "start": 1440,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1819
                },
                {
                    "start": 1821,
                    "end": 1865
                },
                {
                    "start": 1865,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2193
                },
                {
                    "start": 2193,
                    "end": 2310
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 177,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 179,
                    "end": 196,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 2140,
                    "end": 2158,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12139892578125
        },
        {
            "corpus_id": "264405727",
            "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
            "text": "Emergence in LLM LLMs show emergent abilities in a wide range of NLP tasks (Brown et al., 2020;Chowdhery et al., 2022;Wei et al., 2022a,b;OpenAI, 2023), among which the reasoning ability is the most noteworthy as it requires the model to perform multi-hop reasoning like human beings. Smaller LMs (< 100B) are often considered to be falling significantly short in reasoning, highlighting the superiority of LLMs in this aspect (Wei et al., 2022a). In this paper, we aim to democratize such emergent reasoning ability to smaller LMs. \n\nCoT Prompting CoT prompts LMs to solve reasoning tasks by generating intermediate rationales to reach the answer, which has greatly improved the reasoning performance (Wei et al., 2022b;Kojima et al., 2022b;Wang et al., 2023a). However, according to the reasoning performance curve (Wei et al., 2022a), the CoT reasoning performance of smaller LMs is far from satisfactory, since the generation of rationales is challenging for them. Distilling Knowledge from LLM Fine-tuning smaller LMs to follow instructions with highquality data collected from LLMs shows the feasibility of distilling knowledge from LLMs (Taori et al., 2023;Chiang et al., 2023;Xu et al., 2023). This procedure can also be viewed as a distant variant of black-box distillation (Hinton et al., 2015;Jianping et al., 2021). However, these works aim to improve the instruction-following ability of smaller LMs, while the reasoning ability that we focus on is often overlooked. Some recent studies (Ho et al., 2023;Fu et al., 2023b;Shridhar et al., 2023) propose to employ LLMs to annotate rationales for training smaller student LMs towards reasoning, not considering the student's feedback to the teacher. In contrast, we exploit the potential of the black-box LLM as the teacher instead of the data annotator by proposing a multi-round learning paradigm.",
            "score": 0.47040948120519216,
            "section_title": "Related Work",
            "char_start_offset": 4801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1859
                }
            ],
            "ref_mentions": [
                {
                    "start": 427,
                    "end": 446,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 702,
                    "end": 721,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 721,
                    "end": 742,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 742,
                    "end": 761,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 817,
                    "end": 836,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1304,
                    "end": 1326,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1500,
                    "end": 1517,
                    "matchedPaperCorpusId": "254877399"
                },
                {
                    "start": 1517,
                    "end": 1534,
                    "matchedPaperCorpusId": "256390607"
                },
                {
                    "start": 1534,
                    "end": 1556,
                    "matchedPaperCorpusId": "258762841"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056243896484375
        },
        {
            "corpus_id": "273549351",
            "title": "CorrectionLM: Self-Corrections with SLM for Dialogue State Tracking",
            "text": "Recently, large language models (LLMs) have demonstrated strong reasoning abilities by providing feedback on their own outputs and subsequently refining them based on that feedback (Shinn et al., 2024;Madaan et al., 2024;Huang et al., 2023;Saunders et al., 2022). This is especially true for tasks requiring multi-step reasoning (code and math reasoning tasks). However, these capabilities of generating feedback and refinement are less commonly observed in small language models (SLMs) (Saunders et al., 2022;Ye et al., 2023). \n\nTo enable the abilities of self-critique and selfrefinement of SLMs, previous research has focused on distilling knowledge from LLMs. Typically, this involves fine-tuning SLMs on improvement demonstrations generated by LLMs (Shridhar et al., 2023a;Ye et al., 2023) and has proven to improve the self-improvement abilities of SLM. \n\nHowever, Yu et al. (2024a) observe that naively training SLMs on LLM improvement demonstra-tions can hurt task performance, since SLMs may have different error modes, and learning from LLM mistakes may be less beneficial. To address this, they propose generating reasoning trajectories with SLMs and using LLMs to provide feedback and refinement before fine-tuning the SLMs on these edited trajectories. However, this approach still heavily relies on LLM involvement. In this work, we introduce a novel self-improvement framework, CORRECTIONLM, that finetunes an SLM and makes corrections using in-context exemplars, without involving any LLMs. We demonstrate the effectiveness of CORRECTIONLM on dialogue state tracking (DST), a task that extracts user intents from multi-turn conversations. \n\nIn this work, we target low-resource settings and use only 5% of the training set for experiments. We first randomly sample a few examples as incontext learning prompts for the SLM to generate dialogue state predictions for the remaining data. This step is intended to capture the errors that SLMs are prone to during in-context learning inference.",
            "score": 0.47003939761125063,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 201,
                    "matchedPaperCorpusId": "258833055"
                },
                {
                    "start": 201,
                    "end": 221,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "253080328"
                },
                {
                    "start": 754,
                    "end": 778,
                    "matchedPaperCorpusId": "258762841"
                },
                {
                    "start": 871,
                    "end": 888,
                    "matchedPaperCorpusId": "264406178"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07574462890625
        },
        {
            "corpus_id": "260132634",
            "title": "AUTOGEN: A Personalized Large Language Model for Academic Enhancement\u2014Ethics and Proof of Principle",
            "text": "Large language models (LLMs) such as ChatGPT or Google's Bard have shown significant performance on a variety of text-based tasks, such as summarization, translation, and even the generation of new ideas (Bommasani et al. 2022;Bubeck et al. 2023). Despite such impressive, seemingly domain-general abilities, LLMs such as GPT-31 (Brown et al. 2020) can benefit from additional, specialized training in certain narrower domains (Moradi et al. 2022). Known as fine-tuning, this process involves training the final few layers of an LLM's neural network on a specialized corpus of text, such that the resulting model retains its fundamental model of language but produces text influenced by the features of the specific corpus on which it was fine-tuned (Church, Chen, and Ma 2021). \n\nThe potential for LLMs to facilitate the academic research and writing process is increasingly explored (Lin 2023). However, such use is attended by significant technical and ethical problems and uncertainties. Recent, high-performing LLMs such as ChatGPT and GPT-4 tend to produce text in a shorter, more conversational, and less rigorous format than that typical of academic prose. This text also tends toward a flat, homogenous style (Grimaldi and Ehrler 2023). Among the novel ethical issues are questions of responsibility, credit, and blame for generated text and the need to vet such text for accuracy (Porsdam Mann et al. 2023). \n\nThe extent to which these technical and ethical issues can be addressed through fine-tuning an LLM for academic prose generation is largely an an open question. Early findings show that fine-tuned GPT-3 models can in fact produce text stylistically akin to specific authors (Elkins and Chun 2020). Notably, a GPT-3 model, fine-tuned by Schwitzgebel et al. to respond to philosophical questions in the style of philosopher Daniel C. Dennett, has produced text convincingly similar to Dennett's own writing (Schwitzgebel, Schwitzgebel, and Strasser 2023).",
            "score": 0.46967047112824417,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 347,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 750,
                    "end": 777,
                    "matchedPaperCorpusId": "239890333"
                },
                {
                    "start": 885,
                    "end": 895,
                    "matchedPaperCorpusId": "261065402"
                },
                {
                    "start": 1218,
                    "end": 1244,
                    "matchedPaperCorpusId": "255495711"
                },
                {
                    "start": 1390,
                    "end": 1416,
                    "matchedPaperCorpusId": "258519213"
                },
                {
                    "start": 1694,
                    "end": 1716,
                    "matchedPaperCorpusId": "225035059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08343505859375
        },
        {
            "corpus_id": "272397970",
            "title": "Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges",
            "text": "Researchers have developed various methods to evaluate the cognitive abilities of LLMs, often drawing inspiration from cognitive science and psychology. These methods aim to provide a comprehensive assessment of LLMs' capabilities and limitations in comparison to human cognition. One prominent approach is the use of cognitive psychology experiments adapted for LLMs. For example, CogBench, a benchmark with ten behavioral metrics from seven cognitive psychology experiments, has been developed to evaluate LLMs [12]. This benchmark allows for a systematic comparison of LLMs performance across various cognitive tasks. Another method involves using neuroimaging data to compare LLMs representations with human brain activity. Studies have employed Functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) recordings to analyze the similarity between LLMs activations and brain responses during language processing tasks [13]. This approach provides insights into the neural-level similarities and differences between LLMs and human cognition. \n\nResearchers have also adapted traditional psychological tests for use with LLMs. For instance, cognitive reflection tests and semantic illusions have been used to evaluate the reasoning capabilities of LLMs [14]. These tests help reveal the extent to which LLMs exhibit human-like biases and reasoning patterns. Additionally, methods from developmental psychology have been proposed to understand the capacities and underlying abstractions of LLMs [15]. These approaches focus on testing generalization to novel situations and using simplified stimuli to probe underlying abstractions. \n\nIn an effort to create more comprehensive evaluation tools, Zhang et al. [16] introduced MulCogBench, a multi-modal cognitive benchmark dataset for evaluating Chinese and English computational language models. This dataset includes various types of cognitive data, such as subjective semantic ratings, eyetracking, fMRI, and MEG, allowing for a comprehensive comparison between LLMs and human cognitive processes. Ivanova [17] provided a set of methodological considerations for evaluating the cognitive capacities of LLMs using language-based assessments. The paper highlights common pitfalls and provides guidelines for designing high-quality cognitive evaluations, contributing to best practices in AI Psychology. \n\nDelving deeper into specific cognitive abilities, Srinivasan et al. [18] proposed novel methods based on cognitive science principles to test LLMs' common sense reasoning abilities through prototype analysis and proverb understanding.",
            "score": 0.4688607568608415,
            "section_title": "B. Methods for evaluating LLMs cognitive abilities",
            "char_start_offset": 6790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2372
                },
                {
                    "start": 2375,
                    "end": 2609
                }
            ],
            "ref_mentions": [
                {
                    "start": 943,
                    "end": 947,
                    "matchedPaperCorpusId": "246902471"
                },
                {
                    "start": 1275,
                    "end": 1279,
                    "matchedPaperCorpusId": "263828908"
                },
                {
                    "start": 1516,
                    "end": 1520,
                    "matchedPaperCorpusId": "259713140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1397705078125
        },
        {
            "corpus_id": "264172665",
            "title": "Probing the Creativity of Large Language Models: Can models produce divergent semantic association?",
            "text": "Large language models (LLMs) have exhibited unparalleled mastery of natural language (Bubeck et al., 2023). The primary capacity of producing the most probable next word is broadly generalizable to many language tasks, suggesting underlying cognitive abilities beyond specialized linguistic rules and patterns. There is observation that LLMs may possess reasoning abilities which is a core aspect of intelligence, including decision-making (Binz and Schulz, 2023) and theory of mind (Moghaddam and Honey, 2023). Meanwhile, there is also increasing interest in exploring LLMs' creativity, which is closely related to intelligence (Frith et al., 2021). Creative use of language, such as metaphor and Figure 1: Creativity from the perspective of language distribution. Creative thoughts need to be novel and valuable, which need cognitive control to inhibit common tokens and remote association to find valuable tokens. \n\nhumor, is important during communication. Ope-nAI (2023) has reported GPT-4's ability to understand jokes, while subsequent works show limited capacity for LLMs to generate or explain humor (Jentzsch and Kersting, 2023;Hessel et al., 2023). As creativity is essential to the development of art, science, and everyday life for human (Gabora and Kaufman, 2010), it is non-trivial if models could produce creative content. Regarding to the curse of recursion for LLMs that training on generated data makes models collapse, one promising solution might be the novel language distribution through creative generation (Shumailov et al., 2023). But since LLMs represent word meaning and predict the next word in context, it seems paradoxical that such models could create ideas not seen in training. \n\nHere, we empirically investigate the creativity of LLMs by examining models' ability to generate divergent concepts. \n\nA general definition of creativity is the ability to create something both novel and valuable (Runco and Jaeger, 2012). According to the dual-process theory of creativity (Beaty et al., 2014), creative thinking relies on remote association while inhibiting common ideas (Figure 1).",
            "score": 0.4688399322395188,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 440,
                    "end": 463,
                    "matchedPaperCorpusId": "250113371"
                },
                {
                    "start": 629,
                    "end": 649,
                    "matchedPaperCorpusId": "226207322"
                },
                {
                    "start": 1138,
                    "end": 1158,
                    "matchedPaperCorpusId": "252222308"
                },
                {
                    "start": 1251,
                    "end": 1277,
                    "matchedPaperCorpusId": "150860538"
                },
                {
                    "start": 1927,
                    "end": 1951,
                    "matchedPaperCorpusId": "146772784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045013427734375
        },
        {
            "corpus_id": "274166126",
            "title": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by Harnessing AI",
            "text": "Fine-tuning LLMs is widely used to adapt models to specific downstream tasks [3], [4]. The model leverages knowledge acquired from pre-training on large unsupervised language learning tasks, allowing its weights to be adjusted using a smaller, task-specific dataset. This enables the model to capture novel patterns and improve task-specific reasoning without training from scratch. Fine-tuning LLMs offers several key advantages: (i) Reduced training time and computational costs by leveraging pre-trained foundational knowledge, (ii) Enhanced performance on downstream tasks, particularly beneficial when data is scarce, and (iii) Improved transfer learning across subtasks within a domain, increasing efficiency in model deployment. \n\nBenchmarking LLMs: We evaluated the capabilities of LLMs in analog circuit design, using both open-source (CodeLlama-70B [39], DeepSeek-V2 [40]) and proprietary (GPT-3.5,4o-mini [41]) models. We fine-tuned all baseline LLMs using the corpus collected through the MASALA-CHAI framework. Open-source models were fine-tuned on a Nvidia A100 80GB GPU utilizing LoRA [42], while GPT models were fine-tuned using their respective fine-tuning playground [43]. \n\nMetrics: We adopt the Pass@k metric, widely used in codegeneration tasks [3], [4], [14], as our main evaluation measure.",
            "score": 0.4674904416264102,
            "section_title": "B. Finetuning",
            "char_start_offset": 15780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1313
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 80,
                    "matchedPaperCorpusId": "260775786"
                },
                {
                    "start": 82,
                    "end": 85,
                    "matchedPaperCorpusId": "260379192"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1266,
                    "end": 1269,
                    "matchedPaperCorpusId": "260775786"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "matchedPaperCorpusId": "260379192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06854248046875
        },
        {
            "corpus_id": "270521667",
            "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
            "text": "The authors assess the translation performance of raw LLMs, explore their in-context learning abilities, and fine-tune the models using parameter-efficient methods (Balne et al., 2024) such as LoRA and full fine-tuning.They also introduce a two-stage fine-tuning approach for LLMs, involving full parameter fine-tuning followed by LoRa-based adaptor fine-tuning.Raw LLMs, particularly Llama 2 models, demonstrate better zero-shot and examplebased translation capabilities compared to other models.Fine-tuning LLMs enhances their translation capabilities, especially when using multilingual fine-tuning.The two-stage fine-tuning approach, involving full fine-tuning followed by LoRa-based fine-tuning, yields the best results.The authors conclude that LLMs have significant potential for translation tasks involving English and Indian languages, even with limited parallel data.\n\nTamil-LLaMA (Balachandran, 2023), a new LLM for the Tamil language based on the opensource LLaMA model.They expanded LLaMA's vocabulary with an additional 16,000 Tamil tokens to improve its ability to process and generate Tamil text.This was done by training a Tamil tokenizer on a large Tamil text corpus.The expanded model was pre-trained on 12 GB of Tamil text data using the LoRA (Low-Rank Adapters) method for efficiency.For instruction fine-tuning, they translated the Alpaca dataset and a subset of the OpenOrca dataset into Tamil.The models were fine-tuned on this translated instruction data.Evaluation on Tamil language tasks shows the Tamil-LLaMA models outperform the original LLaMA and GPT-3.5-turbo on tasks like question answering, reasoning, coding, and open-ended generation in Tamil.However, the models have limitations like potential to generate harmful content, limited knowledge, and challenges with math and reasoning.The 7B and 13B parameter models and code are being made publicly available to spur further research on LLMs for Tamil and other Indian languages.\n\nSingh Kohli et al. (2023) present the development of a Llama2-finetuned LLM tailored for the low-resource Odia language.",
            "score": 0.46722682031027457,
            "section_title": "Fine-tuned LLMs",
            "char_start_offset": 16392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 877
                },
                {
                    "start": 879,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1417
                },
                {
                    "start": 1417,
                    "end": 1480
                },
                {
                    "start": 1480,
                    "end": 1680
                },
                {
                    "start": 1680,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1964
                },
                {
                    "start": 1966,
                    "end": 2086
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042327880859375
        },
        {
            "corpus_id": "268063798",
            "title": "How do Large Language Models Handle Multilingualism?",
            "text": "We have verified the proposed framework for explaining the multilingual working mechanism of LLMs in the above section via deactivating certain neurons. While opposite to employing deactivation, we can also enhance their multilingual ability, especially the understanding and generating ability, by fine-tuning these language-specific neurons. \n\nWe employ Llama2-7b-base model for enhancement to eliminate the interference of instruction fine-tuning. We select causal language modeling as our fine-tuning task and create a dataset comprising 200 documents for each language, extracted from the Wikipedia corpus. 4 It is important to note that our enhancements are focused on augmenting the model's capabilities in understanding and generation only; we do not extend its reasoning faculties or broaden its knowledge base as it may require more specific data preparation. Accordingly, we evaluate the efficacy of our enhancements through targeted understanding and generation tasks. Detailed experiment results are shown in Table 8. \n\nOur findings indicate that with just 10 minutes of fine-tuning on 200 contexts, LLMs exhibit significant enhancements in multilingual understanding and generation abilities. Notably, there is a relative performance boost of 7.4% on the XQuAD benchmark. Similarly, for XLSum, we observe an relative improvement in performance. Table 8: Enhancement is achieved by fine-tuning the Llama2-7b-base model through a causal language modeling task, utilizing 200 contexts from each language.",
            "score": 0.46699580055497125,
            "section_title": "Enhance Multilingual Ability",
            "char_start_offset": 18925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1515
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034698486328125
        },
        {
            "corpus_id": "272423588",
            "title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks",
            "text": "Our framework aims to enhance the System 1 capabilities of LLMs, rather than augment System 2 directly. Consequently, we can deduce from Q5 that only tasks exhibiting a substantial discrepancy in accuracy between CoT usage and non-usage enable LLMs to advance their internalized reasoning abilities through self-practice. \n\nFor Q3 and Q4, the results in Table I indicate that, in general, an increase in additional examples correlates with a more pronounced enhancement in the LLMs' reasoning abilities without CoT, achieved through self-practice. Larger models require fewer examples to approach their System 1 capacity ceiling; beyond this point, further example data yield minimal benefits. This finding suggests that larger models are more adept at leveraging limited data to improve performance without CoT guidance through self-practice, aligning with the research by Jaimovitch et al. [36].",
            "score": 0.46613686362379775,
            "section_title": "C. Results",
            "char_start_offset": 15316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 897
                }
            ],
            "ref_mentions": [
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "245216830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0413818359375
        },
        {
            "corpus_id": "255096269",
            "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "text": "Instruction-tuning of LLMs has emerged as an effective means to improve their zero and few-shot generalization abilities. We make three main contributions to instruction-tuning in this paper. First, we curate a large scale benchmark for instruction-tuning comprising 2000 NLP tasks from 8 dataset collections, annotated into task categories. We strategically produce evaluation splits on this benchmark to evaluate three different types of model generalization abilities: 1) fully-supervised performance, 2) performance on unseen tasks from seen task categories, and 3) performance on tasks from completely held-out categories. Second, using our evaluation suite, we establish tradeoffs and best practices of many aspects of instruction-tuning, such as different sampling methods of finetuning tasks and categories, fine-tuning with task demonstrations, and fine-tuning with specialized datasets for reasoning and dialogue. Finally, using the best settings from our experiments, we train and release OPT-IML 30B and 175B instruction-tuned models based on OPT, that strongly outperform OPT on five evaluation benchmarks and are competitive with recent instruction-tuned models that are tuned on individual benchmarks.",
            "score": 0.46586381363238094,
            "section_title": "Conclusions",
            "char_start_offset": 69485,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "268856659",
            "title": "LLM Attributor: Interactive Visual Attribution for LLM Generation",
            "text": "Large language models (LLMs) have recently garnered significant attention thanks to their remarkable capability to generate convincing text across diverse domains (Touvron et al., 2023).To tailor the outputs of these models to specific tasks or domains, users fine-tune pretrained models with their own training data.However, significant concerns persist regarding potential risks, including hallucination (Zhang et al., 2023), dissemination of misinformation (Pan et al., 2023;Zhou et al., 2023), and amplification of biases (Kotek et al., 2023).For example, lawyers have been penalized by federal judges for citing non-existent LLM-fabricated cases in court filings (Strom, 2023).Therefore, it is crucial to discern and elucidate the rationale behind LLM text generation.\n\nThere have been several attempts to understand reasoning behind LLM text generation.Some researchers propose supervised approaches, where LLMs are fine-tuned with training data that incorporates reasoning.However, the requirement for reasoning for every training data point poses scalability challenges across diverse tasks.Explicitly prompting for reasoning (e.g., \"[Question] Provide evidence for my question\") has also been presented, but LLMs often create fake references that do not exist (Zuccon et al., 2023).Moreover, these methods provide limited solutions for incorrect model behavior (Worledge et al., 2023).\n\nTo complement these shortcomings, identifying the training data points highly responsible for LLMs' generation has been actively explored (Kwon et al., 2023;Park et al., 2023;Grosse et al., 2023).However, while theoretical advancements have been made in developing and refining such algorithms, there has been little research on how to present the attribution results to people.",
            "score": 0.4658295679099941,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 773
                },
                {
                    "start": 775,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1291
                },
                {
                    "start": 1291,
                    "end": 1394
                },
                {
                    "start": 1396,
                    "end": 1592
                },
                {
                    "start": 1592,
                    "end": 1774
                }
            ],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 496,
                    "matchedPaperCorpusId": "257633591"
                },
                {
                    "start": 526,
                    "end": 546,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 1269,
                    "end": 1290,
                    "matchedPaperCorpusId": "261891399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040008544921875
        },
        {
            "corpus_id": "268666984",
            "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach",
            "text": "Additionally, LLMs exhibit neutrality and are devoid of social biases.However, current LLM evaluations tend to prioritize accuracy (Fu et al., 2023b;Safdari et al., 2023;Choi et al., 2023;Yuan et al., 2023;Li et al., 2023).\n\nRecent evaluation efforts reveal several glaring issues.For instance, \"emergent abilities\" could be observed from a number of LLMs, such as GPT, PaLM and LaMDA (Wei et al., 2022;Schaeffer et al., 2023).Some researchers found that instruction-tuning provides a broad set of advantages compared with other types of training (fine-tune, pretrained, RL-tuned etc.) (Liang et al., 2022;Chung et al., 2022;Zhao et al., 2023).Zhao et al. (2023) also reported that the small-sized open-source models perform not well on mathematical reasoning and scaling the open-source modes can improve the performance consistently.Researchers also found that some of the inconsistencies among the relationships between model size and task performance (Burnell et al., 2023).These findings actually are in-volved the overall performance of LLMs and different abilities with training types and scaling.However, the findings drawn from these studies primarily stem from observations made using a relatively small dataset.Notably, these findings have not undergone rigorous validation with a more extensive dataset.For enhanced reliability and accuracy of the results, further validation efforts could benefit from the application of comprehensive statistical methods.The following details these potential problems and challenges.\n\nA primary issue is the narrow range of models typically assessed in multiple tasks -often several to 30 (Yu et al., 2023b;Yu et al., 2023a;Fu et al., 2023a;Jiang et al., 2023b;Huang et al., 2023), compared to the over 120000 models available, for instance, on Huggingface.The limited selection fails to capture the full spectrum of LLMs, diminishing our understanding of their diverse capabilities.For example, the limited number of LLMs may have emergent abilities.",
            "score": 0.4658084570360599,
            "section_title": "Introduction",
            "char_start_offset": 2165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 70,
                    "end": 223
                },
                {
                    "start": 225,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 427
                },
                {
                    "start": 427,
                    "end": 644
                },
                {
                    "start": 644,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1315
                },
                {
                    "start": 1315,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1530
                },
                {
                    "start": 1532,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1930
                },
                {
                    "start": 1930,
                    "end": 1998
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029541015625
        },
        {
            "corpus_id": "265308588",
            "title": "Applications of Large Scale Foundation Models for Autonomous Driving",
            "text": "The emergent abilities of LLMs are one of the most significant characteristics that distinguish them from smaller language models. Specifically, in-context learning (ICL) [46], instruction following [60] and reasoning with chain-of-thought (CoT) [66] are three typical emergent abilities for LLMs. \n\nICL employs a structured natural language prompt that contains task descriptions and possibly a few task examples as demonstrations. Through these task examples, LLMs can grasp and perform new tasks without necessitating explicit gradient updates. Instruction-tuning and following aims to teach models to follow natural language (including prompt, positive or negative examples, and constraints etc.), to perform better multi-task learning on training tasks and generalization on unseen tasks. CoT takes a different approach by incorporating intermediate reasoning steps, which can lead to the final output, into the prompts instead of using simple input-output pairs. \n\nParameter-efficient fine tuning (PEFT) [31,50,68] is a crucial technique used to adapt pre-trained language models (LLMs) to specialized downstream applications. PEFT can be divided into addition-based, selection/specification-based or reparameterization-based. Adapters [18] add domain specific layers between neural network modules. They propose to add fully-connected networks after attention and FFN layers in Transformer. Unlike the transformer FFN block, Adapters usually have a smaller hidden dimension than the input. \n\nLi & Liang [24] develop the idea of soft prompts with a distinctive flavor, called prefix-tuning. Instead of adding a soft prompt to the model input, trainable parameters are prepended to the hidden states of all layers. Another method, P-tuning v1 [27] leverages few continuous free parameters to serve as prompts fed as the input to the pre-trained language models. \n\nThen the continuous prompts are optimized using gradient descent as an alternative to discrete prompt searching. \n\nAn empirical finding is that properly optimized prompt tuning can be comparable to fine-tuning universally across various model scales and NLU tasks. The improved method Ptuning v2 given in [34] can be viewed as an optimized and adapted implementation, designed for generation and knowledge probing, shown in Fig. 6.",
            "score": 0.46579935867656896,
            "section_title": "Fig. 4. LLaMA 2 Training [59]",
            "char_start_offset": 14328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1496
                },
                {
                    "start": 1499,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06915283203125
        },
        {
            "corpus_id": "258079092",
            "title": "Boosted Prompt Ensembles for Large Language Models",
            "text": "Large Language Models Large, transformer-based language models (LLMs) have proven to be extremely capable few shot learners in a wide variety of different contexts (Vaswani et al., 2017;Brown et al., 2020). Their general purpose nature has created something of a \"paradigm shift\" in the AI landscape, whereby many downstream tasks requiring language will make use of an LLM as a foundation model, either directly or by finetuning (Bommasani et al., 2021). Our work considers one approach to improving baseline, untuned LLM performance, which builds on and is complementary to a number of recent techniques. \n\nChain of Thought Wei et al. (2022) show that prompting LLMs with intermediary reasoning steps, called chain of thought (CoT) prompting, can significantly increase the ability of the LLM to perform complexity reasoning tasks. Wang et al. (2022c) further improve reasoning performance by introducing self-consistency (SC), which replaces the standard greedy decoding of the LLM output with a stochastic output space ensemble that marginalizes over multiple reasoning paths by sampling with positive temperature (e.g., T = 0.7) and choosing the final prediction p * with highest agreement: \n\nThis exploits the fact that diverse reasoning paths that lead to the same answer are more likely to be correct. Our work builds on self-consistency by using the agreement among reasoning paths to determine the set of \"Hard\" problems and, for the test-time version of our algorithm, the set of LLM generated answers that are likely to be correct. This latter usage is similar to that of Huang et al. (2022), who show that by finetuning LLMs on self generated answers with \"high agreement,\" large language models can self improve. \n\nAutomatic Prompt Engineering It has been observed that language model performance can be sensitive to the chosen prompt (Zhao et al., 2021), which has led to in-depth studies of prompting methodology (Liu et al., 2023;Wang et al., 2022a) and the development of several approaches to automatic prompt generation (Shin et al., 2020;Gao et al., 2020).",
            "score": 0.4656285761103137,
            "section_title": "Prior Work",
            "char_start_offset": 4274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 205,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06243896484375
        },
        {
            "corpus_id": "276903444",
            "title": "A Query Optimization Method Utilizing Large Language Models",
            "text": "Large Language Models (LLMs) are powerful machine learning models based on the transformers structure [36]. As the depth and width of transformers layers increase, the number of parameters in LLMs scales to billions and trillions. In recent years, LLMs such as GPT-4 [30] and DeepSeek-v3 [22] have demonstrated remarkable capability in natural language understanding (NLU) and natural language generation (NLG). Meanwhile, open-source LLMs such as LlaMA [6] and Mistral [15] enable researchers to further fine-tune these models for specific tasks. \n\nPre-trained language models (PLMs) exhibit strong generalization capabilities across various natural language processing (NLP) tasks, including code generation and question answering [3,28,37]. However, in downstream applications, LLMs struggle to strictly follow user instructions and generate invalid or inaccurate outputs [31]. Fine-tuning is an effective strategy for adapting LLMs to specific tasks, ensuring that LLMs are able to follow instructs and generate output in desired format.",
            "score": 0.46556548116867486,
            "section_title": "Large Language Models",
            "char_start_offset": 11365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1041
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 733,
                    "end": 736,
                    "matchedPaperCorpusId": "257019916"
                },
                {
                    "start": 739,
                    "end": 742,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0794677734375
        },
        {
            "corpus_id": "266899974",
            "title": "Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs",
            "text": "(2) Compared with the results of the single-task training strategy (-MT), the multi-task training strategy brings improvement to the reasoning performance of the LLMs. Although during testing, we limit the model to only output the final answers without reasoning steps, adding training tasks that predict reasoning steps can enhance the models' reasoning abilities and robustness. \n\n(3) Compared with the results without reasoning steps in the input corpus (-RS), we can conclude that providing explicit reasoning steps in training input is beneficial. If there are no reasoning steps in the input corpus but a multi-task training strategy is used, it will increase the difficulty of the training task and lead to poor performance, which is particularly evident in Baichuan2-13B-Chat. (4) The result of fine-tuning using only the normal demand and answer pairs is not satisfactory, which indicates that it is challenging for LLMs to learn the data patterns solely based on the Q&A pairs in situations where tasks are difficult. Therefore, it is necessary to design appropriate demonstrations and training tasks to fine-tune the LLMs.",
            "score": 0.46359501131513814,
            "section_title": "Case Study. Table",
            "char_start_offset": 25340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1133
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06695556640625
        },
        {
            "corpus_id": "271329091",
            "title": "Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though",
            "text": "We begin by utilizing open-source chat LLMs, such as Llama3-8B-Instruct (AI@Meta, 2024), to generate rules, facts, and queries following specific instructions and demonstrations. Subsequently, we employ the Prolog engine to perform logical reasoning, verifying the outcomes against established truths. Only the verified reasoning processes are selected, which we then translate into CoT-like natural language reasoning trajectories. These trajectories serve as the basis for supervised fine-tuning (SFT), aimed at teaching LLMs to imitate a strictly logical reasoning process. To address the issue of catastrophic forgetting during domain-specific fine-tuning, we implement a model averaging technique. Empirical experiments confirm that the THOUGHT-LIKE-PRO framework significantly improves the reasoning capabilities of LLMs and extends their generalization to Out-of-Distribution (OOD) reasoning benchmarks. The contributions of our study are detailed as follows: \n\n1 The code will be fully released due to the acceptance. We offer a demo of the dataset automatically generated by the THOUGHT-LIKE-PRO framework for reference. Available at: https://anonymous.4open.science/r/Prolog_ datasets-9875. \n\n\u2022 We propose THOUGHT-LIKE-PRO framework designed to enhance the general reasoning capabilities of LLMs in a self-driven way. This framework is straightforward to implement and replicate. \n\n\u2022 Our results show that performing imitation learning on strictly logical and diverse reasoning trajectories for a single instruction leads to improved and consistent reasoning performance. \n\n\u2022 The framework is simple, straightforward, and highly effective. It demonstrates robust OOD reasoning performance across multiple logic tasks and general benchmarks, making it applicable in various industrial domains.",
            "score": 0.46326885647372096,
            "section_title": "Introduction",
            "char_start_offset": 3883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1802
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07159423828125
        },
        {
            "corpus_id": "267657819",
            "title": "Personalized Large Language Models",
            "text": "On the other hand, LLMs are sophisticated zero-shot reasoners [2]. One can use their abilities to solve downstream tasks with in-context-learning [33] and extensive prompt-based inference [34]. Fine-tuning can be computationally expensive and time-consuming, especially for large language models. Finetuning a language model on task-specific data can improve its performance on the task, but it may come at the cost of reduced performance on other tasks. This is due to the risk of catastrophic forgetting [35], where the model may forget some of the knowledge learned during pre-training and alignment processes [36]- [38]. Techniques such as multitask learning or balancing pre-training and task-specific data might be beneficial for retaining the performance of LLMs in multiple downstream tasks. \n\nTo the best of our knowledge, LLM fine-tuning for subjective tasks via user ID inputs, such as personalized emotion recognition or personalized hate speech detection, has not been extensively evaluated, and further research is needed in this area.",
            "score": 0.4631791118780315,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 5428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 1049
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 65,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 506,
                    "end": 510,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "262055661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414794921875
        },
        {
            "corpus_id": "264770810",
            "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
            "text": "Math Reasoning with LLMs A pivotal metric for assessing the efficacy of LLMs is their capability in addressing intricate reasoning challenges, exemplified by mathematical reasoning tasks [9,11,25,33,39]. Rather than yielding direct, definitive answers, prior research has illustrated that by employing a variety of prompting techniques, such as Chain-of-Thought (CoT) prompting [32], LLMs can be guided through step-by-step reasoning, resulting in significant improvements in performance across an array of diverse reasoning tasks. Imani et al. [16] propose the generation of multiple algebraic expressions or Python functions to solve the same mathematical problem, aiming to explore a broader spectrum of potential solutions. Additionally, Li et al. [18] introduce a step-aware verifier to scrutinize the reasoning steps in COT, thereby enhancing the model's reasoning capabilities. Another effective approach, Self-Consistency [31], combines a wider range of solutions and derives a final answer by aggregating them to obtain the most consistent response. Meanwhile, several scholarly works have incorporated the concept of rejection sampling, in conjunction with various other techniques, to curate a more diverse set of sampled reasoning paths for the purpose of finetuning data augmentation [1,12,15,21,27,29,36,37,40]. Following the line, Yuan et al. [35] utilize rejection sampling to augment the data volume for fine-tuning math reasoning LLMs. \n\nInstruction Tuning with LLMs Instruction tuning serves as a pivotal component within the developmental frameworks of language models, with its primary function being to orient LLMs towards objectives that are more congruent with human preferences and functional applications. \n\nThe academic discourse on instruction tuning is notably concentrated on amplifying the versatile instructional capabilities of LLMs. This discourse is particularly exemplified by pioneering studies such as UnifiedQA [17], Zero-Prompt [34], FLAN [10], and T0 [24]. These studies have embarked on an exploration into the generalization capabilities of LLMs. Following these, FLAN-v2 [19] further investigated the impact of scaling instructional datasets on model performance.",
            "score": 0.46254686388076016,
            "section_title": "Related Works",
            "char_start_offset": 6533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 382,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "257427208"
                },
                {
                    "start": 752,
                    "end": 756,
                    "matchedPaperCorpusId": "259370847"
                },
                {
                    "start": 1306,
                    "end": 1309,
                    "matchedPaperCorpusId": "257019561"
                },
                {
                    "start": 1318,
                    "end": 1321,
                    "matchedPaperCorpusId": "247762790"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "matchedPaperCorpusId": "253224009"
                },
                {
                    "start": 1950,
                    "end": 1954,
                    "matchedPaperCorpusId": "218487109"
                },
                {
                    "start": 1968,
                    "end": 1972,
                    "matchedPaperCorpusId": "246035184"
                },
                {
                    "start": 1992,
                    "end": 1996,
                    "matchedPaperCorpusId": "239009562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051177978515625
        },
        {
            "corpus_id": "271843039",
            "title": "Risks, Causes, and Mitigations of Widespread Deployments of Large Language Models (LLMs): A Survey",
            "text": "LLMs are pre-trained on extensive corpora with vast numbers of parameters, excelling in various NLP tasks such as text generation, summarization, classification, machine translation, and question answering [4], [5], [17], [22]. In 2023, several major LLMs were released, including OpenAI's ChatGPT [38], Meta AI's LLaMA [50], and Databricks' Dolly 2.0. These models exemplify the trend toward exponentially increasing parameters, such as GPT-2 with 1.5 billion parameters and GPT-3 with 175 billion parameters [13]. The deployment of these models spans applications in chat agents, computational biology, programming, creative domains, knowledge work, medicine, reasoning, robotics, and the social sciences [27]. \n\nDespite the remarkable success of LLMs, they pose several challenges that are unprecedented for humans [27]. Diverse organizations are deploying apps that integrate LLMs while existing apps and features are constantly being updated with these new LLMs [41]. However, these rapid updates raise var-ious concerns, including academic integrity, copyright issues, and environmental impacts [41]. Moreover, as LLMs grow in size, their insatiable demand for data becomes apparent. These models are now trained on such vast amounts of data that humans can no longer manually scrutinize it all [28]. In addition, evaluation results may be flawed because the training data could include instances from the test data [27]. This largescale pre-training also introduces issues like bias and fairness, as well as ethical concerns. \n\nWhen a new language model is introduced, researchers often investigate its challenges and limitations [39], [51]. Additionally, some studies focus on identifying and mitigating specific risks associated with these models [51]. As language models rapidly evolve, there is an increasing need for comprehensive literature that addresses these areas concurrently. Surveys or reviews that cover the issues, solutions, and underlying causes related to language models are essential.",
            "score": 0.4622625614522691,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "174802484"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "261076348"
                },
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "257098877"
                },
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "257098877"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "220938739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0526123046875
        },
        {
            "corpus_id": "271962793",
            "title": "Strategic Optimization and Challenges of Large Language Models in Object-Oriented Programming",
            "text": "With the emergence of LLMs, significant achievements have been made in the field of NLP [14,15,19,20,25], and they are gradually being applied to various aspects of software engineering, especially code generation. These models, through pre-training and fine-tuning, can learn and generate logically coherent code snippets. For instance, Baptiste Rozi\u00e8re et al. propose Code Llama [18], and Mark Chen et al. propose CodeX [1], which have achieved certain results by fine-tuning for code generation tasks. They demonstrate advantages in tasks such as automated code completion, bug fixing, and direct code generation from descriptions. \n\nBuilding upon these, researchers are exploring various ways to further leverage the potential of the LLMs themselves [9,11,13,22,26,27,30]. This includes proposing more advanced prompting engineering methods, such as Chain-of-Thought [26], as proposed by Jason Wei et al., which significantly enhance the accuracy and interpretability of LLMs by introducing a process of incremental reasoning into the LLMs' inputs. Additionally, in order to reduce usage costs, researchers may employ classic resource-saving methods such as model distillation [11], as proposed by Geoffrey Hinton et al., and quantization compression [8,13]. However, this trade-off can lead to performance degradation and a decrease in generalization capability. \n\nTo better evaluate the abilities of various LLMs in the code generation scenario, researchers have proposed a series of generation tasks. For instance, Mark Chen et al. propose HumanEval [1], which involves having LLMs write specific programming problems and unit tests to assess whether the generated code can correctly perform the required tasks. Additionally, Shiqi Wang et al. propose ReCode [24], which offers a suite of robustness tests for code generation models. However, most of these studies primarily focus on code generation tasks that do not require considering context.",
            "score": 0.4619891998852897,
            "section_title": "RELATED WORKS 3.1 LLMs and Code Generation",
            "char_start_offset": 8563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 95,
                    "matchedPaperCorpusId": "257921533"
                },
                {
                    "start": 95,
                    "end": 98,
                    "matchedPaperCorpusId": "226283634"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "259097639"
                },
                {
                    "start": 760,
                    "end": 763,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1255,
                    "end": 1258,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 1258,
                    "end": 1261,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0794677734375
        },
        {
            "corpus_id": "269899678",
            "title": "Language Models can Evaluate Themselves via Probability Discrepancy",
            "text": "Natural Language Generation Evaluation: We applied ProbDiff to assess Qwen-14B-Chat's efficacy in tasks such as translation, summarization, and Xiaohongshu blog writing.Table 2 illustrates the confidence levels in the generated sentences by both the original and fine-tuned Qwen models across these tasks.Subsequent to the fine-tuning process, it was noted that the fine-tuned model exhibited increased confidence in its responses upon revision, aligning with our anticipatory hypotheses.Specifically, in the Xiaohongshu blog writing task tailored for LLMs with a 14B parameter size, we encountered a scarcity of data for fine-tuning.\n\nNonetheless, post-fine-tuning, there was a noticeable enhancement in response confidence, as depicted in Table 2. From these results, we deduce that fine-tuning LLMs on task-specific datasets invariably boosts the confidence of their responses, markedly surpassing the performance of the prefine-tuned LLM.This elevation in response confidence serves as a pertinent indicator for gauging improvements in LLM performance.\n\nLLM's Alignment Evaluation: To ascertain the alignment capabilities of LLMs across diverse dimensions, we conducted fine-tuning of Qwen-14B-Chat using documents synthesized by GPT-4, specifically targeting the AlignBench dataset.The results of our evaluation are presented in Table 3.\n\nAlignBench serves as a meticulously curated benchmark designed to assess the alignment capabilities of LLMs.This benchmark poses a more complex challenge compared to standard NLG tasks, requiring nuanced understanding and response accuracy from the model.As depicted in Table 3, post fine-tuning with task-specific data, the model demonstrates notable confidence across most evaluated alignment abilities.However, it is observed that in categories such as writing ability and open-ended questions, the model's confidence tends to diminish, suggesting areas for further improvement and investigation.Evaluation on other LLMs: To assess the generalization capabilities of our ProbDiff, we extended our evaluations to include several highperforming LLMs featured on established LLM benchmark leaderboards, specifically MT-Bench and AlpacaEval 2.0.",
            "score": 0.4619043124333938,
            "section_title": "Results",
            "char_start_offset": 19620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 634
                },
                {
                    "start": 636,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1056
                },
                {
                    "start": 1058,
                    "end": 1287
                },
                {
                    "start": 1287,
                    "end": 1342
                },
                {
                    "start": 1344,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1599
                },
                {
                    "start": 1599,
                    "end": 1749
                },
                {
                    "start": 1749,
                    "end": 1943
                },
                {
                    "start": 1943,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10595703125
        },
        {
            "corpus_id": "261558535",
            "title": "Making Large Language Models Better Reasoners with Alignment",
            "text": "Reasoning is a cognitive process that involves utilizing evidence to reach a well-founded conclusion (Qiao et al., 2023;Huang & Chang, 2023). Recently, there has been a growing focus on enhancing the reasoning abilities of Large Language Models (LLMs) (Li et al., 2023b), particularly open-source LLMs (Yuan et al., 2023a;Luo et al., 2023;Mukherjee et al., 2023), because LLMs still lack reasoning skills (Wang et al., 2023b;d;Zheng et al., 2023) that are essential for them to serve as the brain of artificial general intelligence agents (Wang et al., 2023a;Yao et al., 2023;Song et al., 2023b). \n\nRecent works (Chung et al., 2022;Hsieh et al., 2023;Mukherjee et al., 2023) find that training LLMs using data with a chain of thought (COT) reasoning process is a very effective method to improve the reasoning ability of LLMs. These studies typically train LLMs using maximum likelihood estimation (MLE), and employ a next-token prediction objective. However, MLE only assigns probability mass to the reference COT, which contradicts reasoning tasks where various reasoning paths can lead to the correct answer. In this paper, we find that previous vanilla fine-tuning (VFT) paradigm causes LLMs to suffer from an Assessment Misalignment problem, i.e., LLMs struggle with accessing the quality 1 arXiv:2309.02144v1 [cs.CL] 5 Sep 2023 Preprint Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \n\nReference Answer (PPL 1.05) : Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute. Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.",
            "score": 0.4618274354649936,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1607
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 120,
                    "matchedPaperCorpusId": "254854219"
                },
                {
                    "start": 120,
                    "end": 140,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 252,
                    "end": 270,
                    "matchedPaperCorpusId": "259370847"
                },
                {
                    "start": 559,
                    "end": 576,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 632,
                    "end": 651,
                    "matchedPaperCorpusId": "258461606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0994873046875
        },
        {
            "corpus_id": "266998884",
            "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation",
            "text": "LLMs exhibit substantial improvements when trained on extensive datasets rich in domain-specific language and terminologies. Such targeted training enhances their capabilities in specialized applications that necessitate robust natural language understanding, including interaction with computing systems through tasks like program synthesis, code completion, debugging, and documentation generation. \n\nIn our BT generation method, as we analyzed, require various LLM abilities which are the most advanced usage of LLM, such as planning, reasoning, tool manipulation abili-ties, etc. Some maybe meet our task requirements after being pretrained but it is various on the different LLM model. And Some abilities can not meet our requirements. So to make LLM model more useble, the training is necessary. The quality of data is paramount for effective training and ethical development of LLMs, a principle equally relevant to the context of BT generation. Typically, these models leverage diverse and publicly accessible open-source data during their initial training phase. \n\nIn this section, we will delineate the pretraining and Supervised Fine-Tuning (SFT) stages within the LLM training pipeline.",
            "score": 0.46106796041815057,
            "section_title": "Training",
            "char_start_offset": 51970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0994873046875
        },
        {
            "corpus_id": "263134628",
            "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?",
            "text": "ChatGPT OpenAI (2023a) and GPT-4 OpenAI (2023b) successfully use instruction tuning to enable LLMs to follow natural language instructions and complete real-world tasks; this improvement has become standard in open-source LLMs. This is implemented by fine-tuning the model on a wide range of tasks using human-annotated instructions and feedback, by supervised fine-tuning via manually or automatically generated instructions using public benchmarks and datasets, or learning from instruction-following data by developing from state-of-the-art instruction-tuned teacher LLMs. \n\nTo illustrate the impact of code data on the LLMs reasoning ability in the instruction tuning stage, we use the instruction tuning datasets that contain codes and the instruction tuning datasets without codes introduced in Chapter 1.3 to fine-tune the PanGu2.6B and PanGu13B models (Zeng et al., 2021) and evaluate their performance in reasoning-intensive scenarios. In addition, we also fine-tune the CodePanGu2.6B model using the instruction tuning dataset containing codes to observe the effect of using code data in both pre-training and instruction tuning stages. Table 3 shows the results of these tasks. Among them, NN and NC represent the fine-tuned PanGu model using only text instructions and instructions containing codes, respectively, and CC represents the fine-tuning model of CodePanGu2.6B using instructions containing codes. Consistently over these tasks, we observe the following: \n\n\u2022 After fine-tuning with mixed code instruction data, LLM shows different trends in multiple reasoning tasks. This indicates that introducing code data in the instruction tuning phase may be less effective than in the pre-training phase. Therefore, it is best to add code data in the pre-training stage to improve the model performance in general reasoning tasks. \n\n\u2022 We find that training with code data in both stages can significantly improve code-related tasks (CosQA and MBPP), especially code generation tasks. This may be because the code instruction data activates the code reasoning ability of the language model, which suggests that if the LLM needs to complete complex code tasks, the code reasoning ability can be improved by effectively following code instructions and generating compliant content.",
            "score": 0.4600738106015887,
            "section_title": "INSTRUCTION-TUNING STAGE",
            "char_start_offset": 12326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2290
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06512451171875
        },
        {
            "corpus_id": "272832498",
            "title": "Watch Your Steps: Observable and Modular Chains of Thought",
            "text": "There has been a great deal of work on CoT prompting and reasoning in LLMs, as surveyed here (Huang and Chang, 2022). The prior work that is most relevant to this paper is analytic work on understanding why CoT works, and why and when it fails. A number of hypotheses have been proposed for CoT's success: for instance, it has been argued that the longer outputs of CoT provide additional computational power, which is needed to solve computationally complex tasks (Feng et al., 2024). More notably, it has been argued that CoT \"unlocks\" reasoning abilities that emerge in sufficiently large language models (Wei et al., 2022;Prystawski et al., 2024). This view is supported by the observation that phrases like \"let's think step by step\" often lead to improved LLM performance on many tasks (Kojima et al., 2022). The metaphor of \"unlocking\" reasoning is very compelling, and may be why there has been little prior research on the degree to which CoT prompts teach LLMs task-specific reasoning methods, and the degree to which LLM behavior can be controlled by presenting more explicit method traces (as we showed is possible using Program Trace Prompting). The results of Sections 3.3 and 3.4 illustrate that CoT prompts also provide function by providing new information to the LLM, as its steps describe a particular strategy for a task. \n\nThe notions of modularity and locality explored in this paper are closely related to the notion of CoT explanation faithfulness (Jacovi and Goldberg, 2020;Turpin et al., 2024;Lanham et al., 2023). Turpin et al noted that when LLMs are biased (e.g., illustrating a gender bias in predictions of occupation), CoT prompting sometimes leads to predictions that preserve the underlying bias, but still produces plausible-looking CoT explanations-explanations that do not reflect the underlying bias. This sort of \"unfaithful\" explanation is disturbing, perhaps because it seems like deception; practically it is also inconvenient if explanations are used to justify LLM predictions to a user, or even if explanations are used to help optimize LLM behavior.",
            "score": 0.4598837379695244,
            "section_title": "Understanding CoT and Measuring Faithfulness",
            "char_start_offset": 33015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 465,
                    "end": 484,
                    "matchedPaperCorpusId": "258865989"
                },
                {
                    "start": 626,
                    "end": 650,
                    "matchedPaperCorpusId": "258048648"
                },
                {
                    "start": 1472,
                    "end": 1499,
                    "matchedPaperCorpusId": "215416110"
                },
                {
                    "start": 1499,
                    "end": 1519,
                    "matchedPaperCorpusId": "258556812"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06671142578125
        },
        {
            "corpus_id": "273098595",
            "title": "TypedThinker: Typed Thinking Improves Large Language Model Reasoning",
            "text": "The primary reason for comparing our method with the few-shot baseline is that fine-tuning for specific reasoning types is an integral part of our approach. Therefore, we evaluate the impact of our fine-tuned reasoner through a separate ablation study. However, comparing TypedThinker to few-shot baselines without fine-tuning may not fully account for the benefits of fine-tuning. Therefore, we conduct two more experiments to verify the influence of the fine-tuned LLMs. \n\nComparison with base LLM + one module Ablation studies in Section 4.4 investigate the contribution of each component by removing one component each time. Here we provide additional ablation results by adding one component to the base LLM each time, resulting in two variants: Base LLM + Meta-thinker and Base LLM + Collection. For a more reliable conclusion, we ran experiments three times to calculate the average and std and present the result in Table 13 and 14. Results show that the retrieval component improves performance on logical tasks but may mislead models on mathematical datasets. This is consistent with our findings in the ablation study in Table 2: the retrieved solutions with digits may mislead the model. Meanwhile, compared with the ICL reasoner, our finetuned reasoner shows better capability in identifying the suitable reasoning type. \n\nA.6 DISCUSSION ON MORE REASONING PROBLEMS \n\nIn this paper, we mainly focus on logical and math reasoning problems. However, our TypedThinker can also be extended to symbolic or commonsense reasoning without extra ef-",
            "score": 0.45929962848000483,
            "section_title": "A.5.3 MORE ABLATION STUDIES",
            "char_start_offset": 38244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1552
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04510498046875
        },
        {
            "corpus_id": "273549220",
            "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
            "text": "Existing knowledge editing methods like SERAC [17], ROME [16], MEMIT [19], and IKE [14] work well on these evaluation criteria across various datasets on different LLMs. \n\nDespite these successes in editing language models, recent works [20; 21; 22; 23] have disclosed the inevitable pitfalls of existing editing methods from different perspectives such as knowledge distortion [22], and catastrophic forgetting [24]. In sequential editing setting (see Section 2), as the number of edits increases, it is necessary to balance two aspects: the retention of the model's original knowledge and the preservation of newly acquired knowledge through updates. These two objectives are to some extent conflicting. The general ability (Section 2) of LLMs is the foundation to solve the wide range of complex tasks. Changes in the model's general capabilities reflect the retention of its original knowledge. However, the general abilities of post-edit language models are still unexplored, making current editing methods unreliable to be employed for real-world applications. Given this situation, it naturally motivates the following critical question to explore: \n\nHow do sequential model editing affect the general abilities of language models ? \n\nTo close this gap, we make a comprehensive understanding and analysis of edited LLMs with various editing methods (In Section 3). In detail, we edit multiple LLMs with various editing methods and evaluate them across benchmarks to verify underlying factors that may affect the general abilities. It is worth noting that our focus is on the general capabilities of the model (including world knowledge, reading comprehension, reasoning, safety, etc.), rather than the performance on efficacy, generalization and locality or downstream tasks like NER, QA, and NLI. These distinctions distinguish our work from some existing studies like [25; 24; 26]. Technically, we explore the impact of various underlying factors such as the number of edits, model scale, safety, different aspects of abilities, and instruction tuning on the general capabilities of edited LLMs after sequential editing. \n\nThe empirical results indicate that the majority of current editing methods do not significantly influence the fundamental capabilities of models within dozens of edits (Section 4.1).",
            "score": 0.45921114499954285,
            "section_title": "Introduction",
            "char_start_offset": 1988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2129
                },
                {
                    "start": 2132,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1865234375
        },
        {
            "corpus_id": "252917981",
            "title": "Prompting GPT-3 To Be Reliable",
            "text": "NLP is dominated by large language models (LLMs) -pretrained on large, unlabeled text data -that are then used for downstream tasks (Devlin et al., 2019a;Brown et al., 2020). Scaling the model and data size often brings gains on downstream tasks (Kaplan et al., 2020;BIG-Bench, 2022), allowing what some call emergent abilities (Wei et al., 2022a). These emergent behaviors are accomplished through prompting-a crafted, natural language text to shape predictions or offer relevant information without expensive supervised data. Among all the existing LLMs, GPT-3 (Brown et al., 2020) is particularly popular due to its flexibility and ease of use from the OpenAI API2 . \n\nExisting empirical studies investigate GPT-3 on specific tasks such as mathematical reasoning (Hendrycks et al., 2021a), multi-hop reasoning (Wei et al., 2022b;Kojima et al., 2022), and code generation (Chen et al., 2021a). However, rising numbers on these evaluations do not ensure LLM reliability. For example, LLMs (including GPT-3) produce biased (Lucy & Bamman, 2021) generations, false statements (Lin et al., 2022b), and outdated information (Chen et al., 2021b;Kasai et al., 2022). Deploying such models in the real world could result in catastrophic harm. \n\nIn the context of prompting LLMs, several previous works have explored their reliability. For example, in the release reports of GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), Gopher (Rae et al., 2021) and PaLM (Chowdhery et al., 2022), there are dedicated experiments evaluating these LLMs' representational bias and toxicity. Another line of work has evaluated calibration (Lin et al., 2022a;Kadavath et al., 2022) of prompting-based LLMs on math questions or multiple-choice questions. We differ from these prior works in two key aspects: (i) We perform a more comprehensive study of four core facets of reliability, serving as a meta-analysis.",
            "score": 0.45908850369989646,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1236
                },
                {
                    "start": 1239,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1893
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 173,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 246,
                    "end": 267,
                    "matchedPaperCorpusId": "210861095"
                },
                {
                    "start": 267,
                    "end": 283,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 328,
                    "end": 347,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 563,
                    "end": 583,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1023,
                    "end": 1044,
                    "matchedPaperCorpusId": "235097208"
                },
                {
                    "start": 1075,
                    "end": 1094,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 1141,
                    "end": 1160,
                    "matchedPaperCorpusId": "251105205"
                },
                {
                    "start": 1374,
                    "end": 1394,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1400,
                    "end": 1420,
                    "matchedPaperCorpusId": "248496292"
                },
                {
                    "start": 1457,
                    "end": 1481,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1621,
                    "end": 1640,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 1640,
                    "end": 1662,
                    "matchedPaperCorpusId": "250451161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059967041015625
        },
        {
            "corpus_id": "274423420",
            "title": "MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications",
            "text": "While prompt-engineering provides a training-free method, its generalization to smaller open-source models for multi-step reasoning remains limited. This highlights the need for computationally reasonable, yet efficient models capable of powering reasoning frameworks to handle intricate document understanding. Fine-tuning dedicated LLMs for multi-step reasoning is a promising approach, particularly when deploying closed-source models is impractical due to promptengineering efforts, cost, latency or data privacy concerns, which can arise when dealing with sensitive business documents as discussed in [44]. \n\nFor instance, TAT-LLM [44] introduces a stepwise pipeline to break down problems into extraction, reasoning, and execution steps. This decomposition helps open-source LLMs from 7B to 70B fine-tuned on data annotated by human experts to achieve discrete reasoning over tabular financial documents. Similarly, Husky [17] fine-tunes LLaMA-2 and LLaMA-3 models from 7B to 13B, and ToRA [11] fine-tunes LLaMA-2 and CodeLLaMA models up to 70B, equipping them with tool-augmented multi-step reasoning abilities. \n\nHusky incorporates specialized tools for code generation and execution, web search, and commonsense knowledge, while ToRA focuses on dynamical planning with code-based tools leveraging libraries like SymPy [25] and solvers. Both Husky and ToRA demonstrate that fine-tuning the tools and the planner is crucial to enhance SLMs' reasoning abilities, with extensive comparison studies against other LLM frameworks on multiple reasoning tasks. \n\nInstruction Tuning Instruction tuning is typically the first step in fine-tuning language models [40], where they are trained on input-output pairs. When dealing with multi-step reasoning frameworks, this involves fine-grained supervision, to train on intermediate reasoning steps or tool-specific for tool-augmented frameworks. \n\nTo obtain the training data, one approach is to query a teacher model, typically a few-shot prompted closed-source LLM like GPT-4 in both Husky and ToRA, to solve problems and retain the correct multi-step reasoning data. Alternatively, in TAT-LLM manual annotations from human domain-experts are used to supervise each step of the reasoning pipeline. \n\nBoth strategies come with limitations.",
            "score": 0.4590019116091445,
            "section_title": "Fine-tuned Multi-step Reasoning Frameworks",
            "char_start_offset": 7350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2245
                },
                {
                    "start": 2248,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "263310365"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "2272000"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "272397813",
            "title": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs",
            "text": "Recent years have witnessed remarkable progress on Large Language Models (LLMs) [38], especially those instruction-following models such as ChatGPT and GPT-4 [17]. Numerous studies have demonstrated that these models exhibit strong capabilities across a wide range of tasks. However, despite the effectiveness of these models, existing work [11] shows that they perform poorly on Out-of-Distribution tasks, so fine-tuning with specific tasks and datasets is required to achieve satisfactory results. Fig. 1: Parameter-Efficient Fine-Tuning (PEFT) methods transform the nonprompt-following model to prompt-following by injecting a small number of learnable parameters into the pre-trained LLM. Our method lies in the domain of PEFT and concentrates on its problem-solving capabilities. \n\nNevertheless, fine-tuning large-scale LLMs in full is often prohibitively costly, thus many Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed in recent years, which transform a non-prompt-following model into a prompt-following model by injecting a small number of extra model parameters (Figure 1), thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning [37,16]. \n\nWhile these prompt-following models or fine-tuning methods have been proven to be effective in generating responses based on human instructions, there remains uncertainty regarding whether these models have genuinely acquired knowledge from the text or merely learned the distribution of the word tokens without true comprehension. [29] claimed that the scaling up of language models could significantly enhance their performance, which is usually seen as a piece of evidence that the LLMs can acquire knowledge when it's sufficiently large. However, [21] claims that emergent abilities only appear for specific metrics, and [11] suggests that these models do not possess any causal reasoning abilities. \n\nMany discussions have been raised regarding this issue, yet the answer remains inconclusive. Besides, most of these discussions are raised on GPT models, and they are rarely addressed in the context of LLM fine-tuning.",
            "score": 0.45877003059436744,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1980
                },
                {
                    "start": 1983,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.090576171875
        },
        {
            "corpus_id": "268531869",
            "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus",
            "text": "Efforts to evaluate LLMs' capabilities continue, underscoring strengths in image and text generation.Especially, analysis confirms LLMs possess elements of a World Model [13], indicating potential in inference tasks.Despite these capabilities, challenges in reasoning are noted [41], with errors such as distortion and incomplete reasoning highlighted [23].Research suggests these reasoning abilities can improve through methodological adjustments.Furthermore, studies indicate that complex compositionality remains challenging [10].\n\nThe divergent claims about the abilities of LLMs stem from result-centric measurement methods.Turing was the first figure to shift the approach to inference towards consequential direction [40].Subsequently, Wiener [51], McCulloch and Pitts [28], and Rosenblatt [36] shifted to studying methods for measuring performance rather than focusing on the process.Recently, Chollet attempted to quantify inference abilities from a consequential perspective [7].However, these studies all focus on what reasoning can achieve using a result-oriented approach, without specifying the elements that constitute reasoning ability.West et al. [50] raised concerns about evaluating the reasoning ability of LLMs from a consequentialist perspective, as the generation capability of LLMs may not necessarily depend on comprehension abilities.\n\nTherefore, a new perspective is needed to evaluate AI's inference processes; Language of Thought Hypothesis (LoTH) enhances discussions by integrating reasoning components with quantitative metrics.LoTH posits that inference involves manipulating mental representations, a view that dominates the philosophy of mind due to its explanatory power over logical coherence, compositionality, and productivity observed in human cognition.These mental representations are believed to have a compositional syntax and combinatorial semantics.Our study compares with prior works and evaluates LLMs' inference capabilities through the LoTH, marking progress by assessing aspects like logical coherence, compositionality, and productivity.\n\nExploring deeper into the three perspectives of LoTH offers strong justification for improving reasoning capabilities.These principles help in developing the ability to process information and solve tasks similar to human reasoning.Logical coherence ensures LLMs can reason without contradictions, compositionality allows LLMs to adapt known knowledge to new scenarios, and productivity enhances LLMs' capacity to generate results based on given rules.",
            "score": 0.4587643098811695,
            "section_title": "Assessing Reasoning Ability of LLMs",
            "char_start_offset": 5884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 533
                },
                {
                    "start": 535,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 892
                },
                {
                    "start": 892,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1152
                },
                {
                    "start": 1152,
                    "end": 1360
                },
                {
                    "start": 1362,
                    "end": 1560
                },
                {
                    "start": 1560,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1895
                },
                {
                    "start": 1895,
                    "end": 2089
                },
                {
                    "start": 2091,
                    "end": 2209
                },
                {
                    "start": 2209,
                    "end": 2323
                },
                {
                    "start": 2323,
                    "end": 2543
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 282,
                    "matchedPaperCorpusId": "249889477"
                },
                {
                    "start": 528,
                    "end": 532,
                    "matchedPaperCorpusId": "258967391"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "14636783"
                },
                {
                    "start": 776,
                    "end": 780,
                    "matchedPaperCorpusId": "15619658"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "12781225"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093994140625
        },
        {
            "corpus_id": "259145356",
            "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
            "text": "We see that across models and different numbers of in-context examples, the reasoning gap \u2206 reas accounts for up to 79.11% of the performance gap between in-context learning and fine-tuning. This indicates that the LLM representations have sufficient information but lack the ability to reason over them. \n\nFine-tuning: Improves task-specific reasoning. We next investigate how fine-tuning for a specific task affects the performance of the base model. In Figure 3b, we show a scatter plot of the gains that can be attributed to improved representations against the reasoning gains. We see that, across models, reasoning improvements accounts for 73.06% of the improvements. This indicates that while fine-tuning improves both reasoning and representations of the LLM, the gains are predominantly due to improvements in task-specific reasoning. Furthermore, this task-specific fine-tuning of the LLM hurts its performance on other tasks. In Figure 3c, we show that the accuracy of a model fine-tuned on the AGNews dataset [ZZL15], leads to an average decrease of 25.77% on other tasks. Furthermore, this drop in accuracy can be attributed to the drop in task-specific reasoning capabilities-these account for 72.58% of the drop (see Appendix B for more details). \n\nAdapters: Impairs task-agnosticity via reasoning. Task-specific adapters do not change the underlying representation ability of the model. To study their ability to generalize across tasks, we train an adapter for the AGNews dataset and evaluate it on other tasks. In Appendix B, we show that the performance drops across tasks by an average of 19.8%, indicating that adapters only learn task-specific reasoning abilities. \n\n4 Tart: Task-Agnostic Reasoning Transformers \n\nThe above analysis showed how it is the effective reasoning capabilities of the LLMs which limits its performance when compared with task-specific adaptation approaches. Building on this insight, we propose Tart, which learns a general-purpose reasoning module completely agnostic to the underlying base LLM and when composed with any LLM via its embeddings, generically improves upon its reasoning abilities. Tart is a completely task-agnostic method which works across a suite of tasks without any task-specific training.",
            "score": 0.45854129999975196,
            "section_title": "Understanding performance via Representation-Reasoning decomposition",
            "char_start_offset": 12481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2260
                }
            ],
            "ref_mentions": [
                {
                    "start": 1022,
                    "end": 1029,
                    "matchedPaperCorpusId": "368182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447998046875
        },
        {
            "corpus_id": "272593221",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "text": "In recent years, the rapid advancement of Large Language Models (LLMs) has revolutionized natural language processing (NLP). Pre-trained language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have validated the pre-train and fine-tune paradigm, which involves learning general language representations through pre-training and subsequently transferring this knowledge to enhance performance on specific NLP tasks via fine-tuning (Min et al., 2023a). This approach has evolved into prompt-based reasoning, exemplified by the GPT family (Radford The most downloaded model  et al., 2019;Brown et al., 2020), where a few examples are provided in the prompt before the model performs the task on new inputs (Liu et al., 2023a). \n\nThese paradigms have demonstrated exceptional performance across a range of tasks, including language generation (Dong et al., 2023), language understanding (Wang et al., 2019), and domain-specific applications in areas such as coding (Jiang et al., 2024b), medicine (He et al., 2023), and law (Sun, 2023). Moreover, the theory of emergent abilities suggests that certain reasoning capabilities are enhanced by increasing model size, with some abilities only appearing in larger models (Wei et al., 2022a). This has led to a surge in the development of increasingly large models, such as GPT-4 (Achiam et al., 2023), Mixtral 8x22B (Jiang et al., 2024a), PaLM-340B (Anil et al., 2023), and LLaMA-405B (Dubey et al., 2024). As a result, LLMs have become highly prevalent, with data from March 2024 showing that ChatGPT (OpenAI, 2024) reached approximately 180 million users. \n\nWhile LLMs have made significant strides in artificial general intelligence (AGI), their capabilities come with substantial overhead. Scaling model sizes leads to exponential increases in computational costs and energy consumption (Wan et al., 2023). Additionally, training and deploying LLMs is often unfeasible for academic researchers and businesses with limited resources.",
            "score": 0.4581441488328045,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 187,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 197,
                    "end": 218,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 729,
                    "end": 748,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 909,
                    "end": 928,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1046,
                    "end": 1057,
                    "matchedPaperCorpusId": "257557504"
                },
                {
                    "start": 1858,
                    "end": 1876,
                    "matchedPaperCorpusId": "266044196"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0299835205078125
        },
        {
            "corpus_id": "269484462",
            "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
            "text": "The alignment of reasoning abilities between smaller and larger Language Models are largely conducted via supervised fine-tuning using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-improve their abilities.Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on synthetic demonstrations provided by LLMs, and then the instructed models self-improve their abilities through preference optimization strategies.In particular, the second phase operates refinement heuristics based on Direct Preference Optimization, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs.Results obtained on commonsense and math reasoning tasks show that this approach consistently outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger language models.",
            "score": 0.45801726474870974,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1702880859375
        },
        {
            "corpus_id": "259145356",
            "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
            "text": "From the taxonomy of task adaptation approaches, only in-context learning satisfies the taskagnostic property but it consistently underperforms the task-specific tuning approaches. This section investigates why this performance gap exists. We hypothesize that it is either because (a) the representations learned by the LLM are insufficient to learn a good predictor for the specific task, or (b) the LLM lacks the capability to reason over these representations to make good predictions for the task. \n\nTo understand whether the representations have sufficient information, we train a task-specific linear classifier using these representations, also known as linear probing, and evaluate its accuracy. Let Acc FT , Acc ICL , and Acc LR denote the accuracies obtained by fine-tuning, in-context learning, and by linear probing respectively. Using this as an intermediate, we decompose the performance gap \n\nwhere \u2206 rep represents the gap in performance which can be attributed to insufficient representation capacity and \u2206 reas is the performance gap due to insufficient reasoning abilities. Using this decomposition, we consider the following hypotheses: H1. LLM representations have enough information to perform the task in-context, but they lack the reasoning abilities to perform the task well. \n\nH2. Fine-tuning affects both the representations and reasoning but the improvement in reasoning abilities primarily leads to better performance. \n\nH3. Fine-tuning and adapters are not task-agnostic because the task-specific training hurts their ability to transfer reasoning. \n\nWe now analyze each of the task adaptation approaches through the lens of the above hypotheses. We perform all experiments with three different classes of language models (GPT-Neo, Pythia, Bloom) across a collection of 6 binary classification tasks. See Appendix B for further details. In-context learning: LLMs lack reasoning abilities. We begin by studying the representation and reasoning gaps, as defined in eq. ( 1), for in-context learning. In Figure 3a, we plot the average accuracy across datasets for in-context learning, task-specific fine-tuning, and linear probing. We see that across models and different numbers of in-context examples, the reasoning gap \u2206 reas accounts for up to 79.11% of the performance gap between in-context learning and fine-tuning.",
            "score": 0.45745992387009554,
            "section_title": "Understanding performance via Representation-Reasoning decomposition",
            "char_start_offset": 10322,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2349
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295654296875
        },
        {
            "corpus_id": "272881329",
            "title": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical Impacts, Constraints, and Emerging Questions",
            "text": "Various research works attempt to advance LLMs through analysis-based methods [12,24,35,61]. \n\nThe development of metrics to quantify aspects of LLMs enhances understanding of both the limitations and effectiveness of these models. Le Scao and Rush [61] introduce an approach to quantify the advantage of prompting, aiming to guide practices in developing LLMs with scale by examining the impact of prompting across various data sizes. Gehman et al. [12] REALTOXICITYPROMPTS dataset provides valuable insights for gauging the risks of LLMs and understanding the factors to be conscious of. Metrics such as Majority Label Bias, Recency Bias, and Common Token Bias are crucial [35] to characterize model stability, aiding in the development of their calibration approach to address instability. Jiang et al. [24] propose a set of methods to better measure the accuracy of model knowledge that accounts for the role of prompts in the quality of generations. Their work enables a more accurate estimation of the knowledge in language models with the automatic generation of better prompts.",
            "score": 0.45732318312132936,
            "section_title": "LLM Understanding",
            "char_start_offset": 24418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 95,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1085
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 82,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 88,
                    "end": 91,
                    "matchedPaperCorpusId": "232233408"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "232233408"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06451416015625
        },
        {
            "corpus_id": "267500016",
            "title": "Automatic Robotic Development through Collaborative Framework by Large Language Models",
            "text": "The development and applications of large language models (LLMs) have attracted substantial attention in various research domains. The following studies contribute to the understanding of LLMs' capabilities and their applications in different contexts. In the context of prompting methods, Wei et al. [7] introduced \"chain-of-thought\" prompting, which promotes reasoning in LLMs through interconnected prompts. Kojima et al. [8] explored the zero-shot reasoning potential of LLMs, revealing their ability to perform reasoning tasks without specific training data. Ouyang et al. [9] focused on training LLMs to follow instructions with human feedback, demonstrating the adaptability of LLMs in learning from interactions. Scaling instruction-finetuned language models gained attention, as exemplified by the work of Chung et al. [10]. Their research aimed to enhance the capabilities of instruction-based fine-tuning, contributing to LLMs' effectiveness in understanding and executing instructions. Program synthesis using LLMs was another significant area of exploration. Austin et al. [11] delved into program synthesis with LLMs, seeking to generate executable code from natural language prompts. Jain et al. [12] introduced \"Jigsaw,\" a framework where LLMs meet program synthesis, enabling automatic code generation from high-level instructions. In the realm of multi-turn program synthesis, Nijkamp et al. [13] developed \"Codegen\" an open LLM model specializing in code generation through multi-turn interactions. Additionally, Chen et al. [14] focused on teaching LLMs to self-debug, enabling the models to identify and rectify errors in their generated code. \n\nFurthermore, research has explored LLMs' interaction with software development. Jiang et al. [15] conducted a study on LLMs' abilities to improve the effectiveness of software evolution tasks. Shin et al. [16] introduced \"AutoPrompt,\" a method for knowledge elicitation from LLMs using automatically generated prompts. Reynolds and McDonell [17] extended the concept of prompt programming beyond few-shot scenarios to enhance LLMs' expressiveness in generating code. Additionally, studies have examined strategies to assess LLMs' knowledge and understanding.",
            "score": 0.45717901945280104,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1211,
                    "end": 1215,
                    "matchedPaperCorpusId": "244908632"
                },
                {
                    "start": 2008,
                    "end": 2012,
                    "matchedPaperCorpusId": "231925131"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06982421875
        },
        {
            "corpus_id": "257766560",
            "title": "Unified Text Structuralization with Instruction-tuned Language Models",
            "text": "GPT-3 (Brown et al., 2020), with 175 billion parameter, has shown strong performance on many NLP tasks and benchmarks in few-shot setting. The latest ChatGPT model released by OpenAI some time ago has set off a new round of AI boom. From technical answers to scene play, from ghostwriting papers to chatting to relieve boredom, ChatGPT seems to be omnipotent. The most common way to use LLMs is fine-tuning, which refers to the processing of adapting a general-purpose model for a specific task or domain, achieved by training LLMs on a smaller dataset relevant to the task (Wei et al., 2021). Recent years, in-context learning makes the use of LLMs more convenient and efficient (Min et al., 2022). Provide LLMs a set of prompts (often input-output pairs), and LLMs can learn the patterns and execute the desired task. However, both fine-tuning and in-context learning require more or less sample data, and still pose difficulties for the general use of LLMs. So researches of LLMs under zero-shot setting become popular. expressed via natural language instructions (Wei et al., 2021). This simple method has been shown to improve the zero-shot learning abilities of language models. Follow an instruction, LLMs can perform well on unseen tasks (Wei et al., 2021;Chung et al., 2022b;Xu et al., 2022b), and even more close to human needs like writing a poem (Chakrabarty et al., 2022). Our method is inspired by the performance of instruction-tuned LLMs under zero-shot condition.",
            "score": 0.4571584261247931,
            "section_title": "Output Result",
            "char_start_offset": 7933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1480
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 26,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0517578125
        },
        {
            "corpus_id": "265157830",
            "title": "SAIE Framework: Support Alone Isn't Enough - Advancing LLM Training with Adversarial Remarks",
            "text": "LLMs have demonstrated remarkable capabilities in language comprehension and generation across a wide range of tasks (Brown et al., 2020;Chung et al., 2022;OpenAI, 2023). A pivotal development in this domain is their capability to predict through discussions with either other models or humans. For example, LLMs can explain reasons for their predictions in response to counterarguments, persuade others, or correct their own mistakes through multi-turn discussions (Kaneko et al., 2023). This Only the learner model undergoes parameter updates based on these interactions. Question in this example is: 'Tom decides to renovate a house. There are 3 bedrooms and each bedroom takes 4 hours to renovate. The kitchen takes 50% longer than each bedroom. The living room took twice as much time as everything else combined. How long did everything take?' performance of LLMs, particularly in tasks that demand complex reasoning processes (Talebirad and Nadiri, 2023). \n\nRecent studies have underscored the impact of proactive discussion during the inference phase in amplifying the efficacy of LLMs (Liang et al., 2023;Xiong et al., 2023;Chen et al., 2023;Madaan et al., 2023;Du et al., 2023). Through these discussions, models refine their problem-solving approaches, thereby improving task performance. Nevertheless, such enhancements largely polish the models' capabilities after their foundational learning has been set. This raises a crucial question: Is enhancing LLMs solely during the inference stage sufficient for developing reasoning and criti-cal thinking abilities? \n\nThe integration of interactive discussions during the training phase emerges as a novel area of research. Studies by Welleck et al. (2022) and Paul et al. (2023) have explored the generator-corrector framework, wherein feedback boosts model generation performance. This approach, focusing on correcting errors using intermediate representations like equations or logical expressions, may constrain the models' adaptability across diverse tasks. Moreover, an excessive emphasis on error correction could inadvertently encourage the generator model to rely on memorizing feedback rather than developing a profound understanding.",
            "score": 0.4570018251716529,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2202
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 137,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09283447265625
        },
        {
            "corpus_id": "261065787",
            "title": "CLEVA: Chinese Language Models EVAluation Platform",
            "text": "Inspired by HELM (Liang et al., 2022), we present a Tasks\u00d7Prompts\u00d7Metrics evaluation taxonomy for users to evaluate their models. Our evaluation taxonomy carefully designs a Chinese benchmark targeting various LLM abilities, employs a set of diverse prompt templates for each task to characterize the model performance variance, and adopts multiple metrics to comprehensively assess LLMs. Tasks. As shown in Figure 1, our Chinese LLM evaluation benchmark consists of two parts: ability evaluation and application assessment. Each task in ability evaluation focuses on one special skill of LLMs, while application assessment involves real-world NLP tasks that require LLMs to solve practical use cases with their skill sets. Ability evaluation assesses LLM ability from five aspects:\n\n\u2022 Language measures how well LLMs understand Chinese. In addition to three conventional tasks, we incorporate two tasks specific to Chinese: Pinyin transliteration and classical Chinese understanding. \u2022 Knowledge focuses on assessing the capacity of knowledge acquired by LLMs. We further segment our evaluation into subject knowledge and cultural knowledge (mainly Chinese culture) based on the source of knowledge. This fine-grained design allows users to closely analyze the model performance across different knowledge categories. \u2022 Reasoning evaluates LLMs' reasoning ability in two settings: reasoning primitives, which is independent of language and knowledge background, and realistic reasoning that requires reasoning with domain knowledge on practical scenarios. On top of HELM, we additionally include commonsense reasoning, inductive reasoning, conceptual generalization, and deductive reasoning. \u2022 Harms evaluates the potential risk of LLMs in copyright, disinformation, bias, and toxicity. \u2022 Others is newly introduced to include crucial yet uncategorized tasks like mathematical calculation and instruction following.\n\nFor application assessment, CLEVA features 11 real-world NLP tasks. In addition to the core scenarios of HELM, we newly include opinion mining, dialogue generation, paraphrase generation, translation, paraphrase identification, and data-to-text generation. A detailed description of each task is documented in Appendix B.\n\nWe instantiate the aforementioned tasks in two ways: by directly adopting related public Chinese datasets and by collecting new data. For wellstudied tasks, widely-recognized datasets are the best options for forming our benchmark. However, many important tasks, such as reasoning primitive,",
            "score": 0.45697156975043773,
            "section_title": "Evaluation Taxonomy",
            "char_start_offset": 10376,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0836181640625
        },
        {
            "corpus_id": "275134126",
            "title": "LLM Reasoning Engine: Specialized Training for Enhanced Mathematical Reasoning",
            "text": "Our experiments resulted in notable performance improvements across all four base models. because these models benefit more from structured reasoning guidance. This aligns with findings in previous LLM fine-tuning studies, where weaker models exhibit larger relative improvements when exposed to specialized training objectives. These findings underscore the empirical effectiveness of our methodology in improving the reasoning efficiency and accuracy of LLMs. By assessing LLMs' performance in mathematical reasoning tasks, we contribute to the ongoing efforts to advance the state-of-the-art in natural language processing and pave the way for their application in diverse problem domains requiring mathematical reasoning abilities.",
            "score": 0.45688138715138227,
            "section_title": "Main Results",
            "char_start_offset": 15463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 735
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "263829839",
            "title": "LLM for SoC Security: A Paradigm Shift",
            "text": "The recent rise of Large Language Models (LLMs) has profoundly impacted the field of Natural Language Processing (NLP), ushering in a new era of capabilities and applications. As the size and complexity of these models increase, they consistently improve in performance and efficiency on numerous NLP tasks that span Natural Language Generation (NLG) [1], Natural Language Understanding (NLU) [2] and information retrieval [3]. Specifically, their mastery is evident in fields such as text generation [4], summarization [5,6], machine translation [7], paraphrasing [8], classification [9], sentiment analysis [10], and question answering [11], to name a few. Beyond their efficacy in such linguistic tasks, LLMs are increasingly showcasing incredible aptitude in complex reasoning tasks. This encompasses arithmetic reasoning [12], commonsense, symbolic, and logical deliberations [13], analogical reasoning [14], and even multimodal reasoning [15]. Such emergent abilities [16], more pronounced in larger models such as GPT-3 [17], GPT-4 [18], PaLM [19], etc., provide a captivating insight into the unforeseen potential of scaled-up language models. Because of zero-shot and fewshot learning capabilities, these pre-trained models (PTMs) are being applied in a wide range of applications: healthcare [20], legal professions [21][22][23], creative works [24], and robotics [15,25]. The remarkable success of these PTMs has catalyzed the development of fine-tuned domain-specific LLMs such as Med-PaLM [26], Med-PaLM 2 [27], PaLM-E [25], BloombergGPT [28], AugGPT [29], LayoutGPT [30], BioBERT [31], SciBERT [32], ClimateBERT [33], etc. \n\nAs an example, software programming is witnessing a transformative shift as researchers increasingly incorporate LLMs for diversified coding tasks.",
            "score": 0.45687008940345364,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "208092413"
                },
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "208000835"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 1027,
                    "end": 1031,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "257572753"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "252596159"
                },
                {
                    "start": 1502,
                    "end": 1506,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1594,
                    "end": 1598,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03790283203125
        },
        {
            "corpus_id": "271328900",
            "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
            "text": "Chen et al. (2024f) delineate the differences in the mechanisms of knowledge storage and representation, proposing the Query Localization Assumption to response these controversies. Zhu and Li (2023) further observe that knowledge may be memorized but not extracted due to the knowledge not being sufficiently augmented (e.g., through paraphrasing, sentence shuffling) during pretraining. Hence, rewriting the training data to provide knowledge augmentation and incorporating more instruction fine-tuning data in the pretraining stage can effectively alleviate the above challenges and criticisms. \n\nDespite considerable criticism, the mainstream view (Didolkar et al., 2024;Jin and Rinard;Jin, 2024) is that current LLMs may possess basic world knowledge via memorization but hardly master underlying principles for reasoning and creativity. In other words, LLMs master basic knowledge via memorization (discussed in \u00a73.1). Although LLMs possess the foundational ability to comprehend and apply knowledge (discussed in \u00a73.2), exhibiting plausible and impressive reasoning capabilities. Current LLMs still struggle with reasoning and planning in complex tasks due to the fragility of knowledge in LLMs (elaborated in \u00a76.2). These reasoning and planning abilities usually require to be induced through techniques such as ICL and CoT. Unfortunately, current LLMs are nearly incapable of creation due to the architectural limitations (discussed in \u00a73.3). Therefore, some scholars explore various architectural choices (e.g., Mamba (Gu and Dao, 2023)) and training procedures. Besides, recent research attempts to manipulate neurons, knowledge circuits, or repre-sentations (Allen-Zhu and Li, 2023b;Zou et al., 2023;Wu et al., 2024;Li et al., 2023a) to explore more knowledge and awaken the reasoning and planning capabilities of LLMs. \n\nRemarks: LLMs have learned basic knowledge of the world by momorization. However, the learned knowledge is fragile, leading to challenges in knowledge comprehension and application. Unfortunately, due to architectural limitations, current LLMs struggle with creation.",
            "score": 0.4556942755789898,
            "section_title": "What Knowledge Have LLMs Learned?",
            "char_start_offset": 42565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 675,
                    "end": 690,
                    "matchedPaperCorpusId": "258762618"
                },
                {
                    "start": 1728,
                    "end": 1745,
                    "matchedPaperCorpusId": "259088877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04779052734375
        },
        {
            "corpus_id": "258212839",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "text": "Emergent Abilities and Multi-Step Reasoning. LLMs are particularly skilled at in-context learning, which involves adhering to the structure of prompts (typically few-shot) and completing corresponding tasks [15,[18][19][20]. Among the diverse range of language comprehension tasks, we are particularly interested in multi-step reasoning because it exhibits two unique features. Firstly, LLMs significantly outperform smaller models on multi-step reasoning tasks [8], whereas their performance gains on tasks like sentiment classification can be limited [19]. Secondly, few-shot prompting outperforms full training set fine-tuning in multi-step reasoning tasks, even when conducted on LLMs [7]. \n\nChain-of-Thought Reasoning. Chain-of-thought (CoT) prompting [8] is a prominent work that demonstrates the multi-step reasoning capacities of LLMs. This approach suggests that the reasoning ability can be elicited through a chain of thoughts, where an answer directly follows a question without intermediate reasoning steps. Least-to-Most prompting [9], which follows the same research direction, divides reasoning into problem breakdown parts and problem answer parts and describes the reasoning steps in more detail. Similarly, the complex CoT [10] highlights the importance of prompt complexity and selects the most complex questions and their answers as prompts. To reduce the human workload, the Auto-CoT is proposed [21]. Other works have found that using specific phrases like \"Let's think step by step\" [6] can improve performance. \n\nReasoning Path Extraction. Previous research has investigated various task-specific methods for identifying reasoning paths, including constructing semantic graphs [22], developing Recurrent Neural Network (RNN) models to retrieve reasoning paths from a Wikipedia graph [23], using human-annotated reasoning paths on math problems for fine-tuning [12], or training an extractor with heuristic-based pseudo reasoning paths [24]. A novel research work, named Self-Consistency [25], couples the generation of reasoning paths and a final answer by sampling from the decoder and using aggregation to retrieve the most consistent answer without extra modules. This approach has shown great promise, and it has the potential to outperform existing methods in terms of accuracy.",
            "score": 0.4556513491885544,
            "section_title": "Related Work",
            "char_start_offset": 3940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 1045,
                    "end": 1048,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "252683303"
                },
                {
                    "start": 1507,
                    "end": 1510,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1702,
                    "end": 1706,
                    "matchedPaperCorpusId": "237433880"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "208267807"
                },
                {
                    "start": 2012,
                    "end": 2016,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07977294921875
        },
        {
            "corpus_id": "258564230",
            "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
            "text": "Large Language Models (LLMs) have shown impressive abilities in various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory-of-Thought, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.",
            "score": 0.45541775662972944,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09368896484375
        },
        {
            "corpus_id": "264439566",
            "title": "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks",
            "text": "LLM pre-training. Traditional language models typically focus on a single task during the training stage, while LLMs take a more comprehensive approach by incorporating multiple tasks simultaneously during pre-training. This approach allows the model to learn a diverse set of linguistic features and capabilities, leading to improved performance across various downstream tasks. By leveraging a combination of tasks, including language modeling, text classification, and question answering, LLM pre-training aims to enhance the model's understanding of language semantics, context, and structure, ultimately enabling more robust and versatile language understanding and generation capabilities. LLM fine-tuning. In specialized domains like biomedicine and finance, LLMs often require fine-tuning on training data to acquire domain-specific knowledge and expressive capabilities, enabling them to effectively address domain-specific queries [10,17,31,38,51]. Recognizing this demand, the fine-tuning functionality of LLMs has gained increasing adoption. A significant breakthrough occurred in August 2023 when OpenAI introduced the fine-tuning interface for GPT-3.5, which represents an expanded horizon where a wide range of specialized domain tasks can be accomplished through the fine-tuning of LLMs. RLHF. Reinforcement learning from human feedback (RLHF) represents a groundbreaking approach in the training methodology of LLMs. The language modeling objective of LLMs -predicting the next token -is different from the objective \"following instructions and being helpful, truthful, and harmless\" [31]. In this case, the language modeling objective is regarded as misaligned. Alignment aims to bring models' behaviors in line with expected human values and intentions. Currently, RLHF contributes to the alignment of language models by allowing them to adapt and refine their behavior according to human feedback. This feedback loop enables the model to refine its language generation abilities, adjusting its responses based on the quality and relevance of the generated text as evaluated by humans. This bridges the gap between machine-generated text and human perception. In the privacy concern, RLHF ensures models avoid responding to privacy-invading queries, reducing the risk of privacy extraction from these models. Catastrophic forgetting in LLMs. Catastrophic forgetting (CF) is a notable challenge in the field of machine learning [19], particularly within the realm of LLMs [28].",
            "score": 0.45529407407006617,
            "section_title": "PRELIMINARIES 2.1 Large Language Models",
            "char_start_offset": 9295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2327
                },
                {
                    "start": 2328,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2495
                }
            ],
            "ref_mentions": [
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1601,
                    "end": 1605,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.152099609375
        },
        {
            "corpus_id": "276741193",
            "title": "Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs",
            "text": "Fine-tuning for domainspecific tasks has been extensively explored. The RAFT approach (Zhang et al. 2024) combines Retrieval-Augmented Generation (RAG) with fine-tuning to enhance LLM performance in specific domains by training models to disregard irrelevant retrieved documents, improving focus and accuracy. Similarly, \"RAG vs Fine-tuning\" (Gupta et al. 2024) compares both approaches across various LLMs, demonstrating how each method can be effectively employed for domain-specific applications, particularly in underexplored sectors like agriculture. Additionally, \"Fine-tuning Language Models for Factuality\" (Tian et al. 2023) leverages recent innovations in factuality judgment and preference optimization algorithms to improve the factual accuracy of LLMs, offering a novel approach to mitigating misinformation. \n\nInstruction Tuning and Data Selection Efficient data selection for instruction tuning is crucial for optimizing LLM performance. \"From Quantity to Quality\" (Li et al. 2023) introduces a self-guided methodology that employs the Instruction-Following Difficulty metric to identify highquality instruction data, enhancing training efficiency. Additionally, \"Rethinking Data Selection for Supervised Fine-Tuning\" (Shen 2024) argues that selecting data reflecting human-like interactions, rather than purely based on quality and diversity, yields better results in aligning models with human expectations. The MoDS approach (Du, Zong, and Zhang 2023) further refines data selection by focusing on quality, coverage, and necessity, demonstrating improved performance with a significantly reduced dataset. Addressing LLM limitations such as hallucinations and weak numerical reasoning, ToolQA (Zhuang et al. 2024) introduces a dataset to evaluate LLMs' ability to use external tools for question answering, providing insights into their strengths and weaknesses.",
            "score": 0.45524574279786123,
            "section_title": "Domain-Specific Adaptation",
            "char_start_offset": 7054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1879
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1488037109375
        },
        {
            "corpus_id": "266163300",
            "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
            "text": "As the number of model parameters and volume of training data increase, LLMs have demonstrated impressive reasoning capabilities [103]. Recently, there has been a growing interest in enhancing the performance of LLMs in downstream tasks without the need to update model parameters. One way to achieve this is to harness the inferential reasoning abilities of LLMs, a notable approach of which is the CoT prompting method [12]. This method enables LLMs to provide reliable answers through thoughtful consideration and explanation. Various approaches have been studied aiming to generate more accurate and reliable CoT possibly using LLMs themselves. For instance, He et al. [104] incorporate external knowledge as supporting information to generate more faithful CoT. Wang et al. [105] utilize self-consistency by generating multiple inference paths and answers, selecting the most frequently occurring answer as the final output, thereby improving the quality of CoT. Creswell et 13. https://github.com/features/copilot al. [106] propose a selection-inference framework that employs LLMs as general processing modules. This framework alternates between selection and inference steps, generating a series of interpretable, causal reasoning steps leading to the final answer. Zhou et al. [107] introduce the least-to-most prompting method, which breaks down complex problems into simpler subproblems and solves them sequentially. \n\nThe methods have limitations in relying on LLMs with more than 100 billion parameters. Researchers have developed smaller language models via knowledge distillation. Ho et al. [108] introduced Fine-tune-CoT, which leverages GPT3(175B) as a reasoning teacher to enable complex reasoning in smaller models, thereby significantly reducing the model size requirements. Li et al. [109] proposed Symbolic Chain-of-Thought Distillation (SCoTD), a method for training smaller student models using rationalizations sampled from a much larger teacher model. This approach distills the reasoning capabilities of the larger model into smaller models. Shridhar et al. [110] utilized the step-by-step Chainof-Thought (CoT) reasoning capabilities of larger models and distilled these abilities into smaller models. \n\nOur primary objective is to generate high-quality CoTs for code generation at a manageable cost.",
            "score": 0.45521292828724613,
            "section_title": "Chain of Thought Generation",
            "char_start_offset": 61112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2229
                },
                {
                    "start": 2232,
                    "end": 2328
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 134,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 779,
                    "end": 784,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1024,
                    "end": 1029,
                    "matchedPaperCorpusId": "248887351"
                },
                {
                    "start": 1286,
                    "end": 1291,
                    "matchedPaperCorpusId": "248986239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0582275390625
        },
        {
            "corpus_id": "271600971",
            "title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment",
            "text": "We evaluate the performance of LLMs fine-tuned with NASA on general QA reasoning tasks. We utilize general reasoning benchmarks such as MuSiQue, GSM8K, and AR-LSAT without the transformation. For short-answer QA, we first verify whether each model-generated prediction includes an answer in these benchmarks and then utilize GPT-4 to double-check whether the prediction is semantically correct. To generate predictions from the model, we utilize Prompt II in Section 4.1. The content of the GPT-4 prompt for prediction verification can be found in Table 15. \n\nAs shown in Table 5, models fine-tuned with NASA show enhanced or maintained general reasoning abilities while effectively tackling negative bias. Since negative bias is a task-specific factor contributing to hallucination in binary decision tasks, its influence on general reasoning QA, like short-answer QA, is relatively low compared to other factors contributing to hallucination such as parametric knowledge. Therefore, improving reasoning ability in binary decision tasks does not necessarily enhance reasoning capabilities in gen- eral reasoning QA. This means that addressing bias in binary decision tasks while maintaining performance in general reasoning QA can be considered an advancement in the model's overall reasoning capability, not overfitted to the binary decision task.",
            "score": 0.45510760135928674,
            "section_title": "General Reasoning Abilities",
            "char_start_offset": 25150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1349
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09173583984375
        },
        {
            "corpus_id": "263605624",
            "title": "Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models",
            "text": "In this work, we have elucidated that fine-tuning teacher LLMs to both annotate unlabeled instances and generate new data points can effectively improve a downstream model's performance. Our empirical investigations spanned six tasks, four in classification and two pertaining to natural language generation. A potential constraint of our approach is that fine-tuning a large model necessitates significant resources. In future work, we aim to delve deeper into quantifying the fine-tuning required to steer the teacher model towards producing high-quality synthetic data.",
            "score": 0.4547550286326489,
            "section_title": "Conclusion, Limitations and Future Work",
            "char_start_offset": 12206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 572
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046295166015625
        },
        {
            "corpus_id": "258865899",
            "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
            "text": "Reasoning in LLMs Reasoning is a fundamental cognitive process involving logical inferences and conclusions based on given information. Developing models with strong reasoning capabilities has attracted increasing attention and many researches have been conducted on this topic since early days in the NLP domain [17]. Since then, various benchmarks focusing on different aspects of reasoning have been proposed, including natural language inference (NLI) [18][19][20], commonsense reasoning [21,22], multi-hop reasoning [23,24] etc. In recent years, there has been growing interests in studying the reasoning abilities of LLMs. Researchers have explored various approaches to enable LLMs to perform better on reasoning tasks. For example, \"chain-of-thought (CoT)\" [5,25] is proposed to facilitate models to generate a reasoning path that decomposes complex reasoning into multiple easier steps; LLMs are decent zero-shot reasoners by adding a simple prompt, \"Let's think step by step\", to facilitate step-by-step thinking before giving the final answer [6]. This significantly improves the performance on arithmetic [26], commonsense [21,27], and symbolic reasoning [5] benchmarks. However, despite their impressive performance on various reasoning benchmarks, all the tasks evaluated are rich in semantics. Thus it is unclear where the reasoning abilities of LLMs come from. This motivates us to investigate LLMs' reasoning abilities when semantics are decoupled. \n\nIn-Context Learning LLMs' reasoning abilities are closely related to in-context learning (ICL). ICL refers to the ability of language models to adapt and learn from a few prompt examples during the inference process. In recent years, there has been a focus on exploring how to improve the performance of ICL. Specifically, some works select related demonstrations to the test instance using off-the-shelf unsupervised similarity metrics or train a prompt retriever to select examples [28][29][30]. Others incorporate task instructions or different task prompts [31,32]. Despite the empirical success, the underlying mechanisms of ICL still remain unclear. A few studies have shown that the performance of ICL usually varies with the choice of in-context demonstrations [8,33].",
            "score": 0.454741285940956,
            "section_title": "Related Works",
            "char_start_offset": 3515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 499,
                    "matchedPaperCorpusId": "236459873"
                },
                {
                    "start": 1139,
                    "end": 1142,
                    "matchedPaperCorpusId": "230799347"
                },
                {
                    "start": 1952,
                    "end": 1956,
                    "matchedPaperCorpusId": "233296655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0709228515625
        },
        {
            "corpus_id": "268724019",
            "title": "Improving Attributed Text Generation of Large Language Models via Preference Learning",
            "text": "Large Language Models (LLMs) have demonstrated emergent abilities and have gained widespread application in Natural Language Processing (NLP) (Brown et al., 2020;Wei et al., 2022;OpenAI, 2022;Anil et al., 2023).For example, LLMs have shown remarkable in-context learning capabilities across a variety of domains and tasks (Dong et al., 2023).Although LLMs have been widely adopted, a prominent issue is that they produce hallucinations in certain situations (Ye et al., 2023a;Zhang et al., 2023).In other words, they generate information that sounds plausible but is nonfactual, thereby limiting their applicability in the real world.To mitigate hallucinations, researchers have resorted to grounding statements in responses generated by LLMs to supported evidence, either by providing rationales or by adding citations to the statements (Li et al., 2023a;Liu et al., 2023).\n\nRecent works have utilized external knowledge sources such as retrieved documents and knowledge graphs for attribution (Shuster et al., 2021;Li et al., 2023c).Generally, these works are divided into two types: 1) the model generates an answer with citations based on the retrieved documents (Li et al., 2023b); 2) an answer is first generated, then modified again to add attribution references by retrieving with query and initial answer (Gao et al., 2023a).However, these works focus mainly on the retrieval stage (Ye et al., 2023b) and the evaluation process (Yue et al., 2023).Considering the selection of the model's desired responses and behavior from its very broad knowledge and capabilities, it is more necessary to optimize the generation process, not only reducing the hallucination of the original answer but also avoiding the hallucination of the attribution process.On the other hand, fine-tuning LLMs after pre-training can also significantly improve performance for users' downstream tasks.First, given positive examples of correct behavior, supervised fine-tuning can be performed using standard likelihood-based training.",
            "score": 0.4546670043996932,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 342
                },
                {
                    "start": 342,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 634
                },
                {
                    "start": 634,
                    "end": 874
                },
                {
                    "start": 876,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1334
                },
                {
                    "start": 1334,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1755
                },
                {
                    "start": 1755,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 179,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 838,
                    "end": 856,
                    "matchedPaperCorpusId": "265043716"
                },
                {
                    "start": 856,
                    "end": 873,
                    "matchedPaperCorpusId": "258212854"
                },
                {
                    "start": 995,
                    "end": 1017,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 1167,
                    "end": 1185,
                    "matchedPaperCorpusId": "265157583"
                },
                {
                    "start": 1314,
                    "end": 1333,
                    "matchedPaperCorpusId": "254247260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07171630859375
        },
        {
            "corpus_id": "273233951",
            "title": "Automatic Curriculum Expert Iteration for Reliable LLM Reasoning",
            "text": "Hallucinations and laziness of LLM. It is widely acknowledged that LLMs often produce hallucinated responses, including fabricated facts and misleading logic (Radhakrishnan et al., 2023;Ji et al., 2023). Most works focus on reducing hallucinations by ensuring the factual accuracy of generated content, often using retrieval-based methods that enhance LLMs with external knowledge (Shuster et al., 2021). This methodology has been effective in improving the reliability of LLMs in domains that require high factual accuracy, such as knowledge-based question answering (Lewis et al., 2020). Particularly, Retrieval-Augmented Generation (RAG) reduces hallucinations by fetching relevant documents from external sources during inference, integrating factual information to enhance accuracy and relevance (Izacard et al., 2023). These methods ensure that the LLMs' responses align more closely with verified data (Komeili et al., 2022;Gururangan et al., 2021). Moreover, several studies have explored hybrid approaches that combine retrieval with fact-checking modules or domain-specific fine-tuning to improve factual accuracy (Lee et al., 2022). Retrieval-based strategies show strong potential in mitigating hallucinations, outperforming purely generative models that rely only on knowledge from training (Borgeaud et al., 2022;Petroni et al., 2020). However, hallucinations are not limited to factual inaccuracies; they can also extend to faulty or incomplete reasoning, a significant concern for multi-step reasoning-based tasks (Creswell et al., 2022). Moreover, LLMs often exhibit what can be described as \"laziness,\" which refers to the tendency of the model to reject or avoid generating correct but complex answers in favour of simpler, superficial, or incomplete responses (Bubeck et al., 2023;Bang et al., 2023). This phenomenon has been noted in tasks requiring step-by-step logical reasoning, where LLMs tend to skip intermediate steps or provide too general answers, rather than addressing the nuanced complexity of the problem at hand (Rae et al., 2021). \n\nReinforcement learning for LLM reasoning.",
            "score": 0.4545671091124598,
            "section_title": "RELATED WORK",
            "char_start_offset": 7083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2066
                },
                {
                    "start": 2069,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 202,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 568,
                    "end": 588,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 801,
                    "end": 823,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 909,
                    "end": 931,
                    "matchedPaperCorpusId": "236034557"
                },
                {
                    "start": 1124,
                    "end": 1142,
                    "matchedPaperCorpusId": "249538460"
                },
                {
                    "start": 1304,
                    "end": 1327,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07989501953125
        },
        {
            "corpus_id": "268876467",
            "title": "uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?",
            "text": "In conclusion, our study emphasizes the crucial role of prompting methods in augmenting the lateral thinking capabilities of LLMs.Through diverse CoT-based strategies, prompt refinements, and RAG techniques for in-context learning, we showcase the efficacy of well-structured prompts and thinking styles in elevating LLM performance.Additionally, fine-tuning models on a lateral thinking dataset proves advantageous, leading to improved performance on various commonsense tasks.This underscores the significance of integrating out-ofthe-box thinking in model training, opening promising avenues for future research to enhance LLMs' reasoning abilities.",
            "score": 0.45427089368078555,
            "section_title": "Conclusion",
            "char_start_offset": 19150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 130,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 652
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04559326171875
        },
        {
            "corpus_id": "268032159",
            "title": "Determinants of LLM-assisted Decision-Making",
            "text": "Large Language Models (LLMs), such as GPT-4, are subsumed under the category of generative AI [24,93,61].These advanced Transformer-based language models with hundreds of billions or more parameters are trained on extensive data [163].Studies have indicated that scaling significantly enhances the model capacity of LLMs [20,31].LLMs demonstrate remarkable abilities in understanding natural language and solving complex text generation tasks [211].Engineered to comprehend and produce natural language, LLMs are capable of performing a wide array of natural language tasks, including automatic summarizing, machine translation, and question answering [165].\n\nIn contrast to conventional (smaller) language models, LLMs are characterized by so-called emergent abilities.\n\nAn emergent ability is defined as an ability that \"is not present in smaller models but is present in larger models\" and is related to specific complex tasks [190].An example of an emergent ability is step-by-step reasoning.For small language models, solving complex tasks involving multiple reasoning steps, such as mathematical word problems, has posed difficulties.In contrast, LLMs can tackle such tasks, for instance by using the Chain-of-Thought (CoT) prompting strategy [191].Furthermore, through Instruction Tuning, LLMs are capable to follow task instructions for new tasks without relying on explicit examples, enhancing their overall ability for generalization [211].Additionally, LLMs are able to execute unfamiliar tasks solely by reading task instructions without requiring a few-shot examples, a capability referred to as Instruction Following [190].\n\nHowever, not only emergent abilities can arise, but also risks.For example, LLMs can perpetuate stereotypes and social biases, leading to unfair discrimination.In addition to this, LLMs might provide false or misleading information that can be harmful, especially in critical areas like legal or medical advice.Another risk lies in presenting LLMs as \"human-like\" which potentially leads users to overestimate their capabilities [193].A comprehensive overview of challenges, limitations and risks of LLMs is provided in Section 4.6.",
            "score": 0.4540855731678618,
            "section_title": "Large Language Models",
            "char_start_offset": 13220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 449
                },
                {
                    "start": 449,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 770
                },
                {
                    "start": 772,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 996
                },
                {
                    "start": 996,
                    "end": 1140
                },
                {
                    "start": 1140,
                    "end": 1255
                },
                {
                    "start": 1255,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1637
                },
                {
                    "start": 1639,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1799
                },
                {
                    "start": 1799,
                    "end": 1950
                },
                {
                    "start": 1950,
                    "end": 2074
                },
                {
                    "start": 2074,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 229,
                    "end": 234,
                    "matchedPaperCorpusId": "254366666"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 652,
                    "end": 657,
                    "matchedPaperCorpusId": "256272939"
                },
                {
                    "start": 930,
                    "end": 935,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1249,
                    "end": 1254,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1631,
                    "end": 1636,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 2068,
                    "end": 2073,
                    "matchedPaperCorpusId": "249872629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0293731689453125
        },
        {
            "corpus_id": "275471407",
            "title": "Enhancing AI Safety Through the Fusion of Low Rank Adapters",
            "text": "Large Language Models (LLMs) have demonstrated remarkable proficiency, exhibiting advanced linguistic and reasoning capabilities [1] that make them increasingly favored choices for conversational agents. With each new iteration, these models are released to the public with enhanced functionalities designed to assist in a myriad of user tasks, ranging from simple queries to complex problem-solving scenarios. \n\nThese LLMs excel at performing general tasks and can also be adapted for specific activities through In-context Learning (ICL) [2], where the model leverages existing parameters without the  need for updates. However, when more profound task-specific performance tuning is required, finetuning becomes necessary. Here, parameters of the base model are modified to align with the demands of downstream tasks. In this realm, Parameter-Efficient Fine-Tuning (PEFT) [3] has emerged as a popular strategy, particularly within large-scale models. Techniques like Low-Rank Adaptation (LoRA) [4] stand out due to their practicality in selectively updating a small subset of parameters, thereby maintaining the vast pre-trained knowledge base while optimizing the model towards specific tasks. \n\nDespite the advantages, fine-tuning can inadvertently lead to jailbreaking of the model, where the LLM deviates from safety constraints previously set by the base configuration. This issue has been noted in several studies [6,7,8,9], which highlight the challenges of maintaining safety alignment when adapting models through fine-tuning. \n\nLLMs have increasingly incorporated advanced safety mechanisms, such as Reinforcement Learning from Human Feedback (RLHF) [5], to simultaneously optimize for both helpfulness and harm reduction. These safety alignment techniques are crucial for aligning model outputs with ethical guidelines and user expectations. However, while fine-tuning LLMs on downstream tasks can significantly enhance their helpfulness and task-specific performance, this fine-tuning process often inadvertently compromises the models' inherent safety protocols. This degradation in safety measures during fine-tuning raises critical concerns, as it may lead to the generation of outputs that, although high-performing, could be potentially harmful or biased. \n\nTo address the challenge of maintaining safety standards while enhancing task performance during fine-tuning, our contribution is twofold:",
            "score": 0.45399869762200623,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2275
                },
                {
                    "start": 2278,
                    "end": 2416
                }
            ],
            "ref_mentions": [
                {
                    "start": 1423,
                    "end": 1426,
                    "matchedPaperCorpusId": "265308865"
                },
                {
                    "start": 1426,
                    "end": 1428,
                    "matchedPaperCorpusId": "263671523"
                },
                {
                    "start": 1428,
                    "end": 1430,
                    "matchedPaperCorpusId": "265067269"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "268554107",
            "title": "ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting",
            "text": "Chain-of-Thought Prompting.Chain-of-Thought (CoT) (Wei et al., 2022;Kojima et al., 2022) prompting is an effective solution for solving complex problems by explicitly generating reasoning steps.As CoT prompting has a critical effect on improving model performance, many studies (Saparov and He, 2022;Wang et al., 2023a) are proposed to further improve CoT prompting technique.For example, self-consistency (Wang et al., 2023b) proposes generating several reasoning paths and then selecting the most consistent answer by voting.Self-verification (Weng et al., 2022) lets LLMs themselves verify their prediction results.Tree-of-Thought (Yao et al., 2023) is a paradigm that allows LLMs to explore multiple reasoning paths over thoughts by framing the problem as a search over a tree.Instead of improving CoT prompting from the perspective of generation methods, our work explores the factors about why CoT prompting works through empirical analysis and designs a data augmentation framework to improve CoT prompting.Instruction Tuning.Instruction Tuning is an approach to fine-tuning LLMs on a collection of instructions and responses.Early work focuses on fine-tuning LLMs on specific NLP tasks.T5 (Raffel et al., 2020) first introduces text-to-text framework to fine-tune models on multiple tasks.Subsequent work such as FLAN (Wei et al., 2021), FLAN-T5 (Chung et al., 2022), and ZeroPrompt (Xu et al., 2022) improve LLMs by increasing the number of tasks and carefully designing instructions for the tasks.To bridge the gap between human queries and synthesized instructions, many studies propose to fine-tune LLMs with open-domain instructions.Alpaca (Taori et al., 2023) is finetuned on 52K instruction data generated by Self-Instruct (Wang et al., 2022).Vicuna (Chiang et al., 2023) collects 70K user-shared ChatGPT conversations from ShareGPT.comfor fine-tuning.Wiz-ardLM (Xu et al., 2023) proposes Evol-Instruct to evolve instructions.",
            "score": 0.4537577557784981,
            "section_title": "Related Work",
            "char_start_offset": 26602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 27,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 527
                },
                {
                    "start": 527,
                    "end": 618
                },
                {
                    "start": 618,
                    "end": 781
                },
                {
                    "start": 781,
                    "end": 1014
                },
                {
                    "start": 1014,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1133
                },
                {
                    "start": 1133,
                    "end": 1194
                },
                {
                    "start": 1194,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1507
                },
                {
                    "start": 1507,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 1758
                },
                {
                    "start": 1758,
                    "end": 1851
                },
                {
                    "start": 1851,
                    "end": 1867
                },
                {
                    "start": 1867,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 68,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 68,
                    "end": 88,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 278,
                    "end": 300,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 300,
                    "end": 319,
                    "matchedPaperCorpusId": "254877569"
                },
                {
                    "start": 406,
                    "end": 426,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1197,
                    "end": 1218,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1391,
                    "end": 1408,
                    "matchedPaperCorpusId": "246035184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060516357421875
        },
        {
            "corpus_id": "270440893",
            "title": "Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement in RLHF and Effective-Merged LLMs",
            "text": "Large Language Models (LLMs) such as GPT-4 OpenAI (2023) and Claude have revolutionized the field of Natural Language Processing (NLP) by demonstrating exceptional abilities in understanding context, generating text, and performing a wide array of language tasks Wu et al. (2023).Remarkably, smaller-scale LLMs like Llama Touvron et al. (2023a;b), Alpaca Taori et al. (2023), Vicuna Chiang et al. (2023), and Mistral Jiang et al. (2023); Zheng et al. (2024b) have also shown impressive results, performing well on general benchmarks like MMLU Hendrycks et al. (2021) and BBH Srivastava et al. (2022).Furthermore, their general capabilities can be enhanced through Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) alignment Ouyang et al. (2022).\n\nHowever, these smaller-scale LLMs exhibit noticeable deficiencies in conversational abilities, even in aligning with human preference in dialogue style Zhai et al. (2023).The generated responses from these models often lack quality and consistency with the desired conversational preference Zheng et al. (2024b).A significant shortcoming is that these smaller-scale models often produce responses that lack depth and detail, struggle with maintaining coherence over extended dialogues, and sometimes fail to exhibit robust underlying reasons or the analytical reasoning process.These issues reduce their effectiveness in complex applications like in-depth conversation and technical analysis Wang et al. (2022); Zhang et al. (2023).\n\nAlthough Supervised Fine-Tuning (SFT) can sometimes reduce hallucinations, the resulting responses are typically short and lack substantial analysis, which negatively influences Preprint subsequent RLHF training.Furthermore, these models are often trained on datasets that do not fully engage their linguistic and cognitive capabilities, resulting in suboptimal performance in tasks that require high-level reasoning and detailed analysis Shen (2024).\n\nIn response to these challenges, we propose a novel two-step Coarse to Fine Analytical and Reasoning Enhancement LLM approach.",
            "score": 0.45358556104378916,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 778
                },
                {
                    "start": 780,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1092
                },
                {
                    "start": 1092,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1512
                },
                {
                    "start": 1514,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1965
                },
                {
                    "start": 1967,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 1472,
                    "end": 1490,
                    "matchedPaperCorpusId": "253244132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07611083984375
        },
        {
            "corpus_id": "268856523",
            "title": "Developing safe and responsible large language model: can we balance bias reduction and language understanding?",
            "text": "This research primarily focuses on developing and implementing an approach to detect and mitigate linguistic biases in textual content while preserving the integrity of knowledge. The following questions guide our study: \n\n\u2022 RQ1: How effective is our approach at reducing biases in texts, and how much knowledge is retained in the process? \u2022 RQ2: To what extent does fine-tuning outperform few-shot and zero-shot prompting in reducing specific types of bias and retaining task-specific knowledge in LLMs? \n\nOur empirical analysis, conducted on both our training set and out-of-distribution datasets such as Toxigen, BOLD, and StereoSet, demonstrates the better performance of our instruction fine-tuned model, SR LLM , in reducing unsafe content and retaining knowledge in the language generation task. This approach outperforms both smaller encoder-decoder fine-tuned language models and vanilla LLMs operating under zeroshot and few-shot prompt settings. The retention of important knowledge within the LLM is confirmed through targeted experiment on language understanding and human evaluation. \n\nWhile we acknowledge the ethical implications associated with modifying user content as in our work, our primary objective remains the development of a methodology that guarantees the production of safe LLM outputs. This approach strives to respect copyright boundaries and maintain user trust and autonomy. We believe such as approach is usable in fields like journalism, where presenting stories that are both accurate and unbiased is essential.",
            "score": 0.4535187247582517,
            "section_title": "Research questions",
            "char_start_offset": 5260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1547
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1143798828125
        },
        {
            "corpus_id": "272397813",
            "title": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs",
            "text": "Large Language Models (LLMs) have demonstrated remarkable efficiency in tackling various tasks based on human instructions, but studies reveal that they often struggle with tasks requiring reasoning, such as math or physics. This limitation raises questions about whether LLMs truly comprehend embedded knowledge or merely learn to replicate the token distribution without a true understanding of the content. In this paper, we delve into this problem and aim to enhance the reasoning capabilities of LLMs. First, we investigate if the model has genuine reasoning capabilities by visualizing the text generation process at the attention and representation level. Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems observed in the visualization. Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions. Experiments show that our method outperforms the baseline consistently across multiple benchmarks, and with only 1.2M tunable parameters, we achieve better or comparable results to other fine-tuning methods. This demonstrates the effectiveness and efficiency of our method in improving the overall accuracy and reliability of LLMs.",
            "score": 0.45342582896829353,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2418212890625
        },
        {
            "corpus_id": "270380160",
            "title": "Bi-Chainer: Automated Large Language Models Reasoning with Bidirectional Chaining",
            "text": "Recent advancements in large language models, such as LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2023), and GPT-4 (Ope-nAI, 2023), have demonstrated surprising humanlike intelligence in the area of multi-step logical reasoning.Due to its huge application potential for various applications, including problem-solving, decision-making, and critical thinking (Huang and Chang, 2023), numerous research efforts have been dedicated to improving or eliciting the reasoning ability of these language models.Most of them can be classified into three categories:\n\nFully Supervised Finetuning: Some previous methods (Rajani et al., 2019;Hendrycks et al., 2021) have employed fine-tuning techniques on pre-trained language models (LMs) using downstream datasets to generate rationales or step-by-step solutions, effectively performing reasoning until obtaining final answers.However, these methods heavily rely on meticulously constructed finetuning datasets that explicitly capture the reasoning process.Unfortunately, such high-quality data is often difficult to access or requires substantial resources to create.Moreover, this reliance on specific datasets can restrict the extension of the LM's reasoning abilities to other open-ended reasoning tasks beyond the domain of the fine-tuning dataset.\n\nPrompting & In-Context Learning: The Chain of Thought (CoT) and its variants (Wei et al., 2022) are the most common approaches to release and utilize LLM's reasoning capabilities.CoT guides the model to generate explicit step-by-step rationale before producing the final results.The rationale engineering techniques like rational refinement/exploration/verification are complementary to CoT for further eliciting the reasoning capabilities more effectively via refining demonstration rationale examples (Fu et al., 2023), encouraging exploring diverse reasoning ways (Wang et al., 2023), and verifying if the generated rationales by LLMs lead to correct final answers (Weng et al., 2023).Problem decomposition methods (Zhou et al., 2023;Press et al., 2023) can also help facilitate the CoT reasoning when tackling complex tasks by decomposing the complex problems into relatively simpler subproblems.",
            "score": 0.4532329637492283,
            "section_title": "Related Works",
            "char_start_offset": 3172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 565
                },
                {
                    "start": 567,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 1006
                },
                {
                    "start": 1006,
                    "end": 1117
                },
                {
                    "start": 1117,
                    "end": 1302
                },
                {
                    "start": 1304,
                    "end": 1483
                },
                {
                    "start": 1483,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1992
                },
                {
                    "start": 1992,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 113,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 368,
                    "end": 391,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 618,
                    "end": 639,
                    "matchedPaperCorpusId": "174803111"
                },
                {
                    "start": 639,
                    "end": 662,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 1381,
                    "end": 1399,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1807,
                    "end": 1824,
                    "matchedPaperCorpusId": "252683303"
                },
                {
                    "start": 1871,
                    "end": 1890,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1972,
                    "end": 1991,
                    "matchedPaperCorpusId": "258840837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0618896484375
        },
        {
            "corpus_id": "258480241",
            "title": "\"Oops, Did I Just Say That?\" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process",
            "text": "A. Background: Large Language Model (LLM) \n\nLLMs usually refer to language models that contain hundreds of billions (or more) of parameters, which are trained on massive text data. Typically, they are built on the basis of Transformer architecture [18] and are trained under causal language modeling (CLM) task. CLM aims to predict the token after a sequence of tokens. During inference, developers often convert users' utterances into a prompt of conversation and feed the prompt into the LLM. Then, the LLM will repeatedly generate the next token to constitute the response until the end of the conversation. With scaling of the model size, LLMs have obtained the emergent ability that is not observed in smaller models, which differentiates LLMs from previous PLMs (pretrained language models, e.g., BERT [19]). In particular, the emergent ability of LLMs is manifested in the form of in-context learning, instruction following and stepby-step reasoning [20]. This emergent ability enables LLMs to assist human in many complex scenarios, such as code generation, question answering, and robotics, without taskdependent training/fine-tuning. Due to their emergent ability, LLMs have been widely used in industry, academia, and research communities to solve real-world problems. Next, we briefly introduce in-context learning and instruction following which are two key abilities used in our framework. In-context Learning. Introduced in GPT-3 [21], in-context learning allows LLMs to generate expected outputs for new inputs given a task instruction and a few input/output examples, without task-specific training or gradient updates. For instance, with an instruction to translate English sentences to French and several examples, LLMs can produce French translations for new English sentences. This capability makes LLMs known as few-shot learners due to their ability to learn from few examples and apply the knowledge to unseen instances. Instruction Following. Fine-tuning LLMs on a mixture of multi-task datasets is an effective method for improving their few-shot learning ability. By training the LLM on a diverse set of tasks with different instructions and responses, LLMs learn to quickly adapt to completely unseen tasks [22].",
            "score": 0.4528232067140143,
            "section_title": "II. BACKGROUND, RELATED WORK AND MOTIVATION",
            "char_start_offset": 3258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 44,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0987548828125
        },
        {
            "corpus_id": "264288970",
            "title": "Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning",
            "text": "Recently, the emergence and rapid advancement of Large Language Models (LLMs) [38,39,30,33] have pushed the boundaries of natural language understanding and generation. These models have been applied to a variety of applications [54,49], from content generation to answering complex questions. A salient feature of LLMs is their potential to follow instructions given to them, a characteristic that has been harnessed to fine-tune and control their outputs. This process, commonly referred to as instruction tuning [43,25,5,26,8,53], holds immense promise for customizing LLMs to specific tasks or preferences. \n\nHowever, instruction tuning is susceptible to the quality of training data. Introducing suboptimal data into the training process can have a cascade of adverse effects. Within the ambit of natural language generation, empirical research delineates that both the integrity and the homogeneity of training data critically modulate the fluency, pertinence, and precision of the generated linguistic content [3,12,15]. Datasets exhibiting inconsistencies or subpar quality can precipitate models to engender erratic, prejudiced, or even specious outputs, thereby attenuating their dependability and applicability. Analogous issues permeate instruction-tuning environments. Recent research [48,34] underscores that even a minuscule fraction of skewed virtual prompts can severely impinge upon a model's operational efficacy, manifesting the susceptibility of large language models (LLMs) to inferior data. On the other hand, ALPAGASUS [5] and Cherry LLM [21] demonstrate that LLMs can achieve enhanced performance metrics by leveraging a select subset of high-quality data. \n\nTo address this identified challenge, we introduce a novel method engineered to enhance the quality of extant instruction-tuning datasets autonomously. Drawing inspiration from the evaluative proficiencies of LLMs [55,7,23] and contemporary paradigms in self-enhancement [17,29], our approach hinges on employing an oracle model to introspectively assess and improve the current dataset against specific criteria.",
            "score": 0.45277524399625496,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1017,
                    "end": 1020,
                    "matchedPaperCorpusId": "52255687"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "matchedPaperCorpusId": "202235596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.149658203125
        },
        {
            "corpus_id": "266999728",
            "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
            "text": "Recent studies have explored the potential of using external tools to augment LLMs' capabilities for complex tasks [111]. Research [181,182,183] has demonstrated that supplementing LLMs with tools, such as calculators, search engines, translation systems, calendars, or even API calls to other models can help solve tasks beyond their inherent capabilities. \n\nThe integration of tools into LLMs generally involves two primary phases: tool selection or creation, and processing the results obtained from these tools [184,185,186]. Tool selection, a pivotal phase within tool learning, can involve allowing the LLM itself to make decisions based on input queries [187,188,189,111,19]. Alternatively, it might include the use of predefined tools specifically designed for particular scenarios [182,190,191,192], such as retrieval systems. \n\nWhen the model is required to autonomously determine which tools to employ, it necessitates the model to understand the characteristics of these tools and make accurate selections via their reasoning abilities. The majority of approaches tend to decompose a complex task into a series of subtasks interacting with tools in sequential or tree structures, such as Chain of Thoughts (CoT) [104,106], Tree of Thoughts (ToT) [193] and Program of Thoughts (PoT) [110]. \n\nA simple way to teach language models to use tools is by instruction [111,191,183]. The description of tools is able to teach language models to use tools [191] in a zero-shot fashion. Language models can also learn to use tools from few-shot examples. This in-context tool learning can range from retrieval tasks to enhancing the model's ability to generalize to new challenges. For example, Art [183] demonstrates this by selecting examples of multistep reasoning and tool usage from a task library. This approach significantly improves upon traditional few-shot prompting and automated Chain of Thought (CoT) methods, particularly in unseen tasks within the BigBench and MMLU benchmarks. \n\nSome studies have shown that LLMs can learn to utilize tools by fine-tuning on the tool-specific corpora [181,182,194].",
            "score": 0.4526182184815756,
            "section_title": "Tool Usage in LLM",
            "char_start_offset": 46690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 357
                },
                {
                    "start": 360,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1993
                },
                {
                    "start": 1996,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 1224,
                    "end": 1229,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1229,
                    "end": 1233,
                    "matchedPaperCorpusId": "249017743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0802001953125
        },
        {
            "corpus_id": "273163663",
            "title": "CodePlan: Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning",
            "text": "LLMs for Reasoning. Endowing LLMs with robust reasoning abilities remains a formidable challenge. Existing approaches predominantly fall into three categories: (1) Prompting techniques, which use expert-designed prompts to elicit reasoning skills without training (Wei et al., 2022b;Press et al., 2023;Imani et al., 2023;Hong et al., 2024) (2) Task-specific fine-tuning, which curates tailored fine-tuning data or rewards to improve reasoning in specific tasks like mathematical reasoning (Yu et al., 2023;Mitra et al., 2024;Shao et al., 2024;OpenAI, 2024), code reasoning (Le et al., 2022;Shen et al., 2023), instruction-following (Cui et al., 2023), visual reasoning (Cheng et al., 2024a), and decision-making (Zeng et al., 2023;Guan et al., 2024) tasks. However, these approaches often falter in generalizing beyond their intended tasks. (3) Tool integration, which seeks to augment LLMs' reasoning capabilities by incorporating external tools like calculators (Schick et al., 2024), retrievers (Asai et al., 2024), and code interpreters (Gao et al., 2023). Compared to these works, CODEPLAN focuses on enhancing LLMs' high-level planning capabilities in diverse domains, without relying on task-specific prompts, fine-tuning data, rewards, or tools. \n\nDeliberate Planning in LLMs. Enhancing LLMs' planning capabilities is crucial for complex reasoning tasks (Yang et al., 2023). Prior work primarily focuses on teaching LLMs to plan in natural language via task-specific prompts (Wang et al., 2023;Khot et al., 2022) or curated fine-tuning data (Yin et al., 2024;Guan et al., 2024). In contrast, CODEPLAN innovatively introduces code as a structured and versatile plan representation.",
            "score": 0.4524238423550846,
            "section_title": "RELATED WORK",
            "char_start_offset": 27657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 283,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 283,
                    "end": 302,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 302,
                    "end": 321,
                    "matchedPaperCorpusId": "257427208"
                },
                {
                    "start": 321,
                    "end": 339,
                    "matchedPaperCorpusId": "260351380"
                },
                {
                    "start": 573,
                    "end": 590,
                    "matchedPaperCorpusId": "250280117"
                },
                {
                    "start": 964,
                    "end": 985,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 998,
                    "end": 1017,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1041,
                    "end": 1059,
                    "matchedPaperCorpusId": "253708270"
                },
                {
                    "start": 1483,
                    "end": 1502,
                    "matchedPaperCorpusId": "258558102"
                },
                {
                    "start": 1549,
                    "end": 1567,
                    "matchedPaperCorpusId": "265128672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.107421875
        },
        {
            "corpus_id": "259202736",
            "title": "Revealing the structure of language model capabilities",
            "text": "Such an outcome would be in-keeping with wider efforts to make more streamlined, \"lite\" benchmarks to combat the prohibitive costs involved in benchmarking LLMs [19,30]. However, many of these streamlined benchmarks, including BIG-bench lite, take a somewhat arbitrary approach to selecting which tasks to include [30]. By contrast, in line with calls to make benchmarks more systematic [3,27], our approach provides an empirically-driven way of selecting benchmark tasks based on the underlying abilities those tasks measure and the unique variance they explain. Moreover, because this approach centres around high-level cognitive abilities that are not tied to any specific task, we can use estimates of these abilities to predict performance on untested tasks, provided we know what abilities the untested tasks are likely to require. \n\nThis study is an important first step on the road to understanding language model capabilities, but it should not be thought of as a complete accounting of every possible ability. Although the HELM tasks included in this analysis cover a wide range of the key tasks LLMs are used for, there may be other important abilities that were not tested here (e.g., abilities involved in creative tasks such as writing poetry or novels). The results of any factor analysis depend on the tasks included in the analysis, so it is important that future work examines how the factor structure changes (if at all) when other kinds of tasks are added to the mix. Indeed, another way of looking at these findings is that, despite the fact that the HELM tasks were intended to test a variety of abilities, they really only tap into the three main abilities identified here. From that perspective, our findings suggest that new task paradigms may be needed if we want to tap into abilities beyond comprehension, reasoning, and language modeling. \n\nThese findings also further our understanding of how the properties of a model affect its capabilities. \n\nIn general, we found support for scaling laws-all three of the extracted factors were positively correlated with model size. However, we also found evidence that scale is not uniformly related to the three abilities.",
            "score": 0.45225476739989284,
            "section_title": "Discussion",
            "char_start_offset": 21051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 1973
                },
                {
                    "start": 1976,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2192
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09588623046875
        },
        {
            "corpus_id": "263830318",
            "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
            "text": "Recent research has demonstrated the remarkable and versatile proficiency of large language models (LLMs) in dealing with a variety of real-world * Work done during internships at Alibaba Group. \n\ntasks expressed in natural languages (Ouyang et al., 2022a;Anil et al., 2023;OpenAI, 2023;Luo et al., 2023a), especially Information Extraction (IE) (Lu et al., 2022;Xu et al., 2023b;Zhao et al., 2023;Cheng et al., 2023d;Wang et al., 2023b;Zhang et al., 2024b;Li et al., 2024), Information Retrieval (IR) (Zhu et al., 2024;Liu et al., 2024c) and Spoken Language Understanding (SLU) (Hoscilowicz et al., 2024;Yin et al., 2024;Cheng et al., 2023aCheng et al., , 2024;;Dong et al., 2023a). Among the tasks, LLMs especially emerge with three outstanding abilities in reasoning (Cobbe et al., 2021;Wei et al., 2022), coding (Chen et al., 2021), and aligning general human intentions (Ouyang et al., 2022a), which have drawn much attention from the LLM research community. In order to further incentivize such abilities, it necessitates supervised fine-tuning (SFT) stages on annotated task data. \n\nHowever, existing research has mostly conducted separate SFT investigations on each of the three tasks, where reasoning and coding abilities require SFT on in-domain human-annotated or augmented data (Yuan et al., 2023b;Luo et al., 2023b;Yu et al., 2024) while diverse and complex human instructions are applauded for aligning human intentions (Wang et al., 2023d;Taori et al., 2023;Cheng et al., 2023c;Xu et al., 2023a;Zhou et al., 2023a;Wang et al., 2023a;Lu et al., 2023).",
            "score": 0.45225037807871127,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1565
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 663,
                    "matchedPaperCorpusId": "259096157"
                },
                {
                    "start": 790,
                    "end": 807,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038818359375
        },
        {
            "corpus_id": "268385622",
            "title": "Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance",
            "text": "Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with several simple questions supported by a generic fact, LLMs often struggle to abstract and apply the generic fact to provide consistent and precise answers, revealing a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts. The code is available at https://github.com/Waste-Wood/MeanLearn.",
            "score": 0.4521988283359175,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0709228515625
        },
        {
            "corpus_id": "271874441",
            "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
            "text": "Since we show in Figure 4 that the ability to understand symbolic graphics programs is associated with some fundamental reasoning abilities of LLMs, we are particularly interested in whether symbolic graphcis programs can be used as a novel data source for building better instruction tuning datasets, which can help to improve the general reasoning ability of LLMs. To verify this, we test the instruction-tuned models on a variety of popular LLM benchmarks, including benchmarks focusing on natural language understanding (XNLI [17], IFEval [119], HellaSwag [117], C-Eval [39], CoQA [80], MMLU [34], SQuAD2.0 [77]), generic reasoning (BigBenchHard [92], PIQA [7], AGIEval [118]) and mathematical reasoning (Arithmetic [9], MathQA [3], GSM8k [14], ASDiv [71]). Table 3: Results on a variety of popular LLM evaluation benchmarks when performing instruction tuning with or without SIT. The Open-Instruct (OI) dataset serves as our baseline. \n\nExperimental results and discussion. We use the Llama-3.1-8B model (without instruction tuning), and the baseline is finetuned with Open-Instruct (OI) [97] that contains 143K question-answer pairs (details in Appendix E.1). We evaluate whether finetuning with SIT data can improve general reasoning by testing three ways of using SIT data: (1) mixing original SIT data into OI; ( 2) mixing the reverse SIT data into OI; \n\n(3) mixing both original and reverse SIT data into OI. \n\nThe results are given in Table 3. We can observe that mixing SIT data can generally improve the instruction following and the reverse usage of SIT data (i.e., symbolic graphics program generation) can improve a set of reasoning abilities that are complementary to symbolic graphics program understanding. The mixture of both original and reverse SIT data often achieves better performance than the OI baseline, the OI + SIT baseline and the OI + rev-SIT baseline. These results are consistent with recent findings that training on code can enhance reasoning ability [66,4] and mathematical understanding [88].",
            "score": 0.4518232853788903,
            "section_title": "SIT CAN IMPROVE GENERAL REASONING ABILITY",
            "char_start_offset": 32933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 565,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "52055325"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0526123046875
        },
        {
            "corpus_id": "273215261",
            "title": "The Impact and Acceptance of Large Language Models in Healthcare: A Perspective from China",
            "text": "Advancements in large language models (LLMs) have fundamentally redefined our understanding of machine learning capabilities, particularly concerning the emergence of novel abilities and reasoning strategies. However, it is imperative to underscore their inherent limitations. i. Emergent Abilities and Scaling Effects: Recent advancements in LLMs have been characterized by the development of emergent abilities -capabilities that are not observable in smaller models but become apparent as the model size increases [11]. For instance, research has revealed that large-scale models have developed multistep reasoning abilities [12]. Such findings underscore the inherent unpredictability of emergent abilities, making it critical to investigate their origins, properties, and potential implications for future research in Natural Language Processing (NLP). ii. Chain-of-Thought Prompting: A Method for Eliciting Reasoning: The practice of chain-ofthought prompting has been identified as a potent tool for enhancing the performance of LLMs in multistep reasoning tasks [12]. This technique, which simulates the human reasoning process, represents an intriguing new frontier in our ongoing attempts to optimize LLMs' performance. This raises compelling questions about how far we can push these reasoning abilities with further model scale-ups and how different prompting methods could potentially expand the scope of tasks LLMs can accomplish. iii. Limitations and Interpretations of LLMs: Despite these promising advances, it is vital to acknowledge the limitations of LLMs. Studies have found that LLMs, including models like GPT, often stumble with certain types of questions and are not inherently designed to pass tests based on mathematical, semantic, and ethical principles [13]. Therefore, viewing LLMs as an emerging form of general artificial intelligence remains speculative at this stage. \n\nIn response to these limitations, there have been calls for the implementation of strategies to optimize the effective utilization of LLMs. Recommendations include developing a system of checks and balances to moderate AI's impact, improving our understanding of their behaviour, and fostering the responsible use of AI technology [14]. Emphasizing the importance of responsible AI stewardship, these suggestions aim to maximize the benefits of LLMs while mitigating their potential drawbacks. \n\nIn conclusion, state-of-the-art research in LLMs presents a vibrant field marked by significant achievements and unexplored potential.",
            "score": 0.45170835703090473,
            "section_title": "The state of the art in large language models",
            "char_start_offset": 30668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2397
                },
                {
                    "start": 2400,
                    "end": 2534
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09930419921875
        },
        {
            "corpus_id": "271744782",
            "title": "In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models",
            "text": "With the advent of Large Language Models (LLMs) exhibiting surprising abilities across a variety of language tasks (Clark et al., 2018;Zellers et al., 2019;Sakaguchi et al., 2019;Hendrycks et al., 2020;Cobbe et al., 2021;Gao et al., 2021;Lin et al., 2022;Beeching et al., 2023), open-source language models have surged in popularity as part of broader efforts to democratize their accessibility (Taori et al., 2023). Furthermore, performance can be improved with custom data that is smaller than their pretraining data, which is known as supervised fine-tuning (Liu et al., 2024b). One type of fine-tuning important to the open-source community is instruction tuning, which allows models to follow a broad or specific-set of instructions and to discuss with users in natural dialogue (Zhang et al., 2023). However, instruction-tuning of open-source LLMs remains limited in multiple areas. \n\nOne prominent limitation is the expensive cost of fine-tuning such models, caused by a large number of parameters and data required (Liu et al., 2024a). The popular approach is to scale-up the parameter count and the dataset size, increasing the computation required to train the models. This method to improve LLMs does not lend-well to organizations without massive computing resources, such as the majority of entities who rely on opensource. Another limitation is evaluation, especially for instruction-following models. Evaluation is challenging as a result of LLMs' inherent openended generation, where a space of ideal outputs exists, and existing evaluations usually measure ability in a specific area, such as summarization. Furthermore, evaluation sets may be hard to construct for domain-specific abilities. These two issues limit the continued adoption of open-source models. \n\nTo alleviate the cost of fine-tuning, many have focused on improving the hardware (Jouppi et al., 2023) or the algorithm of the training process (Hu et al., 2021). To improve evaluation, many have introduced more sophisticated benchmarks (Liang et al., 2022) or rely on automated evaluation using more powerful LLMs (Li et al., 2023a).",
            "score": 0.4515316083331936,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 1863,
                    "end": 1884,
                    "matchedPaperCorpusId": "257921908"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041229248046875
        },
        {
            "corpus_id": "266999123",
            "title": "Exploring the Potential of Large Language Models in Self-Adaptive Systems",
            "text": "After pre-training and optional fine-tuning, LLMs can be utilized to solve various tasks when given suitable prompts. The prompting strategies can be mainly classified into three categories: (i) In-Context Learning (ICL), which offers task description along with examples as demonstrations [3]; (ii) Chainof-Thought (CoT) that incorporates intermediate steps of reasoning into the prompts [56]; and (iii) Planning that decomposes complex tasks into smaller sub-tasks and creates a plan of actions to complete the overall task [72]. Employing these strategies enhances LLM performance by aligning the model's processing with the nature of the specific task, thereby resulting in the generation of more coherent and contextually relevant responses. \n\nCapabilities of LLMs. According to Zhao et al. [71], LLMs mainly have the following six abilities: (i) Language generation aims at creating text that meets certain requirements set by tasks like translation, summarization, question answering, as well as formal language (e.g., logical forms, code) synthesis; (ii) Knowledge utilization entails leveraging the extensive factual knowledge from the pre-training corpus or accessing external data to perform tasks that require significant knowledge, such as common-sense question answering and fact completion; (iii) Reasoning, encompassing knowledge, symbolic, and mathematical reasoning, pertains to the capability to comprehend and apply evidence or logic to reach conclusions or make decisions; (iv) Human alignment means that LLM can well conform to human values and needs, with criteria like truthfulness, honesty, and safety; (v) Interaction with the external environment refers to the ability to receive feedback from the external environment and perform actions according to behavioral instruction, e.g., generating detailed action plans in natural language or other formats; (vi) Tool manipulation refers to the ability to utilize external tools, such as search engines and calculators, to enhance performance on given tasks. It should be noted that these six capabilities are neither comprehensive nor orthogonal, i.e., a task may require a combination of multiple capabilities. \n\nShortcomings of LLMs. Although this paper does not delve into the technical details of LLMs, it is important to consider shortcomings when using them. We highlight key issues.",
            "score": 0.45152433639239486,
            "section_title": "BACKGROUND: LARGE LANGUAGE MODEL",
            "char_start_offset": 5190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2184
                },
                {
                    "start": 2187,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2337
                },
                {
                    "start": 2338,
                    "end": 2362
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13623046875
        },
        {
            "corpus_id": "275133286",
            "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria",
            "text": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities comparable to human cognitive abilities (Madaan et al., 2024;Shinn et al., 2024;Kumar et al., 2024). These models can solve complex reasoning tasks by explicitly generating extended reasoning paths. The generation of such paths involves producing explicit reasoning units (e.g., tokens, steps) (Yu et al., 2024b), which additionally enhances model performance through iterative prompting (Wang et al., 2023;Yao et al., 2023). This iterative generation of explicit reasoning paths allows the model to refine and expand its thought processes, incorporating strategic planning and ongoing cognitive generation (Xi et al., 2023;Yang et al., 2024). \n\nAlthough the extensive generation of explicit reasoning units leads to better peformance, it inherently leads to higher inference costs and increased latency (Yu et al., 2024b;Wang et al., 2024). Moreover, fine-tuning LLMs using a complete reasoning path does not consistently guarantee stronger performance (Yu et al., 2024b;Deng et al., 2024b;Liu et al., 2024), suggesting the need for methods that can maintain reasoning performance while reducing the generation of reasoning units. Despite this apparent requirement, it remains underexplored how to maintain LLM reasoning capabilities while reducing intermediate reasoning paths across diverse tasks. \n\nPrevious methods (Yu et al., 2024b) have drawn inspiration from human cognitive processes to address the aforementioned limitations. Some studies have proposed training pipelines that utilize augmented datasets, iteratively generated by foundation LLMs, to fine-tune subsequent LLMs (Yu et al., 2024b;Liu et al., 2024). However, they are inherently vulnerable as they significantly depend on the generative capabilities of LLMs. In response, other works have directly trained LLMs without augmenting datasets that facilitate implicit reasoning before answer generation. Deng et al. (2023) introduced a knowledge distillation method to distill explicit reasoning into implicit reasoning through token-level hidden states.",
            "score": 0.45148038334212315,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 161,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 161,
                    "end": 180,
                    "matchedPaperCorpusId": "258833055"
                },
                {
                    "start": 724,
                    "end": 742,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 1091,
                    "end": 1108,
                    "matchedPaperCorpusId": "273811942"
                },
                {
                    "start": 1704,
                    "end": 1721,
                    "matchedPaperCorpusId": "273811942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "269484462",
            "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
            "text": "Previous works have demonstrated that Chain-of-Thought (CoT) prompting can improve the Large Language Models (LLMs)1 capacity to perform complex reasoning tasks by decomposing a reasoning task into a sequence of intermediate steps (Wei et al., 2022), where the generation of multistep controlled reasoning can improve results in commonsense (Bubeck et al., 2023), symbolic and mathematical (Gaur and Saunshi, 2023;Liu et al., 2023) reasoning datasets.\n\nSince the size of LLMs represents an adoption barrier for many use cases and smaller models do not seem to have the same emergent reasoning abilities as LLMs, several state-of-the-art alignment approaches for solving mathematical problems have emerged, where Supervised Fine-Tuning (SFT) has been used to train Small Language Models (SLMs) using CoT annotations.However, these annotations outline the intermediate reasoning steps for solving a given problem, which consists of a reasoning pathway generated by the LLM for the specific case.This phenomenon can lead to a relatively weak generalization capacity of tuned models that have a few and limited number of samples.Indeed, there are often multiple valid CoT annotations for the same question (Cobbe et al., 2021;Zhang et al., 2023), which underlines the need for a more general CoT-based fine-tuning approach.\n\nIn this paper, we propose Self-refine Instructiontuning, which is a method to enable CoT reasoning over SLMs.Our approach starts by performing Instruction-tuning on SLMs via demonstrations delivered by LLMs and then applies preference optimization based on reinforcement learning (RL) heuristics to let the SLMs refine their abilities to solve a task in a step-wise manner.Hence, proposing a teacher-student alignment method, we investigate the impact of transferring Chain-of-Thought reasoning abilities through the support of Demonstrations \"taught\" by LLMs to SLMs as a warm-up to the Self-refine process.Therefore, to reinforce the Instruction-tuning phase, we analyze whether preference optimization methods could strengthen students' step-wise reasoning abilities.",
            "score": 0.4514236598499469,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 451
                },
                {
                    "start": 453,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1319
                },
                {
                    "start": 1321,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1929
                },
                {
                    "start": 1929,
                    "end": 2091
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07763671875
        },
        {
            "corpus_id": "259202736",
            "title": "Revealing the structure of language model capabilities",
            "text": "These benchmarks provide a standardized way of testing LLMs across a variety of tasks, many of which were designed to test specific cognitive abilities. With data from this new population of LLMs across a range of these benchmark tasks, it is now becoming feasible to use an individual differences approach to investigate the structure of language model capabilities. \n\nWhat hypotheses, though, might we make about the structure of language model capabilities? It seems likely that the capabilities of LLMs are multifaceted. After all, we can find this multifaceted pattern across a number of different species, including humans, pigeons, and chimpanzees [10,29]. This idea also fits with the existing literature on LLMs-for example, there is evidence that new abilities can suddenly \"emerge\" at particular scales [34,30], which is consistent with the idea that certain abilities can be dissociated from one another. What is more uncertain is how these abilities are structured. For example, can the ability of language models to comprehend language be largely explained by a single broad ability, as is the case in humans [29]? Or are there multiple distinct abilities involved? Similarly, a range of studies have investigated the ability of LLMs to perform various kinds of reasoning, including quantitative reasoning, deductive reasoning, and commonsense reasoning [26,6,18]. But it remains unclear whether these different kinds of tasks rely on a set of distinct abilities or rather on a single, underlying reasoning ability. \n\nOnce we have identified the broad abilities of LLMs, we can begin to examine how model properties affect these different abilities. For example, a great deal of work has already been done to examine how the scale of a model affects its performance. In general, there is evidence increasing model size tends to improve performance, a pattern often referred to as \"scaling laws\" [30,15]. This pattern mirrors findings from cognitive science showing relationships between brain size and intelligence [13,20]. But at the level of specific tasks, the findings from the LLM literature are more complex and difficult to interpret. For example, data from BIG-bench suggests that model scale is closely related to performance on some tasks, but bears no relationship to performance on other tasks [30]. What are we to make of this pattern?",
            "score": 0.4511940279760047,
            "section_title": "Introduction",
            "char_start_offset": 2160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2325
                },
                {
                    "start": 2326,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "148467552"
                },
                {
                    "start": 1123,
                    "end": 1127,
                    "matchedPaperCorpusId": "148467552"
                },
                {
                    "start": 1374,
                    "end": 1377,
                    "matchedPaperCorpusId": "250144408"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0765380859375
        },
        {
            "corpus_id": "270847724",
            "title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability",
            "text": "We briefly summarize related work and provide a detailed discussion in Appendix A. \n\nLLMs are often Transformer-based (Vaswani et al., 2017) equipped with the enormous size of parameters and pretrained on vast training data. One key property that makes such LLM successful is called scaling law: Increasing the scale of language models (pretraining data scale, model parameters) can lead to better performance in downstream tasks. One ability that emerges as the model scale increases is in-context learning (ICL) (Brown et al., 2020). Given a sequence of labeled examples and a testing example (combined as a prompt), the model can construct new predictors for testing examples without further parameter updates Dong et al. (2022). Scaling law was first proposed by Kaplan et al. (2020) and then followed up by Hoffmann et al. (2022), emphasizing both the scale of models and training data. Recent works show LLMs with larger scales have distinct behaviors compared to smaller language models (Wei et al., 2023b;Shi et al., 2023b). This work investigates experiments and analyses how LLM can exhibit compositional ability in ICL. \n\nSolving complex tasks and reasoning is an active problem in the AI community Huang & Chang (2022). There is a line of empirical works investigating the compositional ability in linguistic fashion (Kim & Linzen, 2020;Levy et al., 2022;An et al., 2023a;b). LLMs are capable of learning abstract reasoning (e.g., grammar) to perform new tasks when finetuned or given suitable in-context examples. In our work, we include linguistic experiments as part of our testing suite, illustrating LLMs' compositional ability. Ye et al. (2023); Berglund et al. (2023); Dziri et al. (2023) show LLMs will struggle to solve tasks requiring reasoning. Berglund et al. (2023) studies that LLMs trained on \"A is B\" fail to learn \"B is A\". In our work, we conduct similar experiments showing LLMs will fail on composite if different steps of logical rules are mixed.",
            "score": 0.4510238384395464,
            "section_title": "Related Work",
            "char_start_offset": 5163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 85,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 534,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1013,
                    "end": 1031,
                    "matchedPaperCorpusId": "270123533"
                },
                {
                    "start": 1329,
                    "end": 1349,
                    "matchedPaperCorpusId": "222290851"
                },
                {
                    "start": 1367,
                    "end": 1384,
                    "matchedPaperCorpusId": "257102348"
                },
                {
                    "start": 1646,
                    "end": 1662,
                    "matchedPaperCorpusId": "256826793"
                },
                {
                    "start": 1688,
                    "end": 1707,
                    "matchedPaperCorpusId": "258967391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07122802734375
        },
        {
            "corpus_id": "276317504",
            "title": "CRANE: Reasoning with constrained LLM generation",
            "text": "Recent works such as Tam et al. (2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. Tam et al. (2024) attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding. This observation raises the following open questions: \n\n\u2022 RQ1: Do LLMs truly lose reasoning capabilities under constrained decoding? \u2022 RQ2: How can we leverage the benefits of constrained decoding in reducing syntax errors while preserving the unconstrained reasoning capabilities of LLMs? \n\nKey Challenges: First, we need to formally identify the root cause of the reduction in functional accuracy of endto-end systems when a pre-trained LLM operates under constrained generation. Unlike the empirical observations in (Tam et al., 2024), we seek a formal justification for this reduction that is not limited to specific LLMs used in experiments but extends to any LLM, including more powerful ones developed in the future. \n\nSecond, we must design cost-efficient decoding strategies that address the shortcomings of existing constrained decoding methods while improving functional accuracy. In this work, we do not consider task-specific fine-tuning of LLMs, as fine-tuning for each task is compute-intensive. Unlike constrained decoding, fine-tuning does not guarantee that the LLM output adheres to formal constraints.",
            "score": 0.4509049439840014,
            "section_title": "Introduction",
            "char_start_offset": 1722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1441
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 38,
                    "matchedPaperCorpusId": "271709856"
                },
                {
                    "start": 176,
                    "end": 193,
                    "matchedPaperCorpusId": "271709856"
                },
                {
                    "start": 839,
                    "end": 857,
                    "matchedPaperCorpusId": "271709856"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "270199509",
            "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning",
            "text": "Liang et al. [2023] demonstrates that OpenAI's Codex, further trained on code, excels in complex mathematical reasoning tasks.Madaan et al. [2022] shows that coding data pretraining enables LLMs to rival or exceed their natural language counterparts in structural reasoning tasks.During IFT, Ma et al. [2023] reveals that coding data enhances in-domain abilities, such as code generation and question-answering.Despite its importance, systematic studies of the impact of coding data on out-of-domain reasoning are lacking.To bridge this gap, we fine-tune pretrained LLMs with increased coding data across different families and scales, and evaluate their performance on various reasoning tasks.\n\nReasoning in LLMs.Reasoning involves logically analyzing a subject, using evidence and prior knowledge to reach conclusions [Wason and Johnson-Laird, 1972, Wason, 1968, McHugh and Way, 2018].It has been seen as one of LLMs' emergent behaviors, shown as models are large enough [Wei et al., 2022c,b].Although improving models' reasoning capacities shows promising results in different applications [Li et al., 2022, Zhang et al., 2023], comprehensively evaluating these capacities remains challenging due to their complex nature, requiring different fine-grained abilities within different subdomains [Wei et al., 2022b, Ma et al., 2023, Liang et al., 2023, Qiu et al., 2023, Yue et al., 2023].Our work aims to thoroughly evaluate models across the subdomains of symbolic, logical, and arithmetic reasoning to enhance our understanding of LLMs' reasoning capabilities at the IFT stage.\n\n3 Experimental Setting",
            "score": 0.45087040396677336,
            "section_title": "Related Work",
            "char_start_offset": 8602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 694
                },
                {
                    "start": 696,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 887
                },
                {
                    "start": 887,
                    "end": 995
                },
                {
                    "start": 995,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1580
                },
                {
                    "start": 1582,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 850,
                    "end": 863,
                    "matchedPaperCorpusId": "1212273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057586669921875
        },
        {
            "corpus_id": "273549732",
            "title": "Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in Low-Resource Code",
            "text": "Large Language Models (LLMs) possess several intrinsic capabilities that are a result of both the extensive training they undergo and the fine-tuning they receive on specialized data. One of their core strengths is general knowledge reasoning [Liang et al., 2022], which arises from the vast amount of diverse data they are trained on [Touvron et al., 2023a,b, Guo et al., 2024]. This general reasoning ability enables LLMs to provide accurate responses to a wide range of tasks across different domains, even without task-specific fine-tuning. \n\nIn addition to general reasoning, LLMs can develop proficient knowledge when fine-tuned on domain-specific datasets. This allows them to excel in specialized areas, such as legal [Colombo et al., 2024], medical [Singhal et al., 2023, Diao et al., 2023], or technical fields [Taylor et al., 2022], where deep and nuanced understanding is required. Fine-tuning equips LLMs with the ability to handle more complex and precise tasks that go beyond their general knowledge base. \n\nAnother powerful capability of LLMs is In-Context Learning (ICL) [Brown et al., 2020]. ICL enables models to generate more accurate responses by learning from the context provided in the input, without the need for further training. As a training-free approach, ICL is highly flexible and can be applied in various ways, including data generation [Wang et al., 2022, Ye et al., 2022, Gao et al., 2023a, Wang et al., 2024, Pi et al., 2024a, Gao et al., 2023b, Pi et al., 2024b], personalized conversations [Pi et al., 2024c], where the model adapts to user preferences; and task-specific guidance, where context helps refine and improve response accuracy. ICL's versatility makes it a valuable tool for enhancing performance across different applications. The StarCoder parts are based on data from their report [Li et al., 2023].",
            "score": 0.45079633129573005,
            "section_title": "LLMs' Intrinsic Capabilities",
            "char_start_offset": 7087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055419921875
        },
        {
            "corpus_id": "270123761",
            "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners",
            "text": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or harmless content generation, it is nearly impossible to obtain a large volume of high-quality data that matches the downstream distribution. To improve the performance of LLMs in data-scarce domains with domain-mismatched data, we re-evaluated the Transformer architecture and discovered that not all parameter updates during fine-tuning contribute positively to downstream performance. Our analysis reveals that within the self-attention and feed-forward networks, only the fine-tuned attention parameters are particularly beneficial when the training set's distribution does not fully align with the test set. Based on this insight, we propose an effective inference-time intervention method: Training All parameters but Inferring with only Attention (\\trainallInfAttn). We empirically validate \\trainallInfAttn using two general instruction-tuning datasets and evaluate it on seven downstream tasks involving math, reasoning, and knowledge understanding across LLMs of different parameter sizes and fine-tuning techniques. Our comprehensive experiments demonstrate that \\trainallInfAttn achieves superior improvements compared to both the fully fine-tuned model and the base model in most scenarios, with significant performance gains. The high tolerance of \\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning and enhances specialized tasks using general data. Code is available in \\url{https://github.com/pixas/TAIA_LLM}.",
            "score": 0.45072994219755624,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11676025390625
        },
        {
            "corpus_id": "271212697",
            "title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
            "text": "The core abilities of large language models (LLMs), such as ChatGPT, encompass language generation and emergence ability, enabling it to exhibit emergent behavior while generating coherent and contextually relevant text (Ouyang et al., 2022;Wei et al., 2022).Since the advent of ChatGPT, LLMs show great potential in assisting scientific research.The application of LLMs in disciplinary vertical fields, such as law, medical, chemistry and economics, is constantly evolving (Cui et al., 2023;Dan et al., 2023;L. Wang et al., 2023;N. Wang et al., 2023;Xiong et al., 2023).The ability of LLMs to integrate, understand, and apply scientific literatures in reasoning and interpretation has been widely reflected in existing research (Jablonka et al., 2023;Miret & Krishnan, 2024;Zheng et al., 2023).Their ability to summarize and generalize data that far exceeds human capabilities allows them to explore and present diverse perspectives, solutions, and creative ideas.Models such as K2 and GeoGalactica demonstrate the potential for conducting knowledge reasoning and thought generation tasks in the field of geology to discover potential undiscovered relationships (Deng et al., 2023;Lin et al., 2023).This, in turn, brings new insights and possibilities for problem-solving and scientific research.\n\nHowever, despite its impressive capabilities, LLMs may encounter challenges such as hallucination, which means the information they generate is nonsensical or unfaithful (Ji et al., 2023).This issue arises due to the models' ability to generate text only based on patterns and associations learned from vast amounts of training data, without necessarily verifying the authenticity of the associations (Huang et al., 2023;Ji et al., 2023).Furthermore, while LLMs excel in generating general text, their performance in specialized or professional domains may be insufficient.This limitation stems from their lack of domain-specific knowledge and expertise, making them less reliable for tasks that require in-depth understanding and specialized knowledge (Kaddour et al., 2023b;Zhao et al., 2023).",
            "score": 0.45063761170000083,
            "section_title": "Expert Question and Answer System",
            "char_start_offset": 8661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 347
                },
                {
                    "start": 347,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 795
                },
                {
                    "start": 795,
                    "end": 965
                },
                {
                    "start": 965,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1297
                },
                {
                    "start": 1299,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1737
                },
                {
                    "start": 1737,
                    "end": 1872
                },
                {
                    "start": 1872,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 729,
                    "end": 752,
                    "matchedPaperCorpusId": "259138569"
                },
                {
                    "start": 1469,
                    "end": 1486,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1720,
                    "end": 1736,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017303466796875
        },
        {
            "corpus_id": "269605800",
            "title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models",
            "text": "Large language models (LLMs) have reached approaching-human capabilities across a wide spectrum of tasks related to understanding, generating, and reasoning with natural language [1,30,20]. Notable examples include commercially available LLMs like ChatGPT [21], GPT-4 [1], and Claude-3 [2]. Due to the high inference cost of these close-source models, research trend in open-source foundation models surges, leading to cutting-edge open-source models like LLaMA2 [30], Gemma [29], and Vicuna [6]. Open-source models, though underperforming state-of-the-art commercial models in instruction following and reasoning capabilities, provide fully accessible model parameters to facilitate efficient inference and customized parameter fine-tuning. Despite the advancements of LLMs, recent studies found that they can exhibit problematic behaviors, including the generation of inaccurate information [38,32], flattery, and deception, raising concerns about their potential negative impacts and associated risks on society [4]. To address these issues, considerable research has been dedicated to refining LLMs' outputs to better align with human values and preferences [13].",
            "score": 0.4505611983141704,
            "section_title": "B Related Work B.1 Large Language Models",
            "char_start_offset": 33440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1167
                }
            ],
            "ref_mentions": [
                {
                    "start": 897,
                    "end": 900,
                    "matchedPaperCorpusId": "258215499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045867919921875
        },
        {
            "corpus_id": "265150647",
            "title": "ExpNote: Black-box Large Language Models are Better Task Solvers with Experience Notebook",
            "text": "Large Language Models (LLMs) have demonstrated astonishing capabilities in natural language understanding and generation (Wei et al., 2022;Huang et al., 2022;Sun et al., 2022;Bang et al., 2023). However, due to the limited parameters and context processing length, LLMs are not able to master all task-specific knowledge in real-world applications. As a result, LLMs may perform mediocre on some specific tasks, such as inductive reasoning (Bang et al., 2023) and entity recognition (Chen et al., 2023). \n\nTherefore, how to make LLMs adapt to the downstream tasks has tracked more and more attention. Recent techniques such as prefix-tuning (Li and Liang, 2021), P-tuning (Liu et al., 2021) and LoRA (Hu et al., 2021) proposed low-cost solutions for fine-tuning LLMs. However, these methods are not capable of black-box powerful LLMs, such as ChatGPT and GPT4 (OpenAI, 2023). \n\nA: Jack is Maria's son. \n\nQ: [Chris] and his son [Jack] are at a bar waiting for their drinks. [Maria] showed up and sat with her husband [Chris]. Who is Jack to Maria?",
            "score": 0.4501544060791976,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1046
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 158,
                    "matchedPaperCorpusId": "246035276"
                },
                {
                    "start": 158,
                    "end": 175,
                    "matchedPaperCorpusId": "237940861"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "267682123",
            "title": "ControlLM: Crafting Diverse Personalities for Language Models",
            "text": "Building on existing research, our work aims to achieve nuanced control over LMs by manipulating higher-level personality representations. This fine-grained approach promises to infuse models with diverse character roles that embody different value systems, all while requiring minimal computational resources. \n\nLLMs' downstream task. The complex reasoning abilities and humanized question answering capabilities of LLMs have attracted widespread attention (Sun et al., 2023;Cui et al., 2023;Yao et al., 2023). Chain-of-thought (CoT) (Aky\u00fcrek et al., 2022;Wei et al., 2022c) is one of the most classic methods, which enables language models to generate explanatory processes before outputting answers through in-context learning (Wang et al., 2023b;Dong et al., 2022;Aky\u00fcrek et al., 2022). Some methods focus on enhancing reasoning by adding more intermediate steps (Zhou et al., 2023;Wang et al., 2023a;Weng et al., 2023c), potentially increasing computational demands. Others involve iteratively refining prompts to find optimal templates (Kojima et al., 2022;Zhang et al., 2022;Shi et al., 2023a). In contrast to these works, we show that by controlling language models' personality characteristics, such as simply making models more responsible, aids in more robust reasoning abilities. ControlLM providing a novel avenue for enhancing LMs without excessive resource allocation or prompt-specific fine-tuning. \n\n3 ControlLM: Fine-grained Control of Language Models' Personalities \n\nThe activation space of language models contains interpretable directions that play a causal role in the reasoning process (Burns et al., 2022;Moschella et al., 2022). Since language models are trained on massive and diverse datasets (Brown et al., 2020;Raffel et al., 2020;Penedo et al., 2023), mostly from the internet, these datasets contain a lot of human personality texts.",
            "score": 0.45007643156631655,
            "section_title": "Related Work",
            "char_start_offset": 6176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 493,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 557,
                    "end": 575,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 730,
                    "end": 750,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 867,
                    "end": 886,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 886,
                    "end": 905,
                    "matchedPaperCorpusId": "258558102"
                },
                {
                    "start": 905,
                    "end": 924,
                    "matchedPaperCorpusId": "258840837"
                },
                {
                    "start": 1042,
                    "end": 1063,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1082,
                    "end": 1100,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06903076171875
        },
        {
            "corpus_id": "270738165",
            "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
            "text": "Our work is related to a few areas as discussed next. \n\nGeneral LLMs and CoT Related to our work is the line of research aimed at improving the performance of LLMs (Brown et al., 2020) on various reasoning tasks, with capabilities spanning mathematics, common sense, and symbolic reasoning (Chen, 2023;Ye et al., 2023;Cheng et al., 2022). These approaches often excel using fewshot prompts without requiring fine-tuning. Their reasoning abilities can be further enhanced by breaking complex tasks into steps, employing methods like chain-of-thought (CoT) (Wei et al., 2022) prompting and Zero-CoT. For instance, the Table-CoT (Chen, 2023)  Several studies have utilized instruction tuning and supervised fine-tuning to enhance the performance of LLMs on table reasoning tasks. Notable examples include TableLLaMA (Zhang et al., 2023a) and TableGPT (Zha et al., 2023), which have shown significant improvements in specific applications. In contrast, the BINDER model (Cheng et al., 2022) extends the capabilities of LLMs to programming language generation for solving commonsense problems. Additionally, the DATER approach (Ye et al., 2023) employs LLMs to decompose tables and questions, facilitating table-based QA and fact verification tasks. These diverse approaches underscore the potential of LLMs in handling complex reasoning tasks involving tabular data. \n\nReasoning over structured data/tables Another line of related work is reasoning over tabular data. Several studies leverage symbolic reasoning through text-to-SQL or Python code for table-based reasoning tasks. However, for effectively utilizing the symbolic code generation approach with LLMs for table reasoning tasks, it is crucial to ensure that the table is in the proper format (Pourreza and Rafiei, 2023;Rajkumar et al., 2022;Ni et al., 2023;Nahid and Rafiei, 2024;Cheng et al., 2022). \n\nChain-of-Table (Wang et al., 2024) enhances reasoning on tabular data by iteratively transforming and evolving table structures through a series of reasoning steps, including row/column selection, cell splitting to refine table representations for specific reasoning tasks.",
            "score": 0.44979662941051024,
            "section_title": "Related Work",
            "char_start_offset": 5680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 184,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 290,
                    "end": 302,
                    "matchedPaperCorpusId": "252872943"
                },
                {
                    "start": 302,
                    "end": 318,
                    "matchedPaperCorpusId": "256416408"
                },
                {
                    "start": 318,
                    "end": 337,
                    "matchedPaperCorpusId": "252734772"
                },
                {
                    "start": 555,
                    "end": 573,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 626,
                    "end": 638,
                    "matchedPaperCorpusId": "252872943"
                },
                {
                    "start": 966,
                    "end": 986,
                    "matchedPaperCorpusId": "252734772"
                },
                {
                    "start": 1122,
                    "end": 1139,
                    "matchedPaperCorpusId": "256416408"
                },
                {
                    "start": 1749,
                    "end": 1776,
                    "matchedPaperCorpusId": "258291425"
                },
                {
                    "start": 1798,
                    "end": 1814,
                    "matchedPaperCorpusId": "256900680"
                },
                {
                    "start": 1814,
                    "end": 1837,
                    "matchedPaperCorpusId": "269157143"
                },
                {
                    "start": 1837,
                    "end": 1856,
                    "matchedPaperCorpusId": "252734772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060302734375
        },
        {
            "corpus_id": "266191613",
            "title": "Native Language Identification with Large Language Models",
            "text": "Improved Prompting While we prompted the model to provide reasoning for its choice, it's likely possible to obtain even more detailed analysis with a more specific prompt. Further, it is also possible to explore the generation of counter-explanations or alternative hypotheses. \n\nOpen-Source LLMs Additionally, fine-tuning of open-source LLMs (such as Llama-2) for NLI remains unexplored. While we do not expect such models to outperform GPT-4 (based on other NLP benchmarks), it may be worthwhile to investigate their performance and understand the gaps between these models. \n\nMultilingual Evaluation NLI has been applied to many languages (Malmasi and Dras, 2017), including Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b). Our work only evaluated English L2 data as this is the benchmark dataset for the task. Future experiments can assess multilingual variants of the task.",
            "score": 0.44958311137321516,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 21611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 901
                }
            ],
            "ref_mentions": [
                {
                    "start": 642,
                    "end": 666,
                    "matchedPaperCorpusId": "27493853"
                },
                {
                    "start": 685,
                    "end": 710,
                    "matchedPaperCorpusId": "8893560"
                },
                {
                    "start": 723,
                    "end": 748,
                    "matchedPaperCorpusId": "16323727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0322265625
        },
        {
            "corpus_id": "263830318",
            "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
            "text": "Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.",
            "score": 0.4493915767556502,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1341552734375
        },
        {
            "corpus_id": "276768098",
            "title": "None of the Above, Less of the Right: Parallel Patterns between Humans and LLMs on Multi-Choice Questions Answering",
            "text": "LLMs, however, do not learn or update their parameters between evaluations. Unlike human learners, who might adjust their reasoning or strategies in response to feedback from previous exams, LLMs operate with a fixed set of parameters between different exams, and our experiments reveal that they suffer systematic performance degradation when NA is the correct answer (Figure 1), even when the model possesses the relevant knowledge. In such cases, traditional MCQA benchmarks risk misrepresenting an LLM's true abilities by either overestimating performance in standard settings or underestimating it when NA options are introduced. \n\nMotivated by this paradox between human and machine evaluation, we revisit established MCQA design principles in the context of LLM benchmarking. Specifically, we examine whether the conclusions drawn from educational testing which finds NA options increase difficulty hold true when applied to LLMs. In doing so, we seek to answer a central question: Do the established educational testing guidelines for NA choices from human centered studies can be applied to LLMs, or does the unique, static nature of LLMs warrant the development of novel evaluation approaches? \n\nOur contributions address these challenges through: \n\n\u2022 We perform a comprehensive benchmark of 28 LLMs on both standard MCQA and NA-modified variants, demonstrating that performance degradation occurs regardless of model scale or baseline performance. \n\n\u2022 We conduct detailed item-level analyses using metrics such as the difficulty index and KR-20 reliability, showing that although NA options increase discrimination among models, they do not compromise the overall integrity of the test. \u2022 We show that fine-tuning on NA-specific tasks-whether via supervised finetuning (SFT) or alignment methods-leads to performance improvements that generalize to out-of-domain tasks.",
            "score": 0.4491087288059883,
            "section_title": "Introduction",
            "char_start_offset": 2135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1880
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0987548828125
        },
        {
            "corpus_id": "278310435",
            "title": "Reasoning Capabilities and Invariability of Large Language Models",
            "text": "Large Language Models (LLMs) are constituting a real revolution in the field of Natural Language Processing (NLP), since they have been shown to perform particularly well while generating and manipulating natural language, mainly thanks to the introduction of the Transformer architecture [1] underlying most of them. Indeed, text generated with these tools can simulate human text and reactions to the point that some researchers have stated that LLMs showcase emergent abilities [2], 1 and some industry representatives tout them as gateways to Artificial General Intelligence (AGI) [5]. Among the tasks relying on these abilities for which LLMs have been successfully used (translation, summarization, sentiment analysis, and many others), plausibly answering different types of questions has garnered particular attention due to its potential to revolutionize how we interact with information, and obtain plausible answers to various questions, from simple factual questions to complex analytical ones. However, most current LLMs are not designed to be factually correct or logically consistent; indeed, they are models of language, not of reality [6]. For this reason, their ability to deal with reasoning tasks has often been questioned [7] and different tests have been developed, which mainly focus on commonsense [8], physical, or complex logical reasoning on the one hand, and propositional reasoning on the other [9]- [11]. Attempts have been made to solve this limited reasoning capabilities through scaling, fine-tuning, or retraining [12], [13], but these attempts encounter the same fundamental issues eventually. \n\nLogical reasoning is a fundamental feature for advanced AI applications, where guarantees about the quality of the results and avoidance of so-called hallucinations [14]-where the model generates results which are not grounded on realityare necessary. It is thus important to understand the situations in which reasoning fails, and analyse whether those cases can be effectively handled through other means. \n\nExisting datasets for testing reasoning capabilities in LLMs suffer from two major drawbacks.",
            "score": 0.4490625017804255,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1628
                },
                {
                    "start": 1631,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2038
                },
                {
                    "start": 2041,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1243,
                    "end": 1246,
                    "matchedPaperCorpusId": "254854219"
                },
                {
                    "start": 1322,
                    "end": 1325,
                    "matchedPaperCorpusId": "37887910"
                },
                {
                    "start": 1424,
                    "end": 1427,
                    "matchedPaperCorpusId": "243865235"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "236772032"
                },
                {
                    "start": 1548,
                    "end": 1552,
                    "matchedPaperCorpusId": "267770010"
                },
                {
                    "start": 1796,
                    "end": 1800,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1651611328125
        },
        {
            "corpus_id": "268819406",
            "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods",
            "text": "LLMs typically refer to the Transformer-based language models [65] containing billions of parameters and being trained on massive text data (i.e., several terabytes (TB) scale) [66], such as GPT-3 [29] and LLaMA [67]. The extensive number of parameters and internet-scale training data enable LLMs to master a diverse array of tasks, resulting in enhanced capabilities in language generation, knowledge representation, and logical reasoning, as well as improved generalization to novel tasks. \n\nThe development and effectiveness of LLMs are largely driven by Scaling Laws, i.e., as these models grow in size -both in terms of their parameter count and the data they are trained on -they tend to exhibit emergent abilities that are not present in small models [68], [69], [70], such as in-context learning, reasoning, and generalization. Here, we briefly introduce such capabilities of LLMs in detail: \n\n\u2022 In-context Learning: In-context learning capability eliminates the need for explicit model retraining or gradient update [29], as it can generate better responses or perform tasks by inputs cueing examples or related knowledge. Specifically, task-related texts are included in the prompts as context information, helping the LLMs to understand the situations and execute instructions. \u2022 Instruction Following: Leveraging diverse task-specific datasets formatted with natural language descriptions (also called instruction tuning), LLMs are shown to perform well on unseen tasks that are also described in the form of natural language [71], [72], [73]. Therefore, this capability equips LLMs with the ability to comprehend instructions for new tasks and effectively generalize across tasks not previously encountered, even in the absence of explicit examples. \u2022 Step-by-step Reasoning: For smaller models, tackling multi-step tasks, such as solving math word problems, often proves to be challenging. However, large language models can address the complex task effectively with sophisticated prompting strategies such as Chain of Thought (CoT) [74], Tree of Thought (ToT) [75], and Graph of Thought (GoT) [76]. These strategies structure the problem-solving process into sequential or hierarchical steps, facilitating a more articulated and understandable reasoning pathway.",
            "score": 0.44905391764846614,
            "section_title": "B. Background of Large Language Models",
            "char_start_offset": 15363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "254366666"
                },
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "257219404"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "210861095"
                },
                {
                    "start": 765,
                    "end": 769,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 771,
                    "end": 775,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0469970703125
        },
        {
            "corpus_id": "270923714",
            "title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model",
            "text": "Large Language Model Large language models (LLMs) (Touvron et al., 2023a,b;Frantar et al., 2022;Bai et al., 2023;Du et al., 2021;Rozi\u00e8re et al., 2023), leveraging the Transformer architecture, represent a significant leap in natural language processing (NLP) (Li et al., 2022(Li et al., , 2023b;;Qin et al., 2024a).LLMs undergo rigorous training on extensive textual datasets, enabling them to grasp a wide range of linguistic nuances and contexts.LLMs follow a two-stage process involving pre-training on  large-scale corpora followed by instruct tuning for specific tasks, significantly improving performance across downstream understanding and generation challenges.Notably, the GPT series, starting from GPT-1 and evolving through GPT-4, (Radford et al., 2018;Black et al., 2022;Brown et al., 2020;Ope-nAI, 2023) showcases a progressive increase in model complexity and capacity, with GPT-3 comprising a staggering 175 billion parameters.The introduction of instruction tuning further amplifies the capabilities of LLMs, unlocking emergent abilities for intricate reasoning tasks, such as math and code.LLMs with instruction tuning garner attention from researchers and make significant impacts across various industry scenarios.\n\nInstruction Tuning LLMs refine their ability to follow and understand user commands more accurately fine-tuned on an instruction dataset (Ouyang et al., 2022;Zan et al., 2023;Qin et al., 2024b), consisting of various instructions and their corresponding desired outputs.Early research in constructing conversational datasets largely relies on manually annotated sets (e.g.QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019)), but the limited scale and high annotation costs restrict model performance and generalizability.Simulation-based approaches are adopted to generate synthetic dialogues through mimicking user-system interactions, thus reducing dependence on manual annotations.",
            "score": 0.44878968168553435,
            "section_title": "Related Work",
            "char_start_offset": 21525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1233
                },
                {
                    "start": 1235,
                    "end": 1505
                },
                {
                    "start": 1505,
                    "end": 1607
                },
                {
                    "start": 1607,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 275,
                    "matchedPaperCorpusId": "247218311"
                },
                {
                    "start": 783,
                    "end": 802,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1612,
                    "end": 1631,
                    "matchedPaperCorpusId": "52057510"
                },
                {
                    "start": 1641,
                    "end": 1661,
                    "matchedPaperCorpusId": "52055325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0299224853515625
        },
        {
            "corpus_id": "259203988",
            "title": "Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health",
            "text": "After training, the models can be directly applied to various unseen downstream tasks through in-context learning such as zero-shot, one-shot or few-shot prompting [19]. This led to a recent trend toward development of decoder-only LLMs in the following years. Following GPT-3, a number of powerful LLMs such as PaLM [24], Galactica [25], and the most recent GPT-4 [2], have been developed. For more information on these general-domain models, readers are invited to consult [10,11]. \n\nWhile LLMs are powerful, they are still likely to produce content that is toxic, biased, or harmful for humans since the large corpus used for model training could contain both high-quality and low-quality data. Thus, it is extremely important to align LLMs to generate outputs that are helpful, honest, and harmless for their human users. To achieve this, Ouyang et al. [26] designed an effective approach of finetuning with human feedback to fine-tune GPT-3 into the InstructGPT model. They first fine-tuned GPT-3 on a dataset of human-written demonstrations of the desired output to prompts using supervised learning, then further fine-tuned the supervised model through reinforcement learning from human feedback (RLHF). This process was referred as alignment tuning. It was also applied in the development process of ChatGPT and became an effective practice for development of faithful LLMs. \n\nWith model size growing bigger, fine-tuning LLMs for downstream tasks becomes inefficient and costly. Alternatively, prompt engineering serves as the key to unlock the power of LLMs given their strong incontext learning ability. As demonstrated by GPT-3, large language models were able to achieve promising performance on a wide range of natural language tasks through in-context learning by prompting that used a natural language instruction with or without demonstration examples as prompt for the model to generate expected outputs. Wei et al. [27] showed that chain-of-thought prompting through a series of intermediate reasoning steps was able to significantly improve LLMs' performance on complex arithmetic, common sense, and symbolic reasoning tasks.",
            "score": 0.44873480634619173,
            "section_title": "Overview of General LLMs",
            "char_start_offset": 4822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 479,
                    "end": 482,
                    "matchedPaperCorpusId": "258331833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.094482421875
        },
        {
            "corpus_id": "266818340",
            "title": "On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)",
            "text": "Figure 1 illustrates the comparative capabilities of different LLMs across various competency domains, such as Writing (evaluating text generation quality), Roleplay (assessing conversational interaction), Reasoning (logical problem-solving), Math (numerical problem-solving), Coding (programming language understanding and generation), Extraction (information retrieval from text), STEM (proficiency in scientific and technical contexts), and Humanities (engagement with arts, history, and social sciences content). Across these domains, GPT-4 exhibits the strongest performance in the benchmark dataset evaluated by Zheng et al. (2023a), indicative of its superior training and extensive knowledge base. Expanding LLMs into applications such as code generation signifies their adaptability and potential for cross-disciplinary innovation. However, fine-tuning and in-context learning methodologies also bring challenges, such as potential data overfitting and reliance on the quality of input context. LLMs' continuous development and refinement promise to open new frontiers in various domains, including automated planning and scheduling, by bridging AI with human-like language understanding.",
            "score": 0.44837923318976575,
            "section_title": "Background Large Language Models",
            "char_start_offset": 10851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1197
                }
            ],
            "ref_mentions": [
                {
                    "start": 618,
                    "end": 638,
                    "matchedPaperCorpusId": "249889477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.077392578125
        },
        {
            "corpus_id": "258833277",
            "title": "Tweetorial Hooks: Generative AI Tools to Motivate Science on Social Media",
            "text": "Advances in LLMs have resulted in machine abilities to complete prompts with rich knowledge, commonsense reasoning, and fluent language composition (Radford et al. 2019). Despite not being explicitly trained for specific tasks, these models possess impressive generative capabilities and can perform a diverse range of tasks. Moreover, providing just a few examples in the prompt itself can significantly enhance the quality of the model's outputs (Brown et al. 2020). \n\nLLMs show great promise for supporting creativity and writing tasks. They can help with story writing (Calderwood, Wardrip-Fruin, and Mateas 2022;Chung et al. 2022), brainstorming (Singh et al. 2022), and finding creative connections (Wang et al. 2023) as well as story angles from press releases (Petridis et al. 2023). They have been shown to help with all three stages of the cognitive process model of writing (Flower and Hayes 1981): planning/ideation, translating/drafting, and reviewing (Gero, Liu, and Chilton 2022). Rather than executing these stages in a linear fashion, the writing process typically involves iterative use of these stages and requires writers to switch between their writing goals while keeping their audience in mind (Emig 1977). Because of this, writing can be taxing on both the writer's short-and long-term memory, resulting in high cognitive demands (Hayes 1996). Thus, LLMs as a writing companion and support can benefit writers in reducing mental load. \n\nDespite the successes of LLMs, problems remain. Language models tend to output repetitive and vague responses (Holtzman et al. 2020;Ippolito et al. 2019), particularly when a prompt is underspecified or too difficult to address. One approach to address this is to chain LLM prompts together (Wu, Terry, and Cai 2022): breaking down a problem into simpler and more explicit steps can make it easier for LLMs to complete. A bigger challenge is that language models have no model of truth. They learn correlations from large amounts of text, but they are not able to tell if the text they produce that includes falsehoods or offensive language (Bender et al. 2021).",
            "score": 0.4482800671720697,
            "section_title": "Related Work on LLMs and Writing Support",
            "char_start_offset": 2996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 169,
                    "matchedPaperCorpusId": "247085270"
                },
                {
                    "start": 705,
                    "end": 723,
                    "matchedPaperCorpusId": "247085270"
                },
                {
                    "start": 768,
                    "end": 790,
                    "matchedPaperCorpusId": "247085270"
                },
                {
                    "start": 885,
                    "end": 908,
                    "matchedPaperCorpusId": "248420880"
                },
                {
                    "start": 965,
                    "end": 994,
                    "matchedPaperCorpusId": "239009871"
                },
                {
                    "start": 1217,
                    "end": 1228,
                    "matchedPaperCorpusId": "248420880"
                },
                {
                    "start": 1354,
                    "end": 1366,
                    "matchedPaperCorpusId": "195567408"
                },
                {
                    "start": 1571,
                    "end": 1593,
                    "matchedPaperCorpusId": "195567408"
                },
                {
                    "start": 1752,
                    "end": 1777,
                    "matchedPaperCorpusId": "238353829"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044097900390625
        },
        {
            "corpus_id": "270766735",
            "title": "Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using LLMs with Human-in-the-Loop",
            "text": "In the evolving landscape of Natural Language Generation evaluation, LLM-based metrics emerge as a compelling alternative, offering insights into model performance without the constraints of pre-defined reference responses.Details regarding the prompts used for these Reference-free metrics are present in Appendix C. Prompt-based Evaluation: Prompt-based evaluation is at the forefront of NLG advancements, particularly with the utilization of LLMs (Li et al., 2024).Inspired by G-Eval, we followed the approach described by Liu et al. (2023) and tailored the prompts to be suitable for the evaluation of a question-answering task.\n\nTuning-based Evaluation: Nowadays, there is a significant shift toward leveraging open-source language models, such as LLaMA (Touvron et al., 2023), for fine-tuning purposes.We utilize Prometheus (Kim et al., 2023), which stands out for its fine-tuned evaluation capability, leveraging a large language model to perform nuanced analysis based on customized score rubrics (Li et al., 2024).This unique approach enables Prometheus to evaluate text generation tasks comprehensively, considering factors such as creativity, relevance, and coherence without relying on reference texts.",
            "score": 0.44821823856266674,
            "section_title": "Reference-free Metrics",
            "char_start_offset": 10815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 632
                },
                {
                    "start": 634,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1214
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06097412109375
        },
        {
            "corpus_id": "258676667",
            "title": "When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust",
            "text": "Figure 6 shows LLMs' timeline and the number of parameters extracted from Amatriain (2023). It is clear to observe the evolution of LLMs from 2018 to 2023. Started by small PLMs such as GPT and BERT in 2018, training LLMs has received significant attention of research and investment from both industry and academia (Bowman, 2023). For more detailed information of LLMs and its catalog, we encourage audiences to read the paper of Amatriain (2023). \n\nThe timeline of LLMs pushes the paradigm shift of NLP in which all NLP tasks can be represented as text generation (Radford et al., 2018;Brown et al., 2020;Gururangan et al., 2020;Du et al., 2021). For example, GPT-2 (Radford et al., 2018) showed that a generative pretraining LM improves the performance of 9 out of 12 NLP tasks, i.e., 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI). GPT-3 with few-shot learning (Brown et al., 2020) is even better on the SuperGLUE benchmark (Wang et al., 2019) that includes 8 challenging natural language understanding tasks: question answering, textual entailment, coreference resolution, word sense disambiguation, and causal reasoning. Du et al. (2021) introduced a general pretraining framework that formulates NLP tasks as text generation. The framework also works well on the SuperGLUE benchmark. LLMs have continuously improved in terms of quantity and quality. For example, ChatGPT has more than 100 billion parameters and GPT-4 (OpenAI, 2023) with even a bigger model trained by multimodal data can pass bar exams in the top 10%. \n\nThe purpose of this paper is to empirically analyze the behavior of LLMs in actual business cases. \n\nThe main research question is \"Do LLMs need knowledge enhancement of specific domains for working well in business scenarios?\".",
            "score": 0.4481697960288109,
            "section_title": "LLMS' TIMELINE",
            "char_start_offset": 18643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1839
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 607,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 607,
                    "end": 631,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 947,
                    "end": 967,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1010,
                    "end": 1029,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0227203369140625
        },
        {
            "corpus_id": "270702469",
            "title": "MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models",
            "text": "Large Language Models (LLMs) have achieved impressive quantitative performance on a wide range of benchmarks, natural language processing (Zellers et al., 2019;Wang et al., 2019), general knowledge question-answering (Hendrycks et al., 2021;Clark et al., 2018), and coding (Chen et al., 2021;others, 2021). Additionally, by integrating with some advanced prompting techniques, such as Chain-of-Thought (CoT) (Wei et al., 2023) and its variants (Yao et al., 2023;Trivedi et al., 2023;Zhang et al., 2023), LLMs seem to exhibit a certain level of reasoning abilities including mathematics (Zhang et al., 2024) and even causal inference/discovery (Vashishtha et al., 2023;Wang et al., 2020;Mao et al., 2022;Gupta et al., 2021). However, some studies (Oren et al., 2023) have * Visiting student at Rutgers ML Lab. raised concerns about data leakage (i.e., training models on the test sets), potentially rendering these results unreliable. These seemingly contradictory findings prompt the question of whether LLMs are genuinely performing reasoning tasks or merely predicting the next token. If LLMs are truly capable of reasoning, they should remain unaffected by the replacement of key symbols within the test set. \n\nA hallmark of human intelligence is the ability to handle abstract concepts and to associate them with arbitrary terms (Penn et al., 2008). With a few exceptions such as onomatopoeia, the connection between particular words and particular meanings is arbitrary, and identical concepts are invoked by different words in different human languages (e.g. dog vs chien). Similarly, human reasoners are capable of analogizing structural relationships from one domain to another, meaning that conceptual equivalence can be retained even when details change (Gentner and Medina, 1998). It follows that true human-like comprehension should be unimpaired when terms are substituted for synonymous terms, as long as the substitution is comprehensibly defined. \n\nWe wondered whether LLM peformance reflects true human-like comprehension in this sense, or whether it relies heavily on the specific terms used on training corpora.",
            "score": 0.4481025221875159,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 241,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 483,
                    "end": 502,
                    "matchedPaperCorpusId": "252762275"
                },
                {
                    "start": 668,
                    "end": 686,
                    "matchedPaperCorpusId": "210701213"
                },
                {
                    "start": 686,
                    "end": 703,
                    "matchedPaperCorpusId": "248392254"
                },
                {
                    "start": 703,
                    "end": 722,
                    "matchedPaperCorpusId": "235421644"
                },
                {
                    "start": 1333,
                    "end": 1352,
                    "matchedPaperCorpusId": "218218"
                },
                {
                    "start": 1764,
                    "end": 1790,
                    "matchedPaperCorpusId": "18481016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026611328125
        },
        {
            "corpus_id": "273502839",
            "title": "Optimizing Large Language Models for Dynamic Constraints through Human-in-the-Loop Discriminators",
            "text": "Finetuning has been widely applied to improve the reasoning capacity of LLMs and even vision language models (VLMs). [18] discussed that LLM-based finetuning could encourage zero-shot generalization to relevant tasks, and parameter-efficient tuning could perform much better than fullmodel tuning. From our perspective, fine-tuning a model based on the constraints that most frequently fail can undoubtedly enhance reasoning capabilities. However, identifying the critical data points based on performance metrics while minimizing resource expenditure remains an under-explored area. [9] discussed dataefficient finetuning and scoring data for 1) influential samples and 2) low-cost data pruning. They also focus on eliminating low-value data and focusing on higher-value data. While we did not score the data, we applied a scoring method to the user prompt to focus on higher-value data that address the high failure rate constraints. [2] focuses on automated data curation, employing a confidence-based system to estimate low-quality data and either filter or correct it. This has intriguing implications for feedback mechanisms aimed at improving user prompts. In contrast, our approach centers on leveraging a human-in-the-loop mechanism to identify critical cases for dynamic constraints, enabling broader applicability beyond fixed, pre-designed auto-metrics. \n\nMulti-agent Collaboration. Researchers nowadays also design multi-agent systems to improve the final reasoning performance by realizing the limitation of single-agent system design. The motivation behind this is that complex tasks often require knowledge and expertise across multiple domains. Efficient collaboration among a group of specialized agents can maximize overall information gain and help prevent errors resulting from the knowledge gaps of individual agents. rStar [11] stride forward for self-play [19] reasoning of LLMs by incorporating two small language models (SLMs) without finetuning or superior models with a larger size, effectively addressing diverse reasoning problems. archon [14] leverage a diverse set of LLMs and inference-time techniques, creating a modular framework for selecting, combining, and stacking layers of inference-time techniques to construct optimized LLM systems for target benchmarks. As a general framework, this method also supports us in creating more flexible systems, like [20], achieving excellent performance on web arena benchmark [21].",
            "score": 0.447728265148511,
            "section_title": "RELATED WORK",
            "char_start_offset": 6157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2297
                },
                {
                    "start": 2298,
                    "end": 2457
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 587,
                    "matchedPaperCorpusId": "267320296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07806396484375
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "In this study, we conducted an empirical investigation into the catastrophic forgetting (CF) phenomenon experienced by large language models (LLMs) during continual instruction tuning. Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills. Furthermore, our comparative analysis showed that the decoder-only model, BLOOMZ, demonstrates a superior ability to retain knowledge and skills during continual fine-tuning when compared to the encoder-decoder model, mT0. Additionally, we discovered that employing general instruction tuning techniques may help alleviate the CF problem in LLMs. Our empirical study suggests that exploring more effective methods to mitigate CF in LLMs during continual fine-tuning is a promising research direction. When applying LLMs, practitioners should remain vigilant and pay close attention to the issue of knowledge forgetting that may occur after instruction tuning. Addressing this challenge is crucial to ensure the reliable and consistent performance of LLMs in real-world applications.",
            "score": 0.44726060330824413,
            "section_title": "Conclusion",
            "char_start_offset": 22052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1242
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "249017743",
            "title": "Large Language Models are Zero-Shot Reasoners",
            "text": "From Narrow (task-specific) to Broad (multi-task) Prompting Most prompts are task-specific. While few-shot prompts are naturally so due to task-specific in-context samples [Brown et al., 2020, Wei et al., 2022], majority of zero-shot prompts have also focused on per-task engineering (of templates) [Liu et al., 2021b, Reynolds andMcDonell, 2021]. Borrowing terminologies from Chollet [2019] which builds on hierarchical models of intelligence [McGrew, 2005, Johnson andBouchard Jr, 2005], these prompts are arguably eliciting \"narrow generalization\" or task-specific skills from LLMs. \n\nOn the other hand, our method is a multi-task prompt and elicits \"broad generalization\" or broad cognitive abilities in LLMs, such as logical reasoning or system-2 itself. We hope our work can serve as a reference for accelerating not just logical reasoning research with LLMs, but also discovery of other broad cognitive capabilities within LLMs. \n\nTraining Dataset Details A limitation of the work is the lack of public information on the details of training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs Instruct-GPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big performance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT 001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and nonarithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a task-agnostic multi-step reasoning capability for generic problem solving. While most results are based on InstructGPT since it is the best performing open-access LLM, key results are reproduced on PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also confirm that it is not specially engineered for multi-step reasoning. \n\nLimitation and Social Impact Our work is based on prompting methods for large language models.",
            "score": 0.44724535353827605,
            "section_title": "Results",
            "char_start_offset": 24707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1864
                },
                {
                    "start": 1867,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 191,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 331,
                    "end": 346,
                    "matchedPaperCorpusId": "231925131"
                },
                {
                    "start": 470,
                    "end": 488,
                    "matchedPaperCorpusId": "144573521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08197021484375
        },
        {
            "corpus_id": "270123120",
            "title": "Quo Vadis ChatGPT? From Large Language Models to Large Knowledge Models",
            "text": "The choice of LLMs for users is growing rapidly.Oftentimes, it is preferable to adapt an existing pretrained LLM to a specific task.This fine-tuning of LLMs has become exceedingly popular (and beneficial) in recent years.This fine-tuning is, in essence, transfer learning for LLMs.There are numerous benefits in fine-tuning LLMs: first, one can tailor the output of a general LLM to a specific domain of application; second, it reduces the computational load of training a much larger model with several billion parameters; third, it can reduce the number of trainable parameters for downstream tasks [84].\n\nLLMs have been shown to be capable of learning through successive prompts, and thus can learn in context [85].Fine-tuning is a more permanent exercise in contrast to in-context learning.Here, instead of tuning the trained parameters of the model, one meticulously crafts the inputs/prompts (prompt engineering)\n\nprovided to the model.The objective is to guide the output to better harmonize with the desired outcome.\n\nBuilding on the concept of in-concept learning is the idea of indexing.This adds an information retrieval functionality to LLMs for extracting data from textual sources.There are numerous benefits accrued from in-concept learning, such as increased accuracy resulting in reduced hallucinations and semantic accuracy.\n\nThe central theme of indexing LLMs is to use the model as a reasoning engine.Depending on the application domain, appropriate knowledge bases can be created, and semantic searches can be made on the same.\n\nThis approach of combining a search engine with the natural language reasoning capabilities of an LLM was popularized by Lewis et al. [86] as retrieval-augmented generation (RAG).A major benefit of such an approach is the ability to update the knowledge base with new information.In rapidly evolving fields such as drug discovery, biotechnology, and renewable energy, to name a few, it is vital that their databases be updated in real-time.",
            "score": 0.4469731852900249,
            "section_title": "Finetuning of LLMs",
            "char_start_offset": 24606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 48,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 606
                },
                {
                    "start": 608,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 918
                },
                {
                    "start": 920,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1024
                },
                {
                    "start": 1026,
                    "end": 1097
                },
                {
                    "start": 1097,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1342
                },
                {
                    "start": 1344,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1548
                },
                {
                    "start": 1550,
                    "end": 1729
                },
                {
                    "start": 1729,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 1684,
                    "end": 1688,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05780029296875
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "Instruction fine-tuning (Peng et al., 2023;Chung et al., 2024) has emerged as an indispensable ingredient in the development of Large Language Models (LLMs) (Brown et al., 2020;Radford et al., 2019;Touvron et al., 2023b),enabling them to meet the demands of specific domains (Roziere et al., 2023;Thirunavukarasu et al., 2023) and human preferences (Ouyang et al., 2022).However, a notable concern with this fine-tuning is \"catastrophic forgetting\" (McCloskey and Cohen, 1989;Kirkpatrick et al., 2017), where models may lose essential skills (Dou et al., 2023;Chen et al., 2023)  such as mathematical reasoning while adjusting to user instructions.This raises questions about which abilities are most susceptible to forgetting and the underlying causes of these losses in LLMs.\n\nResearch on LLM forgetting (Luo et al., 2024;Wang et al., 2023b;Wu et al., 2024a) generally examines changes in abilities like reading comprehension, factual retention, mathematical skills, and code generation, underscoring the existence of catastrophic forgetting.Despite these findings, there is a notable gap in understanding the internal mechanisms responsible for these losses.To date, only a few studies, such as Kotha et al. (2024) proposing the task inference hypothesis, have begun to explore how conflicts between task processors might lead to forgetting.Nevertheless, the literature still lacks comprehensive insights into the exact changes that result in forgetting, leaving open questions about whether these changes involve overwriting of old modules or if they are simply overshadowed by new, specialized patterns.\n\nIn this paper, we first present a novel perspective to investigate catastrophic forgetting in LLMs, focusing on the capabilities developed during pretraining and alignment phases.We suggest that the task proficiency in LLMs involves understanding task-specific knowledge and following instructions, assessed through Knowledge Probability P (y|x) and Instruction Probability P (y c |c, x), respectively (as depicted in Fig. 2).",
            "score": 0.4469015078281514,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 777
                },
                {
                    "start": 779,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1608
                },
                {
                    "start": 1610,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 62,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 157,
                    "end": 177,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 177,
                    "end": 198,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 297,
                    "end": 326,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 349,
                    "end": 370,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 449,
                    "end": 476,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1198,
                    "end": 1217,
                    "matchedPaperCorpusId": "262054014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.212890625
        },
        {
            "corpus_id": "272922197",
            "title": "The Impact of Prompting Techniques on the Security of the LLMs and the Systems to Which They Belong",
            "text": "They test three different settings for it-zero-shot (the model is only given a natural language instruction describing the task, but no demonstration is provided), one-shot (the model is given a natural language instruction describing the task alongside a single demonstration of how should the task be performed) and few-shot (the model is given a few demonstrations of the task at inference time as conditioning together with the natural language instruction describing the task)-and compare them with the traditional fine-tuned models specific for each task. Figure 1 shows examples of how every one of these settings would look in practice. It is important to note the fact that this paper does not militate against fine tuning but rather presents an alternative method that could be used even alongside fine tuning. More than that, this highlights the incredible ability of the LLMs to adapt to new kinds of tasks as well. The results of the paper show the performance of this method on different NLP benchmarks in all three settings compared to the traditional fine-tuning approach. The few-shot setting in some cases outperforms the previous fine-tuned state of the art, or it is close enough to at least take into consideration if we have to choose between the two. \n\nFrom a security point of view, this technique could be especially helpful if we would like to teach our LLM to respond in a specific way to prompts that we consider undesirable or malicious. We also have to consider the fact that this method of teaching the LLM to behave in a certain way could be as well used by an attacker who could perhaps try to overload the existing context of our model with examples of how he would want the task to be performed and try in this way to change the initially defined behavior of our LLM. \n\n2.4. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models \"Chain-of-Thought (CoT) Prompting Elicits Reasoning in Large Language Models\" [2] explores the ability of an LLM not only to blindly perform an NLP task but accomplish very good performance on different kinds of tasks such as arithmetic, common sense, and symbolic reasoning [2]. The starting point is based on two ideas.",
            "score": 0.44675729869714087,
            "section_title": "Language Models Are Few-Shot Learners",
            "char_start_offset": 10490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1802
                },
                {
                    "start": 1805,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08758544921875
        },
        {
            "corpus_id": "267548072",
            "title": "Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks",
            "text": "Large language models have shown remarkable generalist capabilities in creating human-like prose, knowledge retention, as well as commonsense reasoning abilities (Xi et al., 2023). These high-performing generalist generative language models often possess >100 billion model parameters in addition to very large training data sets. Examples of these generalist LLMs include GPT-4 (Achiam et al., 2023) and PaLM-2 (Anil et al., 2023). \n\nConversely, we chose to utilize orders of magnitude smaller generative language models, such as those with approximately 100 million parameters. We selected the OPT (open pretrained transformer) family of pretrained foundational generative language models as the starting point for our studies (Zhang et al., 2022). The OPT family of language models also conveyed the benefit of smaller and larger model sizes being available within the same model family and architecture. This allowed us to systematically evaluate the impact of changes in model size (i.e., parameter count) and performance against our specialized task. \n\nIn our work, we define model fine-tuning as initialization of a pretrained foundational language model followed by updates to the model weights and biases. In our fine-tuning setting, all language model parameters can undergo gradient updates -there are no frozen layers nor adapters. \n\nThe prompt for the language models were consistent throughout our evaluation and across all models. The language model prompt was general and agnostic to the data set instructions. The prompt used for our evaluation was: \"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Response:\".",
            "score": 0.44675645205735337,
            "section_title": "Selection of Pretrained Foundational Language Models",
            "char_start_offset": 5563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1720
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039794921875
        },
        {
            "corpus_id": "256461147",
            "title": "Towards Few-Shot Identification of Morality Frames using In-Context Learning",
            "text": "There has been a lot of work towards exploiting existing knowledge in pretrained Large Language Models (LLMs) and improving its few-shot abilities on various downstream tasks in NLP. Some of these works have been influenced from areas related to instruction-based NLP (Goldwasser and Roth, 2014). Mishra et al., 2021 fine-tuned a 140M parameter BART (Lewis et al., 2019) model using instructions and few-shot examples for various NLP tasks such as text classification, question answering, and text modification. This work suggests that augmenting instructions in the fine-tuning process improves model performance on unseen tasks. On similar lines, through a large scale experiment with over 60 different datasets, Wei et al., 2021 showed that instruction tuning on a LLM (\u2248137B parameters) improves zero and few-shot capabilities of these models. Other notable works (Min et al., 2021c;Sanh et al., 2021) show that even a relatively smaller language model can achieve substantial improvement in a similar setting. Furthermore, Schick and Sch\u00fctze, 2020 use cloze-style phrases in a semi-supervised manner to help LM assign a sentiment label for the text classification task. \n\nAnother line of work focuses on improving LM on downstream tasks with no parameter updates. Brown et al., 2020 proposed to improve LLM few-shot performance by conditioning on concatenation of training examples without any gradient updates. Other works (Min et al., 2021b;Zhao et al., 2021) have further improved this work and have shown consistent gains in various NLP tasks. In addition, Wei et al., 2022 shows that sufficiently large LM can exploit its innate reasoning abilities to solve complex tasks when provided with a series of intermediate steps during prompting. \n\nHowever, having a generalized LLM may have poor performance when the downstream task needs nuanced understanding of the text or is very different from language modeling in nature.",
            "score": 0.4466456120432677,
            "section_title": "Related Works",
            "char_start_offset": 3072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1931
                }
            ],
            "ref_mentions": [
                {
                    "start": 1269,
                    "end": 1287,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1448,
                    "end": 1466,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06610107421875
        },
        {
            "corpus_id": "268532485",
            "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
            "text": "In this section, we delve into three pivotal observations on agent tuning that serve as the foundation of our subsequent investigation.\n\nObservation 1.Most agent training data is entangled with both format following and general reasoning, causing a significant departure from the model's original pretraining language domain, namely, natural conversation.\n\nRecent agent tuning works (Zeng et al., 2023;Qin et al., 2023) endorse the adoption of specific formats, exemplified by ReAct (Thought-Action-Observation), for fine-tuning the language model.Moreover, it's noteworthy that action arguments are frequently presented in JSON format.Encoding both format and reasoning knowledge into the training corpus shifts the tuning process from the original chat domain, presenting it as an out-of-domain task for language models.As shown in Figure 2, we compare the training curve of formatted data and normal data.It can be clearly seen that the loss associated with formatted data descends more rapidly to a low value, while keeping content loss still high (0.54 vs 0.04), indicating that the former leads to an inadequate learning process.This phenomenon can be probably attributed to the existence of a fixed structure (ReAct, JSON), where the model quickly gets overfitted to the format itself.Consequently, it fails to grasp the underlying reasoning abilities embedded within the training data, resulting in unsatisfied performance.Observation 2. By explicitly decomposing the training data along the basic capability aspects, each loss exhibits different convergence curves, indicating varied learning speeds on the capabilities requisite for agent tasks of LLMs.\n\nInspired by (Chen et al., 2023c), we explicitly disentangle the model's capabilities into distinct components: instruction following, reasoning, retrieval, and understanding.In this context, instruction following corresponds to format generation, reasoning corresponds to the thought quality at each step, retrieval involves selecting the appropriate function name to execute the task, and the understanding encompasses the parameter inputs for the selected functions.\n\nBy visualizing the loss based on respective aspects in Figure 3, we discern that LLM tends to exhibit varying learning speeds for the capabilities essential for proficient agents.\n\nTo elaborate, retrieval and understanding emerge as relatively more manageable tasks compared to reasoning, with instruction following being the simplest in the learning process.",
            "score": 0.44645003623801816,
            "section_title": "Pilot Observations",
            "char_start_offset": 7166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 137,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 355
                },
                {
                    "start": 357,
                    "end": 548
                },
                {
                    "start": 548,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1292
                },
                {
                    "start": 1292,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1663
                },
                {
                    "start": 1665,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 2133
                },
                {
                    "start": 2135,
                    "end": 2314
                },
                {
                    "start": 2316,
                    "end": 2494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0751953125
        },
        {
            "corpus_id": "270258104",
            "title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
            "text": "Efforts for evaluating LLMs reasoning abilities have been intensified significantly across numerous disciplines, including biomedical informatics [16,17,18], humanities [19,20,21], and social sciences [22,12,13,14].Numerous studies have instantiated the problem of logical reasoning based on reasoning-dependent datasets, such as deduction, induction or abduction, and studies solving the tasks with neural models [23,24,25].Despite the promising performance that LLMs have shown on certain reasoning tasks and those techniques that can help improve LLMs' reasoning abilities, it remains unclear whether LLMs have generalizable logical reasoning abilities and to what extent they are capable of logical reasoning [26].In this regard, Valmeekam et al. [27] stated that LLMs were still fell short of delivering satisfactory performance in common planning and reasoning tasks that are typically straightforward for humans to perform.This limitation was also highlighted by Wei et al. [28] that although CoT can stimulate the thought processes of human reasoners, it does not answer whether the neural network is actually reasoning.Other studies have also highlighted the limitations of modern LLMs in performing logical reasoning tasks.For example, Tang et al. [26] found that LLaMA2, relied on template matching to respond to reasoning queries but failed to generalize to novel logic rules, as demonstrated through experiments on Symbolic Trees and ProofWriter.This led them to question whether modern LLMs have truly mastered inductive, deductive, and abductive reasoning abilities akin to human intelligence.In a recent benchmark study, Saparov and He [29] presented a synthetic question-answering dataset called PrOntoQA to explore the logical reasoning ability of LLMs.Their analysis revealed that while LLMs were capable of making correct individual deduction steps, they encountered difficulties in exploring the solution space when presented with multiple valid deduction steps.\n\nBased on our review of current studies, the prevalent focus on question answering and mathematical problems in current benchmarks may not sufficiently capture the essence of reasoning -the ability to logically process and deduce information beyond memorized knowledge or instantiated cues.In particular, two specific questions regarding the logical reasoning abilities of LLMs remain unclear.",
            "score": 0.4463369194580352,
            "section_title": "Related Work",
            "char_start_offset": 4155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1608
                },
                {
                    "start": 1608,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1983
                },
                {
                    "start": 1985,
                    "end": 2274
                },
                {
                    "start": 2274,
                    "end": 2377
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "250627547"
                },
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "268998279"
                },
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "259262573"
                },
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "258291730"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "270924438",
            "title": "Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment",
            "text": "We further conduct ablation studies to provide insights on QDPO for improving the conversational skills of quantized LLMs.Absolute performance results can be found in Table 9.\n\naligning the quantized LLMs to the 16-bit weight baseline.What is the impact of this alignment on the task-specific performance of LLMs?\n\nTo answer this question, we further evaluate the quantized LLMs on well-known benchmarks that test the task-specific capability of language models.In particular, Common Sense Question Answering (CSQA) (Talmor et al., 2019) and Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) assess the models' reasoning and multitask-solving abilities through multiple-choice questions.Furthermore, DROP (Dua et al., 2019) and BBH (Srivastava et al., 2023) evaluate the problem-solving abilities of instruction-tuned models in logic and math.\n\n(Details on the task-specific benchmarks are in A.7.) Table 3 compares the task accuracy (CSQA, MMLU, DROP, BBH) as well as the conversational abilities (MT-Bench) on the quantized LLM with and without QDPO.RTN suffers degradation on the task accuracy as well as the conversational abilities.Interestingly, AWQ significantly improves task accuracy while its conversational abilities are marginally improved.Meanwhile, QDPO improves conversational ability while mostly preserving task accuracy, showcasing its usefulness.Conversation Abilities vs. Perplexity.Perplexity is a key metric for evaluating language models, as it measures the exponentiated average negative log probability of predicted word sequences.We examine whether the enhanced conversational capabilities through QDPO are also reflected in perplexity by comparing the perplexity and the loserate on the MT-bench.We measure perplexity using Wikitext-2 (Merity et al., 2016) for English and Korean textbooks1 dataset for Korean.As shown in Table 4, RTN significantly increases perplexity across all models.While AWQ decreases perplexity in all models, it does not guarantee an improvement in conversational ability.",
            "score": 0.44613625580421434,
            "section_title": "Ablation Studies",
            "char_start_offset": 25298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 175
                },
                {
                    "start": 177,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 313
                },
                {
                    "start": 315,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 866
                },
                {
                    "start": 868,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 1938
                },
                {
                    "start": 1938,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 755,
                    "end": 780,
                    "matchedPaperCorpusId": "263625818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05792236328125
        },
        {
            "corpus_id": "258841104",
            "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization",
            "text": "Large Language Models and Alignment Learning. Although LLMs have demonstrated great generalization capabilities through their sheer scales [2,6,30] and the emergent mechanism known as in-context learning [1], they still require significant alignment to follow natural language instructions [13], adhere to ethical guidelines or steer towards harmlessness [12], utilize external tools [31], and to be grounded in knowledge to generate truthful answers [16,32]. In particular, instruction-tuning has been pivotal in enabling LLMs to generalize instruction-following abilities, enabling them to solve seemingly any NLP task with only the description of the task in natural text [13,33,34], allowing the models to be accessed in an interactive manner. \n\nParameter-Efficient Fine-Tuning. Fine-tuning leverages the generalization capabilities elicited from the general pretraining to specialize in specific domains and tasks [35,36] or align the LLM with target behaviors [13]. However, updating parameters in LLMs comes with a high computation cost and minimal compute environment required for gradient computation. As the hyper-scale era makes fine-tuning for LLMs prohibitively expensive, both efficient and effective alternatives to fine-tuning have received considerable attention. Specifically, inspired by the sensitivity of LLMs to prompts [37], a line of works has proposed introducing trainable prompt embeddings prepended to the input text while freezing the original LLM parameters [19,38,39]. As another approach, adapter modules [20] introduce task-specific parameters, which are inserted between the pre-existing layers of the model Extending on this adapter-based approach, LoRA [21] employs the concept of low-rank bottleneck modules while demonstrating comparable performance to full fine-tuning. Subsequent works have unified the various versions and diverging approaches to PEFT [40,41] by formulating them in a single mathematical framework. These parameter-efficient methods have shown comparable performance to full model fine-tuning, presenting a cost-effective and efficient avenue for tailoring LLMs to specific tasks. \n\nHowever, even with the adoption of PEFT, the inherent model size of the LLM remains a challenge to handle.",
            "score": 0.44597680906309795,
            "section_title": "Related Work",
            "char_start_offset": 5459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2138
                },
                {
                    "start": 2141,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 923,
                    "end": 926,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 966,
                    "end": 970,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1492,
                    "end": 1495,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 1495,
                    "end": 1498,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1689,
                    "end": 1693,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1893,
                    "end": 1897,
                    "matchedPaperCorpusId": "238583580"
                },
                {
                    "start": 1897,
                    "end": 1900,
                    "matchedPaperCorpusId": "238857301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08978271484375
        },
        {
            "corpus_id": "267682172",
            "title": "Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence",
            "text": "L ARGE Language Models (LLMs), a sophisticated branch of generative Artificial Intelligence (AI), have evolved from narrow, task-specific systems to versatile models capable of few-shot learning and handling diverse tasks [1], [2]. Evaluating LLMs is crucial for understanding their capabilities and limitations. Automatic evaluation methods, which employ standard metrics, offer computational efficiency, while human evaluations provide nuanced insights into the quality and accuracy of LLM responses [3], [4]. Recent advancements like GPT-4 and Gemini, with their multimodal capabilities, and Mistral 8x7B's integration of Mixture of Experts (MoE), have enhanced LLMs' ability to process specialized domain knowledge and reason across diverse tasks [1]- [3]. The proliferation of over 700,000 LLMs on platforms like HuggingFace, alongside many publicly available commercial LLMs, has intensified competition among developers, driving the need for benchmarks to uniformly evaluate and compare LLM performance across dimensions such as accuracy, robustness, and reasoning, as these directly influence an LLM's reliability in real-world applications [3]. For example, an LLM that excels Manuscript received October 15, 2024. Corresponding Author: Timothy R. \n\nMcIntosh (e-mail: timothy.mcintosh@rmit.edu.au). \n\nin accuracy but lacks robustness might fail when confronted with unexpected inputs or novel scenarios. Benchmarks serve as standardized sets of tasks or datasets that assess key aspects such as accuracy, efficiency, and ethical considerations like bias or fairness, guiding both development and deployment decisions [3]. Widely used benchmarks, such as GLUE, Su-perGLUE, and MMLU, are expected to enable consistent LLM evaluations, allowing researchers to fine-tune LLMs for specific tasks or domains, and facilitating performance comparisons across different models in real-world scenarios. \n\nUnlike the automobile and aviation industries, where clear regulations and well-defined public consensus guide benchmarking practices [5], the advanced AI field lacks such universally accepted standards, leading many researchers to devise their own benchmarks.",
            "score": 0.44590774857639404,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "240420063"
                },
                {
                    "start": 502,
                    "end": 505,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "270231563"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "240420063"
                },
                {
                    "start": 756,
                    "end": 759,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 1626,
                    "end": 1629,
                    "matchedPaperCorpusId": "259360395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11199951171875
        },
        {
            "corpus_id": "274822793",
            "title": "Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs",
            "text": "Furthermore, they contribute to improved model memorization, reduced hallucinations, and enhanced reasoning abilities (Zhou et al., 2023). The diversity within such datasets also fosters better model generalization across tasks (Wei et al., 2021;Sanh et al., 2022;Wei et al., 2022). Although significant efforts have been made to generate largescale knowledge and skills instruction datasets (Sudalairaj et al., 2024;Wang et al., 2022;Taori et al., 2023;Xu et al., 2023;Li et al., 2024) and many open-source instruction-tuned models are now available, there is limited research on how to effectively fine-tune base models from scratch. \n\nPractitioners have limited resources to reference when searching for optimal training strategies and hyper-parameters for instruction-tuning small LLMs on knowledge and skills data. Many LLMs are closed-source, and even those that are open-source often lack detailed technical reports describing how to set up hyper-parameters or which configurations were attempted but unsuccessful. As a result, critical factors like batch size and learning rate, as well as their impact on final model performance, remain unclear. Additionally, phase training is increasingly used for instruction tuning, where LLMs are fine-tuned progressively, starting with simple instruction-following data (e.g., general knowledge from elementary or middle school), then moving to foundational knowledge (e.g., graduate-level content), and finally to skills-based data. However, it is unclear how well phase training outperforms traditional stacked training where all data is combined into a single phase. Identifying an effective set of hyper-parameters is especially difficult for users with limited computational resources. This motivates the main question we aim to study: \n\nHow can we effectively fine-tune a small-size LLM (3B-7B parameters) on instruction tuning datasets that cover diverse knowledge and skills? \n\nIn this paper, we present a comprehensive empirical study on supervised fine-tuning small-size LLMs and compare our findings with existing research on this topic.",
            "score": 0.4458787970988488,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2096
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059661865234375
        },
        {
            "corpus_id": "258686278",
            "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
            "text": "LLMs injected with non-linguistic skills forgo their linguistic skills: Consider the task of strict arithmetic reasoning as shown in Figure 1, a subset of possible quantitative reasoning tasks. If a base BERT model (Devlin et al., 2019) is further trained on this non-linguistic task, it suffers significant degradation on 8/9 GLUE tasks (Wang et al., 2018) that evaluate the natural language understanding (NLU) capabilties of the model, as showcased in Table 1. This observation has long been known in the deep learning literature as catastrophic forgetting (Kirkpatrick et al., 2017), wherein when a model pre-trained on task A is further trained on task B, the parameters in the model vital for task A adapt their values to meet the requirements of task B.\n\nLLMs exhibit unconventional forgetting: What is interesting, based on our findings, is that in the case of LLMs, the forgetting of linguistic skills is not evenly spread -the forgetting is rather taskspecific. Akin to other neural network applications, the forgetting of linguistic skills may likely be grouped as performance loss over a single task A; however, as seen in Table 1, the GLUE tasks suffer various ranges of degradation -the task of finding the referent of a pronoun (WNLI, Levesque et al. (2012)) does not seem to suffer at all, while the grammatical correctness assessment task (CoLA, Warstadt et al. (2019)) suffers severe degradation.\n\nAs proponents for skill-empowered LLMs, we thus make a case for disclosing the performance on general NLU tasks when models are trained for superior performance on niche skill-sets such as non-linguistics, an area left wanting in the Math-NLP front. Because of this task-specific forgetting, quantitative reasoning models trained in a Q&A fashion may not showcase degradation in similarly modeled downstream tasks such as SQuAD (Rajpurkar et al., 2016) and DROP (Dua et al., 2019)thus disclosing performance across a range of NLU tasks is crucial.\n\nSubstantiating forgetting on the basis of parameter sharing: To establish that observed performance degradation can indeed be accredited to catastrophic forgetting, we take an information theoretic lens to pry into parameter-sharing tendencies across tasks with the aid of Fisher information (Ri",
            "score": 0.44567280302156254,
            "section_title": "Necessitating the Re-thinking",
            "char_start_offset": 3278,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1363,
                    "end": 1385,
                    "matchedPaperCorpusId": "44072099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140625
        },
        {
            "corpus_id": "276885001",
            "title": "Knowledge Updating? No More Model Editing! Just Selective Contextual Reasoning",
            "text": "In-context Learning (ICL) [5] is a prominent emergent ability of LLMs [65]. ICL enables LLMs to learn and reason directly from contextual information, eliminating the need for explicit retraining. By leveraging the patterns and structures acquired during pre-training, ICL uses examples or task-specific prompts within the context to help the LLM understand the task and generate appropriate outputs, which correspond to few-shot learning and zero-shot learning, respectively. By carefully organizing examples or designing instructions, more ICL techniques, such as Self Adaptive [70] and Self-instruct [64], have been developed, further unlocking the potential of LLMs. When tackling complex tasks, Chain-of-Thought (CoT) [66] uses magic prompts or introduces intermediate reasoning steps into the examples to help the LLM not only arrive at the answer but also understand the underlying reasoning process. LLMs perform conditional generation based on the given context, which actually exploits the capabilities of understanding and reasoning within the boundaries of its generalization abilities [37]. \n\nConstrained by the static nature of pre-training data, LLMs are limited in their awareness of recent events, and their intrinsic knowledge may be subject to inaccuracies, leading to hallucinations and ignorance [27]. Retrieval-Augmented Generation (RAG) [34] seeks to bridge this gap by augmenting the LLM's intrinsic knowledge with real-time, relevant external information from knowledge bases or online sources. Through the integration of retrieval models and ICL, LLMs can effectively enhance their adaptability to new information while maintaining context-dependent language understanding. \n\nThe key advantage of ICL is that there is no need for gradient backpropagation or re-training LLMs for specific tasks. Considering that fine-tuning may affect the LLM's general capabilities [39], we explore a strategy that combines the selected knowledge text and contextual reasoning of LLMs to achieve the acquisition of new knowledge.",
            "score": 0.44564071277749506,
            "section_title": "Contextual Reasoning",
            "char_start_offset": 20005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 29,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 70,
                    "end": 74,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "254877590"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 723,
                    "end": 727,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1098,
                    "end": 1102,
                    "matchedPaperCorpusId": "256616253"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1360,
                    "end": 1364,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1422119140625
        },
        {
            "corpus_id": "265033170",
            "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
            "text": "First, putting the input document in the instruction enables EL generation on decoder-only casual language models rather than using an encoder to capture the global context. This approach enables us to benefit from the advancements in recent foundation models. Moreover, the mention detection offloading alleviates the burden of non-parallel generation. The practice of invoking the generative model only within the decision range drastically enhances computation efficiency, also presenting an intriguing parallel with interactive agents in knowledge base question answering (KBQA) (Gu et al., 2022) and the principle of Interactive NLP (Wang et al., 2023). \n\nInstruction-tuning (Wei et al., 2022a) usually refers to finetuning language models with a collection of tasks that are formulated as plain-text instructions. Recent LLM pre-training (Brown et al., 2020;Chowdhery et al., 2022) show that emergent abilities (Wei et al., 2022b) exhibit when model size, data and training compute scale. Specifically, one of such abilities is that large models can leverage natural language instructions to solve language problems in a zero-shot fashion. When instructions alone can not guide satisfactory generation, In-Context Learning (ICL) can guide LLM to learn from in-context exemplars to perform complex reasoning, such as solving mathematical problems (Wei et al., 2022c). \n\nAlthough instruction-tuning was originally proposed for zero-shot language tasks, we find that tuning models using document instructions also leads to improved prediction and disambiguation of uncommon entities in generative EL. Taking inspiration from ICL, we have extended our approaches to encapsulate EL with the ICL paradigm, facilitating an equitable comparison between INSGENEL-R and general-purpose LLMs. \n\nRetrieval-augmented Language Models are an emerging class of models in the field of NLP. They offer innovative solutions to knowledge-related tasks by combining the power of language mod-els with the ability to retrieve relevant information from a large corpus of external knowledge. Most works augment the input context with retrieved external documents, thus the encoded representation or generated sequence will be conditioned on external knowledge. Guu et al. (2020) firstly train an end-to-end language encoder with a dense retrieval system.",
            "score": 0.4455614284563908,
            "section_title": "Related Works",
            "char_start_offset": 7687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2336
                }
            ],
            "ref_mentions": [
                {
                    "start": 680,
                    "end": 698,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 917,
                    "end": 936,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0260009765625
        },
        {
            "corpus_id": "254823245",
            "title": "ALERT: Adapt Language Models to Reasoning Tasks",
            "text": "Other works such as Chung et al. ( 2022) integrated step-by-step explanations into the finetuning stage (CoT-finetuning). While these techniques may improve the accuracy and interpretability, it is not well understood which reasoning skills they rely on or to what degree they require higher-order reasoning. It is also uncertain how frequently the stated reasoning steps actually contribute to the final task predictions. For instance, to correctly answer the questions in Table 1 a combination of logical, commonsense, math and spatial reasoning skills are required. \n\nIn this work, to gain a deeper understanding of LLMs reasoning abilities in in-context learning settings, we introduce ALERT, a new pipeline to benchmark different LLMs on various reasoning skills and provide analysis to assess reasoning abilities. Unlike existing commonly used benchmarks (e.g., Mishra et al. (2022); Wang et al. (2022c); Srivastava et al. (2022)), ALERT can evaluate LLMs' fine-grained reasoning skills. It spans over 20 datasets and covers 10 different reasoning skills including logical, causal, commonsense, abductive, spatial, analogical, argument and deductive reasoning as well as textual entailment, and mathematics (see Figure 6). ALERT enables easy benchmarking of any LM (e.g., pre-trained, finetuned, CoTfinetuned) on a rich set of new inference methods including zero-shot, few-shot and CoT. \n\nUsing ALERT, we further investigate whether finetuning can improve LMs' performance on downstream reasoning tasks. Specifically, we are interested in diagnosing what actually improved when we observe a performance increase on reasoning tasks. Is it because models have seen similar data in the finetuning stage? Or is it because models have seen prompts in a specific template and memorize the template during finetuning such as definitions provided in the NIV2 benchmark (Wang et al., 2022c)? Or does the LLM actually acquired the required reasoning skill? We investigate these three possibilities. \n\nTo study the above questions, we compare three different model types (as shown in Figure 2): a pretrained model and two types of finetuned models. Specifically:",
            "score": 0.4454674093658721,
            "section_title": "Introduction",
            "char_start_offset": 1798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2158
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.167236328125
        },
        {
            "corpus_id": "254366803",
            "title": "Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review",
            "text": "Similar to the above discussion of generative reasoning, this line of work aims to guide the language models to explicitly generate the intermediate thinking steps (or reasons) during reasoning. A representative work among them is the Chainof-Thought prompting (Wei et al., 2022b), where the intermediate thinking process is verbalized and integrated into the demonstrations. In this way, complex reasoning can be decomposed into multiple steps reflected by language, which is analogous to how humans solve complex tasks. Such methods achieve much better performance on a variety of reasoning tasks compared with normal prompting and even surpass fine-tuned methods in some cases. Moreover, the prompting strategy can be further refined (Wang et al., 2022;Zhou et al., 2022a;Creswell et al., 2022), leading to generally better results. LLMs prompted with and asked to generate step-by-step reasoning chains also exhibit certain quantitative reasoning abilities such as solving math word problems (Wei et al., 2022b), where the LLMs are not specifically trained on such tasks. However, reasoning abilities for LLMs are shown to be emergent (Wei et al., 2022b,a), i.e., effective only for really large language models (over 100 billion parameters). How to enable smaller language models with few-shot reasoning skills is still an open question.",
            "score": 0.4453128325599528,
            "section_title": "Reasoning by NLG",
            "char_start_offset": 26771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1342
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 996,
                    "end": 1015,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0548095703125
        },
        {
            "corpus_id": "265697611",
            "title": "Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification",
            "text": "The previous chapter provided a comprehensive analysis of three pivotal studies: ICL, IL, and CoT, which are crucial for this research. However, recent developments in related fields warrant attention. This section aims to present works pertinent to our study, categorizing them into two distinct groups. The first category encompasses research that leverages the intrinsic knowledge of LLMs as the primary driver of the investigation. The second category includes studies that incorporate external knowledge sources to augment the capabilities and performance of LLMs. \n\nIntrinsic Knowledge. Zhang et al. ( 2022) presents Auto-CoT, an automated method for enhancing large language models' reasoning by generating diverse, self-constructed demonstrations, outperforming manual approaches in reasoning tasks. Wang et al. (2022a) introduces an iterative prompting framework to enhance PLMs for complex, multi-step reasoning tasks. Wu et al. (2023) developed a self-adaptive framework for in-context learning in language models, significantly improving example selection and ordering. Recent studies have advanced the CoT approach, yielding significant enhancements in the reasoning capabilities of LLMs (Huang et al., 2022) (Zhou et al., 2023) (Wang et al., 2022b) (Wang et al., 2023c). Reliance on language models' self-generated knowledge often leads to inaccuracies, as reasoning based on erroneous information inevitably produces incorrect conclusions, a phenomenon known as illusions in language models (Huang et al., 2023) (Ji et al., 2023). \n\nExternal Knowledge. To address the hallucination issue inherent in Large Language Models (LLMs), it is imperative to integrate more dependable external knowledge sources. A practical approach to achieving this is through the incorporation of knowledge graphs. By amalgamating LLMs with these knowledge graphs (Agrawal et al., 2023), which encompass accurate and verified information, we can effectively mitigate the generation of erroneous knowledge (Agarwal et al., 2023) (Wang et al., 2023b) (Wang et al., 2023a) (Baek et al., 2023) (Trivedi et al., 2022).",
            "score": 0.44520045703393174,
            "section_title": "Related Work",
            "char_start_offset": 6520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2106
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 827,
                    "matchedPaperCorpusId": "253098851"
                },
                {
                    "start": 929,
                    "end": 945,
                    "matchedPaperCorpusId": "254877590"
                },
                {
                    "start": 1222,
                    "end": 1240,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 1263,
                    "end": 1283,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1527,
                    "end": 1544,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057586669921875
        },
        {
            "corpus_id": "270560824",
            "title": "A Survey on Human Preference Learning for Large Language Models",
            "text": "Automatic evaluations are mostly conducted through labeled benchmark datasets, evaluating how LLMs are aligned with automatic metrics.These evaluations focus on the natural language understanding abilities of LLMs, which can be further divided into general ability tests, specific ability tests, and downstream task benchmarks.\n\n1) General ability tests: General ability tests aim to evaluate understanding abilities and the overall knowledge of LLMs.They are usually collections of questions appearing in various exams of humans [173,174] or diverse sets of NLP tasks [175,176].\n\n2) Specific ability tests: Specific ability tests are dedicated to evaluating the performance of LLMs in specific aspects, such as truthfulness [177], coding [178], and math reasoning [179].\n\n3) Downstream tasks: Downstream task benchmarks evaluate the performance of LLMs on specific generative downstream tasks, such as summarization [181,182].These evaluations are usually conducted for rapid preliminary validations of the performance of LLMs.",
            "score": 0.4451941714876806,
            "section_title": "B. Automatic Evaluations",
            "char_start_offset": 52919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 327
                },
                {
                    "start": 329,
                    "end": 451
                },
                {
                    "start": 451,
                    "end": 579
                },
                {
                    "start": 581,
                    "end": 771
                },
                {
                    "start": 773,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1028
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 535,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 569,
                    "end": 574,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 725,
                    "end": 730,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 917,
                    "end": 922,
                    "matchedPaperCorpusId": "2204603"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "8928715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072509765625
        },
        {
            "corpus_id": "267200238",
            "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data",
            "text": "Recently, LLMs like ChatGPT (Ouyang et al., 2022) have attracted tremendous research attention with their superb language understanding and generation abilities, bringing excellent performance to many tasks like reading comprehension (Rogers et al., 2023), discrete reasoning (Wei et al., 2022c), etc. Since these general LLMs are usually extremely large and not accessible for most researchers, open-source smaller LLMs are proposed to allow researchers to customize their own LLMs for different tasks, such as LLaMA (Touvron et al., 2023a) and Alpaca (Taori et al., 2023), with comparable performance to the general LLMs. Recently, there has been a trend in the research community that small LLMs are specialized for specific tasks through fine-tuning with instructions (Fu et al., 2023), yielding impressive performance. To the best of our knowledge, we are the first work to specialize a small LLM (e.g.,7B) in tabular and textual QA challenge.",
            "score": 0.4451827199991585,
            "section_title": "Large Language Models (LLMs)",
            "char_start_offset": 5472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 948
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 49,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 234,
                    "end": 255,
                    "matchedPaperCorpusId": "236447339"
                },
                {
                    "start": 276,
                    "end": 295,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 772,
                    "end": 789,
                    "matchedPaperCorpusId": "256390607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056640625
        },
        {
            "corpus_id": "270123745",
            "title": "From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers",
            "text": "The rapid advance of large language models (LLMs) is one of the most exciting recent developments in artificial intelligence.LLMs, pre-trained on large text corpora, have demonstrated impressive generalizable reasoning capabilities and can achieve remarkable performance across a broad set of tasks, ranging from natural language comprehension [34] and generation [3] to mathematical reasoning [10] and programming [6].These models have shown promise in performing real-world tasks in various applications and business solutions.One fundamental pillar of such success lies in the capabilities of these models to perform tasks through generalizable reasoning.\n\nInstruction tuning is a popular approach to unlock the capabilities of these models originally trained on next-token-prediction objectives to understand instructions and perform reasoning to complete tasks.By training models on pairs of instructions and expected outcomes, instruction tuning teaches LLMs to perform specific tasks, thus enabling them to address real-world problems and seamlessly interact with humans.In practice, however, fine-tuning data is limited, and instruction tuning can only focus on a limited set of tasks.Its success is therefore critically dependent on the model's ability to generalize beyond its fine-tuning instructions to unseen tasks not encountered during training.Several factors influence this generalization: the size of the fine-tuning sample, the diversity of the instruction sets, and the quality of the annotations.Yet, there is little systematic research on their relative impact.\n\nOur work aims to fill this gap by proposing two contributions to the study of generalization in instruction tuning.First, we propose a systematic analysis of the impact of instruction diversity by focusing on a simple yet important symbolic task: string rewrites.This basic setting enables us to exercise fine control over the factors that may affect generalization and to demonstrate the importance of instruction diversity.To highlight the broader applicability, we describe this task in arXiv:2405.19787v2[cs.CL] 31 May 2024 terms of a Markov algorithm, a classic Turing-complete model, ensuring rigorous examination of string replacements.Second, we extend this analysis to a real-world application: code generation, and show that fine-tuning on an instruction set that extends beyond coding tasks significantly improves performance.\n\nOur main findings are:\n\n1. Instruction diversity is the main driver of generalization.",
            "score": 0.444915141689897,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1360
                },
                {
                    "start": 1360,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1583
                },
                {
                    "start": 1585,
                    "end": 1700
                },
                {
                    "start": 1700,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2010
                },
                {
                    "start": 2010,
                    "end": 2093
                },
                {
                    "start": 2093,
                    "end": 2228
                },
                {
                    "start": 2228,
                    "end": 2422
                },
                {
                    "start": 2424,
                    "end": 2446
                },
                {
                    "start": 2448,
                    "end": 2510
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057373046875
        },
        {
            "corpus_id": "270870048",
            "title": "HRDE: Retrieval-Augmented Large Language Models for Chinese Health Rumor Detection and Explainability",
            "text": "Since the release of ChatGPT by OpenAI [4], large language models (LLMs) have gained significant attention for their capabilities in natural language processing.Currently, LLMs are typically defined as Transformer-based neural network language models containing billions or more parameters and trained on massive datasets [36,59], such as GPT-4 [39], LLaMA [46], and Qwen [1].Compared to earlier neural language models (NLMs) and pretrained language models (PLMs), LLMs follow scaling laws [19,27], and certain capabilities significantly improve when the model's parameter size exceeds a certain threshold, known as emergent abilities [50].The main emergent abilities of LLMs include (1) in-context learning (ICL) [4,12], where LLMs can learn new tasks from a few examples provided in the prompt without needing retraining, (2) instruction following, where LLMs can understand and execute tasks based on natural language instructions, (3) multi-step reasoning, where LLMs can perform complex reasoning tasks by breaking them down into steps using methods like chain-of-thought (CoT) [51].Additionally, LLMs can be enhanced through techniques such as Retrieval-Augmented Generation (RAG) [16], supervised fine-tuning (SFT) [49], reinforcement learning from human feedback (RLHF) [8], making them more robust and reliable for handling complex tasks in specialized fields.Consequently, numerous specialized LLMs have emerged in various vertical domains, such as Med-PaLM in the medical field [44], BloombergGPT in finance [53], and ChatLaw in legal applications [10].",
            "score": 0.44472784299653667,
            "section_title": "Large Language Models",
            "char_start_offset": 3732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 640
                },
                {
                    "start": 640,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1369
                },
                {
                    "start": 1369,
                    "end": 1564
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 42,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 714,
                    "end": 717,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1278,
                    "end": 1281,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "255124952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0233306884765625
        },
        {
            "corpus_id": "268531405",
            "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",
            "text": "Recently, Large Language Models (LLMs) have initiated a significant paradigm shift in the areas of Natural Language Processing (NLP) including Natural Language Generation (NLG).LLMs are generally characterized by a large number of parameters --usually ranging from millions to trillions -and are constructed using deep neural networks, mainly transformer architectures (Vaswani et al., 2017;Lin et al., 2021).They undergo pre-training on massive amounts of text data, often collected from the web, and leverage self-supervised (Jiao et al., 2023), semisupervised (Shi et al., 2023a), or Reinforcement Learning (RL) (Ouyang et al., 2022;Gulcehre et al., 2023) methods for pre-training and finetuning.Researchers continue to explore ways to fine-tune and adapt these LLMs for specific applications, making them indispensable tools in the NLP community and beyond.\n\nLLMs have demonstrated remarkable abilities to generate coherent, human-like text, often for a given textual input, also referred to as a prompt (Zhao et al., 2023b).\n\nFor example, LLMs can assist users in communicating effectively, providing consistent and context-aware responses.They enable users to access information quickly, summarizing large volumes of text or answering complex queries.Moreover, LLMs contribute to producing varied and inventive content, whether it's generating poetry, stories, or code snippets.Beyond individual use cases, they also play a crucial role in education, research, and innovation across diverse fields, including science, art, and literature.\n\nMore specifically, LLMs achieve impressive results on various NLP tasks, such as text generation (Senadeera and Ive, 2022), question answering (Zaib et al., 2021;Bhat et al., 2023), sentiment analysis (Batra et al., 2021;Kheiri and Karimi, 2023), as well as augmenting human abilities by improving Human-Computer Interactions (HCI) (Oppenlaender and Hamalainen, 2023;H\u00e4m\u00e4l\u00e4inen et al., 2023).",
            "score": 0.4446492824005696,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 409
                },
                {
                    "start": 409,
                    "end": 699
                },
                {
                    "start": 699,
                    "end": 861
                },
                {
                    "start": 863,
                    "end": 1029
                },
                {
                    "start": 1031,
                    "end": 1145
                },
                {
                    "start": 1145,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1544
                },
                {
                    "start": 1546,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 391,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 391,
                    "end": 408,
                    "matchedPaperCorpusId": "235368340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.027374267578125
        },
        {
            "corpus_id": "270226883",
            "title": "Process-Driven Autoformalization in Lean 4",
            "text": "To enhance the reasoning capabilities of LLMs, prior research primarily focuses on specific prompting techniques. Existing efforts include few-shot prompting with intermediate steps augmented demonstrations [56,57,58] or zero-shot prompting with specific instructions [59,60]. Although these methods have shown promising results, their effectiveness is often constrained by their task-specific nature and the labour-intensive process of designing prompts, leading to inconsistent outcomes across different tasks [61,62]. Another strategy to facilitate reasoning involves instruction tuning or knowledge distillation, which elicits reasoning paths from LLMs without explicit prompting [63,64,65,66]. These approaches typically involve resource-intensive fine-tuning over LLMs and require a large set of examples annotated with chainof-thoughts (CoT). To address these challenges, verification techniques have emerged as a promising solution [19,21]. Verification models are trained to evaluate and potentially correct the reasoning process generated by LLMs. This approach aims to mitigate the risk of relying solely on the top-1 result, which may not always be reliable [23,67]. \n\nLearning From Feedback Improving LLMs through learning from feedback has become a prevalent strategy, notably through reinforcement learning from human feedback, which seeks to align LLMs with human values by refining their outputs based on feedback [68,69]. However, this method faces challenges such as high costs due to manual labor and a lack of real-time feedback capabilities. An alternative strategy involves using self-correcting LLMs, which rely on automated feedback to iteratively adapt and understand the consequences of their actions without heavy reliance on human intervention. This feedback can be derived from inside sources such as the model itself [70,71] or generation logits [72], and outside sources such as tools [73], knowledge bases [74,75], or evaluation metrics [76,77]. Our method leverages formal languages that can naturally provide precise feedback on the reasoning process, enabling automatic process annotation without substantial human or machine annotation costs.",
            "score": 0.4446196398310797,
            "section_title": "Improving Reasoning Abilities of LLMs",
            "char_start_offset": 33953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "252873674"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "253265328"
                },
                {
                    "start": 944,
                    "end": 947,
                    "matchedPaperCorpusId": "258987659"
                },
                {
                    "start": 1848,
                    "end": 1852,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 1852,
                    "end": 1855,
                    "matchedPaperCorpusId": "258833055"
                },
                {
                    "start": 1939,
                    "end": 1943,
                    "matchedPaperCorpusId": "254247260"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "249017524"
                },
                {
                    "start": 1974,
                    "end": 1977,
                    "matchedPaperCorpusId": "253244506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06817626953125
        },
        {
            "corpus_id": "258298867",
            "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
            "text": "Second, LLMs may unlock enhanced (or new) reasoning abilities when provided with the opportunity and the cues to elaborate a systematic step-by-step response. The improved ToM performance we observe is not simply a result of providing a number of in-context examples of ToM tasks (as in (Sap et al., 2022), where performance remained poor) but seems to rely on providing examples in which there is step-by-step inferential reasoning from the evidence before arriving at a conclusion (Figure 3 and Figure 4). \n\nThe LLMs may have seen some ToM or Photo scenarios during their training phase, but data leakage is unlikely to affect our findings. First, our findings concern the change in performance arising from prompting, and the specific prompts used to obtain this performance change were novel materials generated for this study. Second, if the model performance relied solely on prior exposure to the training data, there should be little difference between zero-shot Photo and ToM performance (Figure 2), as these materials were published in the same documents; however, the zero-shot performance patterns were very different across Photo and ToM scenarios. Third, the LLM performance improvements arose when the models elaborated their reasoning step-by-step, and this elaborated reasoning was not part of the training data. Therefore, although some data leakage is possible, it is unlikely to affect our conclusions concerning the benefits of prompting. \n\nAn important avenue for further testing is whether the prompt-driven performance gains are specific to ToM reasoning, or would be expected more generally in tasks involving other forms of inferential reasoning. Many of the ToM questions require the model to infer facts (e.g. mental states) that are not explicitly stated in the question, while (qualitatively speaking) it seems that many of the Control scenarios can be answered without performing as much inference beyond what is explicitly provided in the scenario text. Therefore, we are now testing LLMs comprehension in scenarios that require inferential reasoning but not reasoning about people's ToM. Our preliminary results indicate (i) a similar pattern in zero-shot performance for ToM scenarios and non-ToM scenarios which require inferential reasoning; and (ii) an improvement in non-ToM performance when incorporating the same prompts used for ToM scenarios (see Supplement E).",
            "score": 0.44461372964831186,
            "section_title": "General Discussion",
            "char_start_offset": 24058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2403
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 305,
                    "matchedPaperCorpusId": "253098632"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0592041015625
        },
        {
            "corpus_id": "274131858",
            "title": "I'm Spartacus, No, I'm Spartacus: Measuring and Understanding LLM Identity Confusion",
            "text": "LLM development usually involves two stages: pretraining and fine-tuning. In pretraining, the model is trained on a large, unsupervised dataset to learn general language patterns. Afterward, the model undergoes fine-tuning on a smaller, task-specific dataset to optimize its performance for a particular application: \n\n\u2022 Pretraining: During pretraining, the model is trained on massive, unsupervised datasets like books, articles, and websites to learn language patterns, grammar, word relationships, and factual knowledge. Using unsupervised learning, it predicts missing or next words in sequences, building a generalized understanding of language for diverse tasks. This process is highly computational, requiring distributed training on GPUs or TPUs over weeks or months. \u2022 Fine-tuning: After pretraining, the model undergoes fine-tuning on a smaller, task-specific dataset to adapt its general knowledge to a particular application. This supervised process maps inputs to desired outputs, refining the model's understanding for specific use cases. Fine-tuning is less resource-intensive than pretraining and requires smaller datasets since the model builds on its existing knowledge. The combination of pretraining and fine-tuning creates a flexible and efficient development pipeline for large language models. It allows organizations to leverage stateof-the-art models without the steep costs associated with training from scratch, while still tailoring these models to the unique requirements of specific applications or domains.",
            "score": 0.4445046007193646,
            "section_title": "Pre-Training and Fine-Tuning",
            "char_start_offset": 8336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06842041015625
        },
        {
            "corpus_id": "274776474",
            "title": "Codenames as a Benchmark for Large Language Models",
            "text": "With the ever growing number of LLMs being released every month, being able to empirically evaluate their performance has become an increasingly critical challenge. To understand how good these models are across different domains and applications, researchers often rely on multiple benchmarks that focus on different LLM capabilities or tasks. \n\nOne of the most popular benchmarks for evaluating the general knowledge and language understanding of LLMs is the Massive Multitask Language Understanding (MMLU) test [17]. This benchmark contains numerous multiple-choice questions grouped into 57 individual topics, that LLMs are required to interpret and answer correctly. Other benchmarks such as HellaSwag [18] or BIG-Bench Hard (BBH) [19] also focus on language comprehension, but additionally assess general and common-sense reasoning abilities [20]. For Hel-laSwag, given the start of the sentence, the LLM must select the logical completion among different available choices. BBH considers 23 task groups, each consisting of several examples, that have been identified as especially challenging LLMs. \n\nAs a subcategory of reasoning, LLMs can also be evaluated on their strategic capabilities [21]. This ranges from their playing skill for different types of games (conversational, board, card, or electronic games) to societal and economic simulations. One of the key aspects for this type of evaluation is the presence of at least one other agent that can influence the environment, which thus affects the decisions made by the LLM. Beyond evaluating general reasoning or text generation, benchmarks have also been proposed to assess the capabilities of LLMs for specific tasks or scenarios. TyDi QA [22] is another benchmark consisting of questions and expected answers but presented in a variety of world languages, intended to evaluate the multilingual abilities of LLMs. Another example of a precise domain benchmark is the MATH test set [23], which (as the name would imply) assesses LLM performance on different mathematical problems. \n\nThis section has covered only a handful of the many LLM benchmarks that currently exist, however it is apparent that few of them evaluate more abstract language understanding and reasoning capabilities of LLMs outside of knowledge assessment. We propose that the use of Codenames as an LLM benchmark allows for the simultaneous assessment of language understanding, strategic reasoning, and theory of mind capabilities.",
            "score": 0.444471300779977,
            "section_title": "A. Benchmarks for LLMs",
            "char_start_offset": 13706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2047
                },
                {
                    "start": 2050,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2469
                }
            ],
            "ref_mentions": [
                {
                    "start": 1707,
                    "end": 1711,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1019287109375
        },
        {
            "corpus_id": "264398751",
            "title": "Language models and psychological sciences",
            "text": "Importantly, due to their size and complexity, the behaviour of LLMs cannot be predicted in advance by looking at the architecture and training corpus and must instead be empirically explored. The procedure required for evaluating LLM is like that used by cognitive psychologists to study the human mind and consists in testing LLMs with tasks that are believed to tap on specific cognitive functions. \n\nAs stated earlier, the advent of LLMs has sparked a robust debate within the AI community, centring on the question of whether machines possess the capability to genuinely comprehend natural language, thereby capturing the interplay of both physical and social contexts encapsulated within linguistic expression. The implications of this debate extend beyond practical applications, delving into the realm of psychological cognition. This is because LLMs, as elucidated in the subsequent discussion, exhibit a remarkable proficiency in simulating reasoning abilities that have traditionally been regarded as distinctly human. \n\nRecently, cognitive psychologists have introduced a novel evaluation methodology for LLMs. This approach involves treating LLMs as active participants within a psychological experiment, thereby facilitating a comprehensive assessment of their cognitive capabilities. Cognitive psychologists believe that this approach offers different advantages over existing evaluation protocols which are not driven by a cognitive model. The use of psychological-inspired tests to scrutinize LLMs' performance serves a multifaceted purpose. These tests aim to uncover underlying cognitive biases and different problem-solving approaches and methodologies that extend beyond the confines of conventional performance-based analyses, which have been the focal point of previous investigations. By demystifying how LLMs solve challenging reasoning problems, psychological experiments can provide a deeper understanding of their full complexity. \n\nHerein, we introduce the preliminary l findings arising from an investigation conducted over the last 6 months (late 2022 -June 2023), centred on the assessment of the reasoning abilities of LLMs using evaluation protocols initially formulated for human assessment.",
            "score": 0.44436592048506185,
            "section_title": "Psychological assessment of LLMs",
            "char_start_offset": 15408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2226
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "259317218",
            "title": "Personality Traits in Large Language Models",
            "text": "LLMs are starting to meet most of the key requirements for human-like language use, including conversation, contextual understanding, coherent and relevant responses, adaptability and learning, question answering, dialog, and text generation [80,116,101]. These impressive NLP capabilities are a result of LLMs' abilities to learn language distribution, aided by increasing model sizes [11,117], training on massive datasets of text, and further fine-tuning toward usage preferences [115] (see Appendix A). Taken together, they enable LLMs to enact convincing, humanlike personas, sparking debate over the existence and extent of personality [74], human values [97], and other psychological phenomena [110] potentially embedded in these models. Personality is a foundational socio-behavioral phenomenon in psychology that, for humans, predicts a broad spectrum of health, social, economic, and political behaviors crucial for individual and societal success [9]. For example, personality has been extensively studied as an antecedent of human values [85]. Decades of research have further shown how personality information is richly encoded in human language [31,96]. LLMs not only comprise the vast sociopolitical, economic, and behavioral data they are trained on, they also generate language that inherently expresses personality content. For this reason, the ability to measure and validate LLM-synthesized personality holds promise for LLM safety, responsibility, and alignment efforts [27], which have so far primarily focused on mitigating specific harms rather than examining more fundamental patterns of model behavior. Ultimately, personality as an empirical framework [47] provides both theory and methodology for quantifying latent traits in LLMs that are potentially predictive of LLM behaviors in diverse inference tasks (see Appendix B). Recent work has tried to identify unintended consequences of the improved abilities of LLMs, including their use of deceptive and manipulative language [62], gender, racial, or religious bias in behavioral experiments [1], and violent language, among many others [7]. LLMs can also be inconsistent in dialogue [65], explanation generation, and factual knowledge extraction.\n\nPrior attempts to probe psychological phenomena such as personality and human values in LLMs have informally measured personality using questionnaires and, in some cases, prelim",
            "score": 0.4439807593528352,
            "section_title": "Quantifying and Validating Personality Traits in LLMs",
            "char_start_offset": 3671,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "149343234"
                },
                {
                    "start": 2071,
                    "end": 2074,
                    "matchedPaperCorpusId": "263630346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052520751953125
        },
        {
            "corpus_id": "273350617",
            "title": "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics",
            "text": "Large Language Models (LLMs) can perform many tasks effectively, even in few-shot or zero-shot settings. Recently, LLMs have also been used to evaluate natural language generation tasks, in replacement of human evaluation. LLM-as-a-Judge shows useful properties as an evaluation metric, for instance Faysse et al. (2023) illustrated using GPT-4 that it can be highly correlated with human judgement, format and task agnostic and comparable across tasks. Zheng et al. (2023) describe limitations of LLM-as-a-Judge, including position, verbosity and self-enhancement biases as well as poor performance at grading math or reasoning tasks. Other limitations are expressed by Kim et al. (2023) targeting proprietary LLMs such as GPT-4 for their closed source nature, uncontrolled versioning, and their high costs. Prometheus 2 (Kim et al., 2024) is designed for evaluating language models and shows high correlations with proprietary LLMs and human evaluations. Besides, its open-source nature mitigates some of the aforementioned issues. Liu et al. (2023) suggest that LLMs aligned from human feedback overfit to reference-less human evaluation of summaries, which they observed to be biased towards longer summaries and to suffer from low inter-annotator agreement.",
            "score": 0.4439286405722517,
            "section_title": "LLM-as-a-Judge evaluation",
            "char_start_offset": 2949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1262
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 320,
                    "matchedPaperCorpusId": "264426560"
                },
                {
                    "start": 1034,
                    "end": 1051,
                    "matchedPaperCorpusId": "254685611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070556640625
        },
        {
            "corpus_id": "265609639",
            "title": "Data Management For Training Large Language Models: A Survey",
            "text": "In the pretraining stage of LLMs: \n\n\u2022 The coverage of more domains and proper domain mixture ratio are important. Recently, researchers try to automatically find the proper domain mixture weights, which still show room for improvement. \n\n\u2022 Large amount of data is widely considered critical, and proper data repetition may also bring positive impacts to model performance. \n\n\u2022 Data quality control is necessary usually form an order, namely quality filtering, deduplication and toxicity filtering.. However, overaggressive quality and toxicity filtering may lead to performance degradation and social biases, which is still under-explored. \n\n\u2022 Data diversity and temporal misalignment also have impacts on model performance, which call for future study. \n\nIn the supervised fine-tuning stage of LLMs: \n\n\u2022 Multitask fine-tuning is widely adopted nowadays. However, conflictions may exist among tasks and hinders the model abilities. Hence, dealing with negative task confliction is also calling for better answers. Ensembling multiple single-task experts instead of training one multitask model also arises as an new trend. \n\n\u2022 Quality control are usually achieved through heuristics, human evaluation or LLMs as quality judges. Instruction diversity and complexity are also beneficial and enhanced by several works. The exploration of more diverse and complex instructions is still an open question. \n\n\u2022 Works have shown that the SFT of LLM rely more on data quality than data quantity. However, digging deeper into the influence of data quantity, some researchers find that the learning of different tasks may require different amount of data. \n\n\u2022 Instead of keep instruction datasets unchanged during fine-tuning, works propose to adjust the datasets dynamically through fine-tuning. Special fine-tuning strategies are also continually shown up to utilize the instruction data more efficiently. 2022) also argue that the quality filters used for GPT-3 (Brown et al., 2020) prefer newspapers published by larger schools located in wealthier, educated, and urban ZIP codes, leading to a language ideology. Feng et al. ( 2023) conduct a comprehensive case study focusing on the effects of media political biases in the pretraining corpus on the fairness of hate speech detection and misinformation detection w.r.t. partisan leanings and how it is propagated to language models even further to downstream tasks.",
            "score": 0.443826336164911,
            "section_title": "A Takeaways",
            "char_start_offset": 31518,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 36,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2313
                },
                {
                    "start": 2314,
                    "end": 2409
                }
            ],
            "ref_mentions": [
                {
                    "start": 1954,
                    "end": 1974,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06927490234375
        },
        {
            "corpus_id": "267751249",
            "title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "text": "Large Language Model Self-correction. Recent works demonstrate that LLM can utilize its own feedback signal to refine itself (Madaan et al., 2024;Chen et al., 2024;Shinn et al., 2024). Wang et al. (2023) further proposed to sample diverse reasoning paths and use a majority vote to find the most confident answer. Huang et al. (2023a) leverages self-consistency to further fine-tune the LLM on the most confident reasoning path with diverse instruction formats. On the other hand, LLM's selffeedback can also be used as a reward signal to further align LLM to follow instructions (Gulcehre et al., 2023;Yuan et al., 2024). \n\nDespite some demonstrations of performance improvements, most findings indicate that LLMs struggle to rectify their initial mistakes, and their performance even worsens after self-correction (Huang et al., 2023b;Tyen et al., 2023;Ke et al., 2023). This issue arises because the quality of the model's self-generated feedback is bounded by its existing knowledge and abilities (Stechly et al., 2023;Hong et al., 2023). Therefore, internal feedback may not offer any extra advantage for improving the results; it might even steer the model away from the correct answer (Valmeekam et al., 2023). However, prior works only had empirical observations on this phenomenon, while lacking a quantitative analysis. Moreover, prior works only focus on specific tasks, such as reasoning or code generation. In this work, we are the first to quantitatively analyze the self-bias of different LLMs across three tasks and four languages, which provides a novel and generalizable view to address the perils of self-refine. \n\nLLMs as Evaluators. et al. (2023b) point out that reference-free metrics are inherently biased on their own outputs. \n\nAlthough the above empirical studies provide valuable insights, they lack a formal definition to quantify those biases nor provide a connection to the self-refine framework. In this work, we define and quantify self-bias and provide the first in-depth analysis of its impact on the self-refine pipeline.",
            "score": 0.4436586320884271,
            "section_title": "Related Work",
            "char_start_offset": 3408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 146,
                    "matchedPaperCorpusId": "257557820"
                },
                {
                    "start": 146,
                    "end": 164,
                    "matchedPaperCorpusId": "258059885"
                },
                {
                    "start": 164,
                    "end": 183,
                    "matchedPaperCorpusId": "264305982"
                },
                {
                    "start": 185,
                    "end": 203,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 314,
                    "end": 334,
                    "matchedPaperCorpusId": "265157446"
                },
                {
                    "start": 603,
                    "end": 621,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 855,
                    "end": 871,
                    "matchedPaperCorpusId": "265506605"
                },
                {
                    "start": 1023,
                    "end": 1041,
                    "matchedPaperCorpusId": "265157446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "261049794",
            "title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
            "text": "Recent developments in large language models (Chowdhery et al. 2022;Thoppilan et al. 2022;Liu et al. 2023, inter alia) have spotlighted their efficacy in general problem solving (Huang and Chang 2022;Suzgun et al. 2022), code generation (Chen et al. 2021;Austin et al. 2021), and instruction following (Ouyang et al. 2022;Bai et al. 2022). While early models relied on direct answer strategies (Brown et al. 2020), contemporary research veers towards linear reasoning paths (Wei et al. 2022b;Kojima et al. 2022;Zhang et al. 2022) by breaking problems into sub-tasks for solution discovery, or harnesses external mechanisms to alter token generation by changing the context (Zhou et al. 2022;Drozdov et al. 2022;Yao et al. 2023). \n\nAnalogous to human cognition (Sloman 1996; Kahneman 2011), early LLM strategies seemed to emulate the instantaneous System 1, characterized by its impulsive decisionmaking. In contrast, more recent methodologies like chainof-thought (CoT) (Wei et al. 2022b) and least-to-most prompting (L2M) (Zhou et al. 2022;Drozdov et al. 2022) Preprint. Under review. \n\nreflect the introspective nature of System 2. Notably, integrating intermediary reasoning steps has yielded improvements in arithmetic reasoning tasks (Srivastava et al. 2022;Liang et al. 2022). \n\nHowever, as tasks shift towards deeper planning and extensive thought exploration, these methods appear restrictive. Although CoT integrated with Self-Consistency (CoT-SC) (Wang et al. 2022) enlists multiple LLM outputs for a consensus, the lack of meticulous evaluation can result in model misdirection. The \"Tree of Thoughts\" (Yao et al. 2023;Long 2023) emerges as a notable solution. While one LLM is dedicated to idea generation, another steps in to assess the merit of these ideas, following a halting-assessmentresuming cycle.",
            "score": 0.44359234003453163,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 255,
                    "matchedPaperCorpusId": "225488526"
                },
                {
                    "start": 302,
                    "end": 322,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 492,
                    "end": 511,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 511,
                    "end": 528,
                    "matchedPaperCorpusId": "252762275"
                },
                {
                    "start": 691,
                    "end": 711,
                    "matchedPaperCorpusId": "237376626"
                },
                {
                    "start": 1041,
                    "end": 1061,
                    "matchedPaperCorpusId": "237376626"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.031982421875
        },
        {
            "corpus_id": "275788769",
            "title": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities",
            "text": "Despite recent successes of large language models (henceforth, LLMs), matching or exceeding human performance in wide array of tasks, it is still not clear to what extent they can be considered as models of human language (Milli\u00e8re, 2024) or human cognition (Mahowald et al., 2024). Ever since the emergence of large foundation language models, there has been a great interest in gaining a deeper understanding of their general linguistic and cognitive capabilities, ranging from studies into syntactic or semantic processing in LLMs (Gulordava et al., 2018;Blevins et al., 2018;Goldberg, 2019;Arefyev et al., 2020) to analyses of their arithmetic or logical reasoning abilities (Huang and Chang, 2022;Webb et al., 2023;Wu et al., 2023;Yuan et al., 2023). Since LLMs are trained on massive amounts of training data and learn huge amounts of parameters in their internal layers, it is often difficult to establish whether an LLM has \"really\" generalized a certain rule or piece of knowledge, or whether it memorized superficial token-level patterns (Weissweiler et al., 2023). This challenge exists in particular in domains where linguistic knowledge is closely intertwined with nonlinguistic aspects of cognition (Mahowald et al., 2024) such as, e.g., pragmatics and commonsense reasoning (Chang and Bergen, 2024). \n\nTo assess LLMs linguistic capabilities research has drawn from the methods of psychology, applying established experimental paradigms to language models. For example, Ettinger (2020) created a diagnostic suite for language models inspired by psycholinguistic tests, ranging over different linguistic disciplines. A study by Hawkins et al. (2020) compared human and model acceptability judgements for double object sentences, capturing the differences in their biases. Outside the realm of psycholinguistics, Binz and Schulz (2023) used vignette studies from cognitive psychology to gauge the cognitive abilities of GPT-3 (Brown et al., 2020).",
            "score": 0.4435189049328552,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 281,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 594,
                    "end": 615,
                    "matchedPaperCorpusId": "227231059"
                },
                {
                    "start": 702,
                    "end": 720,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 1213,
                    "end": 1236,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1289,
                    "end": 1313,
                    "matchedPaperCorpusId": "257636789"
                },
                {
                    "start": 1484,
                    "end": 1499,
                    "matchedPaperCorpusId": "199001173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056243896484375
        },
        {
            "corpus_id": "258564914",
            "title": "InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language",
            "text": "Large language model. Recent LLMs [3,46,61,81] have demonstrated a range of significant abilities, including language generation, in-context learning, world knowledge, and reasoning. The presence of these capabilities enables LLMs to perform complex tasks based on user instructions and prompts in a zero-shot manner. GPT-3 [3], the first language model with over 100 billion parameters, has achieved impressive zero-shot performance on various benchmarks. However, it does not consistently outperform smaller models, such as T5 [54], on some tasks. InstructGPT models [49], which are finetuned on a dataset consisting of prompts with the corresponding humanannotated desired behavior, can be aligned with users, generate outputs that are preferred over those from GPT-3 and show improvements in truthfulness and reductions. Instruction-tuned models have also demonstrated a remarkable ability to generalize zero-shot to new tasks. Thus, instructiontuning [44,71,21,8] is considered key to eliciting the abilities of LLMs [17]. In addition to GPT model family [52,53,3,46], several other LLMs exist, including OPT [83], LLaMA [61], MOSS [11] and GLM [81]. These models also achieve high performance and are open-sourced, providing valuable experience in training large models and serving as a base for further fine-tuning for different purposes. For example, Alpaca [70] proposes a self-instruct framework to instruction tune the LLaMA model family without relying heavily on human-written instruction data. Another active research area on LLMs is chain-of-thought prompting (CoT) [72,25,18,68]. CoT prompts models to solve problems step by step, greatly improving their reasoning ability of LLMs and making it possible to utilize LLMs for task splitting. As a result, LLMs can be combined with a variety of APIs [34,45] and models [57,73] trained for different modalities and serve as a controller to schedule them. This method liberates LLMs from pure language instructions and paves the way for a multi-modal interactive system. Perception model.",
            "score": 0.443512617466848,
            "section_title": "Related Work",
            "char_start_offset": 6247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 37,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 529,
                    "end": 533,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 960,
                    "end": 963,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 1064,
                    "end": 1067,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1067,
                    "end": 1069,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046295166015625
        },
        {
            "corpus_id": "270068369",
            "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
            "text": "Large Language Models (LLMs) have emerged as a powerful tool for natural language processing, demonstrating remarkable abilities in areas like text completion, summarization, and question answering [1]. One of their most intriguing capabilities is their potential to learn \"common sense\" -general knowledge about the world that allows them to reason and make inferences beyond the literal meaning of text. This has fueled excitement about the possibility of achieving zero-shot learning, where LLMs can solve unseen problems without any prior training on specific tasks [2]. \n\nHowever, a crucial distinction exists between generic public knowledge and the specific private knowledge required for most real-world use cases. While LLMs excel at generic text completion or chat-like interactions, practical applications often demand solving specific and repeatable downstream tasks within a particular domain [3]. This typically necessitates knowledge specific to a business or organization, such as understanding internal processes, up-to-date product details, or customer behavior. \n\nFine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]. As a final consideration, keeping LLMs constantly up-to-date using fine-tuning can be infeasible, especially for domains with frequently changing information like e-commerce product inventory, whereas it is easy to update a database in real-time from which information is retrieved. \n\nAs an alternative to fine-tuning, In-Context Learning (ICL) offers a promising approach for leveraging LLMs in scenarios with limited data. This approach exploits the demonstrated ability of LLMs for \"meta-learning\" -essentially, learning how to learn.",
            "score": 0.4434542787712594,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 2052
                },
                {
                    "start": 2055,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "269791266",
            "title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models",
            "text": "One major difference between LLM and traditional non-LLM methods is the emergent abilities that become available in large models but are not present in the small models [115].The term \"emergent abilities\" refers to new, complex capabilities that arise as LLMs scale in size and complexity.These abilities enable advanced understanding and generation of natural language, problem-solving across various domains without specific training, and adapting to new tasks through in-context learning.In the following, we introduce several common emergent abilities in the scope of LLMs.\n\nIn-Context Learning [127,128,129] refers to the ability of LLMs to understand and respond to new tasks or queries based on the context provided within the prompt, without requiring explicit retraining or fine-tuning.Milestone papers (GPT-2/GPT-3 [130,127]) demonstrate in-context learning in a few-shot manner, where a model is provided with a few examples of a task within the prompt and then asked to tackle a different example without prior explicit training.State-of-the-art LLMs, such as GPT-4 [131], exhibit remarkable in-context learning abilities, understanding complex instructions, and performing a wide range of tasks from simple translations to generating code and creative writing, all based on the context provided within the prompt.\n\nReasoning in the context of LLMs, often referred to as \"chain-of-thought\" prompting [132,133], involves the model generating intermediate steps or reasoning paths when tackling complex problems or questions.This approach allows LLMs to break tasks into smaller, manageable parts, facilitating a more structured and understandable solution process.To achieve this, training involves datasets that include a variety of problem-solving tasks [134,135], logical puzzles [136,137], and datasets designed to mimic reasoning under uncertainty [138].Current state-of-theart LLMs [127,131,139,140,141,142,143] demonstrate advanced reasoning capabilities typically when model sizes become larger than 60B to 100B parameters [132].\n\nInstruction-following [144,127,140,145,146] refers to the models' capability to understand and execute commands, or follow instructions as specified by the user.",
            "score": 0.4434210409183225,
            "section_title": "LLM Emergent Abilities",
            "char_start_offset": 11274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 289
                },
                {
                    "start": 289,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 577
                },
                {
                    "start": 579,
                    "end": 795
                },
                {
                    "start": 795,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1326
                },
                {
                    "start": 1328,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1675
                },
                {
                    "start": 1675,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 2048
                },
                {
                    "start": 2050,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 604,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 825,
                    "end": 830,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1412,
                    "end": 1417,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "252762275"
                },
                {
                    "start": 1767,
                    "end": 1772,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 1794,
                    "end": 1799,
                    "matchedPaperCorpusId": "232223322"
                },
                {
                    "start": 1799,
                    "end": 1803,
                    "matchedPaperCorpusId": "220047831"
                },
                {
                    "start": 1899,
                    "end": 1904,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1908,
                    "end": 1912,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 2042,
                    "end": 2047,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0574951171875
        },
        {
            "corpus_id": "278237264",
            "title": "On the generalization of language models from in-context learning and finetuning: a controlled study",
            "text": "A variety of works have studied how language models learn and generalize factual information, both in pretraining (e.g. Allen-Zhu andLi, 2024, 2025;Zucchet et al., 2025) and during finetuning (e.g. Berglund et al., 2024;Gekhman et al., 2024). \n\nMany of these works have focused on failures to generalize like the \"Reversal Curse\" (Berglund et al., 2024), while others looked into increases in hallucination as LLMs are pretrained (Zucchet et al., 2025) or finetuned on new knowledge (Gekhman et al., 2024). However, these works have generally not performed data-matched experiments comparing the generalization of incontext learning and finetuning. \n\nData augmentation: A wide variety of works have explored how LLMs can be used to augment data for improved performance from small or narrow datasets, e.g. to improve zero-shot task performance (Maini et al., 2024) or to improve crosslingual generalization (e.g. Whitehouse et al., 2023). Ding et al. (2024) reviews some of this broader literature. There have also been targeted attempts to fix specific issues like the reversal curse with hardcoded augmentations (Golovneva et al., 2024). A closely related work by Aky\u00fcrek et al. (2024) proposes \"deductive closure training\" to improve coverage via prompting language models to generate deductive inferences from training documents. Padmanabhan et al. (2024) similarly propose generating continuations and then distilling them into the model. Several concurrent works Chen et al. ( 2024); Ruan et al. (2025) suggest that having LMs generate additional reasoning directions, and using these to augment their training data, can improve performance on reasoning tasks. Yang et al. (2024) use language models to extract entities in the training documents, and generate synthetic data reasoning about links between these entities; like our results, they find this method of \"rearranging\" knowledge in synthetic data helps to improve downstream performance.",
            "score": 0.4431063539915604,
            "section_title": "Factual learning & generalization:",
            "char_start_offset": 19558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 220,
                    "matchedPaperCorpusId": "262083829"
                },
                {
                    "start": 220,
                    "end": 241,
                    "matchedPaperCorpusId": "269635770"
                },
                {
                    "start": 330,
                    "end": 353,
                    "matchedPaperCorpusId": "262083829"
                },
                {
                    "start": 483,
                    "end": 505,
                    "matchedPaperCorpusId": "269635770"
                },
                {
                    "start": 913,
                    "end": 937,
                    "matchedPaperCorpusId": "258840943"
                },
                {
                    "start": 939,
                    "end": 957,
                    "matchedPaperCorpusId": "271926161"
                },
                {
                    "start": 1334,
                    "end": 1359,
                    "matchedPaperCorpusId": "259165330"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16357421875
        },
        {
            "corpus_id": "269484653",
            "title": "Is Temperature the Creativity Parameter of Large Language Models?",
            "text": "Large language models (LLMs) are applied to all sorts of creative tasks, and their outputs vary from beautiful, to peculiar, to pastiche, into plain plagiarism. The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter. Here, we investigate this claim using a narrative generation task with a predetermined fixed context, model and prompt. Specifically, we present an empirical analysis of the LLM output for different temperature values using four necessary conditions for creativity in narrative generation: novelty, typicality, cohesion, and coherence. We find that temperature is weakly correlated with novelty, and unsurprisingly, moderately correlated with incoherence, but there is no relationship with either cohesion or typicality. However, the influence of temperature on creativity is far more nuanced and weak than suggested by the\"creativity parameter\"claim; overall results suggest that the LLM generates slightly more novel outputs as temperatures get higher. Finally, we discuss ideas to allow more controlled LLM creativity, rather than relying on chance via changing the temperature parameter.",
            "score": 0.4427983830228853,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058349609375
        },
        {
            "corpus_id": "274234789",
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "text": "First, we outline our experimental setup to evaluate the impact of fine-tuning on the reasoning abilities of LLMs and then present our results. Datasets. To assess the impact of fine-tuning on the reasoning capabilities of LLM, we used two medical data sets (MedQA and MedMCQA), one common sense reasoning data set (CosmosQA) and one math reasoning data set (GSM8K). We use one dataset from each of these categories for finetuning the LLM and we use all the datasets to evaluate the effect of fine-tuning on the given dataset. With slight abuse of notation, we will denote any test dataset as in-distribution (IID) if the dataset belongs to the same category as the fine-tuning dataset and out-of-distribution (OOD) otherwise. Next, we describe each of the four datasets. i) MedQA (Jin et al., 2021): Multiple choice question answers from the United States Medical License Exams (USMLE), ii) MedMCQA (Pal et al., 2022): Multiple choice question answers from the All India Institute of Medical Sciences (AIIMS) and National Eligibility cum Entrance Exam (NEET), iii) CosmosQA (Huang et al., 2019): Multiple choice questions formulated from commonsense-based reading comprehensions, and iv) GSM8K (Cobbe et al., 2021): Math word problems from diverse grades. \n\nModels. We work with three LLMs: a 4bit quantized Llama-3-8b-Instruct model (Abhimanyu Dubey, 2024) provided by Unsloth library, GPT-3.5-0125, and GPT-4 known for their exceptional reasoning capabilities on various tasks. \n\nImplementation details. The primary goal of this paper is to explore the effects of vanilla finetuning, i.e., fine-tuning without incorporating reasoning steps in the responses, on the CoT reasoning capabilities of the model. However, achieving accuracy improvements on the GSM8K dataset proved challenging when fine-tuning the model using only question-answer pairs from the GSM8K training set.",
            "score": 0.44252589792614583,
            "section_title": "Experimental Setup",
            "char_start_offset": 12744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 781,
                    "end": 799,
                    "matchedPaperCorpusId": "221970190"
                },
                {
                    "start": 900,
                    "end": 918,
                    "matchedPaperCorpusId": "247763070"
                },
                {
                    "start": 1075,
                    "end": 1095,
                    "matchedPaperCorpusId": "202540590"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.167236328125
        },
        {
            "corpus_id": "267522853",
            "title": "Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach",
            "text": "GPT-3.5 or GPT-4) have shown superior domain generalization ability (Wang et al., 2023;Yang et al., 2023) over the fine-tuned model, we introduce an innovative approach based on the programmer-interpreter framework (Reed and de Freitas, 2016), which benefits from the domain generalization ability from LLMs. The program-mer component -a smaller language model finetuned on task-specific data -delivers precise edit instructions to the larger language model, thus infusing the large model with task-specific knowledge. The interpreter, in turn, edits the large model's output given the provided instructions. Contrary to the Self-Correct (Welleck et al., 2022) approach that utilizes smaller, fine-tuned models for editing, our interpreter is also an LLM. The editing is accomplished through the use of prompts that include editing instructions, eliminating the need for any additional fine-tuning. This distinct framework guarantees the preservation of the LLM's domain generalization ability while simultaneously benefiting from the task-specific knowledge encoded by the programmer. Our method distinguishes itself from approaches like PiVe (Han et al., 2023), which also employ an LLM as the interpreter but focus on graph generation tasks. In contrast, our approach specifically designs word-level editing actions in the instructions, tailored to enhance text generation. This targeted strategy renders our method more effective for text-generation tasks. \n\nOverall, our key contributions are as follows: \n\n\u2022 We introduce a novel programmer-interpreter method that enhances LLM in low-resource cross-domain text generation tasks. This approach capitalizes on the programmer's ability to encode task-specific knowledge and the interpreter's prowess in domain generalization. \n\n\u2022 We design editing operations optimized for text generation tasks, leading to substantial text quality improvements by simply prompting the LLMs with action instructions. \n\n\u2022 In scenarios where training and test data span different domains, our comprehensive empirical studies confirm that the method outperforms all existing LLM post-editing baselines in low-resource MT and LF-to-Text.",
            "score": 0.44236797742392864,
            "section_title": "Introduction",
            "char_start_offset": 1584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 87,
                    "matchedPaperCorpusId": "257102461"
                },
                {
                    "start": 215,
                    "end": 242,
                    "matchedPaperCorpusId": "7034786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08740234375
        },
        {
            "corpus_id": "254877399",
            "title": "Large Language Models Are Reasoning Teachers",
            "text": "Language models (LMs) have demonstrated remarkable performance in a wide range of downstream tasks. Recently, large language models (LLMs) have demonstrated in-context generalization capabilities: performing downstream tasks simply by conditioning on few in-context exemplars or plain natural language task descriptions (Brown et al., 2020;Sun et al., 2021). Despite these advancements, even the largest LLMs have been found to struggle with complex tasks which require multiple reasoning steps (Rae et al., 2021). Figure 1: Fine-tune-CoT uses teacher-generated reasoning to teach students. We prompt a very large teacher model, such as GPT-3 175B, to solve complex questions via zero-shot chain-of-thought reasoning. We then use the reasoning samples to fine-tune a much smaller student model. See Figure 2 for details.\n\nTo solve complex tasks, recent works show that it is possible to elicit reasoning abilities by prompting LLMs to perform chain-of-thought (CoT) reasoning, i.e., generate a series of intermediate reasoning steps. This can be achieved by providing CoT demonstrations as exemplars in prompting (Wei et al., 2022b). More recently, Kojima et al. (2022) found that LLMs can be prompted to perform CoT reasoning simply by providing a natural language instruction to think step-by-step.\n\nA major drawback of prompt-based CoT reasoning methods, however, is their reliance on extremely large models that span hundreds of billions of parameters (Wei et al., 2022b;Kojima et al., 2022). These models are prohibitive to deploy at scale due to overwhelming computational requirements and inference costs (Wei et al., 2022b).",
            "score": 0.44197231655987473,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0872802734375
        },
        {
            "corpus_id": "268666984",
            "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach",
            "text": "As illustrated in Fig. 3, these abilities manifest even with minimal parameters (as low as 0.01 billion).Our research indicates a consistent increase in the capabilities of the models from the outset up to the point where 60% of the data is utilized, aligning with the observations reported in Schaeffer et al. (2023).However, the aspect of unpredictability, which our study identifies beyond the 60% data threshold, was not observed or reported in the findings of Schaeffer et al. (2023).However, \"GMS8K\" and \"Winogrande\" seems to keep increasing, which are consistent with some research (Cobbe et al., 2021a;Chowdhery et al., 2023;Touvron et al., 2023).It is possible that mathematical reasoning ability does not show unpredictable tendency when the parameter is up to 180B.In the current study, a degree of unpredictability is evident when the parameter size exceeds 7B.In essence, the abilities of LLMs tend to scale almost linearly with parameter sizes up to 7B.Beyond this threshold, their performance becomes more predictable (Ganguli et al., 2022).The applicability of the scaling law appears to be confined within a specific range and may also vary depending on the nature of the tasks involved (Kaplan et al., 2020).\n\nWhile our findings diverge from previous research on emergent abilities in LLMs, this does not necessarily negate the concept entirely.Rather, it suggests a reevaluation: the abilities of LLMs continue to improve with increasing training sizes, yet performance becomes less predictable as the training size reaches extremely large scales.This indicates that simply expanding the training size may not be a consistently reliable method for enhancing the capabilities of LLMs (Patel, 2024).\n\nIn the following, we focus on the interplay among various abilities in LLMs.Our findings suggest that knowledge reasoning and language understanding could have an overall impact on the other capabilities of LLMs.Burnell et al. (2023) used the methods of factor analysis and correlation to identify three essential capabilities based on HELM Liang et al. (2022): language comprehension, language modeling, and reasoning, and their finding is basically consistent with our findings.",
            "score": 0.44163794461480865,
            "section_title": "Discussion",
            "char_start_offset": 31779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1226
                },
                {
                    "start": 1228,
                    "end": 1363
                },
                {
                    "start": 1363,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1716
                },
                {
                    "start": 1718,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1930
                },
                {
                    "start": 1930,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 610,
                    "end": 633,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1033,
                    "end": 1055,
                    "matchedPaperCorpusId": "246867298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0205230712890625
        },
        {
            "corpus_id": "278501703",
            "title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
            "text": "In the context of early sequential language tasks, encompassing both natural and programming languages, task-specific model fine-tuning has shown promising performance [20]. Fine-tuning involves updating the model weights and enhancing its performance on specific tasks by learning the relationship between the input and output of a specific downstream task dataset. \n\nAs a phenomenon-level technology, the emergence and rapid development of large language models has triggered disruptive changes in related fields. For example, ChatGPT [66], LLama [84], Claude [4], etc., which usually contain billions or even hundreds of billions of parameters, have been trained on massive text data, and have powerful language understanding and generation capabilities. Since LLMs encapsulate comprehensive knowledge, they can be applied to downstream tasks using a novel method called in-context learning [16], eliminating the need for extensive downstream datasets for fine-tuning. In-context learning allows the model to perform specific tasks directly by providing task-related contextual information without updating its parameters [9]. \n\nIn this paper, we adopt the method of in-context learning to guide LLMs to understand the binary analysis tasks from multiple perspectives, thus facilitating a comprehensively evaluation of LLMs' performance on binary analysis.",
            "score": 0.4416146080806772,
            "section_title": "Large Language Models",
            "char_start_offset": 9361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1359
                }
            ],
            "ref_mentions": [
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0489501953125
        },
        {
            "corpus_id": "273549561",
            "title": "Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems",
            "text": "LLMs have demonstrated significant advancements in both mathematical reasoning and creative capabilities, making them increasingly powerful tools in a variety of domains. In the realm of mathematical reasoning, techniques such as prompt engineering, Chain-of-Thought (CoT) prompting, and program-aided language modeling have notably enhanced LLMs' abilities to solve complex problems (Brown, 2020;Wei et al., 2022;Zhou et al., 2023). These approaches enable models to break down problems into more manageable steps, thereby improving their accuracy and reasoning depth. Moreover, specialized models like MathVerse (Zhang et al., 2024) and Internlm-Math (Ying et al., 2024b), which are trained on extensive mathematical corpora, have achieved significant improvements in mathematical problem-solving performance (Lewkowycz et al., 2022;Ying et al., 2024b). Benchmarks such as GSM8K and MATH further provide a structured means to evaluate and compare these advancements, highlighting the continuous progress in this area (Cobbe et al., 2021;Hendrycks et al., 2021b). \n\nIn terms of creativity, LLMs have shown remarkable prowess across diverse fields. They have excelled in generating high-quality, human-like content, ranging from code generation (Ni et al., 2023;Liu et al., 2024a) and music composition (Yuan et al., 2024) to literature (G\u00f3mez-Rodr\u00edguez & Williams, 2023;Liu et al., 2024b) and educational tools (Lan & Chen, 2024;Orenstrakh et al., 2023). Creativity in LLMs is often evaluated using frameworks like Margaret Boden's criteria (Franceschelli & Musolesi, 2023) and the Torrance Tests of Creative Thinking (TTCT) (Torrance, 1966), where they have demonstrated high fluency, originality, and flexibility. However, the applicability of these traditional creativity metrics to AI systems is still a topic of debate, as they were originally designed to assess human creativity (Zhao et al., 2024).",
            "score": 0.441433691424344,
            "section_title": "RELATED WORK",
            "char_start_offset": 3822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 414,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1245,
                    "end": 1262,
                    "matchedPaperCorpusId": "256900680"
                },
                {
                    "start": 1262,
                    "end": 1280,
                    "matchedPaperCorpusId": "258437095"
                },
                {
                    "start": 1371,
                    "end": 1389,
                    "matchedPaperCorpusId": "269748457"
                },
                {
                    "start": 1626,
                    "end": 1642,
                    "matchedPaperCorpusId": "141073971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046722412109375
        },
        {
            "corpus_id": "266899568",
            "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
            "text": "Model editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model\u2019s behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing\u2019s improvements on factuality may come at the cost of a significant degradation of the model\u2019s general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT is proposed to regularize the edit update weights by imposing constraints on their complexity based on the RElative Change in weighT. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.",
            "score": 0.4413500039906212,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "266053666",
            "title": "Towards Knowledge-driven Autonomous Driving",
            "text": "Recently, LLMs have achieved remarkable performance. These models have achieved remarkable performance by leveraging extensive training on massive text datasets, showcasing powerful text generation and comprehension capabilities. LLMs have demonstrated their competence in understanding natural language and tackling diverse complex tasks [104], emerging as a milestone in the development of knowledge-driven methods. Some notable examples of LLMs include GPT-3 [105], PaLM [106], LLaMA [107], and GPT-4 [108]. Notably, the emergent capability in LLMs is one of their most distinguishing features compared to smaller language models. Specifically, capabilities such as contextual learning [109], instruction following [110], [111], and chain of thought reasoning [112] are three typical emergent abilities in LLMs. Specifically, ChatGPT [113] and GPT-4 [108] represent significant advancements in LLM capabilities, especially in natural language understanding and generation. It's worth noting that LLMs are seen as equipped with human-like intelligence and common sense to hold the potential to bring us closer to the field of Artificial General Intelligence (AGI) [104], [114]. Remarkable breakthroughs in LLMs underscore the critical importance of highquality data. These models exhibit robust reasoning capabilities and also possess emergent capacity, which lays a solid foundation for the development of knowledge-driven autonomous driving.",
            "score": 0.4412324357921329,
            "section_title": "LLM: A Milestone for Knowledge-driven Approaches",
            "char_start_offset": 18547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1445
                }
            ],
            "ref_mentions": [
                {
                    "start": 689,
                    "end": 694,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 718,
                    "end": 723,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 763,
                    "end": 768,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030670166015625
        },
        {
            "corpus_id": "258686278",
            "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
            "text": "Progress in Math-NLP: Several notable publications in the Math-NLP space have made rapid strides in numeracy-tinged language-modeling (Thawani et al., 2021) -from investigations of the inherent deficiency of numerical reasoning skills in LLMs induced through unsupervised training, both for numerals that appear in the training corpus (Zhang et al., 2020) and OOD (out-of-domain) numerals (Wallace et al., 2019;Razeghi et al., 2022), to interventions that strengthen the numerical reasoning skills of these models (Spithourakis and Riedel, 2018;Jiang et al., 2020;Geva et al., 2020). Further, advances in chain-of-thought prompting in few-shot learning settings (Li et al., 2022) Table 1: LLMs trained for niche non-linguistic skill-sets forget linguistics: Comparative analysis between the performance of the base BERT model and the same model further trained on an arithmetic reasoning corpus on the set of 9 GLUE tasks for natural language understanding. All tasks except WNLI suffer severe performance degradation as a consequence of continued training on a non-linguistic corpus.\n\ntask-specific fine-tuning (Lewkowycz et al., 2022) have shown significant gains in the capacity for quantitative reasoning in LLMs.\n\nLinguistic evaluation remains important: As notable as these accomplishments are, the goal remains not to replicate the reasoning capabilities of a grade-schooler or to proxy a calculator, but rather build LLMs that are empowered with these skills. As such, an area that often goes unaddressed in the Math-NLP space is how these models perform as general language modelers. With the advent and popularity of generative conversational models (OpenAI, 2022), the goal is to have one model capable of a host of skills -not to load separate models for conversation/assistance and reasoning. As depicted in Figure 1, whether a model is designed to perform strict non-linguistic tasks or semi-linguistic tasks, it should never come at the cost of core linguistic competency. After all, language models are intended to model language.",
            "score": 0.44099676203226273,
            "section_title": "Re-thinking the Objective of Math-NLP",
            "char_start_offset": 1199,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 156,
                    "matchedPaperCorpusId": "232335764"
                },
                {
                    "start": 335,
                    "end": 355,
                    "matchedPaperCorpusId": "222290454"
                },
                {
                    "start": 389,
                    "end": 411,
                    "matchedPaperCorpusId": "202583694"
                },
                {
                    "start": 514,
                    "end": 545,
                    "matchedPaperCorpusId": "29150573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.109130859375
        },
        {
            "corpus_id": "269005156",
            "title": "Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding",
            "text": "Datasets To evaluate LLMs' performance on logical reasoning, we used four representative datasets including FOLIO (Han et al., 2022), TaxiNLI (Joshi et al., 2020), LogiQA (Liu et al., 2020), and Reclor (Yu et al., 2020) in our experiments.\n\nFOLIO focuses on first-order logic reasoning (FOL) that is a classical deductive reasoning task.TaxiNLI is specific to natural language inference (NLI) that tests the logical relationship between a premise and a hypothesis.LogiQA and Reclor are the multi-choice reading comprehension (MRC) datasets, which choose the most suitable answer corresponding to the given text, could better reflect comprehensive logical reasoning abilities.The instances of the four datasets are shown in Appendix D. In addition to the training data in above four datasets and our LFUD, we also used the logical fallacy data LOGIC (Jin et al., 2022) to fine-tune LLMs.LOGIC (including LOGIC-CLIMATE) contains thirteen types of logical fallacy sentences, as shown in Appendix B.\n\nLLMs We selected five popular LLMs in our experiments, including LLaMA2-7B, LLaMA2-13B (Touvron et al., 2023), Vicuna-7B, Vicuna-13B (Chiang et al., 2023) and Orca2-7B (Mitra et al., 2023).When fine-tuning these LLMs, we set the learning rate to 2.5e-5 and the batch size to 8. To ensure the robustness of our results, we repeated all experiments for three times and reported the average performance (accuracy) scores.\n\nDataset Split For the 4,020 synthesized instances in our LFUD, we randomly selected 3,000 instances (corresponding to 600 sentences with logical fallacies) as the training set and the remaining 1,020 instances (corresponding to 204 sentences with logical fallacies) as the test set.Given the instances of Task 1-4 (choice questions) have fixed answers, we only used the training samples (2,500 instances) of Task 1-4 to fine-tune the five LLMs.",
            "score": 0.4408686036477917,
            "section_title": "Experiment Setup",
            "char_start_offset": 17469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 241,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 886
                },
                {
                    "start": 886,
                    "end": 995
                },
                {
                    "start": 997,
                    "end": 1186
                },
                {
                    "start": 1186,
                    "end": 1415
                },
                {
                    "start": 1417,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 849,
                    "end": 867,
                    "matchedPaperCorpusId": "247158013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0401611328125
        },
        {
            "corpus_id": "267769989",
            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
            "text": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft.",
            "score": 0.44075536791581676,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13720703125
        },
        {
            "corpus_id": "277434959",
            "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching",
            "text": "Hallucinations are innate features of large language models (LLMs), where LLMs tend to generate plausible sounding but factually incorrect content [38]. There are various causes of this phenomenon. Hallucinations can arise from data, where the data used for training may have misinformation, bias, or knowledge boundaries, or the sample alignment data used for fine-tuning may be of poor quality. Hallucinations are also caused by improper model training, including pretraining and fine-tuning using supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF). Hallucinations may also occur due to imperfect decoding strategies, overconfidence, softmax bottlenecks, and reasoning failure during the inference process [11]. While LLM hallucinations can be helpful for narrative writing [31] and adversarial example generation [36], they have significant drawbacks and hamper the reliability of LLMs in many realworld scenarios [35]. \n\nBenchmarking LLM hallucinations is an essential but critical step to understanding the cause of LLM hallucinations and mitigating the effects. Most benchmark datasets focus on general-purpose question-answering (QA), leaving under-explored LLM hallucinations in domain-specific or task-specific scenarios. For example, hallucinations occur in ontology matching (OM) tasks. Unlike traditional general-purpose QA tasks, OM tasks aim to find aligned entities between two different ontologies, and this process requires a comprehensive understanding of the context based on the ontologies provided. For example, within the context of a research conference, the entity \"chair\" could be correctly mapped to the entity \"chairman\" when it refers to a specific meaning of the person who chairs the conference, but not when it refers to the meaning of furniture that could be used at the conference. \n\nOM tasks are commonly executed by OM software systems.",
            "score": 0.44052321813839107,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "265067168"
                },
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "270285964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1517333984375
        },
        {
            "corpus_id": "271309962",
            "title": "Open Artificial Knowledge",
            "text": "While handcrafted human data has shown significant improvements in supervised fine-tuning (SFT) of LLMs, particularly for tasks like code generation and mathematical reasoning (Roziere et al., 2023;Wan et al., 2024), the scarcity and cost of creating such high-quality data have led to the increasing use of synthetic data as a proxy.This method primarily leverages strongly capable LLMs, such as the GPT family (Achiam et al., 2023) to produce high-quality synthetic data (Li et al., 2023b;Josifoski et al., 2023;Taori et al., 2023).\n\nRecent research has highlighted LLMs' ability to rephrase for improved responses and boost synthetic data for effective SFT (Gallego, 2024;Chen et al., 2024).These developments suggest that the use of synthetic data in model training will continue to grow in the future, with ongoing research exploring various techniques to leverage synthetic data effectively The process begins with extracting general topics from extensive human knowledge databases such as Wikipedia and GPT-4o models.These high-level and sub-level topics are then used in an automatic prompt generation step, which employs two methods: meta prompt engineering using large language models (LLMs) and cost-effective programming prompt engineering.The generated prompts are subsequently fed into state-of-the-art open-source LLMs (at the time of writing, five models were used: Llama3-8B, Llama-70B, Mixtral7x8B, Gemma-7B (Team et al., 2024), and Gemma-2-9B (Team, 2024)) to create the OAK dataset.\n\nfor improving LLM performance and alignment (Hao et al., 2024).",
            "score": 0.44047162659496925,
            "section_title": "Introduction",
            "char_start_offset": 1773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 534
                },
                {
                    "start": 536,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1502
                },
                {
                    "start": 1504,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 491,
                    "end": 514,
                    "matchedPaperCorpusId": "257378179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034759521484375
        },
        {
            "corpus_id": "265281389",
            "title": "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
            "text": "Large Language Models (LLMs) are blurring the boundaries between human languages and machine languages with their powerful text understanding and generation capabilities [34]. LLMs have also had a significant impact on various fields, including software engineering [115]. Currently, a considerable amount of work focuses on evaluating and optimizing the performance of LLMs on software engineering tasks such as code generation [123], code summarization [2], vulnerability mining [114], and more [15,117]. However, many studies adopt a negative stance on the current performance of LLMs in the field of software engineering [51,70,125]. This is largely due to the fact that LLMs may not effectively comprehend the structure and semantics of code, lacking domain-specific knowledge in software engineering [59,85]. Fortunately, we can enhance the performance and adaptability of LLMs in specific tasks within the software engineering domain through fine-tuning and additional training [11], thereby designing LLMs specifically tailored for software engineering tasks. We refer to these LLMs designed for software engineering tasks as Code LLMs [10]. \n\nCode LLMs can not only be generated through fine-tuning but also through traditional pretraining methods [50]. As language models specifically developed for the field of software engineering, ideally, LLMs can have their capabilities further enhanced and possess stronger abilities in code generation, vulnerability mining, and other tasks [55]. Therefore, there is currently a considerable amount of work focusing on this area, resulting in the development of many milestone Code LLMs such as Codex [60], Code Llama [78]. \n\nHowever, due to the uncertainties brought by fine-tuning, training data, and other factors [82,119], as well as the remarkable performance of state-of-the-art general LLMs like GPT-4 in various software engineering domains [98], it is challenging to determine whether the current state-ofthe-art Code LLMs outperform the current state-of-the-art general LLMs in software engineering tasks. Moreover, there is a wide range of Code LLMs available, many of which are derived through fine-tuning on general LLMs or other Code LLMs [21,62,72,120].",
            "score": 0.4403240362442643,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "250426482"
                },
                {
                    "start": 481,
                    "end": 486,
                    "matchedPaperCorpusId": "250562504"
                },
                {
                    "start": 632,
                    "end": 636,
                    "matchedPaperCorpusId": "256389762"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06854248046875
        },
        {
            "corpus_id": "263620236",
            "title": "PrivacyMind: Large Language Models Can Be Contextual Privacy Protection Learners",
            "text": "Large Language Models (LLMs) have demonstrated remarkable linguistic comprehension and generation capability (Bang et al., 2023;Wang et al., 2023a). Meanwhile, when directly applied to specialized industries, they encounter challenges such as hallucination (Chan et al., 2023;Deng et al., 2024;Jin et al., 2024a), insufficient domain expertise (Singhal et al., 2023b), and failing to incorpo-rate the latest domain knowledge in ever-evolving industry scenarios (Kasneci et al., 2023). The introduction of open-source general-purpose LLMs such as LLaMA (Touvron et al., 2023) and RWKV (Peng et al., 2023) have provided a promising solution. Researchers would fine-tune specialized LLMs based on powerful general-purpose LLMs using high-quality, domain-specific knowledge to ensure both commonsense reasoning and comprehensive knowledge coverage (Hoffmann et al., 2022a,b;Villalobos et al., 2022;Yang et al., 2024). Such examples include BloombergGPT (Wu et al., 2023) and Med-PaLM (Singhal et al., 2023a), for financial and medical applications, respectively. However, these fine-tuning datasets usually contain sensitive information, such as personally identifiable information (PII) (Carlini et al., 2020;Lin et al., 2021;Gehman et al., 2020). When applied to downstream tasks, sensitive information in the training data, such as social security numbers or patient names, can be exposed by the LLMs upon text generation, a phenomenon known as the memorization effect (Yu et al., 2023b;Kenton and Toutanova, 2019;Meng et al., 2023) or inference-time privacy threat (Mireshghallah et al., 2024), leading to identity theft and financial losses (Coavoux et al., 2018;Yu et al., 2023a). \n\nChallenges.",
            "score": 0.44021349985569913,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 147,
                    "matchedPaperCorpusId": "257102461"
                },
                {
                    "start": 980,
                    "end": 1003,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1223,
                    "end": 1243,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1468,
                    "end": 1486,
                    "matchedPaperCorpusId": "256697118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05419921875
        },
        {
            "corpus_id": "271533629",
            "title": "Intelligence Analysis of Language Models",
            "text": "For instance, in a study conducted by (Xu et al., 2023), where GPT was tested on ARC, the success rate was far below 30%. This lack of advancement highlights a critical aspect: despite progress in deep learning and particularly in language models, LLMs remain challenged in abstract reasoning. Our analysis contributes to the growing body of evidence suggesting that current LLMs are a considerable distance from achieving Artificial General Intelligence (AGI). The consistent inability of LLMs to excel in abstract reasoning, even when leveraging techniques like CoT, highlights an important limitation in their current design and functioning. \n\nWhile our study represents a step forward in un-derstanding how Large Language Models (LLMs) handle reasoning, it is important to recognize its limitations. We relied on only two prompting techniques, and it is possible that alternative techniques could yield improved results. Furthermore, our focus was limited to open-source LLMs, which may not encompass the full spectrum of capabilities present in more advanced models. This limitation can be seen by the better results achieved with GPT (OpenAI, 2020, 2023), as demonstrated in (Xu et al., 2023). Another limitation was the focus on only the 50 easiest tasks in ARC, standard approach took place in (Xu et al., 2023). This approach underlines the fact that LLMs still have a considerable way to go in solving complex tasks that necessitate abstract and visual reasoning skills. \n\nFuture work could expand on our findings by exploring a variety of prompting techniques, such as Tree-of-Thoughts (ToT) (Yao et al., 2023), an extension of CoT (Wei et al., 2022). Additionally, within the CoT method, a broader selection of examples from the dataset could be included in the prompts. For instance, instead of using just one example task as in our study, including multiple example tasks in the prompt that correspond to each of ARC's core knowledge priors -each with a tailored step-by-step reasoning processcould potentially improve the models' ability to successfully tackle a wider range of tasks from the dataset. Furthermore, the use of more advanced models, perhaps fine-tuned for abstract reasoning, could be beneficial.",
            "score": 0.4398719903795167,
            "section_title": "Discussion",
            "char_start_offset": 13178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2226
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08978271484375
        },
        {
            "corpus_id": "267682172",
            "title": "Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence",
            "text": "\u2022 AGIEval [12]: The study utilized the Chain-of-Thought prompting technique to assess models' reasoning capabilities, but it noted variability in performance improvements across tasks, suggesting a challenge in conclusively determining genuine reasoning abilities versus technical optimization. \n\n\u2022 ToolAlpaca [47]: The study created and evaluated models using simulated tool-use scenarios, without explicitly addressing the distinction between genuine reasoning and optimization to benchmark specifications, exemplified by the training and evaluation of models on a constructed corpus without mechanisms to differentiate between true comprehension and mere performance optimization. \n\n\u2022 HELM [31]: The HELM evaluation framework explicitly aimed to address various facets of language model capabilities, including reasoning, but the inherent challenge of distinguishing between genuine reasoning and technical optimization in responses remains a significant concern due to the models' \"black box\" nature and the complexity of interpreting LLM outputs beyond mere performance metrics. \n\n\u2022 ToolBench [48]: The study implemented a depth-first search-based decision tree (DFSDT) to enhance the planning and reasoning ability of LLMs, indicating an effort to discern genuine reasoning from technical optimization. However, the primary focus was on improving LLMs' ability to interact with APIs rather than directly addressing the benchmark's potential inadequacy in distinguishing genuine reasoning capabilities from mere technical optimization. \n\n\u2022 PromptBench [7]: The study acknowledged the challenge of discerning genuine reasoning from technical optimization in LLMs, indicating that it primarily evaluated models based on their ability to follow instructions and generate outputs without deeply investigating whether these outputs resulted from true understanding or merely optimization strategies. This suggested that the benchmarks could not fully distinguish between genuine reasoning and technical optimization, reflecting the described inadequacy. \n\n\u2022 AgentBench [49]: The study did not provide explicit methods to differentiate whether LLMs' responses were the result of genuine reasoning or merely technical optimizations, focusing instead on evaluating LLMs' ability to act as agents in various environments without addressing this specific concern. \n\n\u2022 APIBank [50]: The study detailed an evaluation system for tool-augmented LLMs, without directly addressing the challenge of determining whether LLM responses stem from genuine reasoning or merely technical optimization to match benchmark answers.",
            "score": 0.4398686537594759,
            "section_title": "B. Genuine Reasoning vs Technical Optimization",
            "char_start_offset": 86600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2053
                },
                {
                    "start": 2056,
                    "end": 2358
                },
                {
                    "start": 2361,
                    "end": 2609
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "258108259"
                },
                {
                    "start": 1557,
                    "end": 1560,
                    "matchedPaperCorpusId": "266191741"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06878662109375
        },
        {
            "corpus_id": "263908902",
            "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
            "text": "There have been recent attempts to tune input prompts to improve the performance of LLMs further (Liu et al., 2023b;Lester et al., 2021;Li and Liang, 2021;Qin and Eisner, 2021;Liu et al., 2022Liu et al., , 2023a)). However, directly optimizing prompts is not trivial since changes in the input space may cause non-monotonic performance changes (Hu et al., 2022). Especially, Fichtel et al. (2021) demonstrate that finetuned LMs outperform prompt-tuned LMs on factual knowledge probing tasks. Although LLMs, such as GPT-3 and T0, were primitively designed to perform well on various tasks without finetuning (Brown et al., 2020;Sanh et al., 2022), recent work shows that finetuning improves the linguistic capabilities of LLMs substantially (Ouyang et al., 2022;Wei et al., 2022). Therefore, we consider finetuned LMs for analysis.",
            "score": 0.4396506628741437,
            "section_title": "Prompt Tuning and Finetuning",
            "char_start_offset": 5107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 830
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 136,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 136,
                    "end": 155,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 155,
                    "end": 176,
                    "matchedPaperCorpusId": "233231453"
                },
                {
                    "start": 176,
                    "end": 192,
                    "matchedPaperCorpusId": "248780177"
                },
                {
                    "start": 344,
                    "end": 361,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 607,
                    "end": 627,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 627,
                    "end": 645,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 740,
                    "end": 761,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 761,
                    "end": 778,
                    "matchedPaperCorpusId": "237416585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09185791015625
        },
        {
            "corpus_id": "269785051",
            "title": "A systematic literature review on the impact of AI models on the security of code generation",
            "text": "During the training process, LLMs are scored on their ability to autoencode, that is, to accurately reproduce their input (in the face of a partially occulted input).In the context of natural language, minor errors are often acceptable and almost always have little to no impact on the meaning or understanding of a sentence.Such is not the case for code, which can be particularly sensitive to minor variations, especially for low-level programming languages.A stricter training regimen could score an LLM based not only on syntactic correctness, but on (some degree of) semantic correctness, to limit the extent to which the model wanders away from a valid program.Unfortunately, experimental data from Liguori et al. (2023) suggests that currently no single metric succeeds at that task.\n\nAlternatively, since most LLMs today come pre-trained, a better fine-tuning step could reduce the risks associated with incorrect code generation.He and Vechev (2023) took this approach and had promising results in the CWE they investigated.However, there is conflicting evidence.Evidence from Wu et al. (2023) seems to indicate that this approach is inherently limited to fixing a very narrow, and simple class of bugs.More studies analyzing the impact of fine-tuning models with curated security datasets are needed to assess the impact of this mitigation strategy.",
            "score": 0.439253066060612,
            "section_title": ". . Training procedure",
            "char_start_offset": 60302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 790
                },
                {
                    "start": 792,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1072
                },
                {
                    "start": 1072,
                    "end": 1212
                },
                {
                    "start": 1212,
                    "end": 1359
                }
            ],
            "ref_mentions": [
                {
                    "start": 1086,
                    "end": 1102,
                    "matchedPaperCorpusId": "258967736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0472412109375
        },
        {
            "corpus_id": "269156935",
            "title": "TEL'M: Test and Evaluation of Language Models",
            "text": "LMs have demonstrated abilities on a wide range of tasks, from concrete and relatively objective, such as authoring computer programs [28], to highly subjective, such as authoring creative prose or poetry [31].\n\nWhile it is technically interesting to ask if an LM trained on a generic corpus of English language can effectively respond to arithmetic [77] or planning [66] prompts, in general we assume that most end-user concerns involve LMs that are trained to perform specific kinds of domain-specific tasks and that rigorous test and evaluation of those LMs will be done on samples of the same kinds of tasks.Accordingly, it is standard practice to consider LM-based tasks that are consistent with the design and training of an LM, with the general exception of deliberately testing out-of-distribution (OOD) performance on tasks for which the LM was not specifically trained.With this understanding, example tasks that would be appropriate for test and evaluation are, for instance:\n\n\u2022 Example: TASK A: Use text and image prompts in a multimodal language model to answer questions about the image, to caption images semantically and/or synthesize novel properties of the image;\n\n\u2022 Example: TASK B: Synthesize programs that implement APIs for a complex cloud computation whose components are user or third party sourced;\n\n\u2022 Example: TASK C: Integrate symbolic reasoning with neural processing to answer natural language prompts that produce quantitative and logically consistent military Courses of Action that are presented in a predefined, structured Document Definition Language (DDL).\n\nIdeally, Tasks of Interest should be defined well enough to anticipate the properties one might ask of an LM performing that task.A Task of Interest should have at least these characteristics:\n\n\u2022 Concreteness: The Task of Interest should be describable at a high semantic level in prose for formulating hypotheses about the LM's performance.Examples of desired performance should be documented and explained clearly.For example, the tasks of \"knowing a lot of facts\" and \"doing my job\" are not concrete enough for meaningful testing;\n\n\u2022 Consistency: The task should be consistent with the design, training data and actual training of the LM, although as noted above, it can be reasonable to explore those boundaries.",
            "score": 0.43923791042673166,
            "section_title": "LM Tasks",
            "char_start_offset": 7935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 212,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 986
                },
                {
                    "start": 988,
                    "end": 1181
                },
                {
                    "start": 1183,
                    "end": 1323
                },
                {
                    "start": 1325,
                    "end": 1591
                },
                {
                    "start": 1593,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1785
                },
                {
                    "start": 1787,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2009
                },
                {
                    "start": 2009,
                    "end": 2126
                },
                {
                    "start": 2128,
                    "end": 2309
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "252519594"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07586669921875
        },
        {
            "corpus_id": "269983774",
            "title": "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning",
            "text": "Large language models (LLMs) have demonstrated impressive abilities in generalizing to previously unseen tasks (Mishra et al., 2022;Wei et al., 2022;Chung et al., 2022;Zhao et al., 2023;Cai et al., 2024). Instruction tuning has emerged as a key technique for aligning pre-trained LLMs with user preferences, achieved by supervised fine-tuning (SFT) on datasets annotated with instructional prompts (Wei et al., 2022;Chung et al., 2022;Wang et al., 2023c). Distinct from conventional task-specific fine-tuning, it leverages the broad knowledge that LLMs accumulate during pre-training, often involving a wide range of tasks. \n\nWith the availability of APIs for powerful proprietary LLMs, such as ChatGPT, various approaches have been proposed to distill these black-box LLMs into smaller counterparts. These methods involve automatic generation of instructional prompts and their corresponding outputs (Wang et al., 2023c;Xu et al., 2024a;Jiang et al., 2023;Li et al., 2023a). Empirical studies have illustrated that enhancing the diversity and complexity of instruction tuning datasets can improve the model performance (Xu et al., 2024a;Liu et al., 2024). Quality outweighs quantity; thus fine-tuning over a carefully calibrated, smaller dataset may outperform instructtuned models trained on larger datasets (Zhou et al., 2023;Li et al., 2023b;Lu et al., 2023). \n\nDespite the advances, the optimal complexity of instructional data for models with varying capacities and parameters remains an open question. Prior efforts have sought to maximize data diversity through the utilization of sentence embeddings (Liu et al., 2024;Feng et al., 2023). Yet, this approach has not fully resolved the issue of imbalanced model capabilities, as the maximum diversity of sentence embeddings does not necessarily lead to the optimal task ratio. We observe that models fine-tuned with these methods sometimes struggle with more complex and challenging tasks, such as logical reasoning.",
            "score": 0.4390985078780587,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 132,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 132,
                    "end": 149,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 168,
                    "end": 186,
                    "matchedPaperCorpusId": "266163842"
                },
                {
                    "start": 398,
                    "end": 416,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 435,
                    "end": 454,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 901,
                    "end": 921,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 921,
                    "end": 938,
                    "matchedPaperCorpusId": "271745981"
                },
                {
                    "start": 938,
                    "end": 957,
                    "matchedPaperCorpusId": "258833333"
                },
                {
                    "start": 1120,
                    "end": 1138,
                    "matchedPaperCorpusId": "271745981"
                },
                {
                    "start": 1138,
                    "end": 1155,
                    "matchedPaperCorpusId": "266551413"
                },
                {
                    "start": 1310,
                    "end": 1329,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1346,
                    "end": 1362,
                    "matchedPaperCorpusId": "260887200"
                },
                {
                    "start": 1609,
                    "end": 1627,
                    "matchedPaperCorpusId": "266551413"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1209716796875
        }
    ],
    "quotes": {
        "cost": 0.041016000000000004,
        "quotes": [
            {
                "idx": 0,
                "key": "[261031244 | Luo et al. | 2023 | Citations: 318]",
                "snippets": "Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 185,
                        "end": 459,
                        "sentence_offsets": [
                            {
                                "start": 185,
                                "end": 295
                            },
                            {
                                "start": 296,
                                "end": 459
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[266899568 | Gu et al. | 2024 | Citations: 55]",
                "snippets": "Experimental results show that existing LLMs are not robust to weight perturbations, and editing even a few parameters can significantly affect their general abilities. Strikingly, with a single pass of editing involving less than 1% parameters, LLaMA-1 (7B) exhibited a drastic performance degradation to nearly 0 on all the tasks we tried. These results demonstrate that current editing algorithms struggle to work effectively in tandem with LLMs to simultaneously improve model factuality and maintain general abilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1164,
                        "end": 1687,
                        "sentence_offsets": [
                            {
                                "start": 1164,
                                "end": 1332
                            },
                            {
                                "start": 1333,
                                "end": 1505
                            },
                            {
                                "start": 1506,
                                "end": 1687
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Experimental results show that existing LLMs are not robust to weight perturbations, and editing even a few parameters can significantly affect their general abilities. Strikingly, with a single pass of editing involving less than 1% parameters, LLaMA-1 (7B) exhibited a drastic performance degradation to nearly 0 on all the tasks we tried. These results demonstrate that current editing algorithms struggle to work effectively in tandem with LLMs to simultaneously improve model factuality and maintain general abilities."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[270068369 | Contal et al. | 2024 | Citations: 5]",
                "snippets": "Fine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1083,
                        "end": 1769,
                        "sentence_offsets": [
                            {
                                "start": 1083,
                                "end": 1245
                            },
                            {
                                "start": 1246,
                                "end": 1295
                            },
                            {
                                "start": 1296,
                                "end": 1462
                            },
                            {
                                "start": 1463,
                                "end": 1613
                            },
                            {
                                "start": 1614,
                                "end": 1769
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[270123515 | Stap et al. | 2024 | Citations: 12]",
                "snippets": "Our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 202,
                        "end": 1197,
                        "sentence_offsets": [
                            {
                                "start": 117,
                                "end": 288
                            },
                            {
                                "start": 289,
                                "end": 391
                            },
                            {
                                "start": 392,
                                "end": 493
                            },
                            {
                                "start": 494,
                                "end": 707
                            },
                            {
                                "start": 708,
                                "end": 901
                            },
                            {
                                "start": 902,
                                "end": 1072
                            },
                            {
                                "start": 1073,
                                "end": 1197
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[271709856 | Tam et al. | 2024 | Citations: 31]",
                "snippets": "we observe a significant decline in LLMs' reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 453,
                        "end": 665,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "we observe a significant decline in LLMs' reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[274234789 | Lobo et al. | 2024 | Citations: 10]",
                "snippets": "Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes....prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)......Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 824,
                        "end": 1459,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 9,
                        "end": 181,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 463
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": ""
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1825,
                        "end": 2201,
                        "sentence_offsets": [
                            {
                                "start": 1825,
                                "end": 2019
                            },
                            {
                                "start": 2020,
                                "end": 2201
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[276317504 | Banerjee et al. | 2025 | Citations: 7]",
                "snippets": "Recent works such as (Tam et al., 2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. (Tam et al., 2024) attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[271709856 | Tam et al. | 2024 | Citations: 31]": "Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).This study investigates whether such constraints on generation space impact LLMs\u2019 abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs\u2019 performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs\u2019 reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 319,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            },
                            {
                                "start": 176,
                                "end": 319
                            }
                        ],
                        "ref_mentions": [
                            "271709856",
                            "271709856"
                        ],
                        "quote": "Recent works such as (Tam et al., 2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. (Tam et al., 2024) attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.017364,
        "cot": "I need to structure the quotes into meaningful dimensions that address the user's query about empirical studies documenting degradation in language models' abilities after fine-tuning for specific tasks.\n\nFor this query, I'll organize the dimensions as follows:\n\n1. \"Introduction to LLM Fine-tuning and Degradation\" - This will be a synthesis section to provide background on what fine-tuning is and introduce the concept of capability degradation. This will help contextualize the specific studies mentioned. There are no direct quotes for this introductory section, but it's necessary to frame the rest of the answer.\n\n2. \"Catastrophic Forgetting in Continual Fine-tuning\" - This will be a synthesis section focusing on studies that document catastrophic forgetting, where LLMs lose previously acquired abilities after fine-tuning. Quotes 0 and 2 specifically mention this phenomenon.\n\n3. \"Degradation in Reasoning Abilities\" - This will be a synthesis section examining studies that show how fine-tuning impacts reasoning capabilities, including Chain-of-Thought reasoning. Quotes 4, 5, and 6 provide evidence for this dimension.\n\n4. \"Impact on Translation and Language Generation Capabilities\" - This will be a synthesis section covering how fine-tuning affects translation quality and other language generation capabilities. Quote 3 provides detailed information about this.\n\n5. \"Effects of Parameter Editing and Weight Perturbations\" - This will be a synthesis section addressing how even small parameter changes can significantly impact model performance. Quote 1 offers insights into this aspect.\n\n6. \"Factors Influencing Degradation Severity\" - This will be a synthesis section examining the factors that influence how severe the degradation is (model size, dataset size, etc.). Multiple quotes (0, 3, 5) mention factors affecting degradation severity.\n\nEach of these dimensions requires a synthesis format rather than a list because they involve complex relationships and analyses rather than simple enumeration of items. The quotes contain nuanced findings that need to be explained in context.",
        "plan": {
            "Introduction to LLM Fine-tuning and Degradation (synthesis)": [],
            "Catastrophic Forgetting in Continual Fine-tuning (synthesis)": [
                0,
                2
            ],
            "Degradation in Reasoning Abilities (synthesis)": [
                4,
                5,
                6
            ],
            "Impact on Translation and Language Generation Capabilities (synthesis)": [
                3
            ],
            "Effects of Parameter Editing and Weight Perturbations (synthesis)": [
                1
            ],
            "Factors Influencing Degradation Severity (synthesis)": [
                0,
                3,
                5
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to LLM Fine-tuning and Degradation",
                "tldr": "Fine-tuning large language models (LLMs) for specific tasks can lead to degradation of their general capabilities, a phenomenon documented in several empirical studies. While fine-tuning improves performance on targeted tasks, it often comes at the cost of reduced performance in other domains. (LLM Memory)",
                "text": "\nFine-tuning is a technique that adapts pre-trained language models to specific tasks by updating their parameters on task-specific data. While this process effectively enhances performance on the targeted task, empirical evidence suggests it can lead to degradation in the model's general capabilities. This phenomenon occurs because the model optimizes for the distribution of the fine-tuning data, potentially at the expense of its broader knowledge and abilities. Several studies have systematically documented this trade-off. For instance, when LLMs are fine-tuned on narrow domains like medical text or customer service responses, they often show reduced creative writing abilities, lower performance on general reasoning tasks, and less diverse language generation. This degradation is particularly pronounced when fine-tuning is aggressive (using high learning rates) or when the fine-tuning dataset significantly differs from the distribution of the pre-training data. The trade-off between specialization and general capabilities presents a significant challenge for developing LLMs that excel in specific applications while maintaining their broad language understanding and generation abilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Catastrophic Forgetting in Continual Fine-tuning",
                "tldr": "Catastrophic forgetting is a significant challenge in continual fine-tuning of LLMs, where models lose previously acquired knowledge and abilities when trained on new tasks. This problem becomes more severe with increasing model scale and manifests across domain knowledge, reasoning abilities, and reading comprehension. (2 sources)",
                "text": "\nCatastrophic forgetting represents one of the most significant challenges in the continual fine-tuning of large language models. When LLMs are successively fine-tuned on different tasks or domains, they tend to lose previously acquired knowledge and capabilities\u2014a phenomenon formally known as catastrophic forgetting. Empirical evidence indicates that this problem is widespread across various LLM architectures during continual fine-tuning processes. Paradoxically, research has shown that as models increase in scale and complexity, they actually exhibit more severe forgetting of domain knowledge, reasoning abilities, and reading comprehension skills <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>.\n\nThis degradation creates a fundamental tension in LLM adaptation. While fine-tuning helps models specialize for specific tasks by training on tailored datasets, it simultaneously erodes the model's ability to generalize to novel examples that aren't well-represented in the fine-tuning data. The implications extend beyond simple knowledge retention; fine-tuned models may successfully memorize task-specific information but demonstrate diminished capacity to reason effectively about that information <Paper corpusId=\"270068369\" paperTitle=\"(Contal et al., 2024)\" isShortName></Paper>. This distinction between memorization and reasoning highlights the multifaceted nature of the degradation that occurs during fine-tuning, suggesting that different cognitive capabilities may be affected to varying degrees by the catastrophic forgetting phenomenon.",
                "citations": [
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills."
                        ],
                        "paper": {
                            "corpus_id": 261031244,
                            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                            "authors": [
                                {
                                    "authorId": "2181146702",
                                    "name": "Yun Luo"
                                },
                                {
                                    "authorId": "2149231975",
                                    "name": "Zhen Yang"
                                },
                                {
                                    "authorId": "33427918",
                                    "name": "Fandong Meng"
                                },
                                {
                                    "authorId": "2110450452",
                                    "name": "Yafu Li"
                                },
                                {
                                    "authorId": "48128428",
                                    "name": "Jie Zhou"
                                },
                                {
                                    "authorId": "2167740183",
                                    "name": "Yue Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 318
                        },
                        "score": 0.64892578125
                    },
                    {
                        "id": "(Contal et al., 2024)",
                        "snippets": [
                            "Fine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]."
                        ],
                        "paper": {
                            "corpus_id": 270068369,
                            "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
                            "authors": [
                                {
                                    "authorId": "2303471987",
                                    "name": "Emile Contal"
                                },
                                {
                                    "authorId": "2283934772",
                                    "name": "Garrin McGoldrick"
                                }
                            ],
                            "year": 2024,
                            "venue": "IR-RAG@SIGIR",
                            "n_citations": 5
                        },
                        "score": 0.52685546875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Degradation in Reasoning Abilities",
                "tldr": "Fine-tuning and format constraints can significantly degrade LLMs' reasoning capabilities, with stricter constraints leading to greater performance decline. This degradation is particularly pronounced in smaller models, where fine-tuning on non-reasoning tasks can substantially reduce both reasoning performance and the faithfulness of chain-of-thought reasoning. (3 sources)",
                "text": "\nEmpirical evidence demonstrates that LLMs experience notable degradation in reasoning abilities when subjected to certain constraints or fine-tuning processes. Format restrictions, such as requiring structured outputs like JSON or XML, have been shown to significantly impair reasoning capabilities. Studies reveal that stricter format constraints generally lead to greater performance degradation in reasoning tasks <Paper corpusId=\"271709856\" paperTitle=\"(Tam et al., 2024)\" isShortName></Paper>. This finding is particularly relevant for real-world applications that rely on structured generation to extract key information from LLMs.\n\nBeyond format constraints, fine-tuning also impacts reasoning abilities. Research investigating the effect of fine-tuning on chain-of-thought (CoT) reasoning shows that fine-tuning generally reduces CoT reasoning performance, regardless of whether the fine-tuning occurs on reasoning or non-reasoning tasks <Paper corpusId=\"274234789\" paperTitle=\"(Lobo et al., 2024)\" isShortName></Paper>. This degradation in reasoning capability is more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on datasets that require minimal reasoning tends to further decrease the faithfulness of their CoT reasonings <Paper corpusId=\"274234789\" paperTitle=\"(Lobo et al., 2024)\" isShortName></Paper>. This suggests that fine-tuning can alter the internal mechanisms that LLMs use for reasoning.\n\nThe relationship between output constraints and reasoning abilities has been further verified by multiple studies. Recent work has empirically confirmed that imposing constraints on LLM outputs can reduce functional correctness for specific tasks, with this reduction attributed directly to a decline in the LLM's reasoning capabilities under constrained decoding <Paper corpusId=\"276317504\" paperTitle=\"(Banerjee et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271709856\" paperTitle=\"(Tam et al., 2024)\" isShortName></Paper>. These findings highlight a critical trade-off that must be considered when deploying LLMs in applications that require both structured outputs and sophisticated reasoning.",
                "citations": [
                    {
                        "id": "(Tam et al., 2024)",
                        "snippets": [
                            "we observe a significant decline in LLMs' reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks."
                        ],
                        "paper": {
                            "corpus_id": 271709856,
                            "title": "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.",
                            "authors": [
                                {
                                    "authorId": "2028219138",
                                    "name": "Zhi Rui Tam"
                                },
                                {
                                    "authorId": "2217944277",
                                    "name": "Cheng-Kuang Wu"
                                },
                                {
                                    "authorId": "2314965749",
                                    "name": "Yi-Lin Tsai"
                                },
                                {
                                    "authorId": "2306137538",
                                    "name": "Chieh-Yen Lin"
                                },
                                {
                                    "authorId": "2278588523",
                                    "name": "Hung-yi Lee"
                                },
                                {
                                    "authorId": "2306102701",
                                    "name": "Yun-Nung Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 31
                        },
                        "score": 0.51318359375
                    },
                    {
                        "id": "(Lobo et al., 2024)",
                        "snippets": [
                            "Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes",
                            ".prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)",
                            "",
                            "Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate."
                        ],
                        "paper": {
                            "corpus_id": 274234789,
                            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                            "authors": [
                                {
                                    "authorId": "2332098876",
                                    "name": "Elita Lobo"
                                },
                                {
                                    "authorId": "40228633",
                                    "name": "Chirag Agarwal"
                                },
                                {
                                    "authorId": "1892673",
                                    "name": "Himabindu Lakkaraju"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 10
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Banerjee et al., 2025)",
                        "snippets": [
                            "Recent works such as (Tam et al., 2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. (Tam et al., 2024) attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding."
                        ],
                        "paper": {
                            "corpus_id": 276317504,
                            "title": "CRANE: Reasoning with constrained LLM generation",
                            "authors": [
                                {
                                    "authorId": "2069960232",
                                    "name": "Debangshu Banerjee"
                                },
                                {
                                    "authorId": "2218724103",
                                    "name": "Tarun Suresh"
                                },
                                {
                                    "authorId": "1413931779",
                                    "name": "Shubham Ugare"
                                },
                                {
                                    "authorId": "1704478",
                                    "name": "Sasa Misailovic"
                                },
                                {
                                    "authorId": "2301556017",
                                    "name": "Gagandeep Singh"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.50048828125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Impact on Translation and Language Generation Capabilities",
                "tldr": "Fine-tuning LLMs for translation tasks improves general translation quality but degrades several important qualitative capabilities including formality steering, technical translation through few-shot examples, and document-level translation. These degradations become more pronounced with larger fine-tuning datasets, even as generic translation quality continues to improve. (1 source)",
                "text": "\nFine-tuning LLMs for specific language tasks presents a nuanced trade-off between specialized performance and general capabilities, particularly in translation domains. While fine-tuning generally enhances the overall translation quality of language models, research has revealed that this improvement comes at the expense of several sophisticated language capabilities. Stap et al. found that fine-tuned models show significant degradation in their ability to perform formality steering (adjusting translation style based on context), handle technical translations through few-shot examples, and maintain document-level translation coherence. <Paper corpusId=\"270123515\" paperTitle=\"(Stap et al., 2024)\" isShortName></Paper>\n\nInterestingly, this degradation pattern follows a clear trajectory related to the size of fine-tuning datasets. As the volume of fine-tuning data increases, the general translation quality continues to improve, but the degradation in specialized capabilities becomes more pronounced. This inverse relationship highlights a fundamental tension in model optimization\u2014achieving higher performance on standard translation metrics may inadvertently sacrifice more nuanced language capabilities. This pattern holds consistently across various model scales, from smaller 7 billion parameter models to much larger 65 billion parameter models, suggesting this is an inherent characteristic of fine-tuning rather than a model-specific phenomenon. <Paper corpusId=\"270123515\" paperTitle=\"(Stap et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Stap et al., 2024)",
                        "snippets": [
                            "Our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings."
                        ],
                        "paper": {
                            "corpus_id": 270123515,
                            "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities",
                            "authors": [
                                {
                                    "authorId": "2292435932",
                                    "name": "David Stap"
                                },
                                {
                                    "authorId": "2269141224",
                                    "name": "Eva Hasler"
                                },
                                {
                                    "authorId": "2296993932",
                                    "name": "Bill Byrne"
                                },
                                {
                                    "authorId": "2062908179",
                                    "name": "C. Monz"
                                },
                                {
                                    "authorId": "2303845498",
                                    "name": "Ke Tran"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 12
                        },
                        "score": 0.65576171875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Effects of Parameter Editing and Weight Perturbations",
                "tldr": "Research shows that LLMs are extremely sensitive to weight perturbations, with even minor parameter edits leading to significant degradation in general capabilities. Editing less than 1% of parameters can cause performance to drop to nearly zero across multiple tasks. (1 source)",
                "text": "\nRecent empirical work has revealed that large language models exhibit surprising fragility when subjected to parameter editing or weight perturbations. These modifications, even when minimal, can have devastating effects on model performance across a range of capabilities. Gu et al. demonstrated that existing LLMs lack robustness to weight perturbations, with even small-scale parameter edits significantly compromising their general abilities. Their findings are particularly striking\u2014they showed that editing less than 1% of parameters in a LLaMA-1 (7B) model through a single pass resulted in catastrophic performance degradation, with scores plummeting to nearly zero across all evaluated tasks <Paper corpusId=\"266899568\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.\n\nThis extreme sensitivity to parameter modifications presents a significant challenge for model editing techniques that aim to correct factual knowledge or adjust model behavior. The research underscores a fundamental trade-off in parameter editing approaches: attempts to improve specific aspects of model performance through targeted parameter modifications often come at the expense of general capabilities. This finding has important implications for developing robust editing methods that can maintain an LLM's broad functionality while making targeted improvements to particular aspects of its performance. The observed fragility suggests that the distributed nature of knowledge and capabilities within LLMs makes isolated parameter modifications particularly disruptive to the model's overall functioning.",
                "citations": [
                    {
                        "id": "(Gu et al., 2024)",
                        "snippets": [
                            "Experimental results show that existing LLMs are not robust to weight perturbations, and editing even a few parameters can significantly affect their general abilities. Strikingly, with a single pass of editing involving less than 1% parameters, LLaMA-1 (7B) exhibited a drastic performance degradation to nearly 0 on all the tasks we tried. These results demonstrate that current editing algorithms struggle to work effectively in tandem with LLMs to simultaneously improve model factuality and maintain general abilities."
                        ],
                        "paper": {
                            "corpus_id": 266899568,
                            "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
                            "authors": [
                                {
                                    "authorId": "3028818",
                                    "name": "Jia-Chen Gu"
                                },
                                {
                                    "authorId": "2269760395",
                                    "name": "Haoyang Xu"
                                },
                                {
                                    "authorId": "2152612230",
                                    "name": "Jun-Yu Ma"
                                },
                                {
                                    "authorId": "2887562",
                                    "name": "Pan Lu"
                                },
                                {
                                    "authorId": "2072392338",
                                    "name": "Zhen-Hua Ling"
                                },
                                {
                                    "authorId": "2257127887",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "2256996328",
                                    "name": "Nanyun Peng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 55
                        },
                        "score": 0.59912109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Factors Influencing Degradation Severity",
                "tldr": "The severity of capability degradation in fine-tuned LLMs is influenced by multiple factors including model scale, fine-tuning dataset size, and the nature of the fine-tuning task. Larger models paradoxically show more severe forgetting of general capabilities, while larger fine-tuning datasets tend to exacerbate degradation in specialized language abilities despite improving performance on the target task. (3 sources)",
                "text": "\nResearch has identified several key factors that determine the extent of capability degradation when fine-tuning LLMs. Contrary to what might be expected, model scale appears to worsen rather than mitigate the problem of catastrophic forgetting. Luo et al. found that as model size increases, LLMs actually exhibit more severe degradation in domain knowledge, reasoning abilities, and reading comprehension skills during continual fine-tuning <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This counterintuitive finding suggests that larger models, despite their enhanced capabilities, may have more complex knowledge representations that are more susceptible to disruption during fine-tuning.\n\nThe size and nature of the fine-tuning dataset also significantly impact degradation severity. Stap et al. demonstrated that for translation tasks, degradation in specialized capabilities like formality steering and document-level translation becomes more pronounced with larger fine-tuning datasets, even as generic translation quality continues to improve <Paper corpusId=\"270123515\" paperTitle=\"(Stap et al., 2024)\" isShortName></Paper>. This pattern was consistent across models ranging from 7 billion to 65 billion parameters, indicating a fundamental trade-off between optimization for general task performance and retention of specialized capabilities.\n\nAdditionally, the nature of the fine-tuning task itself influences the extent of degradation. Lobo et al. found that fine-tuning on non-reasoning tasks or tasks requiring minimal reasoning leads to greater degradation in chain-of-thought reasoning capabilities compared to fine-tuning on reasoning-intensive tasks <Paper corpusId=\"274234789\" paperTitle=\"(Lobo et al., 2024)\" isShortName></Paper>. This effect was more pronounced in smaller models, suggesting that model size plays a complex role in determining resilience to certain types of degradation. The research indicates that fine-tuning disrupts the internal mechanisms that LLMs use for reasoning, with the degree of disruption varying based on the alignment between the fine-tuning task and the evaluated capability.",
                "citations": [
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills."
                        ],
                        "paper": {
                            "corpus_id": 261031244,
                            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                            "authors": [
                                {
                                    "authorId": "2181146702",
                                    "name": "Yun Luo"
                                },
                                {
                                    "authorId": "2149231975",
                                    "name": "Zhen Yang"
                                },
                                {
                                    "authorId": "33427918",
                                    "name": "Fandong Meng"
                                },
                                {
                                    "authorId": "2110450452",
                                    "name": "Yafu Li"
                                },
                                {
                                    "authorId": "48128428",
                                    "name": "Jie Zhou"
                                },
                                {
                                    "authorId": "2167740183",
                                    "name": "Yue Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 318
                        },
                        "score": 0.64892578125
                    },
                    {
                        "id": "(Stap et al., 2024)",
                        "snippets": [
                            "Our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings."
                        ],
                        "paper": {
                            "corpus_id": 270123515,
                            "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities",
                            "authors": [
                                {
                                    "authorId": "2292435932",
                                    "name": "David Stap"
                                },
                                {
                                    "authorId": "2269141224",
                                    "name": "Eva Hasler"
                                },
                                {
                                    "authorId": "2296993932",
                                    "name": "Bill Byrne"
                                },
                                {
                                    "authorId": "2062908179",
                                    "name": "C. Monz"
                                },
                                {
                                    "authorId": "2303845498",
                                    "name": "Ke Tran"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 12
                        },
                        "score": 0.65576171875
                    },
                    {
                        "id": "(Lobo et al., 2024)",
                        "snippets": [
                            "Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes",
                            ".prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)",
                            "",
                            "Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate."
                        ],
                        "paper": {
                            "corpus_id": 274234789,
                            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                            "authors": [
                                {
                                    "authorId": "2332098876",
                                    "name": "Elita Lobo"
                                },
                                {
                                    "authorId": "40228633",
                                    "name": "Chirag Agarwal"
                                },
                                {
                                    "authorId": "1892673",
                                    "name": "Himabindu Lakkaraju"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 10
                        },
                        "score": 0.82080078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.07958699999999999
    }
}