{
    "query": "How has classifier-free guidance (CFG) been adapted and applied in NLP-specific generative tasks such as conditional text generation or language modeling, and what benefits or trade-offs have been observed compared to traditional conditioning methods?",
    "user_id": "lib_user",
    "task_id": "52eaa140-7e11-45c7-88dd-0ff0599763eb",
    "timestamp": "2025-06-24T00:18:04.974129",
    "n_retrieval": 256,
    "n_retrieved": 255,
    "n_candidates": 45,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.5172390000000001,
    "decomposed_query": {
        "rewritten_query": "Adaptation and application of classifier-free guidance (CFG) in NLP-specific generative tasks such as conditional text generation or language modeling, and the benefits or trade-offs compared to traditional conditioning methods.",
        "keyword_query": "classifier-free guidance CFG NLP text generation language modeling benefits trade-offs traditional conditioning methods",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010212,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Classifier-Free Guidance: From High-Dimensional Analysis to Generalized Guidance Forms",
            "venue": "",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2188735129",
                    "name": "Krunoslav Lehman Pavasovic"
                },
                {
                    "authorId": "2345007920",
                    "name": "Jakob Verbeek"
                },
                {
                    "authorId": "2285281194",
                    "name": "Giulio Biroli"
                },
                {
                    "authorId": "2338268122",
                    "name": "Marc Mezard"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion and flow-based generative models, enabling high-quality conditional generation. A key theoretical challenge is characterizing the distribution induced by CFG, particularly in high-dimensional settings relevant to real-world data. Previous works have shown that CFG modifies the target distribution, steering it towards a distribution sharper than the target one, more shifted towards the boundary of the class. In this work, we provide a high-dimensional analysis of CFG, showing that these distortions vanish as the data dimension grows. We present a blessing-of-dimensionality result demonstrating that in sufficiently high and infinite dimensions, CFG accurately reproduces the target distribution. Using our high-dimensional theory, we show that there is a large family of guidances enjoying this property, in particular non-linear CFG generalizations. We study a simple non-linear power-law version, for which we demonstrate improved robustness, sample fidelity and diversity. Our findings are validated with experiments on class-conditional and text-to-image generation using state-of-the-art diffusion and flow-matching models.",
            "corpus_id": 276287831,
            "sentences": [
                {
                    "corpus_id": "276287831",
                    "title": "Classifier-Free Guidance: From High-Dimensional Analysis to Generalized Guidance Forms",
                    "text": "An important task for both paradigms is generating data conditioned on a class label or textual description of the image content. This can be achieved through conditioning mechanisms in the model architecture, as well as guidance techniques (Dhariwal and Nichol, 2021;Ho and Salimans, 2022) that steer the generation process towards samples aligned with user intentions or desired properties. \n\nThe notion of guidance was first introduced in classifier guidance (Song et al., 2020a;Dhariwal and Nichol, 2021), where a pre-trained classifier is leveraged to induce class conditioning of the sampling. Although beneficial, relying on a pre-trained classifier can be computationally expensive and may introduce biases inherent to the classifier itself. Classifier-free guidance (CFG) (Ho and Salimans, 2022) was developed as an alternative, and was quickly adopted as a standard technique in state-of-the-art generative models (Nichol et al., 2021;Betker et al., 2023;Saharia et al., 2022;Esser et al., 2024). CFG does not rely on an auxiliary classifier, instead, the model is trained to generate unconditional and conditional samples, and at inference extrapolates the denoising path towards the conditional one. Using CFG, however, it is no longer guaranteed to sample the original conditional distribution. Indeed, CFG modifies it by steering it towards a \"mode\" of high-quality and input-consistent samples, while reducing sample diversity in the process (Astolfi et al., 2024). \n\nThe effectiveness of CFG remains surprising in many ways, and a main theoretical question is to characterize the distributions generated by CFG and how they compare to the target distribution. Recent theoretical works on CFG formally showed that in case of Gaussian mixtures in one and finite dimensions, it results in a sharper distribution than the target one, and more shifted towards the boundary of the class (Chidambaram et al., 2024;Xia et al., 2024;Wu et al., 2024;Bradley and Nakkiran, 2024).",
                    "score": 0.607374232458971,
                    "section_title": "Introduction",
                    "char_start_offset": 1775,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 392
                        },
                        {
                            "start": 395,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 1006
                        },
                        {
                            "start": 1007,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1480
                        },
                        {
                            "start": 1483,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1984
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 268,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 482,
                            "end": 508,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 945,
                            "end": 965,
                            "matchedPaperCorpusId": "264403242"
                        },
                        {
                            "start": 965,
                            "end": 986,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 986,
                            "end": 1005,
                            "matchedPaperCorpusId": "268247980"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97998046875
                }
            ],
            "relevance_judgement": 0.97998046875,
            "relevance_judgment_input_expanded": "# Title: Classifier-Free Guidance: From High-Dimensional Analysis to Generalized Guidance Forms\n# Venue: \n# Authors: Krunoslav Lehman Pavasovic, Jakob Verbeek, Giulio Biroli, Marc Mezard\n## Abstract\nClassifier-Free Guidance (CFG) is a widely adopted technique in diffusion and flow-based generative models, enabling high-quality conditional generation. A key theoretical challenge is characterizing the distribution induced by CFG, particularly in high-dimensional settings relevant to real-world data. Previous works have shown that CFG modifies the target distribution, steering it towards a distribution sharper than the target one, more shifted towards the boundary of the class. In this work, we provide a high-dimensional analysis of CFG, showing that these distortions vanish as the data dimension grows. We present a blessing-of-dimensionality result demonstrating that in sufficiently high and infinite dimensions, CFG accurately reproduces the target distribution. Using our high-dimensional theory, we show that there is a large family of guidances enjoying this property, in particular non-linear CFG generalizations. We study a simple non-linear power-law version, for which we demonstrate improved robustness, sample fidelity and diversity. Our findings are validated with experiments on class-conditional and text-to-image generation using state-of-the-art diffusion and flow-matching models.\n## Introduction\nAn important task for both paradigms is generating data conditioned on a class label or textual description of the image content. This can be achieved through conditioning mechanisms in the model architecture, as well as guidance techniques (Dhariwal and Nichol, 2021;Ho and Salimans, 2022) that steer the generation process towards samples aligned with user intentions or desired properties. \n\nThe notion of guidance was first introduced in classifier guidance (Song et al., 2020a;Dhariwal and Nichol, 2021), where a pre-trained classifier is leveraged to induce class conditioning of the sampling. Although beneficial, relying on a pre-trained classifier can be computationally expensive and may introduce biases inherent to the classifier itself. Classifier-free guidance (CFG) (Ho and Salimans, 2022) was developed as an alternative, and was quickly adopted as a standard technique in state-of-the-art generative models (Nichol et al., 2021;Betker et al., 2023;Saharia et al., 2022;Esser et al., 2024). CFG does not rely on an auxiliary classifier, instead, the model is trained to generate unconditional and conditional samples, and at inference extrapolates the denoising path towards the conditional one. Using CFG, however, it is no longer guaranteed to sample the original conditional distribution. Indeed, CFG modifies it by steering it towards a \"mode\" of high-quality and input-consistent samples, while reducing sample diversity in the process (Astolfi et al., 2024). \n\nThe effectiveness of CFG remains surprising in many ways, and a main theoretical question is to characterize the distributions generated by CFG and how they compare to the target distribution. Recent theoretical works on CFG formally showed that in case of Gaussian mixtures in one and finite dimensions, it results in a sharper distribution than the target one, and more shifted towards the boundary of the class (Chidambaram et al., 2024;Xia et al., 2024;Wu et al., 2024;Bradley and Nakkiran, 2024).",
            "reference_string": "[276287831 | Pavasovic et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
            "venue": "",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2278618341",
                    "name": "Yuzhe Liang"
                },
                {
                    "authorId": "2358111037",
                    "name": "Wenzhe Liu"
                },
                {
                    "authorId": "2358041541",
                    "name": "Chunyu Qiang"
                },
                {
                    "authorId": "2229877177",
                    "name": "Zhikang Niu"
                },
                {
                    "authorId": "2324996330",
                    "name": "Yushen Chen"
                },
                {
                    "authorId": "2116609277",
                    "name": "Ziyang Ma"
                },
                {
                    "authorId": "2278584538",
                    "name": "Wenxi Chen"
                },
                {
                    "authorId": "2358116915",
                    "name": "Nan Li"
                },
                {
                    "authorId": "2358098456",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2321881822",
                    "name": "Xie Chen"
                }
            ],
            "abstract": "Flow matching has demonstrated strong generative capabilities and has become a core component in modern Text-to-Speech (TTS) systems. To ensure high-quality speech synthesis, Classifier-Free Guidance (CFG) is widely used during the inference of flow-matching-based TTS models. However, CFG incurs substantial computational cost as it requires two forward passes, which hinders its applicability in real-time scenarios. In this paper, we explore removing CFG from flow-matching-based TTS models to improve inference efficiency, while maintaining performance. Specifically, we reformulated the flow matching training target to directly approximate the CFG optimization trajectory. This training method eliminates the need for unconditional model evaluation and guided tuning during inference, effectively cutting the computational overhead in half. Furthermore, It can be seamlessly integrated with existing optimized sampling strategies. We validate our approach using the F5-TTS model on the LibriTTS dataset. Experimental results show that our method achieves a 9$\\times$ inference speed-up compared to the baseline F5-TTS, while preserving comparable speech quality. We will release the code and models to support reproducibility and foster further research in this area.",
            "corpus_id": 278171703,
            "sentences": [
                {
                    "corpus_id": "278171703",
                    "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                    "text": "Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation [29]. The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse [30]- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33].",
                    "score": 0.5265856490786446,
                    "section_title": "B. Classifier-Free Guidance",
                    "char_start_offset": 5832,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 539
                        },
                        {
                            "start": 542,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1041
                        },
                        {
                            "start": 1042,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1247
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 343,
                            "end": 347,
                            "matchedPaperCorpusId": "270220558"
                        },
                        {
                            "start": 1108,
                            "end": 1112,
                            "matchedPaperCorpusId": "253581838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                },
                {
                    "corpus_id": "278171703",
                    "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                    "text": "However, another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time. It is important to note that this method was primarily designed for class-conditional tasks, where the model is trained to generate images based on fixed categories; its application to text-to-speech remains underexplored. Inspired by this pioneering work, in this paper, we explore enhancing speech generation by modifying the flow matching training target, specifically for textconditioned and audio-conditioned tasks, such as TTS. Our goal is to enable the model to perform only conditional predictions during inference, eliminating the need for unconditional predictions required by traditional CFG. To validate the feasibility of this approach, we adopt F5-TTS, a representative flow-matching-based TTS model, as a case study. Experimental results show that our method can effectively halve the computational cost per sampling step without degrading the generated speech quality compared to the baseline F5-TTS with CFG. Furthermore, our method can be seamlessly combined with advanced sampling strategies to achieve additional speedup, highlighting its potential for real-time applications. \n\nOur contributions are summarized as follows: \n\n\u2022 We present the first attempt to remove classifier-free guidance from flow-matching-based TTS models at inference time by adopting the model-guidance training to alter the prediction target of flow matching. \u2022 We validate our approach on F5-TTS, effectively halving the inference cost without compromising the quality of the generated speech. Moreover, the proposed method can be seamlessly integrated with existing optimized sampling strategies, resulting in further speedup.",
                    "score": 0.5549931236854728,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 1902,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1545
                        },
                        {
                            "start": 1548,
                            "end": 1592
                        },
                        {
                            "start": 1595,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2072
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.931640625
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Towards Flow-Matching-based TTS without Classifier-Free Guidance\n# Venue: \n# Authors: Yuzhe Liang, Wenzhe Liu, Chunyu Qiang, Zhikang Niu, Yushen Chen, Ziyang Ma, Wenxi Chen, Nan Li, Chen Zhang, Xie Chen\n## Abstract\nFlow matching has demonstrated strong generative capabilities and has become a core component in modern Text-to-Speech (TTS) systems. To ensure high-quality speech synthesis, Classifier-Free Guidance (CFG) is widely used during the inference of flow-matching-based TTS models. However, CFG incurs substantial computational cost as it requires two forward passes, which hinders its applicability in real-time scenarios. In this paper, we explore removing CFG from flow-matching-based TTS models to improve inference efficiency, while maintaining performance. Specifically, we reformulated the flow matching training target to directly approximate the CFG optimization trajectory. This training method eliminates the need for unconditional model evaluation and guided tuning during inference, effectively cutting the computational overhead in half. Furthermore, It can be seamlessly integrated with existing optimized sampling strategies. We validate our approach using the F5-TTS model on the LibriTTS dataset. Experimental results show that our method achieves a 9$\\times$ inference speed-up compared to the baseline F5-TTS, while preserving comparable speech quality. We will release the code and models to support reproducibility and foster further research in this area.\n## I. INTRODUCTION\nHowever, another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time. It is important to note that this method was primarily designed for class-conditional tasks, where the model is trained to generate images based on fixed categories; its application to text-to-speech remains underexplored. Inspired by this pioneering work, in this paper, we explore enhancing speech generation by modifying the flow matching training target, specifically for textconditioned and audio-conditioned tasks, such as TTS. Our goal is to enable the model to perform only conditional predictions during inference, eliminating the need for unconditional predictions required by traditional CFG. To validate the feasibility of this approach, we adopt F5-TTS, a representative flow-matching-based TTS model, as a case study. Experimental results show that our method can effectively halve the computational cost per sampling step without degrading the generated speech quality compared to the baseline F5-TTS with CFG. Furthermore, our method can be seamlessly combined with advanced sampling strategies to achieve additional speedup, highlighting its potential for real-time applications. \n\nOur contributions are summarized as follows: \n\n\u2022 We present the first attempt to remove classifier-free guidance from flow-matching-based TTS models at inference time by adopting the model-guidance training to alter the prediction target of flow matching. \u2022 We validate our approach on F5-TTS, effectively halving the inference cost without compromising the quality of the generated speech. Moreover, the proposed method can be seamlessly integrated with existing optimized sampling strategies, resulting in further speedup.\n\n## B. Classifier-Free Guidance\nClassifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation [29]. The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse [30]- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33].",
            "reference_string": "[278171703 | Liang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 61,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282670286",
                    "name": "Ziqin Zhou"
                },
                {
                    "authorId": "2331570564",
                    "name": "Yifan Yang"
                },
                {
                    "authorId": "2125051198",
                    "name": "Yuqing Yang"
                },
                {
                    "authorId": "2350871281",
                    "name": "Tianyu He"
                },
                {
                    "authorId": "2350821834",
                    "name": "Houwen Peng"
                },
                {
                    "authorId": "2268758860",
                    "name": "Kai Qiu"
                },
                {
                    "authorId": "2329560121",
                    "name": "Qi Dai"
                },
                {
                    "authorId": "2160727304",
                    "name": "Lili Qiu"
                },
                {
                    "authorId": "2294680622",
                    "name": "Chong Luo"
                },
                {
                    "authorId": "2320820962",
                    "name": "Lingqiao Liu"
                }
            ],
            "abstract": "Text-to-video generation poses significant challenges due to the inherent complexity of video data, which spans both temporal and spatial dimensions. It introduces additional redundancy, abrupt variations, and a domain gap between language and vision tokens while generation. Addressing these challenges requires an effective video tokenizer that can efficiently encode video data while preserving essential semantic and spatiotemporal information, serving as a critical bridge between text and vision. Inspired by the observation in VQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for text-to-video generation with hierarchical tokenizers. It utilizes a 3D causal VAE with a multi-layer discrete token framework, encoding video content into hierarchically structured codebooks. Higher layers capture semantic information with higher compression, while lower layers focus on fine-grained spatiotemporal details, striking a balance between compression efficiency and reconstruction quality. Our approach efficiently encodes longer video sequences (e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately 70\\% compared to baseline tokenizers, while maintaining competitive reconstruction quality. We explore the trade-offs between compression and reconstruction, while emphasizing the advantages of high-compressed semantic tokens in text-to-video tasks. HiTVideo aims to address the potential limitations of existing video tokenizers in text-to-video generation tasks, striving for higher compression ratios and simplify LLMs modeling under language guidance, offering a scalable and promising framework for advancing text to video generation. Demo page: https://ziqinzhou66.github.io/project/HiTVideo.",
            "corpus_id": 277043912,
            "sentences": [
                {
                    "corpus_id": "277043912",
                    "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
                    "text": "The selection of an appropriate classifier-free guidance (CFG) scale is a crucial factor influencing the quality of generated videos. Originally proposed in the context of diffusion models, the CFG scale determines the tradeoff between fidelity to the textual prompt and the diversity of generated visual content. Classifier-free guidance integrates conditional (prompt-based) and unconditional (prompt-free) signals during model training, thereby enhancing the model's capacity to adhere to the provided textual descriptions without sacrificing model's generative flexibility. Specifically, the CFG scale directly modulates this balance. Higher CFG values enforce strict adherence to the input prompt, yielding videos that are semantically precise and visually coherent. However, an excessively high CFG scale can limit the output diversity, resulting in repetitive or overly rigid generations. Conversely, lower CFG scales offer greater creative freedom, allowing the model to explore diverse interpretations beyond the prompt, thus enhancing the variability and naturalness of generated videos, albeit potentially reducing semantic accuracy. \n\nInspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity. \n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale \n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10.",
                    "score": 0.43090456154089674,
                    "section_title": "Appendix A. Effect of CFG Scale in Text-to-Video Generation",
                    "char_start_offset": 26399,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 1144
                        },
                        {
                            "start": 1147,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1627
                        },
                        {
                            "start": 1628,
                            "end": 1825
                        },
                        {
                            "start": 1828,
                            "end": 1973
                        },
                        {
                            "start": 1974,
                            "end": 2074
                        },
                        {
                            "start": 2075,
                            "end": 2277
                        },
                        {
                            "start": 2280,
                            "end": 2470
                        },
                        {
                            "start": 2471,
                            "end": 2474
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models\n# Venue: arXiv.org\n# Authors: Ziqin Zhou, Yifan Yang, Yuqing Yang, Tianyu He, Houwen Peng, Kai Qiu, Qi Dai, Lili Qiu, Chong Luo, Lingqiao Liu\n## Abstract\nText-to-video generation poses significant challenges due to the inherent complexity of video data, which spans both temporal and spatial dimensions. It introduces additional redundancy, abrupt variations, and a domain gap between language and vision tokens while generation. Addressing these challenges requires an effective video tokenizer that can efficiently encode video data while preserving essential semantic and spatiotemporal information, serving as a critical bridge between text and vision. Inspired by the observation in VQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for text-to-video generation with hierarchical tokenizers. It utilizes a 3D causal VAE with a multi-layer discrete token framework, encoding video content into hierarchically structured codebooks. Higher layers capture semantic information with higher compression, while lower layers focus on fine-grained spatiotemporal details, striking a balance between compression efficiency and reconstruction quality. Our approach efficiently encodes longer video sequences (e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately 70\\% compared to baseline tokenizers, while maintaining competitive reconstruction quality. We explore the trade-offs between compression and reconstruction, while emphasizing the advantages of high-compressed semantic tokens in text-to-video tasks. HiTVideo aims to address the potential limitations of existing video tokenizers in text-to-video generation tasks, striving for higher compression ratios and simplify LLMs modeling under language guidance, offering a scalable and promising framework for advancing text to video generation. Demo page: https://ziqinzhou66.github.io/project/HiTVideo.\n## Appendix A. Effect of CFG Scale in Text-to-Video Generation\nThe selection of an appropriate classifier-free guidance (CFG) scale is a crucial factor influencing the quality of generated videos. Originally proposed in the context of diffusion models, the CFG scale determines the tradeoff between fidelity to the textual prompt and the diversity of generated visual content. Classifier-free guidance integrates conditional (prompt-based) and unconditional (prompt-free) signals during model training, thereby enhancing the model's capacity to adhere to the provided textual descriptions without sacrificing model's generative flexibility. Specifically, the CFG scale directly modulates this balance. Higher CFG values enforce strict adherence to the input prompt, yielding videos that are semantically precise and visually coherent. However, an excessively high CFG scale can limit the output diversity, resulting in repetitive or overly rigid generations. Conversely, lower CFG scales offer greater creative freedom, allowing the model to explore diverse interpretations beyond the prompt, thus enhancing the variability and naturalness of generated videos, albeit potentially reducing semantic accuracy. \n\nInspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity. \n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale \n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10.",
            "reference_string": "[277043912 | Zhou et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 151,
            "citation_count": 280,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.07909",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.07909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48934876",
                    "name": "Chenshuang Zhang"
                },
                {
                    "authorId": "31044159",
                    "name": "Chaoning Zhang"
                },
                {
                    "authorId": "50495602",
                    "name": "Mengchun Zhang"
                },
                {
                    "authorId": "145017151",
                    "name": "In-So Kweon"
                }
            ],
            "abstract": "This survey reviews the progress of diffusion models in generating images from text, ~\\textit{i.e.} text-to-image diffusion models. As a self-contained work, this survey starts with a brief introduction of how diffusion models work for image synthesis, followed by the background for text-conditioned image synthesis. Based on that, we present an organized review of pioneering methods and their improvements on text-to-image generation. We further summarize applications beyond image generation, such as text-guided generation for various modalities like videos, and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.",
            "corpus_id": 257505012,
            "sentences": [
                {
                    "corpus_id": "257505012",
                    "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
                    "text": "Labels improve image synthesis. Early works on generative adversarial models (GAN) have shown that class labels improve the image synthesis quality [35], [36], [37], [38], [39]. As a pioneering work, Conditional GAN [35] feeds the class label as an additional input layer to the model. Moreover, [40] applies class-conditional normalization statistics in image generation. In addition, AC-GAN [38] explicitly adds an auxiliary classifier loss. In other words, labels can help improve the GAN image synthesis quality by providing a conditional input or guiding the image synthesis via an auxiliary classifier. Following these success practices, [41] introduces class-conditional normalization and an auxiliary classifier into diffusion models. In order to distinguish whether the label information is added as a conditional input or an auxiliary loss with gradients, we follow [41] to define the conditional diffusion model and guided diffusion model as follows. \n\nConditional diffusion model: A conditional diffusion model learns from additional information (e.g., class and text) by taking them as model input. \n\nGuided diffusion model: During the training of a guided diffusion model, the class-induced gradients (e.g. through an auxiliary classfier) are involved in the sampling process. \n\nClassifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.",
                    "score": 0.4270921508980777,
                    "section_title": "Guidance in diffusion-based image synthesis",
                    "char_start_offset": 9118,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 31
                        },
                        {
                            "start": 32,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 961
                        },
                        {
                            "start": 964,
                            "end": 1111
                        },
                        {
                            "start": 1114,
                            "end": 1290
                        },
                        {
                            "start": 1293,
                            "end": 1318
                        },
                        {
                            "start": 1319,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 1825
                        },
                        {
                            "start": 1826,
                            "end": 1989
                        },
                        {
                            "start": 1990,
                            "end": 2121
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 158,
                            "matchedPaperCorpusId": "1099052"
                        },
                        {
                            "start": 160,
                            "end": 164,
                            "matchedPaperCorpusId": "2023211"
                        },
                        {
                            "start": 172,
                            "end": 176,
                            "matchedPaperCorpusId": "52889459"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9716796875
                }
            ],
            "relevance_judgement": 0.9716796875,
            "relevance_judgment_input_expanded": "# Title: Text-to-image Diffusion Models in Generative AI: A Survey\n# Venue: arXiv.org\n# Authors: Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In-So Kweon\n## Abstract\nThis survey reviews the progress of diffusion models in generating images from text, ~\\textit{i.e.} text-to-image diffusion models. As a self-contained work, this survey starts with a brief introduction of how diffusion models work for image synthesis, followed by the background for text-conditioned image synthesis. Based on that, we present an organized review of pioneering methods and their improvements on text-to-image generation. We further summarize applications beyond image generation, such as text-guided generation for various modalities like videos, and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.\n## Guidance in diffusion-based image synthesis\nLabels improve image synthesis. Early works on generative adversarial models (GAN) have shown that class labels improve the image synthesis quality [35], [36], [37], [38], [39]. As a pioneering work, Conditional GAN [35] feeds the class label as an additional input layer to the model. Moreover, [40] applies class-conditional normalization statistics in image generation. In addition, AC-GAN [38] explicitly adds an auxiliary classifier loss. In other words, labels can help improve the GAN image synthesis quality by providing a conditional input or guiding the image synthesis via an auxiliary classifier. Following these success practices, [41] introduces class-conditional normalization and an auxiliary classifier into diffusion models. In order to distinguish whether the label information is added as a conditional input or an auxiliary loss with gradients, we follow [41] to define the conditional diffusion model and guided diffusion model as follows. \n\nConditional diffusion model: A conditional diffusion model learns from additional information (e.g., class and text) by taking them as model input. \n\nGuided diffusion model: During the training of a guided diffusion model, the class-induced gradients (e.g. through an auxiliary classfier) are involved in the sampling process. \n\nClassifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.",
            "reference_string": "[257505012 | Zhang et al. | 2023 | Citations: 280]"
        },
        {
            "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348485713",
                    "name": "Jonathan Jacobi"
                },
                {
                    "authorId": "2333352",
                    "name": "Gal Niv"
                }
            ],
            "abstract": "Understanding and interpreting the internal representations of large language models (LLMs) remains an open challenge. Patchscopes introduced a method for probing internal activations by patching them into new prompts, prompting models to self-explain their hidden representations. We introduce Superscopes, a technique that systematically amplifies superposed features in MLP outputs (multilayer perceptron) and hidden states before patching them into new contexts. Inspired by the\"features as directions\"perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training. This approach provides new insights into how LLMs build context and represent complex concepts, further advancing mechanistic interpretability.",
            "corpus_id": 276774646,
            "sentences": [
                {
                    "corpus_id": "276774646",
                    "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
                    "text": "Classifier-Free Guidance (CFG) (Ho et al. (2022)) is a widely used technique in diffusion models that improves the alignment of generated samples with a given condition, such as a text prompt in text-to-image generation. It has been broadly adopted in diffusion-based models (Chen et al. ( 2024)). \n\nCFG operates by first computing an unconditional output, followed by a conditioned one. It then determines the difference between the conditional and unconditional outputs, defining a direction in the model's latent space. By amplifying this direction, the model is effectively steered toward outputs that better align with the given condition. Increasing the scale factor further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThe CFG amplification is expressed as: \n\nwhere: \n\n\u2022 \u03f5 uncond represents the model's noise prediction when no conditioning information is provided. \n\n\u2022 \u03f5 cond represents the noise prediction when conditioning information (e.g., a text prompt) is included. \n\n\u2022 w is the guidance scale, which controls the strength of conditioning in the generated output. \n\nThe equation essentially performs an amplified directional shift in prediction space, pushing the generated output closer to the conditioned estimate. By adjusting w: \n\n\u2022 If w = 0, the model generates samples unconditionally, without any reliance on the condition. \n\n\u2022 If w = 1, the model follows the standard conditional generation process. \n\n\u2022 If w > 1, the model exaggerates the influence of the condition, leading to more precise alignment with the input prompt. \n\nThe core idea is that when given a condition (e.g., a text prompt like \"blue fluffy dog\") and comparing the output to an unconditioned one, subtracting the two produces a direction that represents \"blue fluffy dog\". \n\nBy repeatedly adding this direction (controlled by the guidance scale factor), the model emphasizes the condition more strongly, resulting in an output that better aligns with the given prompt. As shown in Figure 2, a higher guidance scale strengthens the emphasis on the \"dog.\" However, an excessively high guidance scale is also suboptimal, as demonstrated in the figure .",
                    "score": 0.7133083454244206,
                    "section_title": "Superscopes as a variation of Classifier-Free Guidance (CFG)",
                    "char_start_offset": 20114,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 297
                        },
                        {
                            "start": 300,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 814
                        },
                        {
                            "start": 817,
                            "end": 823
                        },
                        {
                            "start": 826,
                            "end": 922
                        },
                        {
                            "start": 925,
                            "end": 1030
                        },
                        {
                            "start": 1033,
                            "end": 1128
                        },
                        {
                            "start": 1131,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1297
                        },
                        {
                            "start": 1300,
                            "end": 1395
                        },
                        {
                            "start": 1398,
                            "end": 1472
                        },
                        {
                            "start": 1475,
                            "end": 1597
                        },
                        {
                            "start": 1600,
                            "end": 1815
                        },
                        {
                            "start": 1818,
                            "end": 2011
                        },
                        {
                            "start": 2012,
                            "end": 2096
                        },
                        {
                            "start": 2097,
                            "end": 2192
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                },
                {
                    "corpus_id": "276774646",
                    "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
                    "text": "In diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.",
                    "score": 0.7107602106946932,
                    "section_title": "Classifier-Free Guidance",
                    "char_start_offset": 10843,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 228
                        },
                        {
                            "start": 231,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 678
                        },
                        {
                            "start": 681,
                            "end": 864
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 207,
                            "end": 226,
                            "matchedPaperCorpusId": "269043390"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.96826171875,
            "relevance_judgment_input_expanded": "# Title: Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation\n# Venue: arXiv.org\n# Authors: Jonathan Jacobi, Gal Niv\n## Abstract\nUnderstanding and interpreting the internal representations of large language models (LLMs) remains an open challenge. Patchscopes introduced a method for probing internal activations by patching them into new prompts, prompting models to self-explain their hidden representations. We introduce Superscopes, a technique that systematically amplifies superposed features in MLP outputs (multilayer perceptron) and hidden states before patching them into new contexts. Inspired by the\"features as directions\"perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training. This approach provides new insights into how LLMs build context and represent complex concepts, further advancing mechanistic interpretability.\n## Classifier-Free Guidance\nIn diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.\n\n## Superscopes as a variation of Classifier-Free Guidance (CFG)\nClassifier-Free Guidance (CFG) (Ho et al. (2022)) is a widely used technique in diffusion models that improves the alignment of generated samples with a given condition, such as a text prompt in text-to-image generation. It has been broadly adopted in diffusion-based models (Chen et al. ( 2024)). \n\nCFG operates by first computing an unconditional output, followed by a conditioned one. It then determines the difference between the conditional and unconditional outputs, defining a direction in the model's latent space. By amplifying this direction, the model is effectively steered toward outputs that better align with the given condition. Increasing the scale factor further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThe CFG amplification is expressed as: \n\nwhere: \n\n\u2022 \u03f5 uncond represents the model's noise prediction when no conditioning information is provided. \n\n\u2022 \u03f5 cond represents the noise prediction when conditioning information (e.g., a text prompt) is included. \n\n\u2022 w is the guidance scale, which controls the strength of conditioning in the generated output. \n\nThe equation essentially performs an amplified directional shift in prediction space, pushing the generated output closer to the conditioned estimate. By adjusting w: \n\n\u2022 If w = 0, the model generates samples unconditionally, without any reliance on the condition. \n\n\u2022 If w = 1, the model follows the standard conditional generation process. \n\n\u2022 If w > 1, the model exaggerates the influence of the condition, leading to more precise alignment with the input prompt. \n\nThe core idea is that when given a condition (e.g., a text prompt like \"blue fluffy dog\") and comparing the output to an unconditioned one, subtracting the two produces a direction that represents \"blue fluffy dog\". \n\nBy repeatedly adding this direction (controlled by the guidance scale factor), the model emphasizes the condition more strongly, resulting in an output that better aligns with the given prompt. As shown in Figure 2, a higher guidance scale strengthens the emphasis on the \"dog.\" However, an excessively high guidance scale is also suboptimal, as demonstrated in the figure .",
            "reference_string": "[276774646 | Jacobi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Steerable Scene Generation with Post Training and Inference-Time Search",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 50,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.04831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348265706",
                    "name": "Nicholas Pfaff"
                },
                {
                    "authorId": "1758275",
                    "name": "Hongkai Dai"
                },
                {
                    "authorId": "2331626375",
                    "name": "Sergey Zakharov"
                },
                {
                    "authorId": "2359630378",
                    "name": "Shun Iwase"
                },
                {
                    "authorId": "2263905014",
                    "name": "Russ Tedrake"
                }
            ],
            "abstract": "Training robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals. We do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution. Our method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments. Website with videos, code, data, and model weights: https://steerable-scene-generation.github.io/",
            "corpus_id": 278394371,
            "sentences": [
                {
                    "corpus_id": "278394371",
                    "title": "Steerable Scene Generation with Post Training and Inference-Time Search",
                    "text": "We support text-conditioned generation by encoding prompts using a frozen BERT-Base encoder [45]. The resulting embeddings are injected into the conditional input branch of our Flux-style architecture. We use 110 tokens per prompt, corresponding to the maximum prompt length across all scene types in our datasets. \n\nTo enable both conditional and unconditional generation within a single model, we randomly mask the conditioning input with 10% probability during training. This allows classifier-free guidance (CFG) [46] at inference time, where predictions are computed using a weighted combination of conditional and unconditional outputs: \n\nwhere w is the guidance weight, and xcond and xuncond are the model predictions under conditional and unconditional contexts, respectively. A weight of w = \u22121 corresponds to unconditional sampling, w = 0 yields conditional sampling without guidance, and w > 0 applies classifier-free guidance during sampling. We apply CFG to both discrete and continuous components.",
                    "score": 0.5202461133040257,
                    "section_title": "B.6.1 Text-Conditioned Generation",
                    "char_start_offset": 33278,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 314
                        },
                        {
                            "start": 317,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 642
                        },
                        {
                            "start": 645,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1011
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.966796875
                }
            ],
            "relevance_judgement": 0.966796875,
            "relevance_judgment_input_expanded": "# Title: Steerable Scene Generation with Post Training and Inference-Time Search\n# Venue: arXiv.org\n# Authors: Nicholas Pfaff, Hongkai Dai, Sergey Zakharov, Shun Iwase, Russ Tedrake\n## Abstract\nTraining robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals. We do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution. Our method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments. Website with videos, code, data, and model weights: https://steerable-scene-generation.github.io/\n## B.6.1 Text-Conditioned Generation\nWe support text-conditioned generation by encoding prompts using a frozen BERT-Base encoder [45]. The resulting embeddings are injected into the conditional input branch of our Flux-style architecture. We use 110 tokens per prompt, corresponding to the maximum prompt length across all scene types in our datasets. \n\nTo enable both conditional and unconditional generation within a single model, we randomly mask the conditioning input with 10% probability during training. This allows classifier-free guidance (CFG) [46] at inference time, where predictions are computed using a weighted combination of conditional and unconditional outputs: \n\nwhere w is the guidance weight, and xcond and xuncond are the model predictions under conditional and unconditional contexts, respectively. A weight of w = \u22121 corresponds to unconditional sampling, w = 0 yields conditional sampling without guidance, and w > 0 applies classifier-free guidance during sampling. We apply CFG to both discrete and continuous components.",
            "reference_string": "[278394371 | Pfaff et al. | 2025 | Citations: 0]"
        },
        {
            "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 75,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327997129",
                    "name": "Jeongsoo Choi"
                },
                {
                    "authorId": "2292215700",
                    "name": "Ji-Hoon Kim"
                },
                {
                    "authorId": "1395322099",
                    "name": "Kim Sung-Bin"
                },
                {
                    "authorId": "2243191148",
                    "name": "Tae-Hyun Oh"
                },
                {
                    "authorId": "2264317306",
                    "name": "Joon Son Chung"
                }
            ],
            "abstract": "In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT .",
            "corpus_id": 278171688,
            "sentences": [
                {
                    "corpus_id": "278171688",
                    "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
                    "text": "In diffusion-based generative models, classifier-free guidance [25] is well-explored to strengthen the influence of conditioning signals during inference. This is achieved by using both conditional and unconditional predictions from the same model to guide the generation process as follows: \n\nwhere s is guidance scale. Since each modality exhibits different characteristics, we hypothesize that using a single guidance scale for all modalities may be sub-optimal. To allow better control over each modality during inference, we propose multimodal classifier-free guidance by assigning modalityspecific guidance scales: \n\nwhere s t is guidance scale for text modality and s v for remained modality, namely video. By adjusting s t and s v , we can adaptively control the focus between modalities. Higher s t encourages the model to follow the text more closely, improving intelligibility, while higher s v leads to better lip synchronizations. \n\nTo support CFG, we apply modality dropout during training by randomly dropping text, video, or both. This not only enables multimodal CFG but also improves robustness in cases where a modality may be missing.",
                    "score": 0.47088703402374943,
                    "section_title": "Multimodal Classifier-Free Guidance",
                    "char_start_offset": 18966,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 291
                        },
                        {
                            "start": 294,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 620
                        },
                        {
                            "start": 623,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1154
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 63,
                            "end": 67,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96630859375
                }
            ],
            "relevance_judgement": 0.96630859375,
            "relevance_judgment_input_expanded": "# Title: AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation\n# Venue: arXiv.org\n# Authors: Jeongsoo Choi, Ji-Hoon Kim, Kim Sung-Bin, Tae-Hyun Oh, Joon Son Chung\n## Abstract\nIn this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT .\n## Multimodal Classifier-Free Guidance\nIn diffusion-based generative models, classifier-free guidance [25] is well-explored to strengthen the influence of conditioning signals during inference. This is achieved by using both conditional and unconditional predictions from the same model to guide the generation process as follows: \n\nwhere s is guidance scale. Since each modality exhibits different characteristics, we hypothesize that using a single guidance scale for all modalities may be sub-optimal. To allow better control over each modality during inference, we propose multimodal classifier-free guidance by assigning modalityspecific guidance scales: \n\nwhere s t is guidance scale for text modality and s v for remained modality, namely video. By adjusting s t and s v , we can adaptively control the focus between modalities. Higher s t encourages the model to follow the text more closely, improving intelligibility, while higher s v leads to better lip synchronizations. \n\nTo support CFG, we apply modality dropout during training by randomly dropping text, video, or both. This not only enables multimodal CFG but also improves robustness in cases where a modality may be missing.",
            "reference_string": "[278171688 | Choi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2022,
            "reference_count": 115,
            "citation_count": 1133,
            "influential_citation_count": 96,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.10789",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.10789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338016295",
                    "name": "Jiahui Yu"
                },
                {
                    "authorId": "2145139570",
                    "name": "Yuanzhong Xu"
                },
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "1821711",
                    "name": "Thang Luong"
                },
                {
                    "authorId": "1396954703",
                    "name": "Gunjan Baid"
                },
                {
                    "authorId": "2331539",
                    "name": "Zirui Wang"
                },
                {
                    "authorId": "2053781980",
                    "name": "Vijay Vasudevan"
                },
                {
                    "authorId": "31702389",
                    "name": "Alexander Ku"
                },
                {
                    "authorId": "2118771180",
                    "name": "Yinfei Yang"
                },
                {
                    "authorId": "143990191",
                    "name": "Burcu Karagol Ayan"
                },
                {
                    "authorId": "2044655623",
                    "name": "Ben Hutchinson"
                },
                {
                    "authorId": "143911112",
                    "name": "Wei Han"
                },
                {
                    "authorId": "27456119",
                    "name": "Zarana Parekh"
                },
                {
                    "authorId": "2158973314",
                    "name": "Xin Li"
                },
                {
                    "authorId": null,
                    "name": "Han Zhang"
                },
                {
                    "authorId": "1387994164",
                    "name": "Jason Baldridge"
                },
                {
                    "authorId": "48607963",
                    "name": "Yonghui Wu"
                }
            ],
            "abstract": "We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",
            "corpus_id": 249926846,
            "sentences": [
                {
                    "corpus_id": "249926846",
                    "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                    "text": "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
                    "score": 0.613509617947003,
                    "section_title": "Classifier-Free Guidance and Reranking",
                    "char_start_offset": 15352,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 691
                        },
                        {
                            "start": 694,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 986
                        },
                        {
                            "start": 989,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1230
                        },
                        {
                            "start": 1231,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1535
                        },
                        {
                            "start": 1538,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1790
                        },
                        {
                            "start": 1791,
                            "end": 1924
                        },
                        {
                            "start": 1925,
                            "end": 2054
                        },
                        {
                            "start": 2055,
                            "end": 2155
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 29,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1621,
                            "end": 1624,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 1835,
                            "end": 1838,
                            "matchedPaperCorpusId": "232035663"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu\n## Abstract\nWe present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.\n## Classifier-Free Guidance and Reranking\nClassifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
            "reference_string": "[249926846 | Yu et al. | 2022 | Citations: 1133]"
        },
        {
            "title": "Classifier-Free Diffusion Guidance",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 25,
            "citation_count": 3970,
            "influential_citation_count": 582,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.12598",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.12598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2126278",
                    "name": "Jonathan Ho"
                }
            ],
            "abstract": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
            "corpus_id": 249145348,
            "sentences": [
                {
                    "corpus_id": "249145348",
                    "title": "Classifier-Free Diffusion Guidance",
                    "text": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "score": 0.43911600783869886,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: Classifier-Free Diffusion Guidance\n# Venue: arXiv.org\n# Authors: Jonathan Ho\n## Abstract\nClassifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.\n",
            "reference_string": "[249145348 | Ho | 2022 | Citations: 3970]"
        },
        {
            "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 69,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.20240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268399939",
                    "name": "Prin Phunyaphibarn"
                },
                {
                    "authorId": "2328609307",
                    "name": "Phillip Y. Lee"
                },
                {
                    "authorId": "2292419803",
                    "name": "Jaihoon Kim"
                },
                {
                    "authorId": "2292259803",
                    "name": "Minhyuk Sung"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.",
            "corpus_id": 277321603,
            "sentences": [
                {
                    "corpus_id": "277321603",
                    "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
                    "text": "In recent years, diffusion models [29,56,59] have shown great success in generation tasks, becoming the de facto standard generative model across many data modalities such as images [50][51][52][53], video [4,5,30,66], and audio [13,33,44]. The success of diffusion models is not only due to their high-quality results and ease of training, but also the simplicity of adapting them into conditional diffusion models. While previous generative models such as GANs [23] and VAEs [39] require separate training for each conditional generation task, making it costly to create various conditional generative models, diffusion models introduced a considerably more effective approach: training an unconditional model (or a conditional model with simple conditions, such as text) as a base and branching out into multiple conditional models. At the core of the extendability of diffusion models in easily converting an unconditional (or less conditioned) base model into a conditional (or more conditioned) model is the Classifier-Free Guidance (CFG) [28] technique. CFG proposed to learn to predict both unconditional and conditional noises using a single neural network, without introducing another network, such as a classifier, as in the classifier-guidance [15] approach. CFG combines unconditional and conditional noise predictions to generate data conditioned on a given input. It has been widely adopted not only for training a conditional model from scratch but also for fine-tuning a base model to incorporate other conditions, by adding encoders for the conditional input. Many successful conditional generative models have been fine-tuned using CFG from a base model. For example, Zero-1-to-3 [46] and Versatile Diffusion [64] use variants of Stable Diffusion [52] (SD) as a base, with additional encoders to incorporate the input image as conditions, while Instruct-Pix2Pix [7] uses SD1.5 as a base and incorporates text editing instructions and input reference images as conditions to perform instruction-based image editing. \n\nDespite its successes and widespread usage, fine-tuning a conditional model from a base model using the CFG technique has limitations, most notably producing lower-quality results for unconditional generation.",
                    "score": 0.6131042934079795,
                    "section_title": "Introduction",
                    "char_start_offset": 1600,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1673
                        },
                        {
                            "start": 1674,
                            "end": 2033
                        },
                        {
                            "start": 2036,
                            "end": 2245
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 41,
                            "end": 43,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 182,
                            "end": 186,
                            "matchedPaperCorpusId": "254854389"
                        },
                        {
                            "start": 190,
                            "end": 194,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 194,
                            "end": 198,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 209,
                            "end": 211,
                            "matchedPaperCorpusId": "258187553"
                        },
                        {
                            "start": 211,
                            "end": 214,
                            "matchedPaperCorpusId": "248006185"
                        },
                        {
                            "start": 214,
                            "end": 217,
                            "matchedPaperCorpusId": "263151295"
                        },
                        {
                            "start": 229,
                            "end": 233,
                            "matchedPaperCorpusId": "259108357"
                        },
                        {
                            "start": 236,
                            "end": 239,
                            "matchedPaperCorpusId": "260775781"
                        },
                        {
                            "start": 463,
                            "end": 467,
                            "matchedPaperCorpusId": "10319744"
                        },
                        {
                            "start": 1045,
                            "end": 1049,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1699,
                            "end": 1703,
                            "matchedPaperCorpusId": "257631738"
                        },
                        {
                            "start": 1728,
                            "end": 1732,
                            "matchedPaperCorpusId": "253523371"
                        },
                        {
                            "start": 1766,
                            "end": 1770,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1881,
                            "end": 1884,
                            "matchedPaperCorpusId": "253581213"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9638671875
                }
            ],
            "relevance_judgement": 0.9638671875,
            "relevance_judgment_input_expanded": "# Title: Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models\n# Venue: arXiv.org\n# Authors: Prin Phunyaphibarn, Phillip Y. Lee, Jaihoon Kim, Minhyuk Sung\n## Abstract\nClassifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.\n## Introduction\nIn recent years, diffusion models [29,56,59] have shown great success in generation tasks, becoming the de facto standard generative model across many data modalities such as images [50][51][52][53], video [4,5,30,66], and audio [13,33,44]. The success of diffusion models is not only due to their high-quality results and ease of training, but also the simplicity of adapting them into conditional diffusion models. While previous generative models such as GANs [23] and VAEs [39] require separate training for each conditional generation task, making it costly to create various conditional generative models, diffusion models introduced a considerably more effective approach: training an unconditional model (or a conditional model with simple conditions, such as text) as a base and branching out into multiple conditional models. At the core of the extendability of diffusion models in easily converting an unconditional (or less conditioned) base model into a conditional (or more conditioned) model is the Classifier-Free Guidance (CFG) [28] technique. CFG proposed to learn to predict both unconditional and conditional noises using a single neural network, without introducing another network, such as a classifier, as in the classifier-guidance [15] approach. CFG combines unconditional and conditional noise predictions to generate data conditioned on a given input. It has been widely adopted not only for training a conditional model from scratch but also for fine-tuning a base model to incorporate other conditions, by adding encoders for the conditional input. Many successful conditional generative models have been fine-tuned using CFG from a base model. For example, Zero-1-to-3 [46] and Versatile Diffusion [64] use variants of Stable Diffusion [52] (SD) as a base, with additional encoders to incorporate the input image as conditions, while Instruct-Pix2Pix [7] uses SD1.5 as a base and incorporates text editing instructions and input reference images as conditions to perform instruction-based image editing. \n\nDespite its successes and widespread usage, fine-tuning a conditional model from a base model using the CFG technique has limitations, most notably producing lower-quality results for unconditional generation.",
            "reference_string": "[277321603 | Phunyaphibarn et al. | 2025 | Citations: 1]"
        },
        {
            "title": "ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 28,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348738665",
                    "name": "Boseong Jeon"
                }
            ],
            "abstract": "In this report, I present an inpainting framework named \\textit{ControlFill}, which involves training two distinct prompts: one for generating plausible objects within a designated mask (\\textit{creation}) and another for filling the region by extending the background (\\textit{removal}). During the inference stage, these learned embeddings guide a diffusion network that operates without requiring heavy text encoders. By adjusting the relative significance of the two prompts and employing classifier-free guidance, users can control the intensity of removal or creation. Furthermore, I introduce a method to spatially vary the intensity of guidance by assigning different scales to individual pixels.",
            "corpus_id": 276813045,
            "sentences": [
                {
                    "corpus_id": "276813045",
                    "title": "ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning",
                    "text": "Classifier-free guidance (CFG) is an approach proposed to enhance the quality and diversity of generated samples in generative models. This technique involves updating the score function to blend the gradients from both a conditional model and an unconditional model. By eliminating the need for a separate classifier, this method simplifies the training process and increases flexibility. The score update rules, as proposed, optimize the balance between the conditional and unconditional components, resulting in more accurate and diverse outputs. \n\nIn practice, given a condition y = (y neg , y pos ) where y neg is the negative prompt and y pos is the positive prompt, I use the following score update: \u03b5\u03b8(x, y) = (1 + w)\u03f5\u03b8(x, y pos ) \u2212 w\u03f5 \u03b8 (x, y neg ) \n\nwhere w \u2208 R is a scalar weight greater than one. \n\nThe above classifier-free guidance cannot be directly applied in cases where the effect of prompts should vary spatially, as it uses only a single global weight w. In more recent work [24], the authors proposed Semantic-aware Classifier-Free Guidance (S-CFG), which allows for different guidance levels for distinct semantic units within an image, thereby enforcing uniformity of text guidance. Although the purposes differ, I adopt this approach to reflect two different intentions (removal and creation) for each region when multiple masks are provided.",
                    "score": 0.6206818988295608,
                    "section_title": "Classifier-Free Guidance",
                    "char_start_offset": 10157,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 549
                        },
                        {
                            "start": 552,
                            "end": 757
                        },
                        {
                            "start": 760,
                            "end": 808
                        },
                        {
                            "start": 811,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1366
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning\n# Venue: arXiv.org\n# Authors: Boseong Jeon\n## Abstract\nIn this report, I present an inpainting framework named \\textit{ControlFill}, which involves training two distinct prompts: one for generating plausible objects within a designated mask (\\textit{creation}) and another for filling the region by extending the background (\\textit{removal}). During the inference stage, these learned embeddings guide a diffusion network that operates without requiring heavy text encoders. By adjusting the relative significance of the two prompts and employing classifier-free guidance, users can control the intensity of removal or creation. Furthermore, I introduce a method to spatially vary the intensity of guidance by assigning different scales to individual pixels.\n## Classifier-Free Guidance\nClassifier-free guidance (CFG) is an approach proposed to enhance the quality and diversity of generated samples in generative models. This technique involves updating the score function to blend the gradients from both a conditional model and an unconditional model. By eliminating the need for a separate classifier, this method simplifies the training process and increases flexibility. The score update rules, as proposed, optimize the balance between the conditional and unconditional components, resulting in more accurate and diverse outputs. \n\nIn practice, given a condition y = (y neg , y pos ) where y neg is the negative prompt and y pos is the positive prompt, I use the following score update: \u03b5\u03b8(x, y) = (1 + w)\u03f5\u03b8(x, y pos ) \u2212 w\u03f5 \u03b8 (x, y neg ) \n\nwhere w \u2208 R is a scalar weight greater than one. \n\nThe above classifier-free guidance cannot be directly applied in cases where the effect of prompts should vary spatially, as it uses only a single global weight w. In more recent work [24], the authors proposed Semantic-aware Classifier-Free Guidance (S-CFG), which allows for different guidance levels for distinct semantic units within an image, thereby enforcing uniformity of text guidance. Although the purposes differ, I adopt this approach to reflect two different intentions (removal and creation) for each region when multiple masks are provided.",
            "reference_string": "[276813045 | Jeon | 2025 | Citations: 0]"
        },
        {
            "title": "DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 89,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2311.17833",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.17833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49799275",
                    "name": "Maximilian Augustin"
                },
                {
                    "authorId": "2195794433",
                    "name": "Yannic Neuhaus"
                },
                {
                    "authorId": "2268308768",
                    "name": "Matthias Hein"
                }
            ],
            "abstract": "While deep learning has led to huge progress in complex image classification tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call into question how reliably these classifiers work in the wild. Furthermore, for safety-critical tasks the black-box nature of their decisions is problematic, and explanations or at least methods which make decisions plausible are needed urgently. In this paper, we address these problems by generating images that optimize a classifier-derived objective using a framework for guided image generation. We analyze the decisions of image classifiers by visual counterfactual explanations (VCEs), detection of systematic mistakes by analyzing images where classifiers maximally disagree, and visualization of neurons and spurious features. In this way, we validate existing observations, e.g. the shape bias of adversarially robust models, as well as novel failure modes, e.g. systematic errors of zero-shot CLIP classifiers. Moreover, our VCEs outperform previous work while being more versatile.",
            "corpus_id": 265498527,
            "sentences": [
                {
                    "corpus_id": "265498527",
                    "title": "DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations",
                    "text": "Classifier-free guidance (CFG) [36] was introduced as an alternative to classifier guidance [9,45,56,80]. [18] already used a class-conditional denoising model \u03f5 \u03b8 (x t , t, y) that was given the target class as additional input. The class label y was thereby integrated into the model via adaptive group normalization layers. They introduced classifier guidance to enforce the generation of the correct target class by strengthening the influence of y on the output of the generative process. Classifier-free guidance is an alternative that also strengthens the impact of the conditioning signal in combination with a conditional denoising model \u03f5 \u03b8 (x t , t, y) without the requirement of an external classifier. \n\nIn the following, we will first introduce cross-attention (XA) conditioning that is used by Stable Diffusion [64] to condition the denoising model \u03f5 \u03b8 not only on class labels but also other modalities such as text prompts or depth maps. Then we will introduce classifier-free Guidance as a solution to strengthen the impact of the conditioning signal.",
                    "score": 0.6852808456649526,
                    "section_title": "A.3. Classifier-Free Guidance and Cross-Attention Conditioning",
                    "char_start_offset": 35114,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 714
                        },
                        {
                            "start": 717,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1069
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 98,
                            "end": 101,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 101,
                            "end": 104,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 106,
                            "end": 110,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 826,
                            "end": 830,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Maximilian Augustin, Yannic Neuhaus, Matthias Hein\n## Abstract\nWhile deep learning has led to huge progress in complex image classification tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call into question how reliably these classifiers work in the wild. Furthermore, for safety-critical tasks the black-box nature of their decisions is problematic, and explanations or at least methods which make decisions plausible are needed urgently. In this paper, we address these problems by generating images that optimize a classifier-derived objective using a framework for guided image generation. We analyze the decisions of image classifiers by visual counterfactual explanations (VCEs), detection of systematic mistakes by analyzing images where classifiers maximally disagree, and visualization of neurons and spurious features. In this way, we validate existing observations, e.g. the shape bias of adversarially robust models, as well as novel failure modes, e.g. systematic errors of zero-shot CLIP classifiers. Moreover, our VCEs outperform previous work while being more versatile.\n## A.3. Classifier-Free Guidance and Cross-Attention Conditioning\nClassifier-free guidance (CFG) [36] was introduced as an alternative to classifier guidance [9,45,56,80]. [18] already used a class-conditional denoising model \u03f5 \u03b8 (x t , t, y) that was given the target class as additional input. The class label y was thereby integrated into the model via adaptive group normalization layers. They introduced classifier guidance to enforce the generation of the correct target class by strengthening the influence of y on the output of the generative process. Classifier-free guidance is an alternative that also strengthens the impact of the conditioning signal in combination with a conditional denoising model \u03f5 \u03b8 (x t , t, y) without the requirement of an external classifier. \n\nIn the following, we will first introduce cross-attention (XA) conditioning that is used by Stable Diffusion [64] to condition the denoising model \u03f5 \u03b8 not only on class labels but also other modalities such as text prompts or depth maps. Then we will introduce classifier-free Guidance as a solution to strengthen the impact of the conditioning signal.",
            "reference_string": "[265498527 | Augustin et al. | 2023 | Citations: 5]"
        },
        {
            "title": "AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03941, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145172609",
                    "name": "A. Kasymov"
                },
                {
                    "authorId": "46220915",
                    "name": "Marcin Sendera"
                },
                {
                    "authorId": "1379958688",
                    "name": "Michal Stypulkowski"
                },
                {
                    "authorId": "2310705964",
                    "name": "Maciej Zieba"
                },
                {
                    "authorId": "1790922",
                    "name": "P. Spurek"
                }
            ],
            "abstract": "Low-rank adaptation (LoRA) is a fine-tuning technique that can be applied to conditional generative diffusion models. LoRA utilizes a small number of context examples to adapt the model to a specific domain, character, style, or concept. However, due to the limited data utilized during training, the fine-tuned model performance is often characterized by strong context bias and a low degree of variability in the generated images. To solve this issue, we introduce AutoLoRA, a novel guidance technique for diffusion models fine-tuned with the LoRA approach. Inspired by other guidance techniques, AutoLoRA searches for a trade-off between consistency in the domain represented by LoRA weights and sample diversity from the base conditional diffusion model. Moreover, we show that incorporating classifier-free guidance for both LoRA fine-tuned and base models leads to generating samples with higher diversity and better quality. The experimental results for several fine-tuned LoRA domains show superiority over existing guidance techniques on selected metrics.",
            "corpus_id": 273186941,
            "sentences": [
                {
                    "corpus_id": "273186941",
                    "title": "AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models",
                    "text": "Require: \n\npredict the conditioning label y from intermediate noisy samples x t , was used to steer the reverse process. This guidance was achieved by modifying the reverse sampling step as: \n\nwhere w is a scaling factor that adjusts the strength of the classifier's influence, and \u1fb1t = t i=1 1 \u2212 \u03b2 i . While effective, classifier-based guidance introduces several downsides, such as added complexity and potential inaccuracies due to classifier errors. \n\nClassifier-Free Guidance offers a simpler and more robust alternative by eliminating the need for an external classifier. Instead, the diffusion model itself is trained in two modes -conditional and unconditional: \n\n\u2022 conditional mode -the model is trained to predict the denoised data x 0 given noisy data x t and conditioning information y, learning the conditional distribution p \u03b8 (x t\u22121 |x t , y); \u2022 unconditional mode -the same model is also trained without any conditioning, learning the unconditional distribution p \u03b8 (x t\u22121 |x t ). \n\nDuring inference, CFG uses a combination of the conditional and unconditional predictions to guide the generation (see Algorithm 1). Specifically, for a given noisy sample x t , the guidance is achieved by interpolating between the conditional and unconditional predictions as follows: \n\nwhere \u03f5 \u03b8 (x t , \u00f8) is the model's prediction of the noise in x t when no conditioning is provided (unconditional), \u03f5 \u03b8 (x t , y) is the prediction of the noise in x t when conditioned on y, and w is the guidance scale, which controls how strongly the conditional information influences the generation. \n\nBy adjusting w, one can control the balance between sample diversity and adherence to the conditioning y. When w = 1, the process is equivalent to standard conditional generation. When w > 1, the conditional prediction is amplified, guiding the model to produce samples that more closely match the conditioning information, potentially at the cost of diversity. \n\n4 AUTOLORA Karras et al. (2024) introduced a novel technique called AutoGuidance to enhance the image generation capabilities of a diffusion model by guiding it with a bad version of itself -a smaller and less-trained variant. This method leads to more refined results while maintaining diversity in the outputs.",
                    "score": 0.4613289446949011,
                    "section_title": "PRELIMINARIES",
                    "char_start_offset": 14147,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 8
                        },
                        {
                            "start": 11,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 190
                        },
                        {
                            "start": 193,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 453
                        },
                        {
                            "start": 456,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 669
                        },
                        {
                            "start": 672,
                            "end": 996
                        },
                        {
                            "start": 999,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1284
                        },
                        {
                            "start": 1287,
                            "end": 1589
                        },
                        {
                            "start": 1592,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1771
                        },
                        {
                            "start": 1772,
                            "end": 1953
                        },
                        {
                            "start": 1956,
                            "end": 2182
                        },
                        {
                            "start": 2183,
                            "end": 2268
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                }
            ],
            "relevance_judgement": 0.958984375,
            "relevance_judgment_input_expanded": "# Title: AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models\n# Venue: arXiv.org\n# Authors: A. Kasymov, Marcin Sendera, Michal Stypulkowski, Maciej Zieba, P. Spurek\n## Abstract\nLow-rank adaptation (LoRA) is a fine-tuning technique that can be applied to conditional generative diffusion models. LoRA utilizes a small number of context examples to adapt the model to a specific domain, character, style, or concept. However, due to the limited data utilized during training, the fine-tuned model performance is often characterized by strong context bias and a low degree of variability in the generated images. To solve this issue, we introduce AutoLoRA, a novel guidance technique for diffusion models fine-tuned with the LoRA approach. Inspired by other guidance techniques, AutoLoRA searches for a trade-off between consistency in the domain represented by LoRA weights and sample diversity from the base conditional diffusion model. Moreover, we show that incorporating classifier-free guidance for both LoRA fine-tuned and base models leads to generating samples with higher diversity and better quality. The experimental results for several fine-tuned LoRA domains show superiority over existing guidance techniques on selected metrics.\n## PRELIMINARIES\nRequire: \n\npredict the conditioning label y from intermediate noisy samples x t , was used to steer the reverse process. This guidance was achieved by modifying the reverse sampling step as: \n\nwhere w is a scaling factor that adjusts the strength of the classifier's influence, and \u1fb1t = t i=1 1 \u2212 \u03b2 i . While effective, classifier-based guidance introduces several downsides, such as added complexity and potential inaccuracies due to classifier errors. \n\nClassifier-Free Guidance offers a simpler and more robust alternative by eliminating the need for an external classifier. Instead, the diffusion model itself is trained in two modes -conditional and unconditional: \n\n\u2022 conditional mode -the model is trained to predict the denoised data x 0 given noisy data x t and conditioning information y, learning the conditional distribution p \u03b8 (x t\u22121 |x t , y); \u2022 unconditional mode -the same model is also trained without any conditioning, learning the unconditional distribution p \u03b8 (x t\u22121 |x t ). \n\nDuring inference, CFG uses a combination of the conditional and unconditional predictions to guide the generation (see Algorithm 1). Specifically, for a given noisy sample x t , the guidance is achieved by interpolating between the conditional and unconditional predictions as follows: \n\nwhere \u03f5 \u03b8 (x t , \u00f8) is the model's prediction of the noise in x t when no conditioning is provided (unconditional), \u03f5 \u03b8 (x t , y) is the prediction of the noise in x t when conditioned on y, and w is the guidance scale, which controls how strongly the conditional information influences the generation. \n\nBy adjusting w, one can control the balance between sample diversity and adherence to the conditioning y. When w = 1, the process is equivalent to standard conditional generation. When w > 1, the conditional prediction is amplified, guiding the model to produce samples that more closely match the conditioning information, potentially at the cost of diversity. \n\n4 AUTOLORA Karras et al. (2024) introduced a novel technique called AutoGuidance to enhance the image generation capabilities of a diffusion model by guiding it with a bad version of itself -a smaller and less-trained variant. This method leads to more refined results while maintaining diversity in the outputs.",
            "reference_string": "[273186941 | Kasymov et al. | 2024 | Citations: 1]"
        },
        {
            "title": "DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327218388",
                    "name": "Hengwei Bian"
                },
                {
                    "authorId": "2152007435",
                    "name": "Lingdong Kong"
                },
                {
                    "authorId": "2237483104",
                    "name": "Haozhe Xie"
                },
                {
                    "authorId": "2272233402",
                    "name": "Liang Pan"
                },
                {
                    "authorId": "2327935778",
                    "name": "Yu Qiao"
                },
                {
                    "authorId": "2277717448",
                    "name": "Ziwei Liu"
                }
            ],
            "abstract": "Urban scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic 4D scenes with semantics. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion&Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D occupancy generation methods across multiple metrics. The code and models have been released to facilitate future research.",
            "corpus_id": 276741036,
            "sentences": [
                {
                    "corpus_id": "276741036",
                    "title": "DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes",
                    "text": "Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(x|c) and unconditional generation p(x), and guidance during sampling is provided by the following equation: \n\nwhere xt (c) is the result conditioned on c, xt (\u2205) is the unconditioned result, and w is a weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved.",
                    "score": 0.5946703031622833,
                    "section_title": "A.4 CLASSIFIER-FREE GUIDANCE",
                    "char_start_offset": 21593,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 359
                        },
                        {
                            "start": 362,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 639
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9580078125
                }
            ],
            "relevance_judgement": 0.9580078125,
            "relevance_judgment_input_expanded": "# Title: DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes\n# Venue: International Conference on Learning Representations\n# Authors: Hengwei Bian, Lingdong Kong, Haozhe Xie, Liang Pan, Yu Qiao, Ziwei Liu\n## Abstract\nUrban scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic 4D scenes with semantics. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion&Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D occupancy generation methods across multiple metrics. The code and models have been released to facilitate future research.\n## A.4 CLASSIFIER-FREE GUIDANCE\nClassifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(x|c) and unconditional generation p(x), and guidance during sampling is provided by the following equation: \n\nwhere xt (c) is the result conditioned on c, xt (\u2205) is the unconditioned result, and w is a weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved.",
            "reference_string": "[276741036 | Bian et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Stay on topic with Classifier-Free Guidance",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 99,
            "citation_count": 55,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.17806",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.17806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2056723344",
                    "name": "Guillaume Sanchez"
                },
                {
                    "authorId": "2072838294",
                    "name": "Honglu Fan"
                },
                {
                    "authorId": "51444076",
                    "name": "Alexander Spangher"
                },
                {
                    "authorId": "34490455",
                    "name": "Elad Levi"
                },
                {
                    "authorId": "1451644426",
                    "name": "Pawan Sasanka Ammanamanchi"
                },
                {
                    "authorId": "103476203",
                    "name": "Stella Biderman"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.",
            "corpus_id": 259308807,
            "sentences": [
                {
                    "corpus_id": "259308807",
                    "title": "Stay on topic with Classifier-Free Guidance",
                    "text": "We have shown that Classifier-Free Guidance, which was originally conceived of in text-to-image applications, can be an effective way of increasing adherence to the prompt in autoregressive language modeling. In contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model. We have shown that CFG can boost performance across an array of canonical benchmarks in NLP that involve variations of the prompt: basic prompting, chain-of-thought prompting, text-to-text prompting and chatbot prompting. Finally, we sought to explain the effects of CFG by showing it decreased sampling entropy, but not in the same ways that Instruction-tuned models do. Ultimately, we leave for future work the exact effects that CFG is having, but we propose qualitative visualizations that confirm our intuitions around prompt adherence. \n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45,73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train. \n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context. It's also possible that CFG might be misused. We have not tested the effects of CFG if used in conjunction with malicious strategies for hacking language models, including but not limited to: prompt injection and prompts aimed at overriding alignment. It's possible that there are unforeseen effects induced by an increased adherence to parts of the prompt. We tried to explore this at length, both quantitatively and qualitatively, and we designed tasks that might reveal such behavior. However, we cannot conclude this method is risk-free. We advocate for standardized benchmarks aimed more squarely at language-model risk (including, possibly, pairs of models along with known prompt injections). Such standardized benchmarks could help us unit-test an advancement like CFG before releasing it into the wild. \n\nStella Biderman supervised the process.",
                    "score": 0.5343409221647053,
                    "section_title": "Conclusion",
                    "char_start_offset": 20190,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 888
                        },
                        {
                            "start": 891,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1236
                        },
                        {
                            "start": 1239,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1683
                        },
                        {
                            "start": 1684,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 1973
                        },
                        {
                            "start": 1974,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2243
                        },
                        {
                            "start": 2246,
                            "end": 2285
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9560546875
                },
                {
                    "corpus_id": "259308807",
                    "title": "Stay on topic with Classifier-Free Guidance",
                    "text": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.",
                    "score": 0.7649108333157304,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.9560546875,
            "relevance_judgment_input_expanded": "# Title: Stay on topic with Classifier-Free Guidance\n# Venue: arXiv.org\n# Authors: Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, Stella Biderman\n## Abstract\nClassifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.\n## Conclusion\nWe have shown that Classifier-Free Guidance, which was originally conceived of in text-to-image applications, can be an effective way of increasing adherence to the prompt in autoregressive language modeling. In contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model. We have shown that CFG can boost performance across an array of canonical benchmarks in NLP that involve variations of the prompt: basic prompting, chain-of-thought prompting, text-to-text prompting and chatbot prompting. Finally, we sought to explain the effects of CFG by showing it decreased sampling entropy, but not in the same ways that Instruction-tuned models do. Ultimately, we leave for future work the exact effects that CFG is having, but we propose qualitative visualizations that confirm our intuitions around prompt adherence. \n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45,73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train. \n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context. It's also possible that CFG might be misused. We have not tested the effects of CFG if used in conjunction with malicious strategies for hacking language models, including but not limited to: prompt injection and prompts aimed at overriding alignment. It's possible that there are unforeseen effects induced by an increased adherence to parts of the prompt. We tried to explore this at length, both quantitatively and qualitatively, and we designed tasks that might reveal such behavior. However, we cannot conclude this method is risk-free. We advocate for standardized benchmarks aimed more squarely at language-model risk (including, possibly, pairs of models along with known prompt injections). Such standardized benchmarks could help us unit-test an advancement like CFG before releasing it into the wild. \n\nStella Biderman supervised the process.",
            "reference_string": "[259308807 | Sanchez et al. | 2023 | Citations: 55]"
        },
        {
            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261742393",
                    "name": "Seyedmorteza Sadat"
                },
                {
                    "authorId": "2204861903",
                    "name": "Manuel Kansy"
                },
                {
                    "authorId": "1466533438",
                    "name": "Otmar Hilliges"
                },
                {
                    "authorId": "145848224",
                    "name": "Romann M. Weber"
                }
            ],
            "abstract": "Classifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null condition. There is also no clear extension of CFG to unconditional models. In this paper, we revisit the core principles of CFG and introduce a new method, independent condition guidance (ICG), which provides the benefits of CFG without the need for any special training procedures. Our approach streamlines the training process of conditional diffusion models and can also be applied during inference on any pre-trained conditional model. Additionally, by leveraging the time-step information encoded in all diffusion networks, we propose an extension of CFG, called time-step guidance (TSG), which can be applied to any diffusion model, including unconditional ones. Our guidance techniques are easy to implement and have the same sampling cost as CFG. Through extensive experiments, we demonstrate that ICG matches the performance of standard CFG across various conditional diffusion models. Moreover, we show that TSG improves generation quality in a manner similar to CFG, without relying on any conditional information.",
            "corpus_id": 270923987,
            "sentences": [
                {
                    "corpus_id": "270923987",
                    "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
                    "text": "Classifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null condition. There is also no clear extension of CFG to unconditional models. In this paper, we revisit the core principles of CFG and introduce a new method, independent condition guidance (ICG), which provides the benefits of CFG without the need for any special training procedures. Our approach streamlines the training process of conditional diffusion models and can also be applied during inference on any pre-trained conditional model. Additionally, by leveraging the time-step information encoded in all diffusion networks, we propose an extension of CFG, called time-step guidance (TSG), which can be applied to any diffusion model, including unconditional ones. Our guidance techniques are easy to implement and have the same sampling cost as CFG. Through extensive experiments, we demonstrate that ICG matches the performance of standard CFG across various conditional diffusion models. Moreover, we show that TSG improves generation quality in a manner similar to CFG, without relying on any conditional information.",
                    "score": 0.4511391266451122,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9560546875
                },
                {
                    "corpus_id": "270923987",
                    "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
                    "text": "Diffusion models have recently emerged as the main methodology behind many successful generative models [39,14,9,33,42,43].At the core of such models lies a diffusion process that gradually adds noise to the data until data points are indistinguishable from pure noise.At inference, a denoiser is used to gradually remove noise from samples until we reach a generated data point.While the theory suggests that standard sampling from diffusion models should yield high-quality images, this does not generally hold in practice, and guidance methods are often required to increase the quality of generations, albeit at the expense of less diversity [9,13,35].Classifier guidance [9] was the first method to introduce this concept by utilizing the gradient of a classifier trained on noisy images to increase the class-likelihood of generated samples.Later, classifier-free guidance (CFG) [13] was proposed, allowing the diffusion model to simulate the same behavior as classifier guidance without using an explicit classifier.Since then, CFG has been applied to other conditional generation tasks, such as text-to-image synthesis [28] and text-to-3D generation [31].\n\nIn addition to CFG's trading diversity for quality, it has two practical limitations.First, it requires the underlying model to be trained in a specific way to also learn the unconditional score function, typically by replacing the conditioning vector with a null vector with probability p (usually p \u2208 [0.1.0.2]).This additional step hinders training efficiency, as the model needs to be trained on two different tasks.Replacing the condition might also not be straightforward when the model is multimodal and uses different conditioning signals such as text, images, and audio at the same time, or when the null vector (which is usually the zero vector in practice) has a specific meaning.Second, it is not clear how to extend the benefits of classifier-free guidance beyond conditional models to unconditional generation.\n\nIn this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model.The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.",
                    "score": 0.5975205233903729,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 123,
                            "end": 269
                        },
                        {
                            "start": 269,
                            "end": 379
                        },
                        {
                            "start": 379,
                            "end": 656
                        },
                        {
                            "start": 656,
                            "end": 847
                        },
                        {
                            "start": 847,
                            "end": 1023
                        },
                        {
                            "start": 1023,
                            "end": 1163
                        },
                        {
                            "start": 1165,
                            "end": 1250
                        },
                        {
                            "start": 1250,
                            "end": 1479
                        },
                        {
                            "start": 1479,
                            "end": 1585
                        },
                        {
                            "start": 1585,
                            "end": 1856
                        },
                        {
                            "start": 1856,
                            "end": 1989
                        },
                        {
                            "start": 1991,
                            "end": 2180
                        },
                        {
                            "start": 2180,
                            "end": 2341
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 108,
                            "end": 111,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 111,
                            "end": 113,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 113,
                            "end": 116,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 116,
                            "end": 119,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 119,
                            "end": 122,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 646,
                            "end": 649,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 652,
                            "end": 655,
                            "matchedPaperCorpusId": "264490969"
                        },
                        {
                            "start": 676,
                            "end": 679,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1127,
                            "end": 1131,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 1158,
                            "end": 1162,
                            "matchedPaperCorpusId": "252596091"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                },
                {
                    "corpus_id": "270923987",
                    "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
                    "text": "The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.This insight leads us to propose independent condition guidance (ICG), a method that replicates the behavior of CFG at inference time without requiring separate training of an unconditional model.Inspired by the above, we also introduce a novel technique to extend classifier-free guidance to a more general setting that includes unconditional generation.This method, which we call timestep guidance (TSG), employs a perturbed version of the time-step embedding in diffusion models to create a guidance signal similar to CFG.Time-step guidance aims to improve the accuracy of denoising at each sampling step by leveraging the time-step information learned by the diffusion model to steer sampling trajectories toward better noise-removal paths.\n\nOur guidance techniques are easy to implement, do not require additional fine-tuning of the underlying diffusion models, and have the same sampling cost as CFG.Through extensive experiments, we empirically verify that: 1) ICG offers performance similar to CFG and can be readily applied to models that are not trained with the CFG objective in mind, such as EDM [18]; and 2) TSG improves output quality in a manner similar to CFG for both conditional and unconditional generation.\n\nThe core contributions of our work are as follows: (i) We revisit the principles of classifier-free guidance and offer an efficient, theoretically motivated method to employ CFG without the need to train an unconditional module, greatly simplifying the training process of conditional diffusion models.(ii) We offer an extension of CFG that is generally applicable to all diffusion models, whether conditional or unconditional.(iii) We demonstrate empirically that our guidance techniques achieve the quality-boosting benefits of CFG across various setups and network architectures.",
                    "score": 0.4402696595546802,
                    "section_title": "Introduction",
                    "char_start_offset": 2195,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 161,
                            "end": 357
                        },
                        {
                            "start": 357,
                            "end": 516
                        },
                        {
                            "start": 516,
                            "end": 686
                        },
                        {
                            "start": 686,
                            "end": 905
                        },
                        {
                            "start": 907,
                            "end": 1067
                        },
                        {
                            "start": 1067,
                            "end": 1387
                        },
                        {
                            "start": 1389,
                            "end": 1691
                        },
                        {
                            "start": 1691,
                            "end": 1816
                        },
                        {
                            "start": 1816,
                            "end": 1971
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9345703125
                }
            ],
            "relevance_judgement": 0.9560546875,
            "relevance_judgment_input_expanded": "# Title: No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models\n# Venue: International Conference on Learning Representations\n# Authors: Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, Romann M. Weber\n## Abstract\nClassifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null condition. There is also no clear extension of CFG to unconditional models. In this paper, we revisit the core principles of CFG and introduce a new method, independent condition guidance (ICG), which provides the benefits of CFG without the need for any special training procedures. Our approach streamlines the training process of conditional diffusion models and can also be applied during inference on any pre-trained conditional model. Additionally, by leveraging the time-step information encoded in all diffusion networks, we propose an extension of CFG, called time-step guidance (TSG), which can be applied to any diffusion model, including unconditional ones. Our guidance techniques are easy to implement and have the same sampling cost as CFG. Through extensive experiments, we demonstrate that ICG matches the performance of standard CFG across various conditional diffusion models. Moreover, we show that TSG improves generation quality in a manner similar to CFG, without relying on any conditional information.\n## Introduction\nDiffusion models have recently emerged as the main methodology behind many successful generative models [39,14,9,33,42,43].At the core of such models lies a diffusion process that gradually adds noise to the data until data points are indistinguishable from pure noise.At inference, a denoiser is used to gradually remove noise from samples until we reach a generated data point.While the theory suggests that standard sampling from diffusion models should yield high-quality images, this does not generally hold in practice, and guidance methods are often required to increase the quality of generations, albeit at the expense of less diversity [9,13,35].Classifier guidance [9] was the first method to introduce this concept by utilizing the gradient of a classifier trained on noisy images to increase the class-likelihood of generated samples.Later, classifier-free guidance (CFG) [13] was proposed, allowing the diffusion model to simulate the same behavior as classifier guidance without using an explicit classifier.Since then, CFG has been applied to other conditional generation tasks, such as text-to-image synthesis [28] and text-to-3D generation [31].\n\nIn addition to CFG's trading diversity for quality, it has two practical limitations.First, it requires the underlying model to be trained in a specific way to also learn the unconditional score function, typically by replacing the conditioning vector with a null vector with probability p (usually p \u2208 [0.1.0.2]).This additional step hinders training efficiency, as the model needs to be trained on two different tasks.Replacing the condition might also not be straightforward when the model is multimodal and uses different conditioning signals such as text, images, and audio at the same time, or when the null vector (which is usually the zero vector in practice) has a specific meaning.Second, it is not clear how to extend the benefits of classifier-free guidance beyond conditional models to unconditional generation.\n\nIn this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model.The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.\n...\nThe main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.This insight leads us to propose independent condition guidance (ICG), a method that replicates the behavior of CFG at inference time without requiring separate training of an unconditional model.Inspired by the above, we also introduce a novel technique to extend classifier-free guidance to a more general setting that includes unconditional generation.This method, which we call timestep guidance (TSG), employs a perturbed version of the time-step embedding in diffusion models to create a guidance signal similar to CFG.Time-step guidance aims to improve the accuracy of denoising at each sampling step by leveraging the time-step information learned by the diffusion model to steer sampling trajectories toward better noise-removal paths.\n\nOur guidance techniques are easy to implement, do not require additional fine-tuning of the underlying diffusion models, and have the same sampling cost as CFG.Through extensive experiments, we empirically verify that: 1) ICG offers performance similar to CFG and can be readily applied to models that are not trained with the CFG objective in mind, such as EDM [18]; and 2) TSG improves output quality in a manner similar to CFG for both conditional and unconditional generation.\n\nThe core contributions of our work are as follows: (i) We revisit the principles of classifier-free guidance and offer an efficient, theoretically motivated method to employ CFG without the need to train an unconditional module, greatly simplifying the training process of conditional diffusion models.(ii) We offer an extension of CFG that is generally applicable to all diffusion models, whether conditional or unconditional.(iii) We demonstrate empirically that our guidance techniques achieve the quality-boosting benefits of CFG across various setups and network architectures.",
            "reference_string": "[270923987 | Sadat et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 24,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2312.04302",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.04302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145915052",
                    "name": "Yuechen Zhang"
                },
                {
                    "authorId": "152230789",
                    "name": "Shengju Qian"
                },
                {
                    "authorId": "2272493196",
                    "name": "Bohao Peng"
                },
                {
                    "authorId": "25059098",
                    "name": "Shu Liu"
                },
                {
                    "authorId": "2273012826",
                    "name": "Jiaya Jia"
                }
            ],
            "abstract": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and 1552.5 in MME-perception.",
            "corpus_id": 266053531,
            "sentences": [
                {
                    "corpus_id": "266053531",
                    "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
                    "text": "In conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning. Prompt Highlighter extends CFG control in language models in a more generalized manner. The user's selection on the context x is converted into a token-level binary highlight mask m = {m 1 , . . . , m N }. We define m i = 1 if the i-th token x i is highlighted, and m i = 0 otherwise. This mask constructs a two-branch condition: the normal and the unconditional contexts. The normal context operates in the same manner as in vanilla inference.",
                    "score": 0.6064017065891522,
                    "section_title": "Token-Level Highlight Guidance",
                    "char_start_offset": 10683,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 513
                        },
                        {
                            "start": 516,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 814
                        },
                        {
                            "start": 817,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 979
                        },
                        {
                            "start": 982,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1152
                        },
                        {
                            "start": 1155,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1609
                        },
                        {
                            "start": 1610,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1806
                        },
                        {
                            "start": 1807,
                            "end": 1894
                        },
                        {
                            "start": 1895,
                            "end": 1966
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 36,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9541015625
                }
            ],
            "relevance_judgement": 0.9541015625,
            "relevance_judgment_input_expanded": "# Title: Prompt Highlighter: Interactive Control for Multi-Modal LLMs\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia\n## Abstract\nThis study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and 1552.5 in MME-perception.\n## Token-Level Highlight Guidance\nIn conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning. Prompt Highlighter extends CFG control in language models in a more generalized manner. The user's selection on the context x is converted into a token-level binary highlight mask m = {m 1 , . . . , m N }. We define m i = 1 if the i-th token x i is highlighted, and m i = 0 otherwise. This mask constructs a two-branch condition: the normal and the unconditional contexts. The normal context operates in the same manner as in vanilla inference.",
            "reference_string": "[266053531 | Zhang et al. | 2023 | Citations: 24]"
        },
        {
            "title": "Guided Flows for Generative Modeling and Decision Making",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 59,
            "citation_count": 46,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166847",
                    "name": "Qinqing Zheng"
                },
                {
                    "authorId": "2267723599",
                    "name": "Matt Le"
                },
                {
                    "authorId": "2219927868",
                    "name": "Neta Shaul"
                },
                {
                    "authorId": "3232072",
                    "name": "Y. Lipman"
                },
                {
                    "authorId": "2267723293",
                    "name": "Aditya Grover"
                },
                {
                    "authorId": "2253976277",
                    "name": "Ricky T. Q. Chen"
                }
            ],
            "abstract": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
            "corpus_id": 265351587,
            "sentences": [
                {
                    "corpus_id": "265351587",
                    "title": "Guided Flows for Generative Modeling and Decision Making",
                    "text": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
                    "score": 0.515920732083757,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                }
            ],
            "relevance_judgement": 0.95361328125,
            "relevance_judgment_input_expanded": "# Title: Guided Flows for Generative Modeling and Decision Making\n# Venue: arXiv.org\n# Authors: Qinqing Zheng, Matt Le, Neta Shaul, Y. Lipman, Aditya Grover, Ricky T. Q. Chen\n## Abstract\nClassifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.\n",
            "reference_string": "[265351587 | Zheng et al. | 2023 | Citations: 46]"
        },
        {
            "title": "Diffusion Models without Classifier-free Guidance",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12154, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1387822270",
                    "name": "Zhicong Tang"
                },
                {
                    "authorId": "9324504",
                    "name": "Jianmin Bao"
                },
                {
                    "authorId": "2265723915",
                    "name": "Dongdong Chen"
                },
                {
                    "authorId": "2238211403",
                    "name": "Baining Guo"
                }
            ],
            "abstract": "This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at https://github.com/tzco/Diffusion-wo-CFG.",
            "corpus_id": 276421312,
            "sentences": [
                {
                    "corpus_id": "276421312",
                    "title": "Diffusion Models without Classifier-free Guidance",
                    "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality. \n\nThe key design of CFG is to combine the posterior probability and utilize Bayes' rule during inference time. To facilitate this, it is required to train both conditional and unconditional diffusion models. In particular, CFG trains the models to predict \n\nwhere is an additional empty class introduced in common practices. During training, the model switches between the two modes with a ratio \u03bb. \n\nFor inference, the model combines the conditional and unconditional scores and guides the denoising process as \n\nFigure 2: We use a grid 2D distribution with two classes, marked with orange and gray regions, as example and train diffusion models on it. We plot the generated samples, trajectories, and probability density function (PDF) of conditional, unconditional, CFG-guided model, and our approach. \n\n(a) The first row indicates that although CFG improves quality by eliminating outliers, the samples concentrate in the center of data distributions, resulting the loss of diversity. In contrast, our method yields less outliers than the conditional model and a better coverage of data than CFG. \n\n(b) In the second row, the trajectories of CFG show sharp turns at the beginning, e.g. samples inside the red box, while our method directly drives the samples to the closet data distributions. \n\n(c) The PDF plots of the last row also suggest that our method predicts more symmetric contours than CFG, balancing both quality and diversity. \n\nwhere w is the guidance scale that controls the focus on conditional scores and the trade-off between generation performance and sampling diversity. CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation.",
                    "score": 0.504553083941135,
                    "section_title": "Classifier-Free Guidance",
                    "char_start_offset": 5406,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 324
                        },
                        {
                            "start": 327,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 580
                        },
                        {
                            "start": 583,
                            "end": 649
                        },
                        {
                            "start": 650,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 836
                        },
                        {
                            "start": 839,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1129
                        },
                        {
                            "start": 1132,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1425
                        },
                        {
                            "start": 1428,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1621
                        },
                        {
                            "start": 1624,
                            "end": 1767
                        },
                        {
                            "start": 1770,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2046
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 31,
                            "end": 51,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.953125
                }
            ],
            "relevance_judgement": 0.953125,
            "relevance_judgment_input_expanded": "# Title: Diffusion Models without Classifier-free Guidance\n# Venue: arXiv.org\n# Authors: Zhicong Tang, Jianmin Bao, Dongdong Chen, Baining Guo\n## Abstract\nThis paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at https://github.com/tzco/Diffusion-wo-CFG.\n## Classifier-Free Guidance\nClassifier-free guidance (CFG) (Ho & Salimans, 2021) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality. \n\nThe key design of CFG is to combine the posterior probability and utilize Bayes' rule during inference time. To facilitate this, it is required to train both conditional and unconditional diffusion models. In particular, CFG trains the models to predict \n\nwhere is an additional empty class introduced in common practices. During training, the model switches between the two modes with a ratio \u03bb. \n\nFor inference, the model combines the conditional and unconditional scores and guides the denoising process as \n\nFigure 2: We use a grid 2D distribution with two classes, marked with orange and gray regions, as example and train diffusion models on it. We plot the generated samples, trajectories, and probability density function (PDF) of conditional, unconditional, CFG-guided model, and our approach. \n\n(a) The first row indicates that although CFG improves quality by eliminating outliers, the samples concentrate in the center of data distributions, resulting the loss of diversity. In contrast, our method yields less outliers than the conditional model and a better coverage of data than CFG. \n\n(b) In the second row, the trajectories of CFG show sharp turns at the beginning, e.g. samples inside the red box, while our method directly drives the samples to the closet data distributions. \n\n(c) The PDF plots of the last row also suggest that our method predicts more symmetric contours than CFG, balancing both quality and diversity. \n\nwhere w is the guidance scale that controls the focus on conditional scores and the trade-off between generation performance and sampling diversity. CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation.",
            "reference_string": "[276421312 | Tang et al. | 2025 | Citations: 5]"
        },
        {
            "title": "InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.14360",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.14360, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118914313",
                    "name": "Bing Han"
                },
                {
                    "authorId": "2147243559",
                    "name": "Junyu Dai"
                },
                {
                    "authorId": "84556222",
                    "name": "Xuchen Song"
                },
                {
                    "authorId": "3314779",
                    "name": "Weituo Hao"
                },
                {
                    "authorId": "2234532553",
                    "name": "Xinyan He"
                },
                {
                    "authorId": "2234359627",
                    "name": "Dong Guo"
                },
                {
                    "authorId": "2855690",
                    "name": "Jitong Chen"
                },
                {
                    "authorId": "2234520458",
                    "name": "Yuxuan Wang"
                },
                {
                    "authorId": "2480051",
                    "name": "Y. Qian"
                }
            ],
            "abstract": "Music editing primarily entails the modification of instrument tracks or remixing in the whole, which offers a novel reinterpretation of the original piece through a series of operations. These music processing methods hold immense potential across various applications but demand substantial expertise. Prior methodologies, although effective for image and audio modifications, falter when directly applied to music. This is attributed to music's distinctive data nature, where such methods can inadvertently compromise the intrinsic harmony and coherence of music. In this paper, we develop InstructME, an Instruction guided Music Editing and remixing framework based on latent diffusion models. Our framework fortifies the U-Net with multi-scale aggregation in order to maintain consistency before and after editing. In addition, we introduce chord progression matrix as condition information and incorporate it in the semantic space to improve melodic harmony while editing. For accommodating extended musical pieces, InstructME employs a chunk transformer, enabling it to discern long-term temporal dependencies within music sequences. We tested InstructME in instrument-editing, remixing, and multi-round editing. Both subjective and objective evaluations indicate that our proposed method significantly surpasses preceding systems in music quality, text relevance and harmony. Demo samples are available at https://musicedit.github.io/",
            "corpus_id": 261242901,
            "sentences": [
                {
                    "corpus_id": "261242901",
                    "title": "InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models",
                    "text": "For diffusion models, there exist two primary strategies for achieving controllable generation. One of these is classifier guidance (CG) (Dhariwal and Nichol 2021; Liu et al. 2023b), which utilizes a classifier during the sampling process and mixes its input gradient of the log probability with the score estimate of diffusion model. It is flexible and controllable, but tends to suffer a performance degradation (Ho and Salimans 2022). Another approach, named classifierfree guidance (CFG) (Ho and Salimans 2022;Nichol et al. 2021;Ramesh et al. 2022;Saharia et al. 2022), achieves the same effect through training a conditional diffusion model directly without a guidance classifier. This method performs better but requires a large amount of data with diverse text descriptions, which is difficult for our InstructME trained with source-target paired data. In this work, to attain a tradeoff between quality and controllability, we adopt both classifier and classifier-free guidance to achieve the controllable editing of Remix operations. \n\nWe specify instrument and genre tags with CFG by incorporating these tags into text commands to train the conditional diffusion models. During the training, we discard our text condition y randomly with a certain probability p CFG following (Liu et al. 2023a;Wang et al. 2023). Then, in the sampling, we can estimate the noise \u03b5\u03b8 (t, T (y), p s , z s , z t ) with a linear combination of the conditional and unconditional score estimates: \n\nwhere w can determine the strength of guidance. \n\nTo achieve finer-grained semantic control with weaklyassociated, free-form text annotations, we apply classifier guidance during sampling with a pre-trained MuLan (Huang et al. 2022), which can project the music audio and its corresponding text description into the same embedding space. The guidance function we use is: \n\nwhere E L (\u2022) and E M (\u2022) denote the language and music encoders respectively. Then, by adding the gradient on estimated x t , we can guide the generation \n\nwith factor s to control the guidance scale.",
                    "score": 0.4740986194753394,
                    "section_title": "Towards Advanced Music Editing -Remix",
                    "char_start_offset": 13567,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 96,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 1042
                        },
                        {
                            "start": 1045,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1483
                        },
                        {
                            "start": 1486,
                            "end": 1533
                        },
                        {
                            "start": 1536,
                            "end": 1823
                        },
                        {
                            "start": 1824,
                            "end": 1856
                        },
                        {
                            "start": 1859,
                            "end": 1937
                        },
                        {
                            "start": 1938,
                            "end": 2013
                        },
                        {
                            "start": 2016,
                            "end": 2060
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 164,
                            "end": 181,
                            "matchedPaperCorpusId": "245117331"
                        },
                        {
                            "start": 514,
                            "end": 533,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 533,
                            "end": 552,
                            "matchedPaperCorpusId": "248097655"
                        },
                        {
                            "start": 552,
                            "end": 572,
                            "matchedPaperCorpusId": "248986576"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.953125
                }
            ],
            "relevance_judgement": 0.953125,
            "relevance_judgment_input_expanded": "# Title: InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models\n# Venue: arXiv.org\n# Authors: Bing Han, Junyu Dai, Xuchen Song, Weituo Hao, Xinyan He, Dong Guo, Jitong Chen, Yuxuan Wang, Y. Qian\n## Abstract\nMusic editing primarily entails the modification of instrument tracks or remixing in the whole, which offers a novel reinterpretation of the original piece through a series of operations. These music processing methods hold immense potential across various applications but demand substantial expertise. Prior methodologies, although effective for image and audio modifications, falter when directly applied to music. This is attributed to music's distinctive data nature, where such methods can inadvertently compromise the intrinsic harmony and coherence of music. In this paper, we develop InstructME, an Instruction guided Music Editing and remixing framework based on latent diffusion models. Our framework fortifies the U-Net with multi-scale aggregation in order to maintain consistency before and after editing. In addition, we introduce chord progression matrix as condition information and incorporate it in the semantic space to improve melodic harmony while editing. For accommodating extended musical pieces, InstructME employs a chunk transformer, enabling it to discern long-term temporal dependencies within music sequences. We tested InstructME in instrument-editing, remixing, and multi-round editing. Both subjective and objective evaluations indicate that our proposed method significantly surpasses preceding systems in music quality, text relevance and harmony. Demo samples are available at https://musicedit.github.io/\n## Towards Advanced Music Editing -Remix\nFor diffusion models, there exist two primary strategies for achieving controllable generation. One of these is classifier guidance (CG) (Dhariwal and Nichol 2021; Liu et al. 2023b), which utilizes a classifier during the sampling process and mixes its input gradient of the log probability with the score estimate of diffusion model. It is flexible and controllable, but tends to suffer a performance degradation (Ho and Salimans 2022). Another approach, named classifierfree guidance (CFG) (Ho and Salimans 2022;Nichol et al. 2021;Ramesh et al. 2022;Saharia et al. 2022), achieves the same effect through training a conditional diffusion model directly without a guidance classifier. This method performs better but requires a large amount of data with diverse text descriptions, which is difficult for our InstructME trained with source-target paired data. In this work, to attain a tradeoff between quality and controllability, we adopt both classifier and classifier-free guidance to achieve the controllable editing of Remix operations. \n\nWe specify instrument and genre tags with CFG by incorporating these tags into text commands to train the conditional diffusion models. During the training, we discard our text condition y randomly with a certain probability p CFG following (Liu et al. 2023a;Wang et al. 2023). Then, in the sampling, we can estimate the noise \u03b5\u03b8 (t, T (y), p s , z s , z t ) with a linear combination of the conditional and unconditional score estimates: \n\nwhere w can determine the strength of guidance. \n\nTo achieve finer-grained semantic control with weaklyassociated, free-form text annotations, we apply classifier guidance during sampling with a pre-trained MuLan (Huang et al. 2022), which can project the music audio and its corresponding text description into the same embedding space. The guidance function we use is: \n\nwhere E L (\u2022) and E M (\u2022) denote the language and music encoders respectively. Then, by adding the gradient on estimated x t , we can guide the generation \n\nwith factor s to control the guidance scale.",
            "reference_string": "[261242901 | Han et al. | 2023 | Citations: 16]"
        },
        {
            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2340406432",
                    "name": "Weichen Fan"
                },
                {
                    "authorId": "2323508574",
                    "name": "Amber Yijia Zheng"
                },
                {
                    "authorId": "2240073241",
                    "name": "Raymond A. Yeh"
                },
                {
                    "authorId": "2324070583",
                    "name": "Ziwei Liu"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)",
            "corpus_id": 277510202,
            "sentences": [
                {
                    "corpus_id": "277510202",
                    "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
                    "text": "Diffusion and Flow-based Models. Unlike generative ad-versarial methods [7] that rely on one-step generation, diffusion models [4] have demonstrated significantly improved performance in generating high-quality samples. Early diffusion models were primarily score-based generative models, including DDPM [10], DDIM [34], EDM [16], and Stable Diffusion [30], which focused on learning the SDEs governing the diffusion process. \n\nNext, Flow Matching [21] provides an alternative approach by directly modeling sample trajectories using ordinary differential equations (ODEs) instead of SDEs. This enables more stable and efficient generative processes by learning a continuous flow field that smoothly transports samples from a prior distribution to the target distribution. Several works, including Rectified Flow [24], SD3 [5], Lumina-Next [45], Flux [20], Vchitect-2.0 [6], Lumina-Video [23] HunyuanVideo [18], SkyReels-v1 [33], and Wan2.1 [39] have demonstrated that ODE-based methods achieve faster convergence and improved controllability in text-to-image and text-to-video generation. As a result, Flow Matching has become a compelling alternative to stochastic diffusion models, offering better interpretability and training stability. Thus, our analysis is based on Flow Matching models, which aim to provide more accurate classifier-free guidance. Guidance in Diffusion Models. Achieving better control over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance (CG) [4], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifierfree guidance (CFG) [9] was proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. \n\nDespite its effectiveness, CFG relies on an unbounded empirical parameter, known as the guidance scale, which determines how strongly the generated output is influenced by the conditional model.",
                    "score": 0.4604507504722727,
                    "section_title": "Related Work",
                    "char_start_offset": 3527,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 425
                        },
                        {
                            "start": 428,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1466
                        },
                        {
                            "start": 1467,
                            "end": 1605
                        },
                        {
                            "start": 1606,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1864
                        },
                        {
                            "start": 1865,
                            "end": 1992
                        },
                        {
                            "start": 1995,
                            "end": 2189
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 127,
                            "end": 130,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 304,
                            "end": 308,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 315,
                            "end": 319,
                            "matchedPaperCorpusId": "222140788"
                        },
                        {
                            "start": 325,
                            "end": 329,
                            "matchedPaperCorpusId": "249240415"
                        },
                        {
                            "start": 352,
                            "end": 356,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 448,
                            "end": 452,
                            "matchedPaperCorpusId": "252734897"
                        },
                        {
                            "start": 812,
                            "end": 816,
                            "matchedPaperCorpusId": "252111177"
                        },
                        {
                            "start": 822,
                            "end": 825,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 1518,
                            "end": 1521,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9521484375
                }
            ],
            "relevance_judgement": 0.9521484375,
            "relevance_judgment_input_expanded": "# Title: CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models\n# Venue: arXiv.org\n# Authors: Weichen Fan, Amber Yijia Zheng, Raymond A. Yeh, Ziwei Liu\n## Abstract\nClassifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)\n## Related Work\nDiffusion and Flow-based Models. Unlike generative ad-versarial methods [7] that rely on one-step generation, diffusion models [4] have demonstrated significantly improved performance in generating high-quality samples. Early diffusion models were primarily score-based generative models, including DDPM [10], DDIM [34], EDM [16], and Stable Diffusion [30], which focused on learning the SDEs governing the diffusion process. \n\nNext, Flow Matching [21] provides an alternative approach by directly modeling sample trajectories using ordinary differential equations (ODEs) instead of SDEs. This enables more stable and efficient generative processes by learning a continuous flow field that smoothly transports samples from a prior distribution to the target distribution. Several works, including Rectified Flow [24], SD3 [5], Lumina-Next [45], Flux [20], Vchitect-2.0 [6], Lumina-Video [23] HunyuanVideo [18], SkyReels-v1 [33], and Wan2.1 [39] have demonstrated that ODE-based methods achieve faster convergence and improved controllability in text-to-image and text-to-video generation. As a result, Flow Matching has become a compelling alternative to stochastic diffusion models, offering better interpretability and training stability. Thus, our analysis is based on Flow Matching models, which aim to provide more accurate classifier-free guidance. Guidance in Diffusion Models. Achieving better control over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance (CG) [4], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifierfree guidance (CFG) [9] was proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. \n\nDespite its effectiveness, CFG relies on an unbounded empirical parameter, known as the guidance scale, which determines how strongly the generated output is influenced by the conditional model.",
            "reference_string": "[277510202 | Fan et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 73,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.07645",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2072014290",
                    "name": "C. O'Neill"
                },
                {
                    "authorId": "93622633",
                    "name": "Y. Ting"
                },
                {
                    "authorId": "50062876",
                    "name": "I. Ciuc\u0103"
                },
                {
                    "authorId": "2229023715",
                    "name": "Jack William Miller"
                },
                {
                    "authorId": "2229238231",
                    "name": "Thang Bui"
                }
            ],
            "abstract": "Large Language Models (LLMs) hold immense potential to generate synthetic data of high quality and utility, which has numerous applications from downstream model training to practical data utilisation. However, contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data. To address the coherency issue, we introduce contrastive expert guidance, where the difference between the logit distributions of fine-tuned and base language models is emphasised to ensure domain adherence. In order to ensure diversity, we utilise existing real and synthetic examples as negative prompts to the model. We deem this dual-pronged approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding Repositioning. STEER operates at inference-time and systematically guides the LLMs to strike a balance between adherence to the data distribution (ensuring semantic fidelity) and deviation from prior synthetic examples or existing real datasets (ensuring diversity and authenticity). This delicate balancing act is achieved by dynamically moving towards or away from chosen representations in the latent space. STEER demonstrates improved performance over previous synthetic data generation techniques, exhibiting better balance between data diversity and coherency across three distinct tasks: hypothesis generation, toxic and non-toxic comment generation, and commonsense reasoning task generation. We demonstrate how STEER allows for fine-tuned control over the diversity-coherency trade-off via its hyperparameters, highlighting its versatility.",
            "corpus_id": 260899968,
            "sentences": [
                {
                    "corpus_id": "260899968",
                    "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation",
                    "text": "In doing so, the distribution hones in on tokens exhibiting high probability in the presence of early context but low probability when devoid of it. Similarly, Su et al. (2022) used a degeneration penalty, defined as the maximum cosine similarity between the representation of a continuation and that of all previous tokens, to prevent model degeneration. They refer to this method as contrastive search. \n\nBuilding upon the innovative efforts to exert control over generative models, the concept of classifier-free guidance (CFG) emerged as a further refinement to the field. It embodies a methodology aligning closely with previous advancements but distinctively eliminating the reliance on separate classifier models. The genesis of CFG can be traced back to diffusion models, where techniques were developed to reshape the latent sampling distribution to synchronise more harmoniously with the prompt (Dhariwal and Nichol 2021b). Addressing earlier complexities, Ho and Salimans (2022) synthesised the classifier's role into the model's training process itself, paving a new path for efficiency. \n\nIn the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sanchez et al. 2023). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;Rombach et al. 2022;Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022).",
                    "score": 0.6244752809926196,
                    "section_title": "Related work Evolution of Generative Models and Control Over Conditional Generation",
                    "char_start_offset": 7648,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 404
                        },
                        {
                            "start": 407,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1099
                        },
                        {
                            "start": 1102,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1503
                        },
                        {
                            "start": 1506,
                            "end": 1509
                        },
                        {
                            "start": 1512,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1736
                        },
                        {
                            "start": 1737,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2160
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1239,
                            "end": 1260,
                            "matchedPaperCorpusId": "44104089"
                        },
                        {
                            "start": 1865,
                            "end": 1885,
                            "matchedPaperCorpusId": "247958012"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95068359375
                }
            ],
            "relevance_judgement": 0.95068359375,
            "relevance_judgment_input_expanded": "# Title: Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation\n# Venue: arXiv.org\n# Authors: C. O'Neill, Y. Ting, I. Ciuc\u0103, Jack William Miller, Thang Bui\n## Abstract\nLarge Language Models (LLMs) hold immense potential to generate synthetic data of high quality and utility, which has numerous applications from downstream model training to practical data utilisation. However, contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data. To address the coherency issue, we introduce contrastive expert guidance, where the difference between the logit distributions of fine-tuned and base language models is emphasised to ensure domain adherence. In order to ensure diversity, we utilise existing real and synthetic examples as negative prompts to the model. We deem this dual-pronged approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding Repositioning. STEER operates at inference-time and systematically guides the LLMs to strike a balance between adherence to the data distribution (ensuring semantic fidelity) and deviation from prior synthetic examples or existing real datasets (ensuring diversity and authenticity). This delicate balancing act is achieved by dynamically moving towards or away from chosen representations in the latent space. STEER demonstrates improved performance over previous synthetic data generation techniques, exhibiting better balance between data diversity and coherency across three distinct tasks: hypothesis generation, toxic and non-toxic comment generation, and commonsense reasoning task generation. We demonstrate how STEER allows for fine-tuned control over the diversity-coherency trade-off via its hyperparameters, highlighting its versatility.\n## Related work Evolution of Generative Models and Control Over Conditional Generation\nIn doing so, the distribution hones in on tokens exhibiting high probability in the presence of early context but low probability when devoid of it. Similarly, Su et al. (2022) used a degeneration penalty, defined as the maximum cosine similarity between the representation of a continuation and that of all previous tokens, to prevent model degeneration. They refer to this method as contrastive search. \n\nBuilding upon the innovative efforts to exert control over generative models, the concept of classifier-free guidance (CFG) emerged as a further refinement to the field. It embodies a methodology aligning closely with previous advancements but distinctively eliminating the reliance on separate classifier models. The genesis of CFG can be traced back to diffusion models, where techniques were developed to reshape the latent sampling distribution to synchronise more harmoniously with the prompt (Dhariwal and Nichol 2021b). Addressing earlier complexities, Ho and Salimans (2022) synthesised the classifier's role into the model's training process itself, paving a new path for efficiency. \n\nIn the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sanchez et al. 2023). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;Rombach et al. 2022;Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022).",
            "reference_string": "[260899968 | O'Neill et al. | 2023 | Citations: 1]"
        },
        {
            "title": "The Unreasonable Effectiveness of Guidance for Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.10257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267395983",
                    "name": "Tim Kaiser"
                },
                {
                    "authorId": "1832165240",
                    "name": "Nikolas Adaloglou"
                },
                {
                    "authorId": "2065390431",
                    "name": "M. Kollmann"
                }
            ],
            "abstract": "Guidance is an error-correcting technique used to improve the perceptual quality of images generated by diffusion models. Typically, the correction is achieved by linear extrapolation, using an auxiliary diffusion model that has lower performance than the primary model. Using a 2D toy example, we show that it is highly beneficial when the auxiliary model exhibits similar errors as the primary one but stronger. We verify this finding in higher dimensions, where we show that competitive generative performance to state-of-the-art guidance methods can be achieved when the auxiliary model differs from the primary one only by having stronger weight regularization. As an independent contribution, we investigate whether upweighting long-range spatial dependencies improves visual fidelity. The result is a novel guidance method, which we call sliding window guidance (SWG), that guides the primary model with itself by constraining its receptive field. Intriguingly, SWG aligns better with human preferences than state-of-the-art guidance methods while requiring neither training, architectural modifications, nor class conditioning. The code will be released.",
            "corpus_id": 274117064,
            "sentences": [
                {
                    "corpus_id": "274117064",
                    "title": "The Unreasonable Effectiveness of Guidance for Diffusion Models",
                    "text": "Diffusion models (DMs) have emerged as a powerful approach for generative tasks, achieving remarkable success in areas such as image synthesis and text-to-image generation [1,17,24,27,41,44,45]. DMs are a class of generative models that iteratively transform noise samples into samples that are close to a desired data distribution. Despite their success, DMs often fail to generate high-quality samples in the visual domain [3] and require guidance techniques to improve visual fidelity (Fig. 1). The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16]. \n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42]. \n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25].",
                    "score": 0.4842213668257808,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 880
                        },
                        {
                            "start": 883,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1287
                        },
                        {
                            "start": 1288,
                            "end": 1425
                        },
                        {
                            "start": 1428,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1907
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 184,
                            "end": 187,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 1487,
                            "end": 1491,
                            "matchedPaperCorpusId": "254854389"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                }
            ],
            "relevance_judgement": 0.9482421875,
            "relevance_judgment_input_expanded": "# Title: The Unreasonable Effectiveness of Guidance for Diffusion Models\n# Venue: arXiv.org\n# Authors: Tim Kaiser, Nikolas Adaloglou, M. Kollmann\n## Abstract\nGuidance is an error-correcting technique used to improve the perceptual quality of images generated by diffusion models. Typically, the correction is achieved by linear extrapolation, using an auxiliary diffusion model that has lower performance than the primary model. Using a 2D toy example, we show that it is highly beneficial when the auxiliary model exhibits similar errors as the primary one but stronger. We verify this finding in higher dimensions, where we show that competitive generative performance to state-of-the-art guidance methods can be achieved when the auxiliary model differs from the primary one only by having stronger weight regularization. As an independent contribution, we investigate whether upweighting long-range spatial dependencies improves visual fidelity. The result is a novel guidance method, which we call sliding window guidance (SWG), that guides the primary model with itself by constraining its receptive field. Intriguingly, SWG aligns better with human preferences than state-of-the-art guidance methods while requiring neither training, architectural modifications, nor class conditioning. The code will be released.\n## Introduction\nDiffusion models (DMs) have emerged as a powerful approach for generative tasks, achieving remarkable success in areas such as image synthesis and text-to-image generation [1,17,24,27,41,44,45]. DMs are a class of generative models that iteratively transform noise samples into samples that are close to a desired data distribution. Despite their success, DMs often fail to generate high-quality samples in the visual domain [3] and require guidance techniques to improve visual fidelity (Fig. 1). The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16]. \n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42]. \n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25].",
            "reference_string": "[274117064 | Kaiser et al. | 2024 | Citations: 1]"
        },
        {
            "title": "What Makes a Good Diffusion Planner for Decision Making?",
            "venue": "International Conference on Learning Representations",
            "year": 2025,
            "reference_count": 57,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.00535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315320844",
                    "name": "Haofei Lu"
                },
                {
                    "authorId": "2268676735",
                    "name": "Dongqi Han"
                },
                {
                    "authorId": "2152966656",
                    "name": "Yifei Shen"
                },
                {
                    "authorId": "2303586558",
                    "name": "Dongsheng Li"
                }
            ],
            "abstract": "Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks.",
            "corpus_id": 276741986,
            "sentences": [
                {
                    "corpus_id": "276741986",
                    "title": "What Makes a Good Diffusion Planner for Decision Making?",
                    "text": "For decision making tasks, guided sampling algorithms are used to generate desired plans or actions. In this work, we compare three types of different guided sampling methods: classifier guidance (Dhariwal & Nichol, 2021) (CG), classifier-free guidance (CFG) (Ho & Salimans, 2021), and Monte Carlo sampling from selections (MCSS). \n\nClassifier guidance: Classifier guidance (CG) is introduced to guide the unconditional diffusion models q t (x t ) to generate data over condition c. The conditioned score function is formulated as: \n\nwhere the second term is also know as a noised classifier that predict the condition using noised data x t . During sampling, the gradient of the classifier is then applied to the predicted noise \u03f5 \u03b8 (x t , t): \n\nwhere w is a weighting factor that controls the strength of the classifier guidance. For CG sampling, we tuned w in range [0.001, 10] on each task. \n\nClassifier-free guidance: To avoid training classifiers, classifier-free guidance (CFG) is proposed. The main idea of CFG is to train a diffusion model that can be used for both conditional noise predictor \u03f5 \u03b8 (x t , t, c) and unconditional noise predictor \u03f5 \u03b8 (x t , t): \n\nwhere \u03f5 \u03b8 (x t , t) = \u03f5 \u03b8 (x t , t, \u2205). Noise prediction of \u03f5 \u03b8 (x t , t, \u2205) and \u03f5 \u03b8 (x t , t, c) can be jointly learned by randomly discard conditioning with probability of p uncond . For decision making tasks, we can train diffusion models using condition of discounted returns, and using classifier-free guidance for better plan sampling. We can normalize the discounted return in the dataset for training, and use condition of 1 as target return for CFG sampling during inference (Ajay et al., 2022). However, experiments shows that fixing 1 as target may lead to unrealistic or unstable plans.",
                    "score": 0.46144544806532717,
                    "section_title": "A GUIDED SAMPLING ALGORITHMS",
                    "char_start_offset": 27504,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 330
                        },
                        {
                            "start": 333,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 531
                        },
                        {
                            "start": 534,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 744
                        },
                        {
                            "start": 747,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 894
                        },
                        {
                            "start": 897,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1168
                        },
                        {
                            "start": 1171,
                            "end": 1210
                        },
                        {
                            "start": 1211,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1769
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 259,
                            "end": 280,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1655,
                            "end": 1674,
                            "matchedPaperCorpusId": "254044710"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.947265625
                }
            ],
            "relevance_judgement": 0.947265625,
            "relevance_judgment_input_expanded": "# Title: What Makes a Good Diffusion Planner for Decision Making?\n# Venue: International Conference on Learning Representations\n# Authors: Haofei Lu, Dongqi Han, Yifei Shen, Dongsheng Li\n## Abstract\nDiffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks.\n## A GUIDED SAMPLING ALGORITHMS\nFor decision making tasks, guided sampling algorithms are used to generate desired plans or actions. In this work, we compare three types of different guided sampling methods: classifier guidance (Dhariwal & Nichol, 2021) (CG), classifier-free guidance (CFG) (Ho & Salimans, 2021), and Monte Carlo sampling from selections (MCSS). \n\nClassifier guidance: Classifier guidance (CG) is introduced to guide the unconditional diffusion models q t (x t ) to generate data over condition c. The conditioned score function is formulated as: \n\nwhere the second term is also know as a noised classifier that predict the condition using noised data x t . During sampling, the gradient of the classifier is then applied to the predicted noise \u03f5 \u03b8 (x t , t): \n\nwhere w is a weighting factor that controls the strength of the classifier guidance. For CG sampling, we tuned w in range [0.001, 10] on each task. \n\nClassifier-free guidance: To avoid training classifiers, classifier-free guidance (CFG) is proposed. The main idea of CFG is to train a diffusion model that can be used for both conditional noise predictor \u03f5 \u03b8 (x t , t, c) and unconditional noise predictor \u03f5 \u03b8 (x t , t): \n\nwhere \u03f5 \u03b8 (x t , t) = \u03f5 \u03b8 (x t , t, \u2205). Noise prediction of \u03f5 \u03b8 (x t , t, \u2205) and \u03f5 \u03b8 (x t , t, c) can be jointly learned by randomly discard conditioning with probability of p uncond . For decision making tasks, we can train diffusion models using condition of discounted returns, and using classifier-free guidance for better plan sampling. We can normalize the discounted return in the dataset for training, and use condition of 1 as target return for CFG sampling during inference (Ajay et al., 2022). However, experiments shows that fixing 1 as target may lead to unrealistic or unstable plans.",
            "reference_string": "[276741986 | Lu et al. | 2025 | Citations: 11]"
        },
        {
            "title": "Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.12487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268761467",
                    "name": "Angela Castillo"
                },
                {
                    "authorId": "2275352009",
                    "name": "Jonas Kohler"
                },
                {
                    "authorId": "152978592",
                    "name": "Juan C. P'erez"
                },
                {
                    "authorId": "2275543981",
                    "name": "Juan Pablo P'erez"
                },
                {
                    "authorId": "2264453654",
                    "name": "Albert Pumarola"
                },
                {
                    "authorId": "2268676398",
                    "name": "Bernard Ghanem"
                },
                {
                    "authorId": "2264974778",
                    "name": "Pablo Arbel'aez"
                },
                {
                    "authorId": "35869086",
                    "name": "Ali K. Thabet"
                }
            ],
            "abstract": "This paper presents a comprehensive study on the role of Classifier-Free Guidance (CFG) in text-conditioned diffusion models from the perspective of inference efficiency. In particular, we relax the default choice of applying CFG in all diffusion steps and instead search for efficient guidance policies. We formulate the discovery of such policies in the differentiable Neural Architecture Search framework. Our findings suggest that the denoising steps proposed by CFG become increasingly aligned with simple conditional steps, which renders the extra neural network evaluation of CFG redundant, especially in the second half of the denoising process. Building upon this insight, we propose\"Adaptive Guidance\"(AG), an efficient variant of CFG, that adaptively omits network evaluations when the denoising process displays convergence. Our experiments demonstrate that AG preserves CFG's image quality while reducing computation by 25%. Thus, AG constitutes a plug-and-play alternative to Guidance Distillation, achieving 50% of the speed-ups of the latter while being training-free and retaining the capacity to handle negative prompts. Finally, we uncover further redundancies of CFG in the first half of the diffusion process, showing that entire neural function evaluations can be replaced by simple affine transformations of past score estimates. This method, termed LinearAG, offers even cheaper inference at the cost of deviating from the baseline model. Our findings provide insights into the efficiency of the conditional denoising process that contribute to more practical and swift deployment of text-conditioned diffusion models.",
            "corpus_id": 266374764,
            "sentences": [
                {
                    "corpus_id": "266374764",
                    "title": "Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models",
                    "text": "Diffusion Models (DMs) [14] exhibit outstanding generative capacities across domains such as images [45], video [15], audio [20], human pose estimation [5], and even cosmological simulations [48]. DMs generate data by sampling a noise instance and iteratively denoising the instance with a neural network. The sequential nature of this denoising operation makes sampling from DMs a slow and expensive process. In particular, the time required to sample from a given DM is a function of (i) the latency of each denoising iteration, and (ii) the total number of denoising steps. \n\nMany practical applications entail \"conditional generation\", where DMs create samples conditioned on specific criteria such as a class, a text, or an image [37]. DMs achieve conditional generation by replacing regular (i.e., unconditional) denoising steps with conditional ones, in which the neural network processes both the input and the condition. While conditional denoising steps provide competitive results, Ho et al. proposed the technique of Classifier-Free Guidance (CFG) [13] to enhance sample quality. CFG enriches the conditional denoising process by leveraging implicit priors of the diffusion model itself. Despite its simplicity, CFG significantly improves sample quality in tasks such as textto-image [8,37,40], image editing [4,35,50], and textto-3D [26,43]. Yet, the benefits of CFG come at the cost of duplicating the Number of Function Evaluations (NFEs), since each denoising iteration requires evaluating the neural network both conditionally and unconditionally. Adding to the problem, neural networks used in practice for DMs max out the parallelization capacity of production-grade GPUs 1 , preventing simultaneous computation of the conditional and unconditional function evaluations. \n\nIn this paper, we improve the efficiency of text-to-image diffusion models that use Classifier-Free Guidance (CFG). Our analysis reveals that not all denoising steps contribute equally to image quality, suggesting that the traditional policy of applying CFG in all steps is sub-optimal.",
                    "score": 0.4411440716981926,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 576
                        },
                        {
                            "start": 579,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 929
                        },
                        {
                            "start": 930,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1789
                        },
                        {
                            "start": 1792,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2078
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 100,
                            "end": 104,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1302,
                            "end": 1305,
                            "matchedPaperCorpusId": "254854389"
                        },
                        {
                            "start": 1321,
                            "end": 1324,
                            "matchedPaperCorpusId": "253581213"
                        },
                        {
                            "start": 1346,
                            "end": 1350,
                            "matchedPaperCorpusId": "253708074"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.947265625
                }
            ],
            "relevance_judgement": 0.947265625,
            "relevance_judgment_input_expanded": "# Title: Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models\n# Venue: arXiv.org\n# Authors: Angela Castillo, Jonas Kohler, Juan C. P'erez, Juan Pablo P'erez, Albert Pumarola, Bernard Ghanem, Pablo Arbel'aez, Ali K. Thabet\n## Abstract\nThis paper presents a comprehensive study on the role of Classifier-Free Guidance (CFG) in text-conditioned diffusion models from the perspective of inference efficiency. In particular, we relax the default choice of applying CFG in all diffusion steps and instead search for efficient guidance policies. We formulate the discovery of such policies in the differentiable Neural Architecture Search framework. Our findings suggest that the denoising steps proposed by CFG become increasingly aligned with simple conditional steps, which renders the extra neural network evaluation of CFG redundant, especially in the second half of the denoising process. Building upon this insight, we propose\"Adaptive Guidance\"(AG), an efficient variant of CFG, that adaptively omits network evaluations when the denoising process displays convergence. Our experiments demonstrate that AG preserves CFG's image quality while reducing computation by 25%. Thus, AG constitutes a plug-and-play alternative to Guidance Distillation, achieving 50% of the speed-ups of the latter while being training-free and retaining the capacity to handle negative prompts. Finally, we uncover further redundancies of CFG in the first half of the diffusion process, showing that entire neural function evaluations can be replaced by simple affine transformations of past score estimates. This method, termed LinearAG, offers even cheaper inference at the cost of deviating from the baseline model. Our findings provide insights into the efficiency of the conditional denoising process that contribute to more practical and swift deployment of text-conditioned diffusion models.\n## Introduction\nDiffusion Models (DMs) [14] exhibit outstanding generative capacities across domains such as images [45], video [15], audio [20], human pose estimation [5], and even cosmological simulations [48]. DMs generate data by sampling a noise instance and iteratively denoising the instance with a neural network. The sequential nature of this denoising operation makes sampling from DMs a slow and expensive process. In particular, the time required to sample from a given DM is a function of (i) the latency of each denoising iteration, and (ii) the total number of denoising steps. \n\nMany practical applications entail \"conditional generation\", where DMs create samples conditioned on specific criteria such as a class, a text, or an image [37]. DMs achieve conditional generation by replacing regular (i.e., unconditional) denoising steps with conditional ones, in which the neural network processes both the input and the condition. While conditional denoising steps provide competitive results, Ho et al. proposed the technique of Classifier-Free Guidance (CFG) [13] to enhance sample quality. CFG enriches the conditional denoising process by leveraging implicit priors of the diffusion model itself. Despite its simplicity, CFG significantly improves sample quality in tasks such as textto-image [8,37,40], image editing [4,35,50], and textto-3D [26,43]. Yet, the benefits of CFG come at the cost of duplicating the Number of Function Evaluations (NFEs), since each denoising iteration requires evaluating the neural network both conditionally and unconditionally. Adding to the problem, neural networks used in practice for DMs max out the parallelization capacity of production-grade GPUs 1 , preventing simultaneous computation of the conditional and unconditional function evaluations. \n\nIn this paper, we improve the efficiency of text-to-image diffusion models that use Classifier-Free Guidance (CFG). Our analysis reveals that not all denoising steps contribute equally to image quality, suggesting that the traditional policy of applying CFG in all steps is sub-optimal.",
            "reference_string": "[266374764 | Castillo et al. | 2023 | Citations: 13]"
        },
        {
            "title": "Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.21048, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287733778",
                    "name": "Jiatao Gu"
                },
                {
                    "authorId": "2304371756",
                    "name": "Ying Shen"
                },
                {
                    "authorId": "2443456",
                    "name": "Shuangfei Zhai"
                },
                {
                    "authorId": "2254045488",
                    "name": "Yizhe Zhang"
                },
                {
                    "authorId": "3111912",
                    "name": "N. Jaitly"
                },
                {
                    "authorId": "49158771",
                    "name": "J. Susskind"
                }
            ],
            "abstract": "Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process. In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs. Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latent variables, demonstrating its capability to effectively control and direct the image generation process.",
            "corpus_id": 270199289,
            "sentences": [
                {
                    "corpus_id": "270199289",
                    "title": "Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling",
                    "text": "Classifier-free Guidance An intriguing property of conditional diffusion models is that we can easily guide the iterative sampling process for better sampling quality.For instance, Ho and Salimans [2021] introduced Classifier-free Guidance (CFG), which utilizes the diffusion model itself to perform guidance at test time.More specifically, we perform sampling using the following linear combination:\n\nwhere \u03b3 is the guidance weight, and x \u03b8 (x t ) = x \u03b8 (x t , c = \u2205) is the unconditional denoising output.\n\nDuring training, we drop the condition c with certain probability p uncond to facilitate unconditional prediction.When \u03b3 > 1, CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation.\n\nCompared to autoregressive models, diffusion models are more flexible in adjusting sample steps, allowing for the utilization of noise schedules to learn different frequencies.Additionally, with the use of CFG, diffusion models can achieve higher quality images with much fewer parameters than autoregressive models.However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both.",
                    "score": 0.525029940474101,
                    "section_title": "Preliminaries",
                    "char_start_offset": 7014,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 167,
                            "end": 322
                        },
                        {
                            "start": 322,
                            "end": 400
                        },
                        {
                            "start": 402,
                            "end": 507
                        },
                        {
                            "start": 509,
                            "end": 623
                        },
                        {
                            "start": 623,
                            "end": 786
                        },
                        {
                            "start": 788,
                            "end": 964
                        },
                        {
                            "start": 964,
                            "end": 1104
                        },
                        {
                            "start": 1104,
                            "end": 1274
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 181,
                            "end": 203,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                }
            ],
            "relevance_judgement": 0.94580078125,
            "relevance_judgment_input_expanded": "# Title: Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling\n# Venue: Neural Information Processing Systems\n# Authors: Jiatao Gu, Ying Shen, Shuangfei Zhai, Yizhe Zhang, N. Jaitly, J. Susskind\n## Abstract\nDiffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process. In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs. Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latent variables, demonstrating its capability to effectively control and direct the image generation process.\n## Preliminaries\nClassifier-free Guidance An intriguing property of conditional diffusion models is that we can easily guide the iterative sampling process for better sampling quality.For instance, Ho and Salimans [2021] introduced Classifier-free Guidance (CFG), which utilizes the diffusion model itself to perform guidance at test time.More specifically, we perform sampling using the following linear combination:\n\nwhere \u03b3 is the guidance weight, and x \u03b8 (x t ) = x \u03b8 (x t , c = \u2205) is the unconditional denoising output.\n\nDuring training, we drop the condition c with certain probability p uncond to facilitate unconditional prediction.When \u03b3 > 1, CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation.\n\nCompared to autoregressive models, diffusion models are more flexible in adjusting sample steps, allowing for the utilization of noise schedules to learn different frequencies.Additionally, with the use of CFG, diffusion models can achieve higher quality images with much fewer parameters than autoregressive models.However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both.",
            "reference_string": "[270199289 | Gu et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Diffusion Models and Representation Learning: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 183,
            "citation_count": 24,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309173490",
                    "name": "Michael Fuest"
                },
                {
                    "authorId": "2273647740",
                    "name": "Pingchuan Ma"
                },
                {
                    "authorId": "2309172432",
                    "name": "Ming Gui"
                },
                {
                    "authorId": "2273655571",
                    "name": "Johannes S. Fischer"
                },
                {
                    "authorId": "2292259521",
                    "name": "Vincent Tao Hu"
                },
                {
                    "authorId": "2257038709",
                    "name": "Bjorn Ommer"
                }
            ],
            "abstract": "Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models' essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy",
            "corpus_id": 270869763,
            "sentences": [
                {
                    "corpus_id": "270869763",
                    "title": "Diffusion Models and Representation Learning: A Survey",
                    "text": "To address this limitation, Classifier-free guidance (CFG) [67] eliminates the need for a pre-trained classifier.CFG works by training an unconditional diffusion model parametrized by \u03f5 \u03b8 (x t , t, \u03d5) together with a conditional model parametrized by \u03f5 \u03b8 (x t , t, c).For the unconditional model, a null input token \u03d5 is used as a conditioning signal c.The network is trained by randomly dropping out the conditioning signal with probability p uncond .Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network.Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73,100].These methods do not need annotated data, allowing the use of larger unlabelled datasets.\n\nTable 1 shows the requirements of current guidance methods.While classifier and classifier-free guidance improve generation results, they require annotated training data.Self-guidance and online guidance are fully selfsupervised alternatives that achieve competitive performance without annotations.\n\nClassifier and classifier-free guidance are controlled generation methods that rely on conditional training.Trainingfree approaches modify the generation process of a pretrained model by binding multiple diffusion processes [14] or using time-independent energy functions [179].Other controlled generation methods take a variational perspective [54,119,146,164], treating controlled generation as a source point optimization problem [17].The goal is to find samples x that minimize a loss function L(x) and are likely under the model's distribution p.The optimization is formulated as min x0 L(x), where x 0 is the source noise point.The loss function L(x) can be modified for conditional sampling to generate a sample belonging to a particular class y.",
                    "score": 0.49409342390201094,
                    "section_title": "Diffusion Model Guidance",
                    "char_start_offset": 17610,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 113,
                            "end": 268
                        },
                        {
                            "start": 268,
                            "end": 353
                        },
                        {
                            "start": 353,
                            "end": 452
                        },
                        {
                            "start": 452,
                            "end": 557
                        },
                        {
                            "start": 559,
                            "end": 722
                        },
                        {
                            "start": 722,
                            "end": 906
                        },
                        {
                            "start": 906,
                            "end": 995
                        },
                        {
                            "start": 997,
                            "end": 1056
                        },
                        {
                            "start": 1056,
                            "end": 1167
                        },
                        {
                            "start": 1167,
                            "end": 1296
                        },
                        {
                            "start": 1298,
                            "end": 1406
                        },
                        {
                            "start": 1406,
                            "end": 1576
                        },
                        {
                            "start": 1576,
                            "end": 1736
                        },
                        {
                            "start": 1736,
                            "end": 1849
                        },
                        {
                            "start": 1849,
                            "end": 1932
                        },
                        {
                            "start": 1932,
                            "end": 2051
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 59,
                            "end": 63,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1522,
                            "end": 1526,
                            "matchedPaperCorpusId": "256900756"
                        },
                        {
                            "start": 1651,
                            "end": 1655,
                            "matchedPaperCorpusId": "258418154"
                        },
                        {
                            "start": 1655,
                            "end": 1659,
                            "matchedPaperCorpusId": "257757144"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                }
            ],
            "relevance_judgement": 0.94580078125,
            "relevance_judgment_input_expanded": "# Title: Diffusion Models and Representation Learning: A Survey\n# Venue: arXiv.org\n# Authors: Michael Fuest, Pingchuan Ma, Ming Gui, Johannes S. Fischer, Vincent Tao Hu, Bjorn Ommer\n## Abstract\nDiffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models' essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy\n## Diffusion Model Guidance\nTo address this limitation, Classifier-free guidance (CFG) [67] eliminates the need for a pre-trained classifier.CFG works by training an unconditional diffusion model parametrized by \u03f5 \u03b8 (x t , t, \u03d5) together with a conditional model parametrized by \u03f5 \u03b8 (x t , t, c).For the unconditional model, a null input token \u03d5 is used as a conditioning signal c.The network is trained by randomly dropping out the conditioning signal with probability p uncond .Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network.Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73,100].These methods do not need annotated data, allowing the use of larger unlabelled datasets.\n\nTable 1 shows the requirements of current guidance methods.While classifier and classifier-free guidance improve generation results, they require annotated training data.Self-guidance and online guidance are fully selfsupervised alternatives that achieve competitive performance without annotations.\n\nClassifier and classifier-free guidance are controlled generation methods that rely on conditional training.Trainingfree approaches modify the generation process of a pretrained model by binding multiple diffusion processes [14] or using time-independent energy functions [179].Other controlled generation methods take a variational perspective [54,119,146,164], treating controlled generation as a source point optimization problem [17].The goal is to find samples x that minimize a loss function L(x) and are likely under the model's distribution p.The optimization is formulated as min x0 L(x), where x 0 is the source noise point.The loss function L(x) can be modified for conditional sampling to generate a sample belonging to a particular class y.",
            "reference_string": "[270869763 | Fuest et al. | 2024 | Citations: 24]"
        },
        {
            "title": "Exploring the Effectiveness of Dataset Synthesis: An application of Apple Detection in Orchards",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 54,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.11763",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.11763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "104149480",
                    "name": "A. V. Meekeren"
                },
                {
                    "authorId": "51304755",
                    "name": "Maya Aghaei"
                },
                {
                    "authorId": "40109500",
                    "name": "K. Dijkstra"
                }
            ],
            "abstract": "Deep object detection models have achieved notable successes in recent years, but one major obstacle remains: the requirement for a large amount of training data. Obtaining such data is a tedious process and is mainly time consuming, leading to the exploration of new research avenues like synthetic data generation techniques. In this study, we explore the usability of Stable Diffusion 2.1-base for generating synthetic datasets of apple trees for object detection and compare it to a baseline model trained on real-world data. After creating a dataset of realistic apple trees with prompt engineering and utilizing a previously trained Stable Diffusion model, the custom dataset was annotated and evaluated by training a YOLOv5m object detection model to predict apples in a real-world apple detection dataset. YOLOv5m was chosen for its rapid inference time and minimal hardware demands. Results demonstrate that the model trained on generated data is slightly underperforming compared to a baseline model trained on real-world images when evaluated on a set of real-world images. However, these findings remain highly promising, as the average precision difference is only 0.09 and 0.06, respectively. Qualitative results indicate that the model can accurately predict the location of apples, except in cases of heavy shading. These findings illustrate the potential of synthetic data generation techniques as a viable alternative to the collection of extensive training data for object detection models.",
            "corpus_id": 259212148,
            "sentences": [
                {
                    "corpus_id": "259212148",
                    "title": "Exploring the Effectiveness of Dataset Synthesis: An application of Apple Detection in Orchards",
                    "text": "A useful technique for obtaining realistic images in Stable Diffusion involves utilizing Prompt Engineering [40]. This entails creating input prompts that can control the image generation process, including initial images or masks, as well as textual or other cues that can be fed into the model as input. \n\nPrompt engineering enables the creation of highly specific and detailled images using Stable Diffusion. It can be employed to generate images of particular objects, scenes, or styles, as well as to manipulate the image generation process to achieve desired artistic effects. \n\nTo generate high-quality images using Stable Diffusion, carefully crafted positive and negative prompts can be employed. The inclusion of negative prompts in Stable Diffusion [41] was a refinement over Latent Diffusion [12] and involved examining the distinction between the image that is being generated, to steer the final image towards the positive prompt and steer away from the negative prompt. \n\nWhile prompt-based image generation can produce realistic images, using Classifier-free Guidance (CFG) [42] can provide even greater control over the generation process. CFG, in Stable Diffusion, amplifies the effect of the text prompt on the generated image. By default, Stable Diffusion applies a classifier to the text prompt to guide the generation of the image [12]. However, CFG allows for fine-tuning the influence of this classifier, resulting in more creative control over the generated image. Typically, CFG is defined to be in a range between 1 and 30, with lower values generating more creative images. \n\nMore specifically, CFG determines the trade-off between the coverage of modes and image fidelity [42]. When set to 1, the model generates samples based solely on the prior distribution, without any guidance. As the guidance scale increases, the model is instructed to produce samples that better match some given condition. The classifier-free aspect of this technique refers to the fact that it does not require training a classifier to incorporate guidance during generation [42]. Instead, the model is guided by adding a penalty term to the generation process, forcing the generated images to match the desired condition.",
                    "score": 0.4166387137351548,
                    "section_title": "Prompt Engineering",
                    "char_start_offset": 11081,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 305
                        },
                        {
                            "start": 308,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 582
                        },
                        {
                            "start": 585,
                            "end": 705
                        },
                        {
                            "start": 706,
                            "end": 984
                        },
                        {
                            "start": 987,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1489
                        },
                        {
                            "start": 1490,
                            "end": 1601
                        },
                        {
                            "start": 1604,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1811
                        },
                        {
                            "start": 1812,
                            "end": 1927
                        },
                        {
                            "start": 1928,
                            "end": 2086
                        },
                        {
                            "start": 2087,
                            "end": 2228
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9453125
                }
            ],
            "relevance_judgement": 0.9453125,
            "relevance_judgment_input_expanded": "# Title: Exploring the Effectiveness of Dataset Synthesis: An application of Apple Detection in Orchards\n# Venue: arXiv.org\n# Authors: A. V. Meekeren, Maya Aghaei, K. Dijkstra\n## Abstract\nDeep object detection models have achieved notable successes in recent years, but one major obstacle remains: the requirement for a large amount of training data. Obtaining such data is a tedious process and is mainly time consuming, leading to the exploration of new research avenues like synthetic data generation techniques. In this study, we explore the usability of Stable Diffusion 2.1-base for generating synthetic datasets of apple trees for object detection and compare it to a baseline model trained on real-world data. After creating a dataset of realistic apple trees with prompt engineering and utilizing a previously trained Stable Diffusion model, the custom dataset was annotated and evaluated by training a YOLOv5m object detection model to predict apples in a real-world apple detection dataset. YOLOv5m was chosen for its rapid inference time and minimal hardware demands. Results demonstrate that the model trained on generated data is slightly underperforming compared to a baseline model trained on real-world images when evaluated on a set of real-world images. However, these findings remain highly promising, as the average precision difference is only 0.09 and 0.06, respectively. Qualitative results indicate that the model can accurately predict the location of apples, except in cases of heavy shading. These findings illustrate the potential of synthetic data generation techniques as a viable alternative to the collection of extensive training data for object detection models.\n## Prompt Engineering\nA useful technique for obtaining realistic images in Stable Diffusion involves utilizing Prompt Engineering [40]. This entails creating input prompts that can control the image generation process, including initial images or masks, as well as textual or other cues that can be fed into the model as input. \n\nPrompt engineering enables the creation of highly specific and detailled images using Stable Diffusion. It can be employed to generate images of particular objects, scenes, or styles, as well as to manipulate the image generation process to achieve desired artistic effects. \n\nTo generate high-quality images using Stable Diffusion, carefully crafted positive and negative prompts can be employed. The inclusion of negative prompts in Stable Diffusion [41] was a refinement over Latent Diffusion [12] and involved examining the distinction between the image that is being generated, to steer the final image towards the positive prompt and steer away from the negative prompt. \n\nWhile prompt-based image generation can produce realistic images, using Classifier-free Guidance (CFG) [42] can provide even greater control over the generation process. CFG, in Stable Diffusion, amplifies the effect of the text prompt on the generated image. By default, Stable Diffusion applies a classifier to the text prompt to guide the generation of the image [12]. However, CFG allows for fine-tuning the influence of this classifier, resulting in more creative control over the generated image. Typically, CFG is defined to be in a range between 1 and 30, with lower values generating more creative images. \n\nMore specifically, CFG determines the trade-off between the coverage of modes and image fidelity [42]. When set to 1, the model generates samples based solely on the prior distribution, without any guidance. As the guidance scale increases, the model is instructed to produce samples that better match some given condition. The classifier-free aspect of this technique refers to the fact that it does not require training a classifier to incorporate guidance during generation [42]. Instead, the model is guided by adding a penalty term to the generation process, forcing the generated images to match the desired condition.",
            "reference_string": "[259212148 | Meekeren et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Creativity and Machine Learning: A Survey",
            "venue": "ACM Computing Surveys",
            "year": 2021,
            "reference_count": 344,
            "citation_count": 42,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3664595",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.02726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2067291198",
                    "name": "Giorgio Franceschelli"
                },
                {
                    "authorId": "1806767",
                    "name": "Mirco Musolesi"
                }
            ],
            "abstract": "There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.",
            "corpus_id": 233168627,
            "sentences": [
                {
                    "corpus_id": "233168627",
                    "title": "Creativity and Machine Learning: A Survey",
                    "text": "In order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. A possibility is to use classifier guidance [63]: the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance [117]: to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) [198] demonstrates how classifier-free guidance can be effectively used to generate text-conditional images. In addition, it shows how diffusion models can be used for image editing by fine-tuning in order to reconstruct masked regions. Performance improvement can be obtained by means of a cascade of multiple diffusion models performing conditioning augmentation [116]. Notably, the diffusion model can operate on latent vectors instead of real images. Stable Diffusion [232] employs a diffusion model in the latent space of a pre-trained autoencoder. Similarly, DALL-E 2 [221] generates images by conditioning with image representations. At first, it learns a prior diffusion model to generate possible CLIP image embeddings from a given text caption, i.e., conditioned by its CLIP text embedding. Then, a diffusion decoder produces images conditioned by the image embedding. The generation quality can be further improved by means of generated captions for the images in the training set [19]. Imagen [235] uses instead a cascaded diffusion decoder, together with a frozen language model as a text encoder to increase the quality of output. \n\nAlthough the approach is particularly suitable for images, applications to other data sources have been developed as well. DiffWave [152] and WaveGrad [45] use diffusion models to generate audio. They overcome the continuousdiscrete dichotomy by working on waveform.",
                    "score": 0.4199621459439761,
                    "section_title": "Diffusion Models",
                    "char_start_offset": 46859,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 618
                        },
                        {
                            "start": 621,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1039
                        },
                        {
                            "start": 1040,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1800
                        },
                        {
                            "start": 1801,
                            "end": 1947
                        },
                        {
                            "start": 1950,
                            "end": 2072
                        },
                        {
                            "start": 2073,
                            "end": 2145
                        },
                        {
                            "start": 2146,
                            "end": 2216
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 194,
                            "end": 198,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 365,
                            "end": 370,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 803,
                            "end": 808,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 1168,
                            "end": 1173,
                            "matchedPaperCorpusId": "235619773"
                        },
                        {
                            "start": 1275,
                            "end": 1280,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1808,
                            "end": 1813,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 2082,
                            "end": 2087,
                            "matchedPaperCorpusId": "221818900"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9443359375
                }
            ],
            "relevance_judgement": 0.9443359375,
            "relevance_judgment_input_expanded": "# Title: Creativity and Machine Learning: A Survey\n# Venue: ACM Computing Surveys\n# Authors: Giorgio Franceschelli, Mirco Musolesi\n## Abstract\nThere is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.\n## Diffusion Models\nIn order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. A possibility is to use classifier guidance [63]: the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance [117]: to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) [198] demonstrates how classifier-free guidance can be effectively used to generate text-conditional images. In addition, it shows how diffusion models can be used for image editing by fine-tuning in order to reconstruct masked regions. Performance improvement can be obtained by means of a cascade of multiple diffusion models performing conditioning augmentation [116]. Notably, the diffusion model can operate on latent vectors instead of real images. Stable Diffusion [232] employs a diffusion model in the latent space of a pre-trained autoencoder. Similarly, DALL-E 2 [221] generates images by conditioning with image representations. At first, it learns a prior diffusion model to generate possible CLIP image embeddings from a given text caption, i.e., conditioned by its CLIP text embedding. Then, a diffusion decoder produces images conditioned by the image embedding. The generation quality can be further improved by means of generated captions for the images in the training set [19]. Imagen [235] uses instead a cascaded diffusion decoder, together with a frozen language model as a text encoder to increase the quality of output. \n\nAlthough the approach is particularly suitable for images, applications to other data sources have been developed as well. DiffWave [152] and WaveGrad [45] use diffusion models to generate audio. They overcome the continuousdiscrete dichotomy by working on waveform.",
            "reference_string": "[233168627 | Franceschelli et al. | 2021 | Citations: 42]"
        },
        {
            "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 87,
            "citation_count": 556,
            "influential_citation_count": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.00704",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.00704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2914394",
                    "name": "Huiwen Chang"
                },
                {
                    "authorId": "2146204239",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "152630175",
                    "name": "Jarred Barber"
                },
                {
                    "authorId": "2199119286",
                    "name": "AJ Maschinot"
                },
                {
                    "authorId": "143923528",
                    "name": "Jos\u00e9 Lezama"
                },
                {
                    "authorId": "39978626",
                    "name": "Lu Jiang"
                },
                {
                    "authorId": "152790163",
                    "name": "Ming Yang"
                },
                {
                    "authorId": "1702318",
                    "name": "K. Murphy"
                },
                {
                    "authorId": "1768236",
                    "name": "W. Freeman"
                },
                {
                    "authorId": "144544291",
                    "name": "Michael Rubinstein"
                },
                {
                    "authorId": "2167749913",
                    "name": "Yuanzhen Li"
                },
                {
                    "authorId": "1707347",
                    "name": "Dilip Krishnan"
                }
            ],
            "abstract": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
            "corpus_id": 255372955,
            "sentences": [
                {
                    "corpus_id": "255372955",
                    "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
                    "text": "We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u .",
                    "score": 0.4177404521233028,
                    "section_title": "Classifier Free Guidance",
                    "char_start_offset": 13103,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 479
                        },
                        {
                            "start": 482,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 834
                        },
                        {
                            "start": 837,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1153
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94384765625
                }
            ],
            "relevance_judgement": 0.94384765625,
            "relevance_judgment_input_expanded": "# Title: Muse: Text-To-Image Generation via Masked Generative Transformers\n# Venue: International Conference on Machine Learning\n# Authors: Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jos\u00e9 Lezama, Lu Jiang, Ming Yang, K. Murphy, W. Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan\n## Abstract\nWe present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io\n## Classifier Free Guidance\nWe employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u .",
            "reference_string": "[255372955 | Chang et al. | 2023 | Citations: 556]"
        },
        {
            "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 26,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332536645",
                    "name": "Zak Buzzard"
                }
            ],
            "abstract": "Diffusion models have seen immense success in modelling continuous data across a range of domains such as vision and audio. Despite the challenges of adapting diffusion models to discrete data, recent work explores their application to text generation by working in the continuous embedding space. However, these models lack a natural means to control the inherent trade-off between quality and diversity as afforded by the temperature hyperparameter in autoregressive models, hindering understanding of model performance and restricting generation quality. This work proposes the use of classifier-free guidance and stochastic clamping for manipulating the quality-diversity trade-off on sequence-to-sequence tasks, demonstrating that these techniques may be used to improve the performance of a diffusion language model.",
            "corpus_id": 277043967,
            "sentences": [
                {
                    "corpus_id": "277043967",
                    "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
                    "text": "Classifier-free guidance (CFG) [10], is a method for guiding the diffusion process without requiring a separate classifier. For a conditional diffusion model \u01770 = f \u03b8 (y t , t, x), classifier-free guidance perturbs the predicted value as follows2 : \n\nfor some guidance scale s \u2265 0. Larger values of s shift the prediction y 0 further from the unconditional prediction f \u03b8 (y t , t, \u2205) in the direction of the conditional prediction, causing the condition to have increased effect. \n\nCFG has seen wide usage in the domain of images, and is understood to trade off diversity for fidelity [17,21]. While classifiers have been used to guide the diffusion process in text generation [14], to the best of my knowledge classifierfree guidance has not yet been explored.",
                    "score": 0.8158080416534594,
                    "section_title": "Classifier-free Guidance",
                    "char_start_offset": 5636,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 248
                        },
                        {
                            "start": 251,
                            "end": 480
                        },
                        {
                            "start": 483,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 762
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 586,
                            "end": 590,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 590,
                            "end": 593,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94287109375
                }
            ],
            "relevance_judgement": 0.94287109375,
            "relevance_judgment_input_expanded": "# Title: Understanding the Quality-Diversity Trade-off in Diffusion Language Models\n# Venue: arXiv.org\n# Authors: Zak Buzzard\n## Abstract\nDiffusion models have seen immense success in modelling continuous data across a range of domains such as vision and audio. Despite the challenges of adapting diffusion models to discrete data, recent work explores their application to text generation by working in the continuous embedding space. However, these models lack a natural means to control the inherent trade-off between quality and diversity as afforded by the temperature hyperparameter in autoregressive models, hindering understanding of model performance and restricting generation quality. This work proposes the use of classifier-free guidance and stochastic clamping for manipulating the quality-diversity trade-off on sequence-to-sequence tasks, demonstrating that these techniques may be used to improve the performance of a diffusion language model.\n## Classifier-free Guidance\nClassifier-free guidance (CFG) [10], is a method for guiding the diffusion process without requiring a separate classifier. For a conditional diffusion model \u01770 = f \u03b8 (y t , t, x), classifier-free guidance perturbs the predicted value as follows2 : \n\nfor some guidance scale s \u2265 0. Larger values of s shift the prediction y 0 further from the unconditional prediction f \u03b8 (y t , t, \u2205) in the direction of the conditional prediction, causing the condition to have increased effect. \n\nCFG has seen wide usage in the domain of images, and is understood to trade off diversity for fidelity [17,21]. While classifiers have been used to guide the diffusion process in text generation [14], to the best of my knowledge classifierfree guidance has not yet been explored.",
            "reference_string": "[277043967 | Buzzard | 2025 | Citations: 0]"
        },
        {
            "title": "Diffusion Model with Perceptual Loss",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 63,
            "citation_count": 17,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.00110, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "32370203",
                    "name": "Shanchuan Lin"
                },
                {
                    "authorId": "2277412143",
                    "name": "Xiao Yang"
                }
            ],
            "abstract": "Diffusion models without guidance generate very unrealistic samples. Guidance is used ubiquitously, and previous research has attributed its effect to low-temperature sampling that improves quality by trading off diversity. However, this perspective is incomplete. Our research shows that the choice of the loss objective is the underlying reason raw diffusion models fail to generate desirable samples. In this paper, (1) our analysis shows that the loss objective plays an important role in shaping the learned distribution and the MSE loss derived from theories holds assumptions that misalign with data in practice; (2) we explain the effectiveness of guidance methods from a new perspective of perceptual supervision; (3) we validate our hypothesis by training a diffusion model with a novel self-perceptual loss objective and obtaining much more realistic samples without the need for guidance. We hope our work paves the way for future explorations of the diffusion loss objective.",
            "corpus_id": 266693789,
            "sentences": [
                {
                    "corpus_id": "266693789",
                    "title": "Diffusion Model with Perceptual Loss",
                    "text": "Guidance methods alter the model prediction and guide the sample toward desired regions during the generation process. Classifier Guidance [7] adds classifier gradients to the predicted score to guide the sample generation to maximize the classification. It can turn an unconditional diffusion model conditional. However, it is not evident why applying classifier guidance on an already conditional diffusion model can significantly improve sample quality. Previous research has attributed it to low-temperature sampling [15,24]. Classifier-Free Guidance (CFG) [15] uses Bayes' rule and finds that the diffusion model itself can be inverted as an implicit classifier. Specifically, the model is queried both conditionally and unconditionally at every inference step and the difference is amplified toward the conditional direction. Both methods only work for conditional generation and entangle sample quality with conditional alignment [24]. Self-Supervised Guidance [20] uses self-supervised networks to generate synthetic clustering labels for unconditional data. This allows unconditional data to use CFG for improving quality. Guidance-Free Training [4] shows that CFG can be applied at training. This bakes the guided flow into the model and saves computation during inference. More recently, Discriminator Guidance [25] proposes to train a discriminator network to classify real and generated samples and use it as guidance during diffusion generation. Self-Attention Guidance [18] finds that the self-attention map of the diffusion model can be exploited to enhance quality. Autoguidance [24] finds a smaller or less-trained model can be used as negative guidance to improve quality. Although these methods are effective in improving quality, they also present issues such as increased complexity and reduced diversity [15,24], etc. On the other hand, our research aims to explore the underlying issue: why diffusion models without guidance fail in the first place. \n\nThe loss objective of diffusion models has been studied by prior works. Loss weighting is found to influence perceptual quality and likelihood evaluation [6,13,50] but still cannot produce good samples without guidance. Multiscale loss [19] is proposed to improve high-resolution generation. Smoothness penalty [11] is proposed to enforce smoother latent traversal. l1 distance is explored for colorization and in-painting tasks [41].",
                    "score": 0.49026550739628494,
                    "section_title": "Related Work",
                    "char_start_offset": 3312,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1066
                        },
                        {
                            "start": 1067,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1459
                        },
                        {
                            "start": 1460,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1691
                        },
                        {
                            "start": 1692,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 1973
                        },
                        {
                            "start": 1976,
                            "end": 2047
                        },
                        {
                            "start": 2048,
                            "end": 2195
                        },
                        {
                            "start": 2196,
                            "end": 2267
                        },
                        {
                            "start": 2268,
                            "end": 2341
                        },
                        {
                            "start": 2342,
                            "end": 2410
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 139,
                            "end": 142,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 521,
                            "end": 525,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 561,
                            "end": 565,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1322,
                            "end": 1326,
                            "matchedPaperCorpusId": "254096299"
                        },
                        {
                            "start": 1484,
                            "end": 1488,
                            "matchedPaperCorpusId": "252683688"
                        },
                        {
                            "start": 1827,
                            "end": 1831,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 2130,
                            "end": 2133,
                            "matchedPaperCorpusId": "247922317"
                        },
                        {
                            "start": 2133,
                            "end": 2136,
                            "matchedPaperCorpusId": "257557255"
                        },
                        {
                            "start": 2136,
                            "end": 2139,
                            "matchedPaperCorpusId": "235352469"
                        },
                        {
                            "start": 2212,
                            "end": 2216,
                            "matchedPaperCorpusId": "256274516"
                        },
                        {
                            "start": 2287,
                            "end": 2291,
                            "matchedPaperCorpusId": "266054322"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9404296875
                }
            ],
            "relevance_judgement": 0.9404296875,
            "relevance_judgment_input_expanded": "# Title: Diffusion Model with Perceptual Loss\n# Venue: arXiv.org\n# Authors: Shanchuan Lin, Xiao Yang\n## Abstract\nDiffusion models without guidance generate very unrealistic samples. Guidance is used ubiquitously, and previous research has attributed its effect to low-temperature sampling that improves quality by trading off diversity. However, this perspective is incomplete. Our research shows that the choice of the loss objective is the underlying reason raw diffusion models fail to generate desirable samples. In this paper, (1) our analysis shows that the loss objective plays an important role in shaping the learned distribution and the MSE loss derived from theories holds assumptions that misalign with data in practice; (2) we explain the effectiveness of guidance methods from a new perspective of perceptual supervision; (3) we validate our hypothesis by training a diffusion model with a novel self-perceptual loss objective and obtaining much more realistic samples without the need for guidance. We hope our work paves the way for future explorations of the diffusion loss objective.\n## Related Work\nGuidance methods alter the model prediction and guide the sample toward desired regions during the generation process. Classifier Guidance [7] adds classifier gradients to the predicted score to guide the sample generation to maximize the classification. It can turn an unconditional diffusion model conditional. However, it is not evident why applying classifier guidance on an already conditional diffusion model can significantly improve sample quality. Previous research has attributed it to low-temperature sampling [15,24]. Classifier-Free Guidance (CFG) [15] uses Bayes' rule and finds that the diffusion model itself can be inverted as an implicit classifier. Specifically, the model is queried both conditionally and unconditionally at every inference step and the difference is amplified toward the conditional direction. Both methods only work for conditional generation and entangle sample quality with conditional alignment [24]. Self-Supervised Guidance [20] uses self-supervised networks to generate synthetic clustering labels for unconditional data. This allows unconditional data to use CFG for improving quality. Guidance-Free Training [4] shows that CFG can be applied at training. This bakes the guided flow into the model and saves computation during inference. More recently, Discriminator Guidance [25] proposes to train a discriminator network to classify real and generated samples and use it as guidance during diffusion generation. Self-Attention Guidance [18] finds that the self-attention map of the diffusion model can be exploited to enhance quality. Autoguidance [24] finds a smaller or less-trained model can be used as negative guidance to improve quality. Although these methods are effective in improving quality, they also present issues such as increased complexity and reduced diversity [15,24], etc. On the other hand, our research aims to explore the underlying issue: why diffusion models without guidance fail in the first place. \n\nThe loss objective of diffusion models has been studied by prior works. Loss weighting is found to influence perceptual quality and likelihood evaluation [6,13,50] but still cannot produce good samples without guidance. Multiscale loss [19] is proposed to improve high-resolution generation. Smoothness penalty [11] is proposed to enforce smoother latent traversal. l1 distance is explored for colorization and in-painting tasks [41].",
            "reference_string": "[266693789 | Lin et al. | 2023 | Citations: 17]"
        },
        {
            "title": "Scaling up Masked Diffusion Models on Text",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 81,
            "citation_count": 30,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18514, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2191077545",
                    "name": "Shen Nie"
                },
                {
                    "authorId": "2305318534",
                    "name": "Fengqi Zhu"
                },
                {
                    "authorId": "2325201427",
                    "name": "Chao Du"
                },
                {
                    "authorId": "19201674",
                    "name": "Tianyu Pang"
                },
                {
                    "authorId": "2284062049",
                    "name": "Qian Liu"
                },
                {
                    "authorId": "2309181383",
                    "name": "Guangtao Zeng"
                },
                {
                    "authorId": "2253977831",
                    "name": "Min Lin"
                },
                {
                    "authorId": "2253823025",
                    "name": "Chongxuan Li"
                }
            ],
            "abstract": "Masked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored. This paper establishes the first scaling law for MDMs, demonstrating a scaling rate comparable to autoregressive models (ARMs) and a relatively small compute gap. Motivated by their scalability, we train a family of MDMs with up to 1.1 billion (B) parameters to systematically evaluate their performance against ARMs of comparable or larger sizes. Fully leveraging the probabilistic formulation of MDMs, we propose a simple yet effective unsupervised classifier-free guidance that effectively exploits large-scale unpaired data, boosting performance for conditional inference. In language understanding, the 1.1B MDM outperforms the 1.1B TinyLlama model trained on the same data across four of eight zero-shot benchmarks. Notably, it achieves competitive math reasoning ability with the 7B Llama-2 model on the GSM8K dataset. In text generation, MDMs with 16 times more pre-training time offer a flexible trade-off against ARMs with the accelerated sampling technique KV-Cache: MDMs match ARMs in performance while being 1.4 times faster during sampling. Moreover, MDMs address challenging tasks for ARMs by effectively handling bidirectional reasoning and adapting to temporal shifts in data. Notably, a 1.1B MDM breaks the reverse curse encountered by much larger ARMs with significantly more data and computation, such as 13B Llama-2 and 175B GPT-3. Our code is available at https://github.com/ML-GSAI/SMDM.",
            "corpus_id": 273549320,
            "sentences": [
                {
                    "corpus_id": "273549320",
                    "title": "Scaling up Masked Diffusion Models on Text",
                    "text": "We propose a surprisingly simple yet effective approach that leverages unlabeled data to boost performance in various language tasks, dubbed unsupervised classifier-free guidance (CFG). \n\nCFG. CFG (Ho & Salimans, 2022) is an effective and versatile technique widely used in both continuous and discrete diffusion models, with applications spanning image (Ho & Salimans, 2022;Chang et al., 2023) and text generation (Lovelace et al., 2024). Rooted in Bayes' rule, CFG simultaneously trains a conditional and an unconditional diffusion model, introducing a rescaled distribution for inference. Specifically, at a given timestep t \u2208 [0, 1], CFG (Chang et al., 2023) is defined as: \n\nwhere c is the condition, w is a hyperparameter that flexibly controls the strength of c, and p \u03b8 (x 0 |c, x t ) and p \u03b8 (x 0 |x t ) are the conditional and unconditional models respectively. \n\nNotably, it seems that the conditional model must be trained on paired data before applying CFG. Consequently, to the best of our knowledge, all existing work (Ho & Salimans, 2022;Chang et al., 2023;Lovelace et al., 2024) fall into supervised settings, where paired data are readily available. \n\nUnsupervised CFG. We extend CFG to an unsupervised setting by introducing a new formulation: \n\nwhere m is a mask sequence of the same length as c. Compared to Eq. ( 6), the dummy variable m translates the unconditional distribution to a conditional format without adding new information. For simplicity, we continue to refer to p \u03b8 (x 0 |m, x t ) as the unconditional distribution in unsupervised CFG throughout this paper. \n\nThe core insight is that an MDM already characterizes both distributions employed in Eq. ( 7) during unsupervised pretraining. Specifically, in language tasks, both c and x can be viewed as segments of a whole sequence, following the same distribution of unsupervised samples for pretraining.2",
                    "score": 0.5647381143476567,
                    "section_title": "UNSUPERVISED CLASSIFIER-FREE GUIDANCE",
                    "char_start_offset": 12383,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 188,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 677
                        },
                        {
                            "start": 680,
                            "end": 871
                        },
                        {
                            "start": 874,
                            "end": 970
                        },
                        {
                            "start": 971,
                            "end": 1167
                        },
                        {
                            "start": 1170,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1262
                        },
                        {
                            "start": 1265,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1593
                        },
                        {
                            "start": 1596,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1889
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: Scaling up Masked Diffusion Models on Text\n# Venue: International Conference on Learning Representations\n# Authors: Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, Chongxuan Li\n## Abstract\nMasked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored. This paper establishes the first scaling law for MDMs, demonstrating a scaling rate comparable to autoregressive models (ARMs) and a relatively small compute gap. Motivated by their scalability, we train a family of MDMs with up to 1.1 billion (B) parameters to systematically evaluate their performance against ARMs of comparable or larger sizes. Fully leveraging the probabilistic formulation of MDMs, we propose a simple yet effective unsupervised classifier-free guidance that effectively exploits large-scale unpaired data, boosting performance for conditional inference. In language understanding, the 1.1B MDM outperforms the 1.1B TinyLlama model trained on the same data across four of eight zero-shot benchmarks. Notably, it achieves competitive math reasoning ability with the 7B Llama-2 model on the GSM8K dataset. In text generation, MDMs with 16 times more pre-training time offer a flexible trade-off against ARMs with the accelerated sampling technique KV-Cache: MDMs match ARMs in performance while being 1.4 times faster during sampling. Moreover, MDMs address challenging tasks for ARMs by effectively handling bidirectional reasoning and adapting to temporal shifts in data. Notably, a 1.1B MDM breaks the reverse curse encountered by much larger ARMs with significantly more data and computation, such as 13B Llama-2 and 175B GPT-3. Our code is available at https://github.com/ML-GSAI/SMDM.\n## UNSUPERVISED CLASSIFIER-FREE GUIDANCE\nWe propose a surprisingly simple yet effective approach that leverages unlabeled data to boost performance in various language tasks, dubbed unsupervised classifier-free guidance (CFG). \n\nCFG. CFG (Ho & Salimans, 2022) is an effective and versatile technique widely used in both continuous and discrete diffusion models, with applications spanning image (Ho & Salimans, 2022;Chang et al., 2023) and text generation (Lovelace et al., 2024). Rooted in Bayes' rule, CFG simultaneously trains a conditional and an unconditional diffusion model, introducing a rescaled distribution for inference. Specifically, at a given timestep t \u2208 [0, 1], CFG (Chang et al., 2023) is defined as: \n\nwhere c is the condition, w is a hyperparameter that flexibly controls the strength of c, and p \u03b8 (x 0 |c, x t ) and p \u03b8 (x 0 |x t ) are the conditional and unconditional models respectively. \n\nNotably, it seems that the conditional model must be trained on paired data before applying CFG. Consequently, to the best of our knowledge, all existing work (Ho & Salimans, 2022;Chang et al., 2023;Lovelace et al., 2024) fall into supervised settings, where paired data are readily available. \n\nUnsupervised CFG. We extend CFG to an unsupervised setting by introducing a new formulation: \n\nwhere m is a mask sequence of the same length as c. Compared to Eq. ( 6), the dummy variable m translates the unconditional distribution to a conditional format without adding new information. For simplicity, we continue to refer to p \u03b8 (x 0 |m, x t ) as the unconditional distribution in unsupervised CFG throughout this paper. \n\nThe core insight is that an MDM already characterizes both distributions employed in Eq. ( 7) during unsupervised pretraining. Specifically, in language tasks, both c and x can be viewed as segments of a whole sequence, following the same distribution of unsupervised samples for pretraining.2",
            "reference_string": "[273549320 | Nie et al. | 2024 | Citations: 30]"
        },
        {
            "title": "4M: Massively Multimodal Masked Modeling",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 133,
            "citation_count": 74,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.06647, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2111623708",
                    "name": "David Mizrahi"
                },
                {
                    "authorId": "153825349",
                    "name": "Roman Bachmann"
                },
                {
                    "authorId": "2273474116",
                    "name": "Ouguzhan Fatih Kar"
                },
                {
                    "authorId": "143895090",
                    "name": "Teresa Yeo"
                },
                {
                    "authorId": "2273661239",
                    "name": "Mingfei Gao"
                },
                {
                    "authorId": "2273361790",
                    "name": "Afshin Dehghan"
                },
                {
                    "authorId": "40029556",
                    "name": "Amir Zamir"
                }
            ],
            "abstract": "Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens. 4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility. Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.",
            "corpus_id": 266162752,
            "sentences": [
                {
                    "corpus_id": "266162752",
                    "title": "4M: Massively Multimodal Masked Modeling",
                    "text": "Chained generation. Because any generated modality can be used as a conditioning too, we can perform chained generation of several modalities, one after another, with each fully generated one being added to the conditioning of the next (see Figure 3). Performing generation in this chained manner results in each additional modality being generated in a consistent manner, as shown in Figures 9 and 10. In addition, we found that for certain generative tasks, such as caption-to-RGB, generating intermediate modalities such as CLIP tokens can further improve image fidelity (see Figure 9). \n\nClassifier-free guidance. Classifier-free guidance is crucial for improving both image fidelity and how well the generation matches the conditioning. It is most commonly used in diffusion models [50], but can be applied in token-based models as well [40,123,18]. We perform classifier-free guidance by computing a weighted combinations of the logits of a forward pass with the conditioning and one without the conditioning: logits guided = logits uncond + w (logits cond \u2212 logits uncond ) . \n\nHere, w is the guidance scale. When performing chained generation, we add each fully generated modality to the set of guided modalities. \n\nMultimodal guidance. While guidance has been shown to significantly improve image quality, it can still happen that generative models ignore parts of the input, unpredictably focus on some parts more than others, or generate undesired concepts. Negative prompting [78] is a popular way of keeping the model from generating undesired concepts. Liu et al. [68] show that performing compositional guidance on multiple conditions can further improve text-image similarity. In a similar way, we can perform compositional generation by weighting different (parts of) modalities by different continous amounts -even negatively. We can do this by computing a weighted sum of the logits of an unconditional case and the logits of each conditional case: \n\nHere, w i are the guidance scales for the different conditions. For example, this allows 4M to generate semantically or geometrically similar variants of images by weakly conditioning on their extracted segmentation, normal, or depth maps (see Figure 13).",
                    "score": 0.4828008726857025,
                    "section_title": "A.3 Generation procedure details",
                    "char_start_offset": 37472,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 19
                        },
                        {
                            "start": 20,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 589
                        },
                        {
                            "start": 592,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 854
                        },
                        {
                            "start": 855,
                            "end": 1082
                        },
                        {
                            "start": 1085,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1221
                        },
                        {
                            "start": 1224,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1566
                        },
                        {
                            "start": 1567,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 1967
                        },
                        {
                            "start": 1970,
                            "end": 2033
                        },
                        {
                            "start": 2034,
                            "end": 2225
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 842,
                            "end": 846,
                            "matchedPaperCorpusId": "247628171"
                        },
                        {
                            "start": 846,
                            "end": 850,
                            "matchedPaperCorpusId": "249926846"
                        },
                        {
                            "start": 850,
                            "end": 853,
                            "matchedPaperCorpusId": "255372955"
                        },
                        {
                            "start": 1578,
                            "end": 1582,
                            "matchedPaperCorpusId": "249375227"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93798828125
                }
            ],
            "relevance_judgement": 0.93798828125,
            "relevance_judgment_input_expanded": "# Title: 4M: Massively Multimodal Masked Modeling\n# Venue: Neural Information Processing Systems\n# Authors: David Mizrahi, Roman Bachmann, Ouguzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir\n## Abstract\nCurrent machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens. 4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility. Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.\n## A.3 Generation procedure details\nChained generation. Because any generated modality can be used as a conditioning too, we can perform chained generation of several modalities, one after another, with each fully generated one being added to the conditioning of the next (see Figure 3). Performing generation in this chained manner results in each additional modality being generated in a consistent manner, as shown in Figures 9 and 10. In addition, we found that for certain generative tasks, such as caption-to-RGB, generating intermediate modalities such as CLIP tokens can further improve image fidelity (see Figure 9). \n\nClassifier-free guidance. Classifier-free guidance is crucial for improving both image fidelity and how well the generation matches the conditioning. It is most commonly used in diffusion models [50], but can be applied in token-based models as well [40,123,18]. We perform classifier-free guidance by computing a weighted combinations of the logits of a forward pass with the conditioning and one without the conditioning: logits guided = logits uncond + w (logits cond \u2212 logits uncond ) . \n\nHere, w is the guidance scale. When performing chained generation, we add each fully generated modality to the set of guided modalities. \n\nMultimodal guidance. While guidance has been shown to significantly improve image quality, it can still happen that generative models ignore parts of the input, unpredictably focus on some parts more than others, or generate undesired concepts. Negative prompting [78] is a popular way of keeping the model from generating undesired concepts. Liu et al. [68] show that performing compositional guidance on multiple conditions can further improve text-image similarity. In a similar way, we can perform compositional generation by weighting different (parts of) modalities by different continous amounts -even negatively. We can do this by computing a weighted sum of the logits of an unconditional case and the logits of each conditional case: \n\nHere, w i are the guidance scales for the different conditions. For example, this allows 4M to generate semantically or geometrically similar variants of images by weakly conditioning on their extracted segmentation, normal, or depth maps (see Figure 13).",
            "reference_string": "[266162752 | Mizrahi et al. | 2023 | Citations: 74]"
        },
        {
            "title": "GATS: Gather-Attend-Scatter",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.08525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2065684527",
                    "name": "K. Zolna"
                },
                {
                    "authorId": "2066047403",
                    "name": "Serkan Cabi"
                },
                {
                    "authorId": "2279763460",
                    "name": "Yutian Chen"
                },
                {
                    "authorId": "2279718795",
                    "name": "Eric Lau"
                },
                {
                    "authorId": "2080928701",
                    "name": "Claudio Fantacci"
                },
                {
                    "authorId": "31143488",
                    "name": "J. Pa\u0161ukonis"
                },
                {
                    "authorId": "2060551",
                    "name": "Jost Tobias Springenberg"
                },
                {
                    "authorId": "2279752493",
                    "name": "Sergio G\u00f3mez Colmenarejo"
                }
            ],
            "abstract": "As the AI community increasingly adopts large-scale models, it is crucial to develop general and flexible tools to integrate them. We introduce Gather-Attend-Scatter (GATS), a novel module that enables seamless combination of pretrained foundation models, both trainable and frozen, into larger multimodal networks. GATS empowers AI systems to process and generate information across multiple modalities at different rates. In contrast to traditional fine-tuning, GATS allows for the original component models to remain frozen, avoiding the risk of them losing important knowledge acquired during the pretraining phase. We demonstrate the utility and versatility of GATS with a few experiments across games, robotics, and multimodal input-output systems.",
            "corpus_id": 267027965,
            "sentences": [
                {
                    "corpus_id": "267027965",
                    "title": "GATS: Gather-Attend-Scatter",
                    "text": "Classifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4).",
                    "score": 0.4778785999527738,
                    "section_title": "Classifier-free guidance",
                    "char_start_offset": 19616,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 225
                        },
                        {
                            "start": 228,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 414
                        },
                        {
                            "start": 417,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 708
                        },
                        {
                            "start": 709,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 999
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93798828125
                }
            ],
            "relevance_judgement": 0.93798828125,
            "relevance_judgment_input_expanded": "# Title: GATS: Gather-Attend-Scatter\n# Venue: arXiv.org\n# Authors: K. Zolna, Serkan Cabi, Yutian Chen, Eric Lau, Claudio Fantacci, J. Pa\u0161ukonis, Jost Tobias Springenberg, Sergio G\u00f3mez Colmenarejo\n## Abstract\nAs the AI community increasingly adopts large-scale models, it is crucial to develop general and flexible tools to integrate them. We introduce Gather-Attend-Scatter (GATS), a novel module that enables seamless combination of pretrained foundation models, both trainable and frozen, into larger multimodal networks. GATS empowers AI systems to process and generate information across multiple modalities at different rates. In contrast to traditional fine-tuning, GATS allows for the original component models to remain frozen, avoiding the risk of them losing important knowledge acquired during the pretraining phase. We demonstrate the utility and versatility of GATS with a few experiments across games, robotics, and multimodal input-output systems.\n## Classifier-free guidance\nClassifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4).",
            "reference_string": "[267027965 | Zolna et al. | 2024 | Citations: 1]"
        },
        {
            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 35,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.08070, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2110872233",
                    "name": "Hyungjin Chung"
                },
                {
                    "authorId": "2109216792",
                    "name": "Jeongsol Kim"
                },
                {
                    "authorId": "153118937",
                    "name": "Geon Yeong Park"
                },
                {
                    "authorId": "2268758810",
                    "name": "Hyelin Nam"
                },
                {
                    "authorId": "2254155658",
                    "name": "Jong Chul Ye"
                }
            ],
            "abstract": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.",
            "corpus_id": 270391454,
            "sentences": [
                {
                    "corpus_id": "270391454",
                    "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
                    "text": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.",
                    "score": 0.5461622856428119,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93603515625
                }
            ],
            "relevance_judgement": 0.93603515625,
            "relevance_judgment_input_expanded": "# Title: CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models\n# Venue: arXiv.org\n# Authors: Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, Jong Chul Ye\n## Abstract\nClassifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.\n",
            "reference_string": "[270391454 | Chung et al. | 2024 | Citations: 35]"
        },
        {
            "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
            "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2407.04800",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.04800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2075053",
                    "name": "K. Azarian"
                },
                {
                    "authorId": "49950690",
                    "name": "Debasmit Das"
                },
                {
                    "authorId": "2293594635",
                    "name": "Qiqi Hou"
                },
                {
                    "authorId": "2253777162",
                    "name": "F. Porikli"
                }
            ],
            "abstract": "We introduce segmentation-free guidance, a novel method designed for text-to-image diffusion models like Stable Diffusion. Our method does not require retraining of the diffusion model. At no additional compute cost, it uses the diffusion model itself as an implied segmentation network, hence named segmentation-free guidance, to dynamically adjust the negative prompt for each patch of the generated image, based on the patch\u2019s relevance to concepts in the prompt. We evaluate segmentation-free guidance both objectively, using FID, CLIP, IS, and PickScore, and subjectively, through human evaluators. For the subjective evaluation, we also propose a methodology for subsampling the prompts in a dataset like MS COCO-30K to keep the number of human evaluations manageable while ensuring that the selected subset is both representative in terms of content and fair in terms of model performance. The results demonstrate the superiority of our segmentation-free guidance to the widely used classifier-free method. Human evaluators preferred segmentation-free guidance over classifier-free 60% to 19%, with 18% of occasions showing a strong preference. Additionally, PickScore win-rate, a recently proposed metric mimicking human preference, also indicates a preference for our method over classifier-free.",
            "corpus_id": 271051241,
            "sentences": [
                {
                    "corpus_id": "271051241",
                    "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
                    "text": "Diffusion models are powerful generative models for creating visual content from textual prompts.Their success stems from extensive training data and their ability to handle various modalities and signals, enabling diverse applications such as content editing, inpainting, and personalization.\n\nControlling a diffusion model can be achieved primarily in two ways -conditioning and guidance.When a diffusion model is conditioned, it is typically trained to accept a particular form of additional conditioning input, such as a text prompt, image edges, segmentation map, and class labels.However, adapting the model to a different condition often necessitates retraining from scratch.This reliance on expensive retraining poses challenges for end-users seeking to adopt and employ conditioning techniques to control diffusion models.\n\nAn alternative way to control a diffusion model is through a guidance mechanism.Unlike conditioning techniques, this approach does not rely on an external conditioning signal.Instead, it associates a guidance function with the diffusion model to fulfill a specific target criterion, which could be as simple as minimizing the CLIP distance between the generated image and the provided text description.When sampling an image, the reverse process iterations are steered in the direction of the guidance function's gradient, resulting in constrained image generation.\n\nWhen comparing control techniques for diffusion models, guidance emerges as a more versatile approach.It treats the diffusion network as a foundational model which can accommodate different use cases.An earlier method in this domain involved classifier guidance [8], where an explicit classifier functioned as the guidance mechanism.This method utilized the classifier's gradients to drive the image generation process.However, classifier guidance has transitioned to classifier-free guidance [12], eliminating the need for an explicit classifier.In classifier-free guidance approaches, the network is trained to adapt class-label information and conditioning signals without relying on a fixed network architecture.\n\nIn this paper, we propose enhancing image generation quality beyond classifier-free guidance by introducing a novel and universal segmentation-free guidance approach.This methodology aims to improve image quality of diffusion models without necessitating costly retraining, architectural changes, or additional computing during inference.\n\nImage generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.",
                    "score": 0.41709503711511653,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 97,
                            "end": 293
                        },
                        {
                            "start": 295,
                            "end": 390
                        },
                        {
                            "start": 390,
                            "end": 586
                        },
                        {
                            "start": 586,
                            "end": 682
                        },
                        {
                            "start": 682,
                            "end": 831
                        },
                        {
                            "start": 833,
                            "end": 913
                        },
                        {
                            "start": 913,
                            "end": 1008
                        },
                        {
                            "start": 1008,
                            "end": 1235
                        },
                        {
                            "start": 1235,
                            "end": 1398
                        },
                        {
                            "start": 1400,
                            "end": 1502
                        },
                        {
                            "start": 1502,
                            "end": 1600
                        },
                        {
                            "start": 1600,
                            "end": 1733
                        },
                        {
                            "start": 1733,
                            "end": 1819
                        },
                        {
                            "start": 1819,
                            "end": 1947
                        },
                        {
                            "start": 1947,
                            "end": 2116
                        },
                        {
                            "start": 2118,
                            "end": 2284
                        },
                        {
                            "start": 2284,
                            "end": 2456
                        },
                        {
                            "start": 2458,
                            "end": 2634
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1893,
                            "end": 1897,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93408203125
                }
            ],
            "relevance_judgement": 0.93408203125,
            "relevance_judgment_input_expanded": "# Title: Segmentation-Free Guidance for Text-to-Image Diffusion Models\n# Venue: 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n# Authors: K. Azarian, Debasmit Das, Qiqi Hou, F. Porikli\n## Abstract\nWe introduce segmentation-free guidance, a novel method designed for text-to-image diffusion models like Stable Diffusion. Our method does not require retraining of the diffusion model. At no additional compute cost, it uses the diffusion model itself as an implied segmentation network, hence named segmentation-free guidance, to dynamically adjust the negative prompt for each patch of the generated image, based on the patch\u2019s relevance to concepts in the prompt. We evaluate segmentation-free guidance both objectively, using FID, CLIP, IS, and PickScore, and subjectively, through human evaluators. For the subjective evaluation, we also propose a methodology for subsampling the prompts in a dataset like MS COCO-30K to keep the number of human evaluations manageable while ensuring that the selected subset is both representative in terms of content and fair in terms of model performance. The results demonstrate the superiority of our segmentation-free guidance to the widely used classifier-free method. Human evaluators preferred segmentation-free guidance over classifier-free 60% to 19%, with 18% of occasions showing a strong preference. Additionally, PickScore win-rate, a recently proposed metric mimicking human preference, also indicates a preference for our method over classifier-free.\n## Introduction\nDiffusion models are powerful generative models for creating visual content from textual prompts.Their success stems from extensive training data and their ability to handle various modalities and signals, enabling diverse applications such as content editing, inpainting, and personalization.\n\nControlling a diffusion model can be achieved primarily in two ways -conditioning and guidance.When a diffusion model is conditioned, it is typically trained to accept a particular form of additional conditioning input, such as a text prompt, image edges, segmentation map, and class labels.However, adapting the model to a different condition often necessitates retraining from scratch.This reliance on expensive retraining poses challenges for end-users seeking to adopt and employ conditioning techniques to control diffusion models.\n\nAn alternative way to control a diffusion model is through a guidance mechanism.Unlike conditioning techniques, this approach does not rely on an external conditioning signal.Instead, it associates a guidance function with the diffusion model to fulfill a specific target criterion, which could be as simple as minimizing the CLIP distance between the generated image and the provided text description.When sampling an image, the reverse process iterations are steered in the direction of the guidance function's gradient, resulting in constrained image generation.\n\nWhen comparing control techniques for diffusion models, guidance emerges as a more versatile approach.It treats the diffusion network as a foundational model which can accommodate different use cases.An earlier method in this domain involved classifier guidance [8], where an explicit classifier functioned as the guidance mechanism.This method utilized the classifier's gradients to drive the image generation process.However, classifier guidance has transitioned to classifier-free guidance [12], eliminating the need for an explicit classifier.In classifier-free guidance approaches, the network is trained to adapt class-label information and conditioning signals without relying on a fixed network architecture.\n\nIn this paper, we propose enhancing image generation quality beyond classifier-free guidance by introducing a novel and universal segmentation-free guidance approach.This methodology aims to improve image quality of diffusion models without necessitating costly retraining, architectural changes, or additional computing during inference.\n\nImage generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.",
            "reference_string": "[271051241 | Azarian et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Object-Centric Slot Diffusion",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 86,
            "citation_count": 61,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2303.10834",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.10834, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2211907956",
                    "name": "Jindong Jiang"
                },
                {
                    "authorId": "2212038042",
                    "name": "Fei Deng"
                },
                {
                    "authorId": "2212051383",
                    "name": "Gautam Singh"
                },
                {
                    "authorId": "2112164991",
                    "name": "S. Ahn"
                }
            ],
            "abstract": "The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. In addition, we conduct a preliminary investigation into the integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world image segmentation and generation. Project page is available at https://latentslotdiffusion.github.io",
            "corpus_id": 257632090,
            "sentences": [
                {
                    "corpus_id": "257632090",
                    "title": "Object-Centric Slot Diffusion",
                    "text": "We further evaluate the image generation quality of LSSD. \n\nIn Figure 8, we show the image reconstruction results (denoted by cfg = 1.0). The results demonstrate that the model output suffers from visual artifacts which constrained both the image reconstruction and generation capabilities. To further improve the generation results, we explore the technique of classifier-free guidance which is widely employed in text-to-image diffusion models. \n\nClassifier-free guidance [34] uses a mixing weight (abbreviated as cfg) to control the weighted sum of noise predictions between a conditional and an unconditional diffusion model. With cf g = 1, it operates similarly to standard conditional generation. When cf g > 1, the model generates images that are more align to the text input, but comes with a cost of reduced image diversity. Practically speaking, an optimal cf g > 1 can yield images that not only better match the conditions but also exhibit enhanced quality. Since LSSD provides object slots as text tokens to the pre-trained DMs, classifier-free guidance can be applied directly to the input slot, akin to how it's applied to input text in standard DMs, without any model modifications. And our experiment shows that a cf g > 1 for slots can also significantly improve the image generation quality. \n\nIn Figure 8, we tested various cfg values with LSSD to optimize image generation quality. As we can see in the figure, from cfg = 1.0 to 1.3, the images appear cleaner with less visual artifacts, delivering conceptually more consistent images with the input slots. Nevertheless, we also observe that higher cfg values strip away intricate visual details, leading to overly simplified images. We settled on a cfg value of 1.3 for following experiments. \n\nIn Figure 9, we show image samples generated based on the same set of learned slots. The variance in these samples comes from differing the initial noise maps in the denoising process. These results demonstrate that with a proper cfg value, LSSD achieves slot-based conditional image generation with unconstrained real-world objects, which is for the first time in object-centric generative models.",
                    "score": 0.41847987193055747,
                    "section_title": "Real-World Image Generation Results",
                    "char_start_offset": 43992,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 57
                        },
                        {
                            "start": 60,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 446
                        },
                        {
                            "start": 449,
                            "end": 629
                        },
                        {
                            "start": 630,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1310
                        },
                        {
                            "start": 1313,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1704
                        },
                        {
                            "start": 1705,
                            "end": 1764
                        },
                        {
                            "start": 1767,
                            "end": 1851
                        },
                        {
                            "start": 1852,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2165
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 474,
                            "end": 478,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.931640625
                }
            ],
            "relevance_judgement": 0.931640625,
            "relevance_judgment_input_expanded": "# Title: Object-Centric Slot Diffusion\n# Venue: Neural Information Processing Systems\n# Authors: Jindong Jiang, Fei Deng, Gautam Singh, S. Ahn\n## Abstract\nThe recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. In addition, we conduct a preliminary investigation into the integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world image segmentation and generation. Project page is available at https://latentslotdiffusion.github.io\n## Real-World Image Generation Results\nWe further evaluate the image generation quality of LSSD. \n\nIn Figure 8, we show the image reconstruction results (denoted by cfg = 1.0). The results demonstrate that the model output suffers from visual artifacts which constrained both the image reconstruction and generation capabilities. To further improve the generation results, we explore the technique of classifier-free guidance which is widely employed in text-to-image diffusion models. \n\nClassifier-free guidance [34] uses a mixing weight (abbreviated as cfg) to control the weighted sum of noise predictions between a conditional and an unconditional diffusion model. With cf g = 1, it operates similarly to standard conditional generation. When cf g > 1, the model generates images that are more align to the text input, but comes with a cost of reduced image diversity. Practically speaking, an optimal cf g > 1 can yield images that not only better match the conditions but also exhibit enhanced quality. Since LSSD provides object slots as text tokens to the pre-trained DMs, classifier-free guidance can be applied directly to the input slot, akin to how it's applied to input text in standard DMs, without any model modifications. And our experiment shows that a cf g > 1 for slots can also significantly improve the image generation quality. \n\nIn Figure 8, we tested various cfg values with LSSD to optimize image generation quality. As we can see in the figure, from cfg = 1.0 to 1.3, the images appear cleaner with less visual artifacts, delivering conceptually more consistent images with the input slots. Nevertheless, we also observe that higher cfg values strip away intricate visual details, leading to overly simplified images. We settled on a cfg value of 1.3 for following experiments. \n\nIn Figure 9, we show image samples generated based on the same set of learned slots. The variance in these samples comes from differing the initial noise maps in the denoising process. These results demonstrate that with a proper cfg value, LSSD achieves slot-based conditional image generation with unconstrained real-world objects, which is for the first time in object-centric generative models.",
            "reference_string": "[257632090 | Jiang et al. | 2023 | Citations: 61]"
        },
        {
            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 60,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144306665",
                    "name": "Xiaoming Zhao"
                },
                {
                    "authorId": "2281750850",
                    "name": "Alexander G. Schwing"
                }
            ],
            "abstract": "Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.",
            "corpus_id": 276961040,
            "sentences": [
                {
                    "corpus_id": "276961040",
                    "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
                    "text": "Conditional generation, e.g., class-to-image, text-to-image, or image-to-video, is omnipresent as it provides a compelling way to control the output. Ideally, conditional generation results are both diverse and of high-fidelity. Namely, the generative models' outputs align with the conditioning information perfectly and diligently follow the training data diversity. However, there is a trade-off between high-fidelity and diversity: without constraining diversity there are always possibilities to sample from areas on the data distribution manifold that are not well-trained. Thus, trading diversity for fidelity is a long-standing problem and the community has developed various approaches, e.g., the truncation trick for generative adversarial nets (GANs) [7,28], low-temperature sampling for probabilistic models [2], or temperature control in large language models [1,19]. \n\nMore recently, to trade diversity and fidelity in denois-ing diffusion models [25,35,53,58], several techniques have been developed [16,26,31], from which classifierfree guidance [24] emerged as the de-facto standard. For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image [50] and text-to-3D [47] generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing. Recently, several efforts provide insights by studying classifier-free guidance from a theoretical perspective [6,12,61] showing that sampling from classifier-free guidance is not the same as sampling from a sharpened distribution. \n\nInstead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance [16]. It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training [24]. This connection motivates us to carefully study classifier guidance's derivation and its behavior.",
                    "score": 0.5600228076866292,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 880
                        },
                        {
                            "start": 883,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1269
                        },
                        {
                            "start": 1270,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1596
                        },
                        {
                            "start": 1599,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1926
                        },
                        {
                            "start": 1927,
                            "end": 2080
                        },
                        {
                            "start": 2081,
                            "end": 2179
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 762,
                            "end": 765,
                            "matchedPaperCorpusId": "52889459"
                        },
                        {
                            "start": 765,
                            "end": 768,
                            "matchedPaperCorpusId": "54482423"
                        },
                        {
                            "start": 820,
                            "end": 823,
                            "matchedPaperCorpusId": "12174018"
                        },
                        {
                            "start": 876,
                            "end": 879,
                            "matchedPaperCorpusId": "271571434"
                        },
                        {
                            "start": 961,
                            "end": 965,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 965,
                            "end": 968,
                            "matchedPaperCorpusId": "235694314"
                        },
                        {
                            "start": 968,
                            "end": 971,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 971,
                            "end": 974,
                            "matchedPaperCorpusId": "5560643"
                        },
                        {
                            "start": 1015,
                            "end": 1019,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1019,
                            "end": 1022,
                            "matchedPaperCorpusId": "252683688"
                        },
                        {
                            "start": 1022,
                            "end": 1025,
                            "matchedPaperCorpusId": "254096299"
                        },
                        {
                            "start": 1062,
                            "end": 1066,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1233,
                            "end": 1237,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1253,
                            "end": 1257,
                            "matchedPaperCorpusId": "252596091"
                        },
                        {
                            "start": 1476,
                            "end": 1479,
                            "matchedPaperCorpusId": "271903235"
                        },
                        {
                            "start": 1479,
                            "end": 1482,
                            "matchedPaperCorpusId": "272770713"
                        },
                        {
                            "start": 1482,
                            "end": 1485,
                            "matchedPaperCorpusId": "273549917"
                        },
                        {
                            "start": 1773,
                            "end": 1777,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 2075,
                            "end": 2079,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9306640625
                }
            ],
            "relevance_judgement": 0.9306640625,
            "relevance_judgment_input_expanded": "# Title: Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective\n# Venue: arXiv.org\n# Authors: Xiaoming Zhao, Alexander G. Schwing\n## Abstract\nClassifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.\n## Introduction\nConditional generation, e.g., class-to-image, text-to-image, or image-to-video, is omnipresent as it provides a compelling way to control the output. Ideally, conditional generation results are both diverse and of high-fidelity. Namely, the generative models' outputs align with the conditioning information perfectly and diligently follow the training data diversity. However, there is a trade-off between high-fidelity and diversity: without constraining diversity there are always possibilities to sample from areas on the data distribution manifold that are not well-trained. Thus, trading diversity for fidelity is a long-standing problem and the community has developed various approaches, e.g., the truncation trick for generative adversarial nets (GANs) [7,28], low-temperature sampling for probabilistic models [2], or temperature control in large language models [1,19]. \n\nMore recently, to trade diversity and fidelity in denois-ing diffusion models [25,35,53,58], several techniques have been developed [16,26,31], from which classifierfree guidance [24] emerged as the de-facto standard. For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image [50] and text-to-3D [47] generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing. Recently, several efforts provide insights by studying classifier-free guidance from a theoretical perspective [6,12,61] showing that sampling from classifier-free guidance is not the same as sampling from a sharpened distribution. \n\nInstead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance [16]. It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training [24]. This connection motivates us to carefully study classifier guidance's derivation and its behavior.",
            "reference_string": "[276961040 | Zhao et al. | 2025 | Citations: 1]"
        },
        {
            "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.13893, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238138795",
                    "name": "Dongchao Yang"
                },
                {
                    "authorId": "2306974316",
                    "name": "Rongjie Huang"
                },
                {
                    "authorId": "2307148701",
                    "name": "Yuanyuan Wang"
                },
                {
                    "authorId": "66855276",
                    "name": "Haohan Guo"
                },
                {
                    "authorId": "2316948137",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "51263928",
                    "name": "Songxiang Liu"
                },
                {
                    "authorId": "2107999711",
                    "name": "Xixin Wu"
                },
                {
                    "authorId": "2273659859",
                    "name": "Helen M. Meng"
                }
            ],
            "abstract": "Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or Non-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3). Although these works demonstrate good performance, they still have potential weaknesses. For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design. In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2 effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed. Compared to our previous publication, we present ({\\romannumeral1}) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; ({\\romannumeral2}) four distinct types of sentence duration predictors; ({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion model. With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. Demos are available on: {https://dongchaoyang.top/SimpleSpeech2\\_demo/}.",
            "corpus_id": 271957385,
            "sentences": [
                {
                    "corpus_id": "271957385",
                    "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
                    "text": "Classifier-free guidance (CFG) [24] has been demonstrated as an effective way to enhance the generation quality in both image and audio domains [35], [53]. CFG can be formulated as: \n\nwhere \u03bb denotes the guidance scale. CFC tries to model conditional distribution p(x|y) and unconditional distribution p(x) in the training. In the inference stage, \u03bb = 1 denotes that we do not use classifier-free guidance, when \u03bb > 1 the model decreases the unconditional likelihood of the sample while increasing the conditional likelihood. In other words, classifierfree guidance conducts this by decreasing the unconditional likelihood with a negative score term. During the training stage, previous works try to mask the condition information of some samples (e.g. set the training sample's text as empty with 10% probability), so that these samples can be used to optimize unconditional distribution p(x). \n\nMany prior studies [10], [22] demonstrate that text transcriptions generated by an ASR system can be utilized to train a TTS system. Inevitably, the text transcription from the ASR system is not flawless, referred to as noisy or weak labels. Despite this, existing literature does not thoroughly explain why TTS systems can be effectively trained using such dataset. In this study, we show that including a small part of the noisy label in the training set is equivalent to introducing the CFC training strategy, thus we do not need to deliberately construct masked samples during training stage. In the following, we give the proof. \n\nAssume the condition is y, the target is x, and the generative model tries to model p(x|y). Consider the text y includes n words, and y can be split into n parts, and we have correspondence (y i , x i ). Given that the TTS task tries to learn the mapping between y i andx i , y i . In general, we assume x j are mutually independent if i \u0338 = j. The target of the generative model is to learn the distribution p \u03b8 (x|y). Based on Bayes formula, we have: \n\nWe can discard the last term when we calculate the derivative of x.",
                    "score": 0.5648140488433426,
                    "section_title": "E. Classifier-Free Guidance",
                    "char_start_offset": 24850,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 181
                        },
                        {
                            "start": 184,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 894
                        },
                        {
                            "start": 897,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1493
                        },
                        {
                            "start": 1494,
                            "end": 1530
                        },
                        {
                            "start": 1533,
                            "end": 1624
                        },
                        {
                            "start": 1625,
                            "end": 1736
                        },
                        {
                            "start": 1737,
                            "end": 1814
                        },
                        {
                            "start": 1815,
                            "end": 1877
                        },
                        {
                            "start": 1878,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 1985
                        },
                        {
                            "start": 1988,
                            "end": 2055
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 144,
                            "end": 148,
                            "matchedPaperCorpusId": "256416291"
                        },
                        {
                            "start": 150,
                            "end": 154,
                            "matchedPaperCorpusId": "268247980"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.927734375
                }
            ],
            "relevance_judgement": 0.927734375,
            "relevance_judgment_input_expanded": "# Title: SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models\n# Venue: arXiv.org\n# Authors: Dongchao Yang, Rongjie Huang, Yuanyuan Wang, Haohan Guo, Dading Chong, Songxiang Liu, Xixin Wu, Helen M. Meng\n## Abstract\nScaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or Non-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3). Although these works demonstrate good performance, they still have potential weaknesses. For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design. In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2 effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed. Compared to our previous publication, we present ({\\romannumeral1}) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; ({\\romannumeral2}) four distinct types of sentence duration predictors; ({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion model. With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. Demos are available on: {https://dongchaoyang.top/SimpleSpeech2\\_demo/}.\n## E. Classifier-Free Guidance\nClassifier-free guidance (CFG) [24] has been demonstrated as an effective way to enhance the generation quality in both image and audio domains [35], [53]. CFG can be formulated as: \n\nwhere \u03bb denotes the guidance scale. CFC tries to model conditional distribution p(x|y) and unconditional distribution p(x) in the training. In the inference stage, \u03bb = 1 denotes that we do not use classifier-free guidance, when \u03bb > 1 the model decreases the unconditional likelihood of the sample while increasing the conditional likelihood. In other words, classifierfree guidance conducts this by decreasing the unconditional likelihood with a negative score term. During the training stage, previous works try to mask the condition information of some samples (e.g. set the training sample's text as empty with 10% probability), so that these samples can be used to optimize unconditional distribution p(x). \n\nMany prior studies [10], [22] demonstrate that text transcriptions generated by an ASR system can be utilized to train a TTS system. Inevitably, the text transcription from the ASR system is not flawless, referred to as noisy or weak labels. Despite this, existing literature does not thoroughly explain why TTS systems can be effectively trained using such dataset. In this study, we show that including a small part of the noisy label in the training set is equivalent to introducing the CFC training strategy, thus we do not need to deliberately construct masked samples during training stage. In the following, we give the proof. \n\nAssume the condition is y, the target is x, and the generative model tries to model p(x|y). Consider the text y includes n words, and y can be split into n parts, and we have correspondence (y i , x i ). Given that the TTS task tries to learn the mapping between y i andx i , y i . In general, we assume x j are mutually independent if i \u0338 = j. The target of the generative model is to learn the distribution p \u03b8 (x|y). Based on Bayes formula, we have: \n\nWe can discard the last term when we calculate the derivative of x.",
            "reference_string": "[271957385 | Yang et al. | 2024 | Citations: 15]"
        },
        {
            "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
            "venue": "",
            "year": 2024,
            "reference_count": 83,
            "citation_count": 9,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.04332, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146667089",
                    "name": "Junfeng Wu"
                },
                {
                    "authorId": "2294789302",
                    "name": "Yi Jiang"
                },
                {
                    "authorId": "2186151254",
                    "name": "Chuofan Ma"
                },
                {
                    "authorId": "2266428398",
                    "name": "Yuliang Liu"
                },
                {
                    "authorId": "2310758544",
                    "name": "Hengshuang Zhao"
                },
                {
                    "authorId": "2244754235",
                    "name": "Zehuan Yuan"
                },
                {
                    "authorId": "2257280698",
                    "name": "Song Bai"
                },
                {
                    "authorId": "2273663142",
                    "name": "Xiang Bai"
                }
            ],
            "abstract": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using a single large language model (LLM), eliminating the need for external pretrained visual embeddings such as CLIP. For the first time, Liquid uncovers a scaling law that performance drop unavoidably brought by the unified training of visual and language tasks diminishes as the model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We show that existing LLMs can serve as strong foundations for Liquid, saving 100x in training costs while outperforming Chameleon in multimodal capabilities and maintaining language performance comparable to mainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2 are powerful multimodal generators, offering a scalable solution for enhancing both vision-language understanding and generation. The code and models will be released at https://github.com/FoundationVision/Liquid.",
            "corpus_id": 274514993,
            "sentences": [
                {
                    "corpus_id": "274514993",
                    "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
                    "text": "Impact of Classifier-free Guidance. Classifier-Free Guidance (CFG) scale is hyperparameter that control the trade-off between sample quality and diversity in conditional generative models. The visual variations of generated images with different CFG scales t are illustrated in Fig. 10 As observed, higher CFG scales lead to better alignment between the generated images and the text prompts, but cause more chaotic object structures, stronger stylization, and worse photorealism. For example, when CFG=15, the structure of the book in the image becomes disordered. Conversely, lower CFG scales result in poorer consistency between the image content and the prompt, but improve the photorealism and fine-grained texture details. \n\nComparison with Other Models. In Fig. 11, we present a comparison between Liquid and other unified multi-modal large models in terms of visual generation quality. Compared to models based on discrete multi-codebook (VILA-U), diffusion processes (Show-o), and multimodal tokenizers (Janus), Liquid demonstrates superior performance in knowledge-aware image generation (first row), scene generation accuracy and small-scale facial details (second row), as well as structural coherence of objects (third row, vehicles). This visual comparison quantitatively validates Liquid's advancements in generating high-fidelity, semantically consistent images while maintaining strong multi-modal understanding capabilities.",
                    "score": 0.5305952407350116,
                    "section_title": "Visual Comparative Analysis",
                    "char_start_offset": 29021,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 35
                        },
                        {
                            "start": 36,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 728
                        },
                        {
                            "start": 731,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 893
                        },
                        {
                            "start": 894,
                            "end": 1247
                        },
                        {
                            "start": 1248,
                            "end": 1442
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92529296875
                }
            ],
            "relevance_judgement": 0.92529296875,
            "relevance_judgment_input_expanded": "# Title: Liquid: Language Models are Scalable and Unified Multi-modal Generators\n# Venue: \n# Authors: Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, Xiang Bai\n## Abstract\nWe present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using a single large language model (LLM), eliminating the need for external pretrained visual embeddings such as CLIP. For the first time, Liquid uncovers a scaling law that performance drop unavoidably brought by the unified training of visual and language tasks diminishes as the model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We show that existing LLMs can serve as strong foundations for Liquid, saving 100x in training costs while outperforming Chameleon in multimodal capabilities and maintaining language performance comparable to mainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2 are powerful multimodal generators, offering a scalable solution for enhancing both vision-language understanding and generation. The code and models will be released at https://github.com/FoundationVision/Liquid.\n## Visual Comparative Analysis\nImpact of Classifier-free Guidance. Classifier-Free Guidance (CFG) scale is hyperparameter that control the trade-off between sample quality and diversity in conditional generative models. The visual variations of generated images with different CFG scales t are illustrated in Fig. 10 As observed, higher CFG scales lead to better alignment between the generated images and the text prompts, but cause more chaotic object structures, stronger stylization, and worse photorealism. For example, when CFG=15, the structure of the book in the image becomes disordered. Conversely, lower CFG scales result in poorer consistency between the image content and the prompt, but improve the photorealism and fine-grained texture details. \n\nComparison with Other Models. In Fig. 11, we present a comparison between Liquid and other unified multi-modal large models in terms of visual generation quality. Compared to models based on discrete multi-codebook (VILA-U), diffusion processes (Show-o), and multimodal tokenizers (Janus), Liquid demonstrates superior performance in knowledge-aware image generation (first row), scene generation accuracy and small-scale facial details (second row), as well as structural coherence of objects (third row, vehicles). This visual comparison quantitatively validates Liquid's advancements in generating high-fidelity, semantically consistent images while maintaining strong multi-modal understanding capabilities.",
            "reference_string": "[274514993 | Wu et al. | 2024 | Citations: 9]"
        },
        {
            "title": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2312.04655",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.04655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "81331041",
                    "name": "Maitreya Patel"
                },
                {
                    "authorId": "2116705496",
                    "name": "C. Kim"
                },
                {
                    "authorId": "2116727866",
                    "name": "Sheng Cheng"
                },
                {
                    "authorId": "2064619864",
                    "name": "Chitta Baral"
                },
                {
                    "authorId": "1784500",
                    "name": "Yezhou Yang"
                }
            ],
            "abstract": "Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g., DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional T2I benchmarks, at the cost of significant computational resources. The unCLIP stack comprises T2I prior and diffusion image decoder. The T2I prior model alone adds a billion parameters compared to the Latent Diffusion Models, which increases the computational and high-quality data requirements. We introduce ECLIPSE11Our strategy, ECLIPSE, draws an analogy from the way a smaller prior model, akin to a celestial entity, offers a glimpse of the grandeur within the larger pre-trained vision-language model, mirroring how an eclipse reveals the vastness of the cosmos., a novel contrastive learning method that is both parameter and dataefficient. ECLIPSE leverages pre-trained vision-language models (e.g., CLIP) to distill the knowledge into the prior model. We demonstrate that the ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere 2.8% of the data, surpasses the baseline T2I priors with an average of 71.6% preference score under resource-limited setting. It also attains performance on par with SOTA big models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions. Extensive experiments on two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE priors consistently deliver high performance while significantly reducing resource dependency. Project page: https://eclipse-t2i.vercel.app/",
            "corpus_id": 266149498,
            "sentences": [
                {
                    "corpus_id": "266149498",
                    "title": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations",
                    "text": "Both training objectives, L prior and L decoder , integrate Classifier-Free Guidance (CFG) [11], enhancing the model's generative capabilities. During training, conditions are omitted 10% of the time to foster unconditional generation, subsequently improving test performance as CFG works as implicit classifier guidance [11].",
                    "score": 0.41450130713776434,
                    "section_title": "Preliminaries",
                    "char_start_offset": 10175,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 326
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Maitreya Patel, C. Kim, Sheng Cheng, Chitta Baral, Yezhou Yang\n## Abstract\nText-to-image (T2I) diffusion models, notably the unCLIP models (e.g., DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional T2I benchmarks, at the cost of significant computational resources. The unCLIP stack comprises T2I prior and diffusion image decoder. The T2I prior model alone adds a billion parameters compared to the Latent Diffusion Models, which increases the computational and high-quality data requirements. We introduce ECLIPSE11Our strategy, ECLIPSE, draws an analogy from the way a smaller prior model, akin to a celestial entity, offers a glimpse of the grandeur within the larger pre-trained vision-language model, mirroring how an eclipse reveals the vastness of the cosmos., a novel contrastive learning method that is both parameter and dataefficient. ECLIPSE leverages pre-trained vision-language models (e.g., CLIP) to distill the knowledge into the prior model. We demonstrate that the ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere 2.8% of the data, surpasses the baseline T2I priors with an average of 71.6% preference score under resource-limited setting. It also attains performance on par with SOTA big models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions. Extensive experiments on two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE priors consistently deliver high performance while significantly reducing resource dependency. Project page: https://eclipse-t2i.vercel.app/\n## Preliminaries\nBoth training objectives, L prior and L decoder , integrate Classifier-Free Guidance (CFG) [11], enhancing the model's generative capabilities. During training, conditions are omitted 10% of the time to foster unconditional generation, subsequently improving test performance as CFG works as implicit classifier guidance [11].",
            "reference_string": "[266149498 | Patel et al. | 2023 | Citations: 19]"
        },
        {
            "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13040, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297729854",
                    "name": "Xi Wang"
                },
                {
                    "authorId": "2212307989",
                    "name": "Nicolas Dufour"
                },
                {
                    "authorId": "2142463387",
                    "name": "Nefeli Andreou"
                },
                {
                    "authorId": "1710314",
                    "name": "Marie-Paule Cani"
                },
                {
                    "authorId": "2297672100",
                    "name": "Victoria Fern\u00e1ndez Abrevaya"
                },
                {
                    "authorId": "145897899",
                    "name": "David Picard"
                },
                {
                    "authorId": "1881509",
                    "name": "Vicky Kalogeiton"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional predictions using a fixed weight. However, recent works vary the weights throughout the diffusion process, reporting superior results but without providing any rationale or analysis. By conducting comprehensive experiments, this paper provides insights into CFG weight schedulers. Our findings suggest that simple, monotonically increasing weight schedulers consistently lead to improved performances, requiring merely a single line of code. In addition, more complex parametrized schedulers can be optimized for further improvement, but do not generalize across different models and tasks.",
            "corpus_id": 269283056,
            "sentences": [
                {
                    "corpus_id": "269283056",
                    "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
                    "text": "Particularly, CFG has catalyzed advancements in text-conditional generation, a domain where training a noisy text classifier is less convenient and performs worse. This approach breathed new life into the text-to-image application, initially proposed in several works such as (Reed et al., 2016;Mansimov et al., 2015). Numerous works (Rombach et al., 2022;Ramesh et al., 2022;Nichol et al., 2022;Avrahami et al., 2022) have leveraged text-to-image generation with CFG diffusion models conditioned on text encoders like CLIP (Radford et al., 2021), showcasing significant progress in the field, e.g. the Latent Diffusion Model (Dhariwal & Nichol, 2021) and Stable Diffusion (Rombach et al., 2022) employ VAE latent space diffusion with CFG with CLIP encoder. SDXL, an enhanced version, leverages a larger model and an additional text encoder for high-resolution synthesis. \n\nImprovements on Diffusion Guidance. Noticed that in Classifier Guidance (CG), the classifier's gradient tends to vanish towards the early and final stages due to overconfidence, Zheng et al. (2022) leverages the entropy of the output distribution as an indication of vanishing gradient and rescales the gradient accordingly. To prevent such adversarial behaviours, Dinh et al. (2023b) explored using multiple class conditions, guiding the image generation from a noise state towards an average of image classes before focusing on the In Classifier-Free Guidance (CFG), Li et al. ( 2023) used CFG to recover a zero-shot classifier by sampling across timesteps and averaging the guidance magnitude for different labels, with the lowest magnitude corresponding to the most probable label. However, they observed a discrepancy in performance across timesteps with early stages yielding lower accuracy than intermediate ones. Chang et al. (2023) observed that a linear increase in guidance scale enhances diversity. Similarly, Gao et al. (2023) developed a parameterized powercosine-like curve, optimizing a specific parameter for their dataset and method.",
                    "score": 0.6114596011353367,
                    "section_title": "Related Work",
                    "char_start_offset": 5638,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 757
                        },
                        {
                            "start": 758,
                            "end": 871
                        },
                        {
                            "start": 874,
                            "end": 909
                        },
                        {
                            "start": 910,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1884
                        },
                        {
                            "start": 1885,
                            "end": 2025
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 276,
                            "end": 295,
                            "matchedPaperCorpusId": "1563370"
                        },
                        {
                            "start": 334,
                            "end": 356,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 396,
                            "end": 418,
                            "matchedPaperCorpusId": "244714366"
                        },
                        {
                            "start": 524,
                            "end": 546,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 673,
                            "end": 695,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1052,
                            "end": 1071,
                            "matchedPaperCorpusId": "249953799"
                        },
                        {
                            "start": 1239,
                            "end": 1258,
                            "matchedPaperCorpusId": "268042461"
                        },
                        {
                            "start": 1795,
                            "end": 1814,
                            "matchedPaperCorpusId": "255372955"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92333984375
                }
            ],
            "relevance_judgement": 0.92333984375,
            "relevance_judgment_input_expanded": "# Title: Analysis of Classifier-Free Guidance Weight Schedulers\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fern\u00e1ndez Abrevaya, David Picard, Vicky Kalogeiton\n## Abstract\nClassifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional predictions using a fixed weight. However, recent works vary the weights throughout the diffusion process, reporting superior results but without providing any rationale or analysis. By conducting comprehensive experiments, this paper provides insights into CFG weight schedulers. Our findings suggest that simple, monotonically increasing weight schedulers consistently lead to improved performances, requiring merely a single line of code. In addition, more complex parametrized schedulers can be optimized for further improvement, but do not generalize across different models and tasks.\n## Related Work\nParticularly, CFG has catalyzed advancements in text-conditional generation, a domain where training a noisy text classifier is less convenient and performs worse. This approach breathed new life into the text-to-image application, initially proposed in several works such as (Reed et al., 2016;Mansimov et al., 2015). Numerous works (Rombach et al., 2022;Ramesh et al., 2022;Nichol et al., 2022;Avrahami et al., 2022) have leveraged text-to-image generation with CFG diffusion models conditioned on text encoders like CLIP (Radford et al., 2021), showcasing significant progress in the field, e.g. the Latent Diffusion Model (Dhariwal & Nichol, 2021) and Stable Diffusion (Rombach et al., 2022) employ VAE latent space diffusion with CFG with CLIP encoder. SDXL, an enhanced version, leverages a larger model and an additional text encoder for high-resolution synthesis. \n\nImprovements on Diffusion Guidance. Noticed that in Classifier Guidance (CG), the classifier's gradient tends to vanish towards the early and final stages due to overconfidence, Zheng et al. (2022) leverages the entropy of the output distribution as an indication of vanishing gradient and rescales the gradient accordingly. To prevent such adversarial behaviours, Dinh et al. (2023b) explored using multiple class conditions, guiding the image generation from a noise state towards an average of image classes before focusing on the In Classifier-Free Guidance (CFG), Li et al. ( 2023) used CFG to recover a zero-shot classifier by sampling across timesteps and averaging the guidance magnitude for different labels, with the lowest magnitude corresponding to the most probable label. However, they observed a discrepancy in performance across timesteps with early stages yielding lower accuracy than intermediate ones. Chang et al. (2023) observed that a linear increase in guidance scale enhances diversity. Similarly, Gao et al. (2023) developed a parameterized powercosine-like curve, optimizing a specific parameter for their dataset and method.",
            "reference_string": "[269283056 | Wang et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.18664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2071816370",
                    "name": "J. Hyung"
                },
                {
                    "authorId": "2333250087",
                    "name": "Kinam Kim"
                },
                {
                    "authorId": "2186865215",
                    "name": "Susung Hong"
                },
                {
                    "authorId": "2117955517",
                    "name": "Minjeong Kim"
                },
                {
                    "authorId": "1795455",
                    "name": "J. Choo"
                }
            ],
            "abstract": "Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance.",
            "corpus_id": 274422874,
            "sentences": [
                {
                    "corpus_id": "274422874",
                    "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
                    "text": "Classifier-Free Guidance (CFG) [10] uses Bayes' rule to replace a classifier-guided score with a linear combination of conditional and unconditional score estimates: \n\nCFG jointly trains the unconditional model \u03f5 \u03b8 (x t |\u03d5) and the conditional model \u03f5 \u03b8 (x t |c) (= \u03f5 \u03b8 (x t )) within a single model by setting the condition c to a null token \u03d5. Using this guided denoising process, the model better captures the conditions and often generates high-fidelity outputs.",
                    "score": 0.4483924175418692,
                    "section_title": "Classifier-Free Guidance",
                    "char_start_offset": 7688,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 168,
                            "end": 345
                        },
                        {
                            "start": 346,
                            "end": 466
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9228515625
                }
            ],
            "relevance_judgement": 0.9228515625,
            "relevance_judgment_input_expanded": "# Title: Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling\n# Venue: arXiv.org\n# Authors: J. Hyung, Kinam Kim, Susung Hong, Minjeong Kim, J. Choo\n## Abstract\nDiffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance.\n## Classifier-Free Guidance\nClassifier-Free Guidance (CFG) [10] uses Bayes' rule to replace a classifier-guided score with a linear combination of conditional and unconditional score estimates: \n\nCFG jointly trains the unconditional model \u03f5 \u03b8 (x t |\u03d5) and the conditional model \u03f5 \u03b8 (x t |c) (= \u03f5 \u03b8 (x t )) within a single model by setting the condition c to a null token \u03d5. Using this guided denoising process, the model better captures the conditions and often generates high-fidelity outputs.",
            "reference_string": "[274422874 | Hyung et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Upsample Guidance: Scale Up Diffusion Models without Training",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 15,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2029157095",
                    "name": "Juno Hwang"
                },
                {
                    "authorId": "2209948157",
                    "name": "Yong-Hyun Park"
                },
                {
                    "authorId": "2280064153",
                    "name": "Junghyo Jo"
                }
            ],
            "abstract": "Diffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio. However, they encounter difficulties in directly generating high-resolution samples. Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages. These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work. In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process. Remarkably, this technique does not necessitate any additional training or relying on external models. We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion models. We also observed that the proper selection of guidance scale can improve image quality, fidelity, and prompt alignment.",
            "corpus_id": 268856926,
            "sentences": [
                {
                    "corpus_id": "268856926",
                    "title": "Upsample Guidance: Scale Up Diffusion Models without Training",
                    "text": "Techniques have been proposed for conditionally sampling images corresponding to specific classes or text prompts by adding a guidance term to the predicted noise.Ho & Salimans (2022) added the gradient of the log probability predicted by a classifier to \u03f5(x t , t), enabling an unconditional diffusion model to generate class-conditioned images.Subsequently, classifier-free guidance (CFG) was proposed.Instead of using a classifier, the noise predictor's architecture was modified to accept condition c as an input.The following formula is then used as the predicted noise:\n\nHere, w represents the guidance scale.It has been commonly observed that proper adjustment of the scale improves the alignment with the condition and the fidelity of the generated images.",
                    "score": 0.4612512504759482,
                    "section_title": "Guidances for Diffusion Models",
                    "char_start_offset": 6286,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 163,
                            "end": 346
                        },
                        {
                            "start": 346,
                            "end": 404
                        },
                        {
                            "start": 404,
                            "end": 517
                        },
                        {
                            "start": 517,
                            "end": 575
                        },
                        {
                            "start": 577,
                            "end": 615
                        },
                        {
                            "start": 615,
                            "end": 764
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92236328125
                }
            ],
            "relevance_judgement": 0.92236328125,
            "relevance_judgment_input_expanded": "# Title: Upsample Guidance: Scale Up Diffusion Models without Training\n# Venue: arXiv.org\n# Authors: Juno Hwang, Yong-Hyun Park, Junghyo Jo\n## Abstract\nDiffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio. However, they encounter difficulties in directly generating high-resolution samples. Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages. These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work. In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process. Remarkably, this technique does not necessitate any additional training or relying on external models. We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion models. We also observed that the proper selection of guidance scale can improve image quality, fidelity, and prompt alignment.\n## Guidances for Diffusion Models\nTechniques have been proposed for conditionally sampling images corresponding to specific classes or text prompts by adding a guidance term to the predicted noise.Ho & Salimans (2022) added the gradient of the log probability predicted by a classifier to \u03f5(x t , t), enabling an unconditional diffusion model to generate class-conditioned images.Subsequently, classifier-free guidance (CFG) was proposed.Instead of using a classifier, the noise predictor's architecture was modified to accept condition c as an input.The following formula is then used as the predicted noise:\n\nHere, w represents the guidance scale.It has been commonly observed that proper adjustment of the scale improves the alignment with the condition and the fidelity of the generated images.",
            "reference_string": "[268856926 | Hwang et al. | 2024 | Citations: 15]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "265150356",
            "title": "Music ControlNet: Multiple Time-Varying Controls for Music Generation",
            "text": "To improve the flexibility of text conditioning, classifierfree guidance (CFG) is commonly employed. CFG is used to simultaneously learn a conditional and unconditional generative model together and trade-off conditioning strength, mode coverage, and sample quality [24]. Practically speaking, during training CFG is achieved by randomly setting conditioning information to a special null value c \u2205 for a fraction of the time during training. Then during inference, an image is generated using conditional control inputs, unconditional control inputs, or a linear combination of both. In most cases, a forward pass of f \u03b8 (x (m) , m, c text ) and f \u03b8 (x (m) , m, c \u2205 ) per sampling step are needed and subsequent weighted averaging.",
            "score": 0.8739519331318237,
            "section_title": "C. Classifier-free Guidance",
            "char_start_offset": 8399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 732
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "277043967",
            "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
            "text": "Classifier-free guidance (CFG) [10], is a method for guiding the diffusion process without requiring a separate classifier. For a conditional diffusion model \u01770 = f \u03b8 (y t , t, x), classifier-free guidance perturbs the predicted value as follows2 : \n\nfor some guidance scale s \u2265 0. Larger values of s shift the prediction y 0 further from the unconditional prediction f \u03b8 (y t , t, \u2205) in the direction of the conditional prediction, causing the condition to have increased effect. \n\nCFG has seen wide usage in the domain of images, and is understood to trade off diversity for fidelity [17,21]. While classifiers have been used to guide the diffusion process in text generation [14], to the best of my knowledge classifierfree guidance has not yet been explored.",
            "score": 0.8158080416534594,
            "section_title": "Classifier-free Guidance",
            "char_start_offset": 5636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 762
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 590,
                    "end": 593,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "259308807",
            "title": "Stay on topic with Classifier-Free Guidance",
            "text": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.",
            "score": 0.7649108333157304,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "276774646",
            "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
            "text": "Classifier-Free Guidance (CFG) (Ho et al. (2022)) is a widely used technique in diffusion models that improves the alignment of generated samples with a given condition, such as a text prompt in text-to-image generation. It has been broadly adopted in diffusion-based models (Chen et al. ( 2024)). \n\nCFG operates by first computing an unconditional output, followed by a conditioned one. It then determines the difference between the conditional and unconditional outputs, defining a direction in the model's latent space. By amplifying this direction, the model is effectively steered toward outputs that better align with the given condition. Increasing the scale factor further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThe CFG amplification is expressed as: \n\nwhere: \n\n\u2022 \u03f5 uncond represents the model's noise prediction when no conditioning information is provided. \n\n\u2022 \u03f5 cond represents the noise prediction when conditioning information (e.g., a text prompt) is included. \n\n\u2022 w is the guidance scale, which controls the strength of conditioning in the generated output. \n\nThe equation essentially performs an amplified directional shift in prediction space, pushing the generated output closer to the conditioned estimate. By adjusting w: \n\n\u2022 If w = 0, the model generates samples unconditionally, without any reliance on the condition. \n\n\u2022 If w = 1, the model follows the standard conditional generation process. \n\n\u2022 If w > 1, the model exaggerates the influence of the condition, leading to more precise alignment with the input prompt. \n\nThe core idea is that when given a condition (e.g., a text prompt like \"blue fluffy dog\") and comparing the output to an unconditioned one, subtracting the two produces a direction that represents \"blue fluffy dog\". \n\nBy repeatedly adding this direction (controlled by the guidance scale factor), the model emphasizes the condition more strongly, resulting in an output that better aligns with the given prompt. As shown in Figure 2, a higher guidance scale strengthens the emphasis on the \"dog.\" However, an excessively high guidance scale is also suboptimal, as demonstrated in the figure .",
            "score": 0.7133083454244206,
            "section_title": "Superscopes as a variation of Classifier-Free Guidance (CFG)",
            "char_start_offset": 20114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2192
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "276774646",
            "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
            "text": "In diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.",
            "score": 0.7107602106946932,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 10843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 864
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "269043390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "258427128",
            "title": "Class-Balancing Diffusion Models",
            "text": "Diffusion models Diffusion models are recently proposed generative models [44] based on non-equilibrium thermodynamics. Conditional diffusion models [7] encode label information into the generation process and improve largely the generation performance. The guidance structure proposed in [7] makes it possible to control the generation process through an external module. Based on a similar intuition, researchers arrive to realize diverse functions, such as guided adversarial purification [35], few-shot generation [9] and so on [38,43]. The drawback of classifier guidance (noted as CG) [7] lies in its requirement of training another auxiliary classifier. To address the issue, classifier-free guidance (noted as CFG) [12] proposed a mechanism that uses the generator itself to express the class guidance information. CFG is proved to be a resource efficient method and achieves outstanding performance on large models [33]. Moreover, CFG only requires to add one line in training, which can be easily transplanted on different models.",
            "score": 0.7105462045313653,
            "section_title": "Related Works",
            "char_start_offset": 3899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1040
                }
            ],
            "ref_mentions": [
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 536,
                    "end": 539,
                    "matchedPaperCorpusId": "247839278"
                },
                {
                    "start": 924,
                    "end": 928,
                    "matchedPaperCorpusId": "245335086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "270286056",
            "title": "BitsFusion: 1.99 bits Weight Quantization of Diffusion Model",
            "text": "During the inference time, classifier-free guidance (CFG) [22] is usually applied to improve the generation, \n\nwhere w \u2265 1 and \u03b5\u03b8int,s (t, z t , \u2205) denotes the generation conditioned on the null text prompt \u2205.",
            "score": 0.7101833451792112,
            "section_title": "Preliminaries",
            "char_start_offset": 6557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "265498527",
            "title": "DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations",
            "text": "Classifier-free guidance (CFG) [36] was introduced as an alternative to classifier guidance [9,45,56,80]. [18] already used a class-conditional denoising model \u03f5 \u03b8 (x t , t, y) that was given the target class as additional input. The class label y was thereby integrated into the model via adaptive group normalization layers. They introduced classifier guidance to enforce the generation of the correct target class by strengthening the influence of y on the output of the generative process. Classifier-free guidance is an alternative that also strengthens the impact of the conditioning signal in combination with a conditional denoising model \u03f5 \u03b8 (x t , t, y) without the requirement of an external classifier. \n\nIn the following, we will first introduce cross-attention (XA) conditioning that is used by Stable Diffusion [64] to condition the denoising model \u03f5 \u03b8 not only on class labels but also other modalities such as text prompts or depth maps. Then we will introduce classifier-free Guidance as a solution to strengthen the impact of the conditioning signal.",
            "score": 0.6852808456649526,
            "section_title": "A.3. Classifier-Free Guidance and Cross-Attention Conditioning",
            "char_start_offset": 35114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1069
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 101,
                    "end": 104,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "275906852",
            "title": "TFG-Flow: Training-free Guidance in Multimodal Generative Flow",
            "text": "As training-free guidance is an emerging area in generative model research, this section highlights its significance as a critical and timely topic that warrants greater attention and effort, particularly in leveraging off-the-shelf models. \n\nComparison with Conditional Generative Models. Conditional generative models, such as Cond-EDM and Cond-Flow, require labeled data for training. This reliance on annotated datasets can limit training efficiency and scalability. In contrast, training-free guidance allows the use of any unconditional generative model, which can be trained in an unsupervised manner. This flexibility enables the scaling of generative models without the need for task-specific annotations. Furthermore, conditional generative models are tied to predefined tasks during training, making them unsuitable for new tasks post-training. \n\nComparison with Classifier Guidance. Classifier guidance (Dhariwal & Nichol, 2021) involves training a time-dependent classifier aligned with the noise schedule of the generative model. While this approach can utilize pre-trained unconditional flow or diffusion models, it still requires labeled data for classifier training on a per-task basis. This constraint makes it less adaptable for scenarios where the target is defined as a loss or energy function without associated data, or when leveraging pretrained foundation models for guidance. In contrast, training-free guidance offers greater flexibility, allowing for broader and more dynamic applications without the need for task-specific classifier training. \n\nComparison with Classifier-Free Guidance. Classifier-free guidance (Ho & Salimans, 2022) has become a popular approach for building generative foundation models. However, it still requires task definitions to be determined prior to model training. A more versatile alternative is instruction tuning, where text instructions act as an interface for a variety of user-defined tasks. As shown by Ye et al. (2024), training-free guidance surpasses text-to-X models (e.g., text-to-image or text-to-video) in flexibility and robustness, as demonstrated by two failure cases of GPT-4. These models often struggle with interpreting complex targets in text form or constraining generated distributions using simple text prompts.",
            "score": 0.6827158958589322,
            "section_title": "C.3 THE MOTIVATION OF STUDYING TRAINING-FREE GUIDANCE",
            "char_start_offset": 42184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 915,
                    "end": 940,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "259308807",
            "title": "Stay on topic with Classifier-Free Guidance",
            "text": "In this section we show that Classifier-Free Guidance reliably boosts performance across a variety of common prompting approaches. In Section 3.1 we show that CFG boosts zero-shot performance on a variety of standard NLP benchmarks, including achieving state-of-the-art performance on LAMBADA with LLaMa-7B. In Section 3.2 we apply CFG to Chain-of-Thought prompts [55,82] an approach to allows the model to reason first before answering the question. Next, we test the performance of CFG on text-to-text generation prompts in Section 3.3. Finally, we show in Section 3.4 that CFG can be applied to assistant prompts (i.e. prompts with system-instructions).",
            "score": 0.6636420460715706,
            "section_title": "Experiments",
            "char_start_offset": 3744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 656
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "244773644"
                },
                {
                    "start": 368,
                    "end": 371,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "277313590",
            "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
            "text": "Classifier-free guidance (CFG) [55] is a versatile technique originally developed for diffusion generative models. Its effectiveness has also been demonstrated in language modeling applications, including AudioGen [56] and MusicGen [3]. In our research, we have identified significant benefits of employing CFG within both the semantic language model and the diffusion model, even though the authors of MeLoDy [12] did not explore CFG-based sampling for the semantic LM. For MusiCoT, we introduce a dual-scale CFG sampling strategy that modifies the log probabilities as follows: ] represents the flattened CLAP RVQ tokens.",
            "score": 0.6624989476809146,
            "section_title": "Dual-Scale Classifier-Free Guidance",
            "char_start_offset": 18075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 623
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "259108357"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "258887792"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "276421337",
            "title": "CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation",
            "text": "To enhance the alignment between generated images and textual descriptions, Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) modifies the sampling distribution as: \n\nwhere c denotes the textual input, p \u03b8 (z t |c) represents the conditional generative distribution, p \u03b8 (z t ) is the unconditional distribution and s is a scale scalar. CFG guides the generation by estimating the diffusion score (Song et al., 2022) as: \n\nCFG biases the generation process towards the conditional distribution while reducing reliance on the unconditional counterpart, effectively aligning the generated content with the provided textual prompt.",
            "score": 0.646462852078307,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 10309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 631
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 419,
                    "matchedPaperCorpusId": "227209335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "268524153",
            "title": "Scaling Instructable Agents Across Many Simulated Worlds",
            "text": "We use Classifier-Free Guidance (CFG; Ho and Salimans, 2022;Lifshitz et al., 2023) to improve the language-conditionality of a trained agent when running it in an environment. CFG was originally proposed for strengthening text-conditioning in diffusion models (Ho and Salimans, 2022), but has also proven useful for similar purposes with language models (Sanchez et al., 2023) and languageconditioned agents (Lifshitz et al., 2023). That is, we compute the policy, , with and without language conditioning, and shift the policy logits in the direction of the difference between the two:   =  (image, language) +  ( (image, language) \u2212  (image, \u2022)) .",
            "score": 0.6446250919628468,
            "section_title": "Agent",
            "char_start_offset": 31087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 649
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 376,
                    "matchedPaperCorpusId": "91184540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "260899968",
            "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation",
            "text": "In doing so, the distribution hones in on tokens exhibiting high probability in the presence of early context but low probability when devoid of it. Similarly, Su et al. (2022) used a degeneration penalty, defined as the maximum cosine similarity between the representation of a continuation and that of all previous tokens, to prevent model degeneration. They refer to this method as contrastive search. \n\nBuilding upon the innovative efforts to exert control over generative models, the concept of classifier-free guidance (CFG) emerged as a further refinement to the field. It embodies a methodology aligning closely with previous advancements but distinctively eliminating the reliance on separate classifier models. The genesis of CFG can be traced back to diffusion models, where techniques were developed to reshape the latent sampling distribution to synchronise more harmoniously with the prompt (Dhariwal and Nichol 2021b). Addressing earlier complexities, Ho and Salimans (2022) synthesised the classifier's role into the model's training process itself, paving a new path for efficiency. \n\nIn the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sanchez et al. 2023). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;Rombach et al. 2022;Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022).",
            "score": 0.6244752809926196,
            "section_title": "Related work Evolution of Generative Models and Control Over Conditional Generation",
            "char_start_offset": 7648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 1239,
                    "end": 1260,
                    "matchedPaperCorpusId": "44104089"
                },
                {
                    "start": 1865,
                    "end": 1885,
                    "matchedPaperCorpusId": "247958012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "276813045",
            "title": "ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning",
            "text": "Classifier-free guidance (CFG) is an approach proposed to enhance the quality and diversity of generated samples in generative models. This technique involves updating the score function to blend the gradients from both a conditional model and an unconditional model. By eliminating the need for a separate classifier, this method simplifies the training process and increases flexibility. The score update rules, as proposed, optimize the balance between the conditional and unconditional components, resulting in more accurate and diverse outputs. \n\nIn practice, given a condition y = (y neg , y pos ) where y neg is the negative prompt and y pos is the positive prompt, I use the following score update: \u03b5\u03b8(x, y) = (1 + w)\u03f5\u03b8(x, y pos ) \u2212 w\u03f5 \u03b8 (x, y neg ) \n\nwhere w \u2208 R is a scalar weight greater than one. \n\nThe above classifier-free guidance cannot be directly applied in cases where the effect of prompts should vary spatially, as it uses only a single global weight w. In more recent work [24], the authors proposed Semantic-aware Classifier-Free Guidance (S-CFG), which allows for different guidance levels for distinct semantic units within an image, thereby enforcing uniformity of text guidance. Although the purposes differ, I adopt this approach to reflect two different intentions (removal and creation) for each region when multiple masks are provided.",
            "score": 0.6206818988295608,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 10157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "273228630",
            "title": "Diversity-Rewarded CFG Distillation",
            "text": "Classifier-free guidance (CFG). Initially introduced for diffusion models (Ho and Salimans, 2022) and later adapted for autoregressive LLMs (Gafni et al., 2022;Sanchez et al., 2023;Wings, 2022), CFG has found widespread application in various generative domains, including image (Nichol et al., 2021;Ramesh et al., 2022;Rombach et al., 2022;Saharia et al., 2022;Yu et al., 2022), video (Blattmann et al., 2023;Ho et al., 2022), and audio (Copet et al., 2023;Kreuk et al., 2023) generation. However, the detrimental impact of CFG on diversity is well-documented (Dhariwal and Nichol, 2021;Kreuk et al., 2023;Nichol et al., 2021), limiting its application when exploration is key. \n\nDistillation. Knowledge distillation (Hinton et al., 2015) is emerging as a powerful technique to train state-of-the-art models (Gemma Team et al., 2024). By transferring knowledge from a teacher, the student can perform better than with standard training on the same data (Gu et al., 2023;Lin et al., 2020;Sanh et al., 2019). Closer to our work, Meng et al. (2023) propose a two-stage offline procedure to distill a CFG-augmented diffusion model. In contrast, we employ a single-stage on-policy distillation procedure (Agarwal et al., 2024) to distill a CFG-augmented LLM, while introducing a novel diversity-promoting RL algorithm and model merging for improved quality-diversity trade-off. \n\nQuality-diversity in LLMs. Zhang et al. (2021) compare the quality-diversity trade-offs of various inference-time strategies for LLMs, including temperature sampling, top-k sampling (Fan et al., 2018), and nucleus sampling (Holtzman et al., 2020).",
            "score": 0.619562455148588,
            "section_title": "Related work",
            "char_start_offset": 26735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1623
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 300,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 320,
                    "end": 341,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 341,
                    "end": 362,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 362,
                    "end": 378,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 386,
                    "end": 410,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 438,
                    "end": 458,
                    "matchedPaperCorpusId": "259108357"
                },
                {
                    "start": 458,
                    "end": 477,
                    "matchedPaperCorpusId": "252668761"
                },
                {
                    "start": 561,
                    "end": 588,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 588,
                    "end": 607,
                    "matchedPaperCorpusId": "252668761"
                },
                {
                    "start": 607,
                    "end": 627,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 718,
                    "end": 739,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1028,
                    "end": 1046,
                    "matchedPaperCorpusId": "252762155"
                },
                {
                    "start": 1200,
                    "end": 1222,
                    "matchedPaperCorpusId": "263610088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "249926846",
            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
            "text": "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
            "score": 0.613509617947003,
            "section_title": "Classifier-Free Guidance and Reranking",
            "char_start_offset": 15352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1835,
                    "end": 1838,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "277321603",
            "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
            "text": "In recent years, diffusion models [29,56,59] have shown great success in generation tasks, becoming the de facto standard generative model across many data modalities such as images [50][51][52][53], video [4,5,30,66], and audio [13,33,44]. The success of diffusion models is not only due to their high-quality results and ease of training, but also the simplicity of adapting them into conditional diffusion models. While previous generative models such as GANs [23] and VAEs [39] require separate training for each conditional generation task, making it costly to create various conditional generative models, diffusion models introduced a considerably more effective approach: training an unconditional model (or a conditional model with simple conditions, such as text) as a base and branching out into multiple conditional models. At the core of the extendability of diffusion models in easily converting an unconditional (or less conditioned) base model into a conditional (or more conditioned) model is the Classifier-Free Guidance (CFG) [28] technique. CFG proposed to learn to predict both unconditional and conditional noises using a single neural network, without introducing another network, such as a classifier, as in the classifier-guidance [15] approach. CFG combines unconditional and conditional noise predictions to generate data conditioned on a given input. It has been widely adopted not only for training a conditional model from scratch but also for fine-tuning a base model to incorporate other conditions, by adding encoders for the conditional input. Many successful conditional generative models have been fine-tuned using CFG from a base model. For example, Zero-1-to-3 [46] and Versatile Diffusion [64] use variants of Stable Diffusion [52] (SD) as a base, with additional encoders to incorporate the input image as conditions, while Instruct-Pix2Pix [7] uses SD1.5 as a base and incorporates text editing instructions and input reference images as conditions to perform instruction-based image editing. \n\nDespite its successes and widespread usage, fine-tuning a conditional model from a base model using the CFG technique has limitations, most notably producing lower-quality results for unconditional generation.",
            "score": 0.6131042934079795,
            "section_title": "Introduction",
            "char_start_offset": 1600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 43,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 209,
                    "end": 211,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "248006185"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "263151295"
                },
                {
                    "start": 229,
                    "end": 233,
                    "matchedPaperCorpusId": "259108357"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "260775781"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1699,
                    "end": 1703,
                    "matchedPaperCorpusId": "257631738"
                },
                {
                    "start": 1728,
                    "end": 1732,
                    "matchedPaperCorpusId": "253523371"
                },
                {
                    "start": 1766,
                    "end": 1770,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1881,
                    "end": 1884,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "269283056",
            "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
            "text": "Particularly, CFG has catalyzed advancements in text-conditional generation, a domain where training a noisy text classifier is less convenient and performs worse. This approach breathed new life into the text-to-image application, initially proposed in several works such as (Reed et al., 2016;Mansimov et al., 2015). Numerous works (Rombach et al., 2022;Ramesh et al., 2022;Nichol et al., 2022;Avrahami et al., 2022) have leveraged text-to-image generation with CFG diffusion models conditioned on text encoders like CLIP (Radford et al., 2021), showcasing significant progress in the field, e.g. the Latent Diffusion Model (Dhariwal & Nichol, 2021) and Stable Diffusion (Rombach et al., 2022) employ VAE latent space diffusion with CFG with CLIP encoder. SDXL, an enhanced version, leverages a larger model and an additional text encoder for high-resolution synthesis. \n\nImprovements on Diffusion Guidance. Noticed that in Classifier Guidance (CG), the classifier's gradient tends to vanish towards the early and final stages due to overconfidence, Zheng et al. (2022) leverages the entropy of the output distribution as an indication of vanishing gradient and rescales the gradient accordingly. To prevent such adversarial behaviours, Dinh et al. (2023b) explored using multiple class conditions, guiding the image generation from a noise state towards an average of image classes before focusing on the In Classifier-Free Guidance (CFG), Li et al. ( 2023) used CFG to recover a zero-shot classifier by sampling across timesteps and averaging the guidance magnitude for different labels, with the lowest magnitude corresponding to the most probable label. However, they observed a discrepancy in performance across timesteps with early stages yielding lower accuracy than intermediate ones. Chang et al. (2023) observed that a linear increase in guidance scale enhances diversity. Similarly, Gao et al. (2023) developed a parameterized powercosine-like curve, optimizing a specific parameter for their dataset and method.",
            "score": 0.6114596011353367,
            "section_title": "Related Work",
            "char_start_offset": 5638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 295,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 334,
                    "end": 356,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 396,
                    "end": 418,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 524,
                    "end": 546,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 673,
                    "end": 695,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1052,
                    "end": 1071,
                    "matchedPaperCorpusId": "249953799"
                },
                {
                    "start": 1239,
                    "end": 1258,
                    "matchedPaperCorpusId": "268042461"
                },
                {
                    "start": 1795,
                    "end": 1814,
                    "matchedPaperCorpusId": "255372955"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "276287831",
            "title": "Classifier-Free Guidance: From High-Dimensional Analysis to Generalized Guidance Forms",
            "text": "An important task for both paradigms is generating data conditioned on a class label or textual description of the image content. This can be achieved through conditioning mechanisms in the model architecture, as well as guidance techniques (Dhariwal and Nichol, 2021;Ho and Salimans, 2022) that steer the generation process towards samples aligned with user intentions or desired properties. \n\nThe notion of guidance was first introduced in classifier guidance (Song et al., 2020a;Dhariwal and Nichol, 2021), where a pre-trained classifier is leveraged to induce class conditioning of the sampling. Although beneficial, relying on a pre-trained classifier can be computationally expensive and may introduce biases inherent to the classifier itself. Classifier-free guidance (CFG) (Ho and Salimans, 2022) was developed as an alternative, and was quickly adopted as a standard technique in state-of-the-art generative models (Nichol et al., 2021;Betker et al., 2023;Saharia et al., 2022;Esser et al., 2024). CFG does not rely on an auxiliary classifier, instead, the model is trained to generate unconditional and conditional samples, and at inference extrapolates the denoising path towards the conditional one. Using CFG, however, it is no longer guaranteed to sample the original conditional distribution. Indeed, CFG modifies it by steering it towards a \"mode\" of high-quality and input-consistent samples, while reducing sample diversity in the process (Astolfi et al., 2024). \n\nThe effectiveness of CFG remains surprising in many ways, and a main theoretical question is to characterize the distributions generated by CFG and how they compare to the target distribution. Recent theoretical works on CFG formally showed that in case of Gaussian mixtures in one and finite dimensions, it results in a sharper distribution than the target one, and more shifted towards the boundary of the class (Chidambaram et al., 2024;Xia et al., 2024;Wu et al., 2024;Bradley and Nakkiran, 2024).",
            "score": 0.607374232458971,
            "section_title": "Introduction",
            "char_start_offset": 1775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 268,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 482,
                    "end": 508,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 945,
                    "end": 965,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 965,
                    "end": 986,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 986,
                    "end": 1005,
                    "matchedPaperCorpusId": "268247980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "266053531",
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "text": "In conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning. Prompt Highlighter extends CFG control in language models in a more generalized manner. The user's selection on the context x is converted into a token-level binary highlight mask m = {m 1 , . . . , m N }. We define m i = 1 if the i-th token x i is highlighted, and m i = 0 otherwise. This mask constructs a two-branch condition: the normal and the unconditional contexts. The normal context operates in the same manner as in vanilla inference.",
            "score": 0.6064017065891522,
            "section_title": "Token-Level Highlight Guidance",
            "char_start_offset": 10683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "270923987",
            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
            "text": "Diffusion models have recently emerged as the main methodology behind many successful generative models [39,14,9,33,42,43].At the core of such models lies a diffusion process that gradually adds noise to the data until data points are indistinguishable from pure noise.At inference, a denoiser is used to gradually remove noise from samples until we reach a generated data point.While the theory suggests that standard sampling from diffusion models should yield high-quality images, this does not generally hold in practice, and guidance methods are often required to increase the quality of generations, albeit at the expense of less diversity [9,13,35].Classifier guidance [9] was the first method to introduce this concept by utilizing the gradient of a classifier trained on noisy images to increase the class-likelihood of generated samples.Later, classifier-free guidance (CFG) [13] was proposed, allowing the diffusion model to simulate the same behavior as classifier guidance without using an explicit classifier.Since then, CFG has been applied to other conditional generation tasks, such as text-to-image synthesis [28] and text-to-3D generation [31].\n\nIn addition to CFG's trading diversity for quality, it has two practical limitations.First, it requires the underlying model to be trained in a specific way to also learn the unconditional score function, typically by replacing the conditioning vector with a null vector with probability p (usually p \u2208 [0.1.0.2]).This additional step hinders training efficiency, as the model needs to be trained on two different tasks.Replacing the condition might also not be straightforward when the model is multimodal and uses different conditioning signals such as text, images, and audio at the same time, or when the null vector (which is usually the zero vector in practice) has a specific meaning.Second, it is not clear how to extend the benefits of classifier-free guidance beyond conditional models to unconditional generation.\n\nIn this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model.The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.",
            "score": 0.5975205233903729,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 847
                },
                {
                    "start": 847,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1163
                },
                {
                    "start": 1165,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1479
                },
                {
                    "start": 1479,
                    "end": 1585
                },
                {
                    "start": 1585,
                    "end": 1856
                },
                {
                    "start": 1856,
                    "end": 1989
                },
                {
                    "start": 1991,
                    "end": 2180
                },
                {
                    "start": 2180,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 111,
                    "end": 113,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 116,
                    "end": 119,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 119,
                    "end": 122,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 652,
                    "end": 655,
                    "matchedPaperCorpusId": "264490969"
                },
                {
                    "start": 676,
                    "end": 679,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "252596091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "276741036",
            "title": "DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes",
            "text": "Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(x|c) and unconditional generation p(x), and guidance during sampling is provided by the following equation: \n\nwhere xt (c) is the result conditioned on c, xt (\u2205) is the unconditioned result, and w is a weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved.",
            "score": 0.5946703031622833,
            "section_title": "A.4 CLASSIFIER-FREE GUIDANCE",
            "char_start_offset": 21593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "271600562",
            "title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention",
            "text": "Classifier-free guidance (CFG) [14], first proposed as a replacement for classifier guidance (CG) [8] is controlled by a scale parameter. The higher we set classifier-free guidance, the more we get faithful, high-quality images. However, it requires external labels, such as text [30] or class [8] labels, making it impossible to apply to unconditional diffusion models. Also, it requires specific traning procedure with label dropping and it is known that high CFG causes saturation [42]. \n\nFigure 4: Conditional generation using ControlNet [51] and SEG. Table 1: Quantitative comparison of SEG with vanilla SDXL [35], SAG [17], and PAG [1] for unconditional generation.",
            "score": 0.5857395831619032,
            "section_title": "Discussion on related work",
            "char_start_offset": 15881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 671
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "252683688"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "267636873",
            "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
            "text": "Controllable text generation (Prabhumoye et al., 2020;Hu and Li, 2021;Zhang et al., 2023a) has emerged as a vital research domain, focusing on the generation of natural sentences with controllable attributes such as persona (Prabhumoye et al., 2020;Hu and Li, 2021;Zhang et al., 2023a), politeness (Niu and Bansal, 2018;Madaan et al., 2020), and story ending (Peng et al., 2018). Among the various approaches, fine-tuning has been recognized as the most straightforward approach, achieved either through the tuning of model parameters (Li and Liang, 2021;Ouyang et al., 2022;Carlsson et al., 2022) or the integration of tunable adaptor modules (Lin et al., 2021;Ribeiro et al., 2021). While fine-tuning has been effective in a wide range of applications, it is also expensive in computational cost as the size of LLMs is growing tremendously. Recently, there has been a development on controllable generation with diffusion models (Li et al., 2022;Lin et al., 2023), extending to controllable text-to-image generation (Yang et al., 2023). Particularly, the use of classifier guidance (Dhariwal and Nichol, 2021) and classifier-free guidance (Ho and Salimans, 2021) has become prominent in refining the quality of generated outputs. While classifier guidance employs an auxiliary classifier model to evaluate and improve the generation Kawar et al. (2022); Kim et al. (2022); Shi et al. (2023), classifier-free guidance integrates control directly into the generative model, offering an efficient approach for real-time applications with computational constraints (Saharia et al., 2022;Lin et al., 2024). Most recently, Sanchez et al. (2023) applied classifier-free guidance to language models in the single-modal setting to improve their performance at inference time.",
            "score": 0.5838795886154594,
            "section_title": "Controllable Generation",
            "char_start_offset": 7528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 54,
                    "matchedPaperCorpusId": "218502475"
                },
                {
                    "start": 54,
                    "end": 70,
                    "matchedPaperCorpusId": "243843906"
                },
                {
                    "start": 70,
                    "end": 90,
                    "matchedPaperCorpusId": "245986550"
                },
                {
                    "start": 224,
                    "end": 249,
                    "matchedPaperCorpusId": "218502475"
                },
                {
                    "start": 249,
                    "end": 265,
                    "matchedPaperCorpusId": "243843906"
                },
                {
                    "start": 265,
                    "end": 285,
                    "matchedPaperCorpusId": "245986550"
                },
                {
                    "start": 298,
                    "end": 320,
                    "matchedPaperCorpusId": "13690180"
                },
                {
                    "start": 320,
                    "end": 340,
                    "matchedPaperCorpusId": "215811473"
                },
                {
                    "start": 359,
                    "end": 378,
                    "matchedPaperCorpusId": "51729727"
                },
                {
                    "start": 535,
                    "end": 555,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 555,
                    "end": 575,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 575,
                    "end": 597,
                    "matchedPaperCorpusId": "248780067"
                },
                {
                    "start": 644,
                    "end": 662,
                    "matchedPaperCorpusId": "221370525"
                },
                {
                    "start": 662,
                    "end": 683,
                    "matchedPaperCorpusId": "232240435"
                },
                {
                    "start": 931,
                    "end": 948,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 948,
                    "end": 965,
                    "matchedPaperCorpusId": "257019559"
                },
                {
                    "start": 1018,
                    "end": 1037,
                    "matchedPaperCorpusId": "254043880"
                },
                {
                    "start": 1084,
                    "end": 1111,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1141,
                    "end": 1164,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1335,
                    "end": 1354,
                    "matchedPaperCorpusId": "251643669"
                },
                {
                    "start": 1356,
                    "end": 1373,
                    "matchedPaperCorpusId": "246430592"
                },
                {
                    "start": 1375,
                    "end": 1392,
                    "matchedPaperCorpusId": "258309719"
                },
                {
                    "start": 1563,
                    "end": 1585,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1585,
                    "end": 1602,
                    "matchedPaperCorpusId": "258714883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "271974317",
            "title": "VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling",
            "text": "The success of classifier-free guidance (CFG) in text-to-image generation [12] has demonstrated the effectiveness of combining unconditional and conditional generation within diffusion models. Recent advancement in unimodal text generation has further illustrated that CFG can also be used in LLMs [26], improving both coherence and alignment with the given prompt. Motivated by this, we first attempt to introduce CFG into speech codec language models, to enhance the control over human instruction-to-speech generation. Specifically, the condition in Equation ( 1) and ( 2) are replaced with an empty prompt at a certain probability during AR model training. That is, we mask text embedding sequences when predicting semantic tokens, and we mask text embedding sequences or semantic token sequences when predicting coarse-grained acoustic tokens, both of which are considered forms of unconditional generation. Consequently, for inference, we can sample the -th semantic token in the logits space combined with unconditional guidance: \n\nwhere  is the guidance strength. When sampling the -th coarsegrained acoustic token, we can utilize two types of CFG at the same time, the generation of AT to focus on different aspects: \n\n P ( (,1) |E  , , ,  (<,1) ) =  ( (,1) |\u2205, , ,  (<,1) ) + ( ( (,1) |E  , , ,  (<,1) ) \u2212  ( (,1) |\u2205, , ,  (<,1) )) \n\n P\u2032 ( (,1) |E  , , ,  (<,1) ) =  P ( (,1) |E  , , \u2205,  (<,1) ) + ( P ( (,1) |E  , , ,  (<,1) ) \u2212  ( (,1) |E  , , \u2205,  (<,1) )) \n\nwhere  and  are the guidance strength corresponding to the human instruction and the semantic tokens. The guidance strength is usually set to be over 1. \n\nIntuitively, enhancing guidance on instructions contributes to better control over the voice characteristics of generated speech, while intensifying guidance on ST helps increase the intelligibility of the speech content, which can be shown in experiments.",
            "score": 0.5798687011397521,
            "section_title": "Classifier-Free Guidance for Codec Language Model",
            "char_start_offset": 18692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "253384594",
            "title": "I Hear Your True Colors: Image Guided Audio Generation",
            "text": "To further improve the generation performance, and steer the generation process towards the input images, we apply the Classifier-Free Guidance (CFG) method. It was recently shown by the authors in [16,5] that using the CFG method is an effective mechanism for controlling the trade-off between sample quality and diversity. We follow the same setup as in [8] in which during training for each sample in the batch with probability p = 0.5 we replace \n\nwith a learned-null embedding of the same size y \u2205 = \u27e8f \u2205 \u27e9 #f rames m=1 . We empirically found that applying CFG to the LOW model only is enough to greatly improve the performance. During inference we produce token distributions with and without visual conditioning, and we sample from the following, \n\nwhere \u03b7 \u2265 1 is the guidance scale that determines the tradeoff between diversity and quality of the generated audio characteristics. We use \u03b7 = 3 which showed to perform the best in prior works in the fields of text-to-image generation [5] and text-to-audio generation [8].",
            "score": 0.5707283234603698,
            "section_title": "Classifier Free Guidance.",
            "char_start_offset": 7746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1029
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "256231177",
            "title": "Imitating Human Behaviour with Diffusion Models",
            "text": "Classifier-Free Guidance (CFG) has become a core ingredient for text-to-image models, allowing one to trade-off image typicality with diversity (Ho and Salimans, 2021). In CFG, a neural network is trained as both a conditional and unconditional generative model. During sampling, by introducing a 'guidance weight' w, one places a higher weight (w > 0) on the prediction conditioned on some context (here, o), and a negative weight on the unconditional prediction, \n\n(3) \n\nOne might anticipate that CFG would also be beneficial in the sequential setting, with larger w producing trajectories of higher likelihood, but at the cost of the diversity. Surprisingly, we find that CFG can actually encourage less common trajectories, and degrade performance. . This can lead to less common trajectories being sampled more often. \n\nIn Figure 3 we visualise p(a|o) for the claw machine game, under varying guidance strengths, w. \n\nAn interpretation of CFG is that it encourages sampling of actions that would maximise an implicit classifier, p(o|a). Hence, CFG encourages selection of actions that were unique to a particular observation (Ho et al., 2022). Whilst this is useful for text-to-image models (generate images that are more specific to a prompt), in sequential environments this leads to an agent rejecting higherlikelihood actions in favour of less usual ones that were paired with some observation. In later experiments (section 4.1) we demonstrate empirically that this can lead to less common trajectories being favoured, while degrading overall performance. Appendix E provides a didactic example of when CFG fails in this way.",
            "score": 0.567926282221112,
            "section_title": "WHY CLASSIFIER-FREE GUIDANCE FAILS",
            "char_start_offset": 13091,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1635
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 167,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "271957385",
            "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
            "text": "Classifier-free guidance (CFG) [24] has been demonstrated as an effective way to enhance the generation quality in both image and audio domains [35], [53]. CFG can be formulated as: \n\nwhere \u03bb denotes the guidance scale. CFC tries to model conditional distribution p(x|y) and unconditional distribution p(x) in the training. In the inference stage, \u03bb = 1 denotes that we do not use classifier-free guidance, when \u03bb > 1 the model decreases the unconditional likelihood of the sample while increasing the conditional likelihood. In other words, classifierfree guidance conducts this by decreasing the unconditional likelihood with a negative score term. During the training stage, previous works try to mask the condition information of some samples (e.g. set the training sample's text as empty with 10% probability), so that these samples can be used to optimize unconditional distribution p(x). \n\nMany prior studies [10], [22] demonstrate that text transcriptions generated by an ASR system can be utilized to train a TTS system. Inevitably, the text transcription from the ASR system is not flawless, referred to as noisy or weak labels. Despite this, existing literature does not thoroughly explain why TTS systems can be effectively trained using such dataset. In this study, we show that including a small part of the noisy label in the training set is equivalent to introducing the CFC training strategy, thus we do not need to deliberately construct masked samples during training stage. In the following, we give the proof. \n\nAssume the condition is y, the target is x, and the generative model tries to model p(x|y). Consider the text y includes n words, and y can be split into n parts, and we have correspondence (y i , x i ). Given that the TTS task tries to learn the mapping between y i andx i , y i . In general, we assume x j are mutually independent if i \u0338 = j. The target of the generative model is to learn the distribution p \u03b8 (x|y). Based on Bayes formula, we have: \n\nWe can discard the last term when we calculate the derivative of x.",
            "score": 0.5648140488433426,
            "section_title": "E. Classifier-Free Guidance",
            "char_start_offset": 24850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 1985
                },
                {
                    "start": 1988,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "256416291"
                },
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "268247980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "273549320",
            "title": "Scaling up Masked Diffusion Models on Text",
            "text": "We propose a surprisingly simple yet effective approach that leverages unlabeled data to boost performance in various language tasks, dubbed unsupervised classifier-free guidance (CFG). \n\nCFG. CFG (Ho & Salimans, 2022) is an effective and versatile technique widely used in both continuous and discrete diffusion models, with applications spanning image (Ho & Salimans, 2022;Chang et al., 2023) and text generation (Lovelace et al., 2024). Rooted in Bayes' rule, CFG simultaneously trains a conditional and an unconditional diffusion model, introducing a rescaled distribution for inference. Specifically, at a given timestep t \u2208 [0, 1], CFG (Chang et al., 2023) is defined as: \n\nwhere c is the condition, w is a hyperparameter that flexibly controls the strength of c, and p \u03b8 (x 0 |c, x t ) and p \u03b8 (x 0 |x t ) are the conditional and unconditional models respectively. \n\nNotably, it seems that the conditional model must be trained on paired data before applying CFG. Consequently, to the best of our knowledge, all existing work (Ho & Salimans, 2022;Chang et al., 2023;Lovelace et al., 2024) fall into supervised settings, where paired data are readily available. \n\nUnsupervised CFG. We extend CFG to an unsupervised setting by introducing a new formulation: \n\nwhere m is a mask sequence of the same length as c. Compared to Eq. ( 6), the dummy variable m translates the unconditional distribution to a conditional format without adding new information. For simplicity, we continue to refer to p \u03b8 (x 0 |m, x t ) as the unconditional distribution in unsupervised CFG throughout this paper. \n\nThe core insight is that an MDM already characterizes both distributions employed in Eq. ( 7) during unsupervised pretraining. Specifically, in language tasks, both c and x can be viewed as segments of a whole sequence, following the same distribution of unsupervised samples for pretraining.2",
            "score": 0.5647381143476567,
            "section_title": "UNSUPERVISED CLASSIFIER-FREE GUIDANCE",
            "char_start_offset": 12383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1889
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "276961040",
            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
            "text": "Conditional generation, e.g., class-to-image, text-to-image, or image-to-video, is omnipresent as it provides a compelling way to control the output. Ideally, conditional generation results are both diverse and of high-fidelity. Namely, the generative models' outputs align with the conditioning information perfectly and diligently follow the training data diversity. However, there is a trade-off between high-fidelity and diversity: without constraining diversity there are always possibilities to sample from areas on the data distribution manifold that are not well-trained. Thus, trading diversity for fidelity is a long-standing problem and the community has developed various approaches, e.g., the truncation trick for generative adversarial nets (GANs) [7,28], low-temperature sampling for probabilistic models [2], or temperature control in large language models [1,19]. \n\nMore recently, to trade diversity and fidelity in denois-ing diffusion models [25,35,53,58], several techniques have been developed [16,26,31], from which classifierfree guidance [24] emerged as the de-facto standard. For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image [50] and text-to-3D [47] generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing. Recently, several efforts provide insights by studying classifier-free guidance from a theoretical perspective [6,12,61] showing that sampling from classifier-free guidance is not the same as sampling from a sharpened distribution. \n\nInstead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance [16]. It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training [24]. This connection motivates us to carefully study classifier guidance's derivation and its behavior.",
            "score": 0.5600228076866292,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 762,
                    "end": 765,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "12174018"
                },
                {
                    "start": 876,
                    "end": 879,
                    "matchedPaperCorpusId": "271571434"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "235694314"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 971,
                    "end": 974,
                    "matchedPaperCorpusId": "5560643"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "254096299"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "252596091"
                },
                {
                    "start": 1476,
                    "end": 1479,
                    "matchedPaperCorpusId": "271903235"
                },
                {
                    "start": 1479,
                    "end": 1482,
                    "matchedPaperCorpusId": "272770713"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "matchedPaperCorpusId": "273549917"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 2075,
                    "end": 2079,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9306640625
        },
        {
            "corpus_id": "277150926",
            "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling",
            "text": "Classifier-Free Guidance details. To implement cameraconditioned generation, we follow the formulation presented in [3]. Since our generation task involves multi-view generation conditioned on both a text prompt and a camera trajectory, we decompose these two conditions within the Classifier-Free Guidance (CFG) framework like Har-monyView [80]: \n\nwhere s R and s c represent the guidance strengths for the camera pose and text conditions, respectively, and c null denotes the null text condition used in CFG. Also, following DiffusionForcing [8], we slightly noise the camera pose condition to the better conditional mechanism as t R = 0.05.",
            "score": 0.5561425886609136,
            "section_title": "A.3. Details of Camera Conditioned Generation",
            "char_start_offset": 31331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 643
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 119,
                    "matchedPaperCorpusId": "257496235"
                },
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "266550956"
                },
                {
                    "start": 544,
                    "end": 547,
                    "matchedPaperCorpusId": "270869622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "278171703",
            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
            "text": "However, another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time. It is important to note that this method was primarily designed for class-conditional tasks, where the model is trained to generate images based on fixed categories; its application to text-to-speech remains underexplored. Inspired by this pioneering work, in this paper, we explore enhancing speech generation by modifying the flow matching training target, specifically for textconditioned and audio-conditioned tasks, such as TTS. Our goal is to enable the model to perform only conditional predictions during inference, eliminating the need for unconditional predictions required by traditional CFG. To validate the feasibility of this approach, we adopt F5-TTS, a representative flow-matching-based TTS model, as a case study. Experimental results show that our method can effectively halve the computational cost per sampling step without degrading the generated speech quality compared to the baseline F5-TTS with CFG. Furthermore, our method can be seamlessly combined with advanced sampling strategies to achieve additional speedup, highlighting its potential for real-time applications. \n\nOur contributions are summarized as follows: \n\n\u2022 We present the first attempt to remove classifier-free guidance from flow-matching-based TTS models at inference time by adopting the model-guidance training to alter the prediction target of flow matching. \u2022 We validate our approach on F5-TTS, effectively halving the inference cost without compromising the quality of the generated speech. Moreover, the proposed method can be seamlessly integrated with existing optimized sampling strategies, resulting in further speedup.",
            "score": 0.5549931236854728,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "268667035",
            "title": "DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow",
            "text": "Text-to-image diffusion models.Text-to-image diffusion models (Nichol et al., 2021;Saharia et al., 2022;Rombach et al., 2022) are conditional generative models that are trained with text embeddings.They utilize classifier-free guidance (CFG) (Ho & Salimans, 2022), which learns both conditional and unconditional models, and guide the sampling by interpolating the predictions with guidance scale \u03c9: D \u03c9 (x; \u03c3, y) = (1 + \u03c9)D(x; \u03c3, y) \u2212 \u03c9D(x; \u03c3), where y is a text prompt.Empirically, \u03c9 > 0 controls the tradeoff between sample fidelity and diversity.CFG scale is important in text-to-3D generation, as for the convergence of 3D optimization (Poole et al., 2022;Wang et al., 2023b).",
            "score": 0.5535514951728024,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 681
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 104,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 104,
                    "end": 125,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "273098412",
            "title": "Plug-and-Play Controllable Generation for Discrete Masked Models",
            "text": "Conditional generation based on guidance. As an important task in controllable generation, conditional generation aims to generate a random variable X \u223c p(x) (e.g., image) given another random variable Y known as the condition (e.g., the text description of an image). A popular approach is guidance: for example, in continuous diffusion model, the classifier guidance (Dhariwal & Nichol, 2021) learns a classifier p(y|x) that predicts the condition Y given a noisy sample of X and leverages its information to generate X conditional on Y = y. Classifier-free guidance (Ho & Salimans, 2022) trains a score model that approximates both the conditional and unconditional score functions, Preprint using a combination of them for conditional generation. These approaches can be extended to the discrete diffusion model (see Nisonoff et al. (2024)). \n\nControllable generation for discrete generative models. The study of controllable generation is an emerging area in language modeling (see, e.g., Zhang et al. (2023) for a review). A notable work, Dathathri et al. (2020), proposed applying gradient updates to the key-value cache in transformers, a task-agnostic approach but requiring fine-tuning during inference. For diffusion models, a recent work Li et al. (2024) introduced soft value-based decoding, a derivative-free algorithm that requires pre-sampled trajectories x 0 , x 1 , . . . , x T of discrete diffusion model to estimate a conditional expectation. This method does not exploit the special properties of the masking process. To the best of our knowledge, there are no fine-tuning-free samplers for controllable generation in discrete masked models.",
            "score": 0.5492337892774981,
            "section_title": "RELATED WORK",
            "char_start_offset": 16791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1662
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 393,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 994,
                    "end": 1013,
                    "matchedPaperCorpusId": "245986550"
                },
                {
                    "start": 1045,
                    "end": 1068,
                    "matchedPaperCorpusId": "208617790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "270391454",
            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
            "text": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.",
            "score": 0.5461622856428119,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "270391454",
            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) forms the key basis of modern text-guided generation with diffusion models (Dhariwal & Nichol, 2021;Rombach et al., 2022). Nowadays, it is common practice to train a diffusion model with large-scale paired text-image data (Schuhmann et al., 2022), so that sampling (i.e. generating) a signal (e.g. image, video) from a diffusion model can either be done unconditionally from p \u03b8 (x|\u2205) \u2261 p \u03b8 (x), or conditionally from p \u03b8 (x|c), where c is the text conditioning. Once trained, it seems natural that one would acquire samples from the conditional distribution by simply solving the probability-flow ODE or SDE sampling (Song et al., 2021b;a;Karras et al., 2022) with the conditional score function. In practice, however, it is observed that the conditioning signal is insufficient when used naively. To emphasize the guidance, one uses the guidance scale \u03c9 > 1, where the direction can be defined by the direction from the unconditional score to the conditional score (Ho & Salimans, 2021). \n\nIn modern text-to-image (T2I) diffusion models, the guidance scale \u03c9 is typically set within the range of [5.0, 30], referred to as the moderately high range of CFG guidance (Chen et al., 2024;Podell et al., 2023). The insufficiency in guidance also holds for classifier guidance (Dhariwal & Nichol, 2021;Song et al., 2021b) so that a scale of 10 was used. While using a high guidance scale yields higher-quality images with better alignment to the condition, it is also prone to mode collapse, reduces sample diversity, and yields an inevitable accumulation of errors during the sampling process.",
            "score": 0.5454378217785334,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1642
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 52,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 128,
                    "end": 153,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 153,
                    "end": 174,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 275,
                    "end": 299,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 671,
                    "end": 691,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 693,
                    "end": 713,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 1020,
                    "end": 1041,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1219,
                    "end": 1238,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 1325,
                    "end": 1350,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1350,
                    "end": 1368,
                    "matchedPaperCorpusId": "227209335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "269283056",
            "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
            "text": "Diffusion models have demonstrated prominent generative capabilities in various domains e.g. images (Ho et al., 2020), videos (Luo et al., 2023), acoustic signals (Kang et al., 2023b), or 3D avatars (Chen et al., 2023). Conditional generation with diffusion (e.g. text-conditioned image generation) has been explored in numerous works (Saharia et al., 2022;Ruiz et al., 2023;Balaji et al., 2022), and is achieved in its simplest form by adding an extra condition input to the model (Nichol & Dhariwal, 2021). To increase the influence of the condition on the generation process, Classifier Guidance (Dhariwal & Nichol, 2021) proposes to linearly combine the gradients of a separately trained image classifier with those of a diffusion model. Alternatively, Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) simultaneously trains conditional and unconditional models, and exploits a Bayesian implicit classifier to condition the generation without an external classifier. \n\nIn both cases, a weighting parameter \u03c9 controls the importance of the generative and guidance terms and is directly applied at all timesteps. Varying \u03c9 is a trade-off between fidelity and condition reliance, as an increase in condition reliance often results in a decline in both fidelity and diversity. In some recent literature, the concept of dynamic guidance instead of constant one has been mentioned: MUSE (Chang et al., 2023) observed that a linearly increasing guidance weight could enhance performance and potentially increase diversity. This approach has been adopted in subsequent works, such as in Stable Video Diffusion (Blattmann et al., 2023), and further mentioned in Gao et al. (2023) through an exhaustive search for a parameterized cosine-based curve (pcs4) that performs very well on a specific pair of model and task. Intriguingly, despite the recent appearance of this topic in the literature, none of the referenced studies has conducted any empirical experiments or analyses to substantiate the use of a guidance weight scheduler.",
            "score": 0.5415517899692207,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 117,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 126,
                    "end": 144,
                    "matchedPaperCorpusId": "257532642"
                },
                {
                    "start": 163,
                    "end": 183,
                    "matchedPaperCorpusId": "253581601"
                },
                {
                    "start": 199,
                    "end": 218,
                    "matchedPaperCorpusId": "254408910"
                },
                {
                    "start": 335,
                    "end": 357,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 357,
                    "end": 375,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 788,
                    "end": 809,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1388,
                    "end": 1408,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 1660,
                    "end": 1677,
                    "matchedPaperCorpusId": "257767316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "274598164",
            "title": "Self-Guidance: Boosting Flow and Diffusion Generation on Their Own",
            "text": "The pioneering work Ho and Salimans [11] introduces the classifier-free guidance (CFG), which replaces the vanilla probability q t (x t |c) with a new one q t (x t |c) := q t (x t )( qt(xt|c) qt(xt) ) \u03c9 . q t (x t ) represents the unconditional distribution which can be implemented by inputting an empty label \u2205 \u2205 \u2205 as q t (x t |\u2205 \u2205 \u2205). With this, the new score function turns to \n\nSuch a guidance strategy has become a default setting for diffusion models, such as the Stable Diffusion series [52]. However, the CFG strategy has its limitations. First, CFG requires both a conditional and an unconditional score network, which either requires two separate models or is challenging to train with a single model. Second, the CFG is not available for tasks such as purely unconditional generation.",
            "score": 0.5346439337827418,
            "section_title": "Preliminary Diffusion Models",
            "char_start_offset": 10327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 796
                }
            ],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "259308807",
            "title": "Stay on topic with Classifier-Free Guidance",
            "text": "We have shown that Classifier-Free Guidance, which was originally conceived of in text-to-image applications, can be an effective way of increasing adherence to the prompt in autoregressive language modeling. In contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model. We have shown that CFG can boost performance across an array of canonical benchmarks in NLP that involve variations of the prompt: basic prompting, chain-of-thought prompting, text-to-text prompting and chatbot prompting. Finally, we sought to explain the effects of CFG by showing it decreased sampling entropy, but not in the same ways that Instruction-tuned models do. Ultimately, we leave for future work the exact effects that CFG is having, but we propose qualitative visualizations that confirm our intuitions around prompt adherence. \n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45,73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train. \n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context. It's also possible that CFG might be misused. We have not tested the effects of CFG if used in conjunction with malicious strategies for hacking language models, including but not limited to: prompt injection and prompts aimed at overriding alignment. It's possible that there are unforeseen effects induced by an increased adherence to parts of the prompt. We tried to explore this at length, both quantitatively and qualitatively, and we designed tasks that might reveal such behavior. However, we cannot conclude this method is risk-free. We advocate for standardized benchmarks aimed more squarely at language-model risk (including, possibly, pairs of models along with known prompt injections). Such standardized benchmarks could help us unit-test an advancement like CFG before releasing it into the wild. \n\nStella Biderman supervised the process.",
            "score": 0.5343409221647053,
            "section_title": "Conclusion",
            "char_start_offset": 20190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1236
                },
                {
                    "start": 1239,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2243
                },
                {
                    "start": 2246,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "264172506",
            "title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation",
            "text": "Diffusion probabilistic model (DPM) [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song et al., 2020b] is a powerful generative model that employs a forward diffusion process to gradually add noise to data and generate new data from noise through a reversed process. DPM's exceptional sample quality and scalability have significantly contributed to the success of Artificial Intelligence Generated Content (AIGC) in various domains, including images [Saharia et al., 2022, Ramesh et al., 2022, 2021, Rombach et al., 2022], videos [Ho et al., 2022b, Singer et al., 2022, Ho et al., 2022a, Molad et al., 2023], and 3D objects [Poole et al., 2022, Lin et al., 2023, Wang et al., 2023]. \n\nConditional generation is one of the core tasks of AIGC. With the diffusion formulation, condition injection, especially the classical class condition, becomes more transparent as it can be modeled as an extra term during the reverse process. To align with the diffusion process, Dhariwal and Nichol [2021] proposed classifier guidance (CG) to train a time/noise-dependent classifier and demonstrated significant quality improvement over the unguided baseline. Ho and Salimans [2022] later proposed classifier-free guidance (CFG) to implicitly implement the classifier gradient with the score function difference and achieved superior performance in the classical class-conditional image generation. However, both CG and CFG require extra training with labeled data, which is not only time-consuming but also practically cumbersome, especially when adapting to new conditions. To reduce computational costs, training-free guidance methods have been proposed [Bansal et al., 2023] that take advantage of pretrained discriminative models. \n\nDespite the improved flexibility, training-free guidance has not demonstrated convincing performance compared to CG & CFG in formal quantitative evaluation of guiding diffusion generation. There seems to be an irreconcilable trade-off between performance and flexibility and the current guidance schemes are still to be desired.",
            "score": 0.5308703148191048,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 81,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 499,
                    "end": 522,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 548,
                    "end": 569,
                    "matchedPaperCorpusId": "252595919"
                },
                {
                    "start": 644,
                    "end": 662,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 966,
                    "end": 992,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1644,
                    "end": 1665,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "274514993",
            "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
            "text": "Impact of Classifier-free Guidance. Classifier-Free Guidance (CFG) scale is hyperparameter that control the trade-off between sample quality and diversity in conditional generative models. The visual variations of generated images with different CFG scales t are illustrated in Fig. 10 As observed, higher CFG scales lead to better alignment between the generated images and the text prompts, but cause more chaotic object structures, stronger stylization, and worse photorealism. For example, when CFG=15, the structure of the book in the image becomes disordered. Conversely, lower CFG scales result in poorer consistency between the image content and the prompt, but improve the photorealism and fine-grained texture details. \n\nComparison with Other Models. In Fig. 11, we present a comparison between Liquid and other unified multi-modal large models in terms of visual generation quality. Compared to models based on discrete multi-codebook (VILA-U), diffusion processes (Show-o), and multimodal tokenizers (Janus), Liquid demonstrates superior performance in knowledge-aware image generation (first row), scene generation accuracy and small-scale facial details (second row), as well as structural coherence of objects (third row, vehicles). This visual comparison quantitatively validates Liquid's advancements in generating high-fidelity, semantically consistent images while maintaining strong multi-modal understanding capabilities.",
            "score": 0.5305952407350116,
            "section_title": "Visual Comparative Analysis",
            "char_start_offset": 29021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "278171703",
            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
            "text": "Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation [29]. The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse [30]- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33].",
            "score": 0.5265856490786446,
            "section_title": "B. Classifier-Free Guidance",
            "char_start_offset": 5832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "270220558"
                },
                {
                    "start": 1108,
                    "end": 1112,
                    "matchedPaperCorpusId": "253581838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "270095412",
            "title": "Going beyond Compositions, DDPMs Can Produce Zero-Shot Interpolations",
            "text": "In the main body of this work, we study conditional generation using classifier guidance (Song et al., 2021;Dhariwal & Nichol, 2021) as it lets us reuse unconditional models during our explorations.Since classifier-free guidance (CFG) (Ho & Salimans, 2022) typically achieves superior FID (Heusel et al., 2017) and IS (Salimans et al., 2016), we study CFG and present results in Appendix J.",
            "score": 0.5256879324985996,
            "section_title": "Conditional Generation with Classifier Guidance",
            "char_start_offset": 7414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 390
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "270199289",
            "title": "Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling",
            "text": "Classifier-free Guidance An intriguing property of conditional diffusion models is that we can easily guide the iterative sampling process for better sampling quality.For instance, Ho and Salimans [2021] introduced Classifier-free Guidance (CFG), which utilizes the diffusion model itself to perform guidance at test time.More specifically, we perform sampling using the following linear combination:\n\nwhere \u03b3 is the guidance weight, and x \u03b8 (x t ) = x \u03b8 (x t , c = \u2205) is the unconditional denoising output.\n\nDuring training, we drop the condition c with certain probability p uncond to facilitate unconditional prediction.When \u03b3 > 1, CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation.\n\nCompared to autoregressive models, diffusion models are more flexible in adjusting sample steps, allowing for the utilization of noise schedules to learn different frequencies.Additionally, with the use of CFG, diffusion models can achieve higher quality images with much fewer parameters than autoregressive models.However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both.",
            "score": 0.525029940474101,
            "section_title": "Preliminaries",
            "char_start_offset": 7014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 322
                },
                {
                    "start": 322,
                    "end": 400
                },
                {
                    "start": 402,
                    "end": 507
                },
                {
                    "start": 509,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 786
                },
                {
                    "start": 788,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1274
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 203,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "269283056",
            "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
            "text": "Generative and Diffusion Models. Before the advent of diffusion models, several generative models were developed to create new data that mimics a given dataset, either unconditionally or with conditional guidance. Notable achievements include Variational AutoEncoders (VAEs) (Kingma & Welling, 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which have recorded significant progress in various generative tasks (Brock et al., 2018;Kang et al., 2023a;Dufour et al., 2022;Donahue et al., 2018). Recently, diffusion models have demonstrated a remarkable capacity to produce high-quality and diverse samples. They have achieved state-of-the-art results in several generation tasks, notably in image synthesis (Song et al., 2020;Ho et al., 2020), text-to-image applications (Dhariwal & Nichol, 2021;Rombach et al., 2022;Podell et al., 2023;Pernias et al., 2023) and text-to-motion (Chen et al., 2023). \n\nGuidance in Diffusion and Text-to-Image. Making generative models controllable and capable of producing user-aligned outputs requires making the generation conditional on a given input. Conditioned diffusion models have been vastly explored (Saharia et al., 2022;Ruiz et al., 2023;Balaji et al., 2022). The condition is achieved in its simplest form by adding extra input, typically with residual connections (Nichol & Dhariwal, 2021). To reinforce the model's fidelity to specific conditions, two main approaches prevail: Classifier Guidance (CG) (Dhariwal & Nichol, 2021), which involves training an image classifier externally, and Classifier-Free Guidance (CFG) (Ho & Salimans, 2021), that relies on an implicit classifier through joint training of conditional and unconditional models (using dropout on the condition). \n\nParticularly, CFG has catalyzed advancements in text-conditional generation, a domain where training a noisy text classifier is less convenient and performs worse.",
            "score": 0.5228477139243262,
            "section_title": "Related Work",
            "char_start_offset": 3885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 459,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 459,
                    "end": 478,
                    "matchedPaperCorpusId": "257427461"
                },
                {
                    "start": 478,
                    "end": 498,
                    "matchedPaperCorpusId": "252780703"
                },
                {
                    "start": 498,
                    "end": 519,
                    "matchedPaperCorpusId": "52890982"
                },
                {
                    "start": 733,
                    "end": 752,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 752,
                    "end": 768,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 822,
                    "end": 843,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 904,
                    "end": 923,
                    "matchedPaperCorpusId": "254408910"
                },
                {
                    "start": 1168,
                    "end": 1190,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1190,
                    "end": 1208,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1593,
                    "end": 1614,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "278394371",
            "title": "Steerable Scene Generation with Post Training and Inference-Time Search",
            "text": "We support text-conditioned generation by encoding prompts using a frozen BERT-Base encoder [45]. The resulting embeddings are injected into the conditional input branch of our Flux-style architecture. We use 110 tokens per prompt, corresponding to the maximum prompt length across all scene types in our datasets. \n\nTo enable both conditional and unconditional generation within a single model, we randomly mask the conditioning input with 10% probability during training. This allows classifier-free guidance (CFG) [46] at inference time, where predictions are computed using a weighted combination of conditional and unconditional outputs: \n\nwhere w is the guidance weight, and xcond and xuncond are the model predictions under conditional and unconditional contexts, respectively. A weight of w = \u22121 corresponds to unconditional sampling, w = 0 yields conditional sampling without guidance, and w > 0 applies classifier-free guidance during sampling. We apply CFG to both discrete and continuous components.",
            "score": 0.5202461133040257,
            "section_title": "B.6.1 Text-Conditioned Generation",
            "char_start_offset": 33278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "276742045",
            "title": "InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation",
            "text": "model. In-spireMusic has capacity to learn long-form coherence of the music and structural patterns in music for music generation tasks. \n\nClassifier-free guidance (CFG) (Ho & Salimans (2021)) proves effective in improving the generation quality of generative models. Therefore, we adapt the CFG into the AR transformer model. During training, we randomly drop the conditions with a fixed probability of 0.7, enabling the AR model to learn both conditional and unconditional distributions. During inference, CFG also applies to the outputs of the AR model, and we recommend using a guidance scale of 3.0. During the decoding process, the top-K sampling method samples the generated tokens with the default value of 350.",
            "score": 0.5194051589396591,
            "section_title": "AUTOREGRESSIVE TRANSFORMER",
            "char_start_offset": 13348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 719
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 191,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "272368110",
            "title": "Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing",
            "text": "Classifier guidance relies on a separate model, classifier p(y|z t ), trained on noised latents. As long as this classifier is differentiable with respect to the current latent, we can subtract score function \u2207 zt log p(y|z t ) from the diffusion model prediction to sample from the conditional distribution p(z t |y) instead of the data distribution p(z t ). \n\nTraining a separate classifier on specific data may sometimes be a resourceful task. Given a conditional diffusion model, one can rely on this model's knowledge about additional data, that it inquires with conditioning capabilities. This guidance technique is called classifier-free guidance (CFG) [8]. To determine the sampling direction, which leads to correspondence with conditioning data y, CFG compares a conditional prediction of the model with an unconditional one. In case of text-to-image models, the latter can be easily obtained by conditioning the model on the empty text \u2205 = \"\". With CFG incorporated, the diffusion model prediction, used in both sampling, Equation 1, and inversion, Equation 2, takes the form of \u03b5\u03b8 (z t , t, y): \n\nwhere w is a guidance scale, that controls to which extent additional data y influences the generation process. For SD model, the guidance scale is typically chosen as w = 7.5. It is important to note, that with w = 1 CFG sampling step equals regular conditional sampling. Self-guidance. As proposed in [4], the choice in guidance sources is not limited to either the classifier or the diffusion model itself. One can use any energy function g to guide the sampling process, as long as there exists a gradient with respect to z t . When the energy function uses outputs of internal layers of the diffusion model, the guidance process is called self-guidance. The authors of the method suggest defining energy function g on top of cross-attention maps A cross := cross attn.[\u03b5 \u03b8 (z t , t, y)] and the output of penultimate layer of the diffusion model decoder, features \u03a8 := features[\u03b5 \u03b8 (z t , t, y)].",
            "score": 0.5172755307231365,
            "section_title": "Preliminaries",
            "char_start_offset": 11895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "276249104",
            "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
            "text": "This issue is akin to challenges faced in textgeneration LLMs, where outputs may range from highly coherent to erroneous, depending on the model's response to complex prompts. \n\nFor text-generation, preference alignment techniques (Christiano et al., 2017;Ouyang et al., 2022;Shao et al., 2024;Rafailov et al., 2024;Adler et al., 2024) have been proposed to guide models to produce outputs that better match human preferences in coherence, relevance, and clarity. This is achieved through training with human feedback or automated scoring, based on criteria such as factual correctness and fluency. Driven by these advances, recent research employs preference alignment algorithms, including RLHF (Ouyang et al., 2022) and offline preference ranking methods (Rafailov et al., 2024;Azar et al., 2024), to refine audio LLM outputs. For instance, SpeechAlign (Zhang et al., 2024), proposes an iterative strategy to align speech language models with human preferences by addressing the distribution gap between golden AR tokens (from real speech) and synthetic AR tokens (generated during inference). Although real speech from ground truth can be used to guide training, we will show that it introduces inconsistencies due to its fundamentally different distribution from model-generated tokens. This issue makes preference-based optimization such as DPO (Rafailov et al., 2024) less effective. Nonetheless, this approach has been applied in scenarios where obtaining high-quality positive examples is particularly challenging (Chen et al., 2024b;Zhang et al., 2024). \n\nAnother research direction to amplify the influence of conditioning inputs in generative models is Classifier-Free Guidance (CFG). CFG was originally introduced to trade-off sample fidelity and diversity without relying on a separate classifier in diffusion models (Ho & Salimans, 2021). Recently, CFG has been successfully explored in LLMbased text-generation models (Sanchez et al., 2023;Fonseca & Cohen, 2024;Smirnov, 2024).",
            "score": 0.5164379313750538,
            "section_title": "Introduction",
            "char_start_offset": 1633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 256,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 256,
                    "end": 276,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 294,
                    "end": 316,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 697,
                    "end": 718,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 758,
                    "end": 781,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 781,
                    "end": 799,
                    "matchedPaperCorpusId": "264288854"
                },
                {
                    "start": 856,
                    "end": 876,
                    "matchedPaperCorpusId": "269004785"
                },
                {
                    "start": 1351,
                    "end": 1374,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1543,
                    "end": 1562,
                    "matchedPaperCorpusId": "269004785"
                },
                {
                    "start": 1831,
                    "end": 1852,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "271218151",
            "title": "Multi-Modal and Multi-Attribute Generation of Single Cells with CFGen",
            "text": "Classifier-Free Guidance (CFG). One can guide data generation on a condition y by learning the conditional marginal field u t (z|y) via a time-conditioned neural network v t,\u03be (z, y). Given a guidance strength hyperparameter \u03c9 \u2208 R, Zheng et al. (2023) show that generating data points following the vector field \u0169t (\u2022|y) = (1 \u2212 \u03c9)u t (\u2022) + \u03c9u t (\u2022|y) approximates sampling from the distribution q(z|y) \u221d q(z) 1\u2212\u03c9 q(z|y) \u03c9 , where q(z) and q(z|y) are, respectively, the unconditional and conditional data distributions. The parameter \u03c9 controls the trade-off between diversity and adherence to the condition. This approach enables guidance by interpolating between conditional and unconditional vector fields, both learned jointly during training.",
            "score": 0.5159864539590489,
            "section_title": "CONTINUOUS NORMALIZING FLOWS AND FLOW MATCHING",
            "char_start_offset": 10399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 746
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 251,
                    "matchedPaperCorpusId": "195877346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "265351587",
            "title": "Guided Flows for Generative Modeling and Decision Making",
            "text": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
            "score": 0.515920732083757,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "269005366",
            "title": "Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance",
            "text": "Recently, the text-guided generation in diffusion models has reached an unprecedented level, like DallE-3 [2].This generative power stems from three aspects.First, to represent the unstructured text, expressive language embedding models are used to embed each token in the given text, such as CLIP [28] in Stable Diffusion [34], and T5 [29] in Imagen [37].Second, to facilitate the interaction between text and image information, diffusion models typically enhance the network backbone, such as the U-net backbone [35], with the cross-attention mechanism.This mechanism involves utilizing the image embedding as the query and the key and value embeddings derived from the text.Third, Classifier-Free Guidance (CFG) [11] has recently been widely involved as a lightweight and robust technique to encourage text prompt adherence in generations.Instead of training extra classifiers [7,22], CFG mixes the score estimates of the diffusion model with or without the conditional prompt.Some other works [15,21] further separate a prompt into multiple concepts and generate an image by combining a set of diffusion models with each of them conditioning on a certain concept component.Here, we further emphasize the importance of varying CFG scales across different image semantic regions and design the semantic-ware CFG strategy to improve image quality.",
            "score": 0.5157379952260146,
            "section_title": "Text-guided Generation",
            "char_start_offset": 6952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 842
                },
                {
                    "start": 842,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1348
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 302,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 883,
                    "end": 886,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 1001,
                    "end": 1004,
                    "matchedPaperCorpusId": "249375227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "266053531",
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "text": "Prompt engineering and interactions. Based on the autoregressive property of LLMs, users aim to control the generation results by modifying the input contexts. This largely determines the test-time interactions with LLMs, primarily executed through prompt engineering. Representative methods such as CoT [17] introduce demonstrations in the context to enhance reasoning ability. Other multi-branch designs like ToT and GoT [16,18,19,33,34] have been proposed for rich and reliable context generation and self-checking. Aside from prompt engineering, human-model interactions have not been extensively explored in VLMs. Methods like Kosmos-2 [31], LLaVAInteractive [35], LISA [36], and AlphaCLIP [37] enable grounding perception tasks such as detection, segmentation, caption, and image editing through interaction with LLMs. These task-oriented interactions require additional data collection and task-specific tuning. In contrast, Prompt Highlighter is plug-and-play for general text generation in pretrained models. Classifier-free guidance and controllable generation. Classifier-Free Guidance (CFG) [20] enables a control on Diffusion Models' generation process without a conven-  tional classifier. Specifically, CFG's step-wise sampling allows users to employ a negative prompt within the unconditional branch, effectively guiding the generation away from harmful distributions. This approach has been extended to language models by LLM-CFG [21], allowing a controllable text generation and improved performance. However, LLM-CFG still requires a pair-wise prompt design and does not support partial token-level reweighting within the context, which is vital for controlling VLM's generation. Besides, methods in Diffusion Models [38,39] achieve finegrained control over image generation using text prompts by emphasizing areas within cross-attention maps. Finegrained control over autoregressive generation in LLMs and VLMs is still challenging. Later concurrent works CRG and MARINE [40,41], adopt CFG in VLMs for grounding and mitigating hallucination, but employ a different design for positive-negative pairs compared to our approach.",
            "score": 0.5153158354578142,
            "section_title": "Interactions with Multi-Modal LLMs",
            "char_start_offset": 7961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "258108187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "263609296",
            "title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) is a guidance method for conditional generation, which requires both a conditioned noise predictor \u03f5 \u03d5 (x t , t, c) and an unconditioned one \u03f5 \u03d5 (x t , t), where c is the condition variable. By setting a guidance scale w and giving a condition c, we use \u03b5\u03d5 (x t , t, c) = (1 + w)\u03f5 \u03d5 (x t , t, c) \u2212 w\u03f5 \u03d5 (x t , t) to predict noise during the reverse process.",
            "score": 0.5152441020817581,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 9129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 410
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 51,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "273346832",
            "title": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment",
            "text": "Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose \\textit{Condition Contrastive Alignment} (CCA) to facilitate guidance-free AR visual generation with high performance and analyze its theoretical connection with guided sampling methods. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning ($\\sim$ 1\\% of pretraining epochs) on the pretraining dataset, on par with guided sampling methods. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. Code and model weights: https://github.com/thu-ml/CCA.",
            "score": 0.5125319038680983,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "268680839",
            "title": "An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models",
            "text": "Guided generation of data involves creating new data samples, x, based on given pair of data (x 0 , y 0 ).We can condition the score function on a label y to learn conditional generative models for class-specific data generation.This objective can be done by leveraging the Bayes' rule to decompose the conditional score function [5]:\n\nClassifier guidance.By providing a weight term \u03bb to adjust the balance between the unconditional score function and the classifier guidance during the sampling phase, we can recast Eq. 6 as follows:\n\nunconditional score function (7) where the first term can be learned by a classifier.Classifier-free guidance (CFG).We can also model the unconditional score function \u2207 xt log p(x t ) and the joint score function \u2207 xt log p(x t , y) simultaneously to substitute the classifier guidance [7] for obtaining a trade-off between the quality and diversity of samples.We replace \u03bb in Eq. 7 with 1 + \u03c9 to be consistent with the range [0, +\u221e] of the guidance scale \u03c9 proposed in the original paper:\n\nThe conditional generation in our study is achieved through classifier-free guidance from caption y to image x, while unconditional generation uses the same approach with an empty caption embedding.",
            "score": 0.5121183304366588,
            "section_title": "Guided diffusion models",
            "char_start_offset": 7396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 334
                },
                {
                    "start": 336,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 534
                },
                {
                    "start": 536,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1025
                },
                {
                    "start": 1027,
                    "end": 1225
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "273549917",
            "title": "Rectified Diffusion Guidance for Conditional Generation",
            "text": "DPMs and conditional generation. Diffusion probabilistic model (DPM) introduces a new scheme of generative modeling, formulated by forward diffusing and reverse denoising processes (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2020). It is trained by optimizing the variational lower bound. Benefiting from this breakthrough, DPM achieves high generation fidelity, and even beat GANs on image generation. Conditional generation (Choi et al., 2021;Huang et al., 2023) takes better advantage of intrinsic intricate knowledge of data distribution, making DPM easier to scale up and the most promising option for generative modeling. Among the literature, text-toimage generation injects the embedding of text prompts to DPM, faithfully demonstrating the text content (Podell et al., 2024;Chen et al., 2024;Esser et al., 2024). \n\nClassifier-Free Guidance. Classifier-Free Guidance (CFG) serves as the successor of Classifier Guidance (CG) (Dhariwal & Nichol, 2021), circumventing the usage of a classifier for noisy images. Both CFG and CG attempt to formulate the underlying distribution by concentrating more on condition influence, achieving better conditional fidelity. Despite great success in large-scale conditional generation, CFG faces a technical flaw that the guided distribution is not theoretically guaranteed to recover the ground-truth conditional distribution (Du et al., 2023;Karras et al., 2024a;Bradley & Nakkiran, 2024). There exists a shifting issue that the expectation of guided distribution is drifted away from the correct one. This phenomenon may harm the condition faithfulness, especially for extremely broad distribution (e.g., open-vocabulary synthesis).",
            "score": 0.5085067397345634,
            "section_title": "RELATED WORK",
            "char_start_offset": 5288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1692
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 210,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 210,
                    "end": 226,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 440,
                    "end": 459,
                    "matchedPaperCorpusId": "236950721"
                },
                {
                    "start": 459,
                    "end": 478,
                    "matchedPaperCorpusId": "257038979"
                },
                {
                    "start": 947,
                    "end": 972,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1384,
                    "end": 1401,
                    "matchedPaperCorpusId": "257078922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "272397725",
            "title": "Multi-Track MusicLDM: Towards Versatile Music Generation with Latent Diffusion Model",
            "text": "To have control over the generation p \u03b8 (z 0 ), one can introduce some condition c to the diffusion process, resulting in p \u03b8 (z 0 |c). The conditional diffusion process is defined similarly to the unconditional process p \u03b8 (z 0:N ) as follows: \n\nAdditionally, to enable more controllable generation, LDM models often employ classifier-free guidance (CFG) [10]. CFG is a technique in diffusion models that enhances control over the adherence to conditioning information during inference. This is achieved by randomly dropping the conditioning information during training, thereby simultaneously training both conditional and unconditional versions of the LDM model. In the inference time, the strength of the conditioning can be modulated by the CFG weight \u03b5 = w\u03f5 c + (1 \u2212 w)\u03f5 u , where w is the guidance scale weight that balances the model's unconditional \u03f5 u and conditional \u03f5 c predictions. \n\nIn this study, we employ CLAP to convert text prompts and musical tracks into embeddings, which serve as the basis for conditioning the LDM. For example, users can specify the type of guitar by conditioning on the CLAP embedding of a reference track. Additionally, the strength of the conditioning can be adjusted using the CFG weight, increasing their creative options.",
            "score": 0.5049700929475291,
            "section_title": "Conditional Generation",
            "char_start_offset": 14830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1267
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "256390486",
            "title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models",
            "text": "After classifier guidance (Song et al., 2021;Nichol & Dhariwal, 2021), classifier-free guidance (Ho & Salimans, 2021;Nichol et al., 2021) (CFG) has been the state-of-the-art technique for guiding diffusion models. During training, we randomly discard our condition E x with a fixed probability, e.g., 10% to train both the conditional LDMs \u03f5 \u03b8 (z n , n, E x ) and the unconditional LDMs \u03f5 \u03b8 (z n , n). In generation, we use text embedding E y as condition and perform sampling with a modified noise estimation \u03b5\u03b8 (z n , n, E y ):",
            "score": 0.5046047447354631,
            "section_title": "Conditioning Augmentation",
            "char_start_offset": 14397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 529
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 45,
                    "matchedPaperCorpusId": "227209335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "276421312",
            "title": "Diffusion Models without Classifier-free Guidance",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality. \n\nThe key design of CFG is to combine the posterior probability and utilize Bayes' rule during inference time. To facilitate this, it is required to train both conditional and unconditional diffusion models. In particular, CFG trains the models to predict \n\nwhere is an additional empty class introduced in common practices. During training, the model switches between the two modes with a ratio \u03bb. \n\nFor inference, the model combines the conditional and unconditional scores and guides the denoising process as \n\nFigure 2: We use a grid 2D distribution with two classes, marked with orange and gray regions, as example and train diffusion models on it. We plot the generated samples, trajectories, and probability density function (PDF) of conditional, unconditional, CFG-guided model, and our approach. \n\n(a) The first row indicates that although CFG improves quality by eliminating outliers, the samples concentrate in the center of data distributions, resulting the loss of diversity. In contrast, our method yields less outliers than the conditional model and a better coverage of data than CFG. \n\n(b) In the second row, the trajectories of CFG show sharp turns at the beginning, e.g. samples inside the red box, while our method directly drives the samples to the closet data distributions. \n\n(c) The PDF plots of the last row also suggest that our method predicts more symmetric contours than CFG, balancing both quality and diversity. \n\nwhere w is the guidance scale that controls the focus on conditional scores and the trade-off between generation performance and sampling diversity. CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation.",
            "score": 0.504553083941135,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 5406,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 51,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "246241126",
            "title": "Learning to Complete Code with Sketches",
            "text": "First, we consider the idea of generating code sketches using a standard generative model for language. To this end, we simply extend the vocabulary with the special \" \" token. An obvious problem is that while we have plenty of training data for a standard generative model, we do not have training data for outputs y that contain the token. Consequently, we cannot train the model in a fully supervised fashion, and instead turn to reinforcement learning. Concretely, we devise a reward function r(\u22c5) that averages REGEXACC and ROUGE, i.e. for a predicted output sketch \u0177 and a ground truth output (without tokens) y * , we define \n\nAlgorithm 1 GRAMMFORMER generative process, given an input sequence x (0) . t) does not contain non-terminals or none was selected by P s break \u25b7 stop generation u \n\n\u25b7 sample expansion of non-terminal at position i \n\n\u25b7 create x (t+1) by replacing non-terminal at i (t) by u (t) \u229ai (t)   return NONTERMINALSTOHOLES(x (t) )\u25b7 convert remaining non-terminals to holes and return \"szconv. )\" does not contain a left parenthesis and requires the user to fill it in.). To resolve this, we developed GRAMMFORMER, a grammar-guided model. It generates code by following the structure of the context-free grammar (CFG) defining the programming language syntax, iteratively expanding non-terminal symbols. Crucially, it can choose to not expand some non-terminal symbols, which can then be presented as to the user. In traditional grammar-based generation of text (Cohen et al., 2012) or code (Maddison & Tarlow, 2014;Yin & Neubig, 2017;Allamanis & Sutton, 2014;Bielik et al., 2016), the CFG is followed by sequentially expanding the left-most, bottom-most nonterminal symbol, using one of the production rules of the grammar. GRAMMFORMER changes this and instead selects which (if any) non-terminal symbol to expand. An example generation is displayed in Fig. 2.",
            "score": 0.5026256486252909,
            "section_title": "LINEAR CODE SKETCH GENERATION",
            "char_start_offset": 5070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1885
                }
            ],
            "ref_mentions": [
                {
                    "start": 1486,
                    "end": 1506,
                    "matchedPaperCorpusId": "6729691"
                },
                {
                    "start": 1515,
                    "end": 1540,
                    "matchedPaperCorpusId": "5737841"
                },
                {
                    "start": 1559,
                    "end": 1584,
                    "matchedPaperCorpusId": "2923536"
                },
                {
                    "start": 1584,
                    "end": 1604,
                    "matchedPaperCorpusId": "13020969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06829833984375
        },
        {
            "corpus_id": "258564566",
            "title": "Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer",
            "text": "Classifier-free guidance [17] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifier-free guidance effectively shifts probability mass toward \n\nGuidance for conditions. For our task, the scoring network \u03b5  (  ,   ,  T ) has three conditions: the input image   , text prompt  T , and self-attention map   . We find it beneficial to leverage classifier-free guidance: \n\nconcerning each condition. Liu et al. [32] demonstrate that a conditional diffusion model can compose score estimates from multiple different conditioning values. We introduce three guidance scales,   ,  T , and   , which can be adjusted to trade off how strongly the generated samples correspond with the conditions. Our modified score estimate is as follows:",
            "score": 0.5026020997952376,
            "section_title": "Condition Guidance",
            "char_start_offset": 15784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 980
                }
            ],
            "ref_mentions": [
                {
                    "start": 658,
                    "end": 662,
                    "matchedPaperCorpusId": "249375227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "256390486",
            "title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models",
            "text": "where w determines the guidance scale. Compared with Au-dioGen (Kreuk et al., 2022), we have two differences. First, they leverage CFG on a transformer-based auto-regressive model, while our LDMs retain the theoretical formulation behind the CFG (Ho & Salimans, 2021). Second, our text embedding E y is extracted from unprocessed natural language and therefore enables CFG to make use of the detailed text descriptions as guidance for audio generation. However, AudioGen removed the text details showing spatial or temporal relationships with text preprocessing methods.",
            "score": 0.4998294939559079,
            "section_title": "Classifier-free Guidance",
            "char_start_offset": 14955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 570
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3701171875
        },
        {
            "corpus_id": "273346832",
            "title": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment",
            "text": "Despite the core motivation of developing a unified model for language and vision, the AR sampling strategies for visual and text contents differ in one key aspect: AR visual generation necessitates a sampling technique named Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). During inference, CFG adjusts the sampling logits \u2113 sample for each token as: \n\nwhere \u2113 c and \u2113 u are the conditional and unconditional logits provided by two separate AR models, p \u03d5 (x|c) and p \u03d5 (x). The condition c can be class labels or text captions, formalized as prompt tokens. \n\nThe scalar s is termed guidance scale. Since token logits represent the (unnormalized) log-likelihood in AR models, Ho & Salimans (2022) prove that the sampling distribution satisfies: \n\nAt s = 0, the sampling model becomes exactly the pretrained conditional model p \u03d5 . However, previous works (Ho & Salimans, 2022;Podell et al., 2023;Chang et al., 2023;Sun et al., 2024) have widely observed that an appropriate s > 0 is critical for an ideal trade-off between visual fidelity and diversity, making training another unconditional model p \u03d5 necessary. In practice, the unconditional model usually shares parameters with the conditional one, and can be trained concurrently by randomly dropping condition prompts c during training. \n\nOther guidance methods, such as Classifier Guidance (Ho & Salimans, 2022) and Energy Guidance (Lu et al., 2023) have similar effects of CFG. The target sampling distribution of these methods can all be unified under Eq. 3.",
            "score": 0.49888013659128727,
            "section_title": "GUIDED SAMPLING FOR VISUAL GENERATION",
            "char_start_offset": 5880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 357
                },
                {
                    "start": 360,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1523
                }
            ],
            "ref_mentions": [
                {
                    "start": 1395,
                    "end": 1412,
                    "matchedPaperCorpusId": "258309302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "273403725",
            "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
            "text": "During inference, our model adopts a next-token prediction approach. For pure text understanding and multimodal understanding, we follow the standard practice of sampling tokens sequentially from the predicted distribution. For image generation, we utilize classifier-free guidance (CFG) 2 , similar to prior works [8,26,73]. Specifically, for each token, the logit   is calculated as: \n\n, where   is the conditional logit,   is the unconditional logit, and  is the scale for the classifier-free guidance. The default number of  is 5 for the following evaluation.",
            "score": 0.49825005006806145,
            "section_title": "Inference",
            "char_start_offset": 12003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 563
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "247628171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "257496235",
            "title": "One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) combines a conditional and an unconditional model linearly during sampling (see Eq. ( 4)). It is simple yet effective to improve the sample quality and image-text alignment in diffusion models. Notably, CFG is directly applicable to the conditional and joint sampling of UniDiffuser without modifying the training process (see Figure 3 for results). \n\nFormally, we denote the output of \u03f5 \u03b8 as the concatenation of \u03f5 x \u03b8 and \u03f5 y \u03b8 , i.e. \n\nwhere we omit the input for simplicity. UniDiffuser can perform CFG for free in conditional sampling because it captures both the conditional and unconditional models. For example, we can generate x 0 conditioned on y 0 similarly to Eq. ( 4) as follows: \n\nwhere \u03f5 x \u03b8 (x t , y 0 , t, 0) and \u03f5 x \u03b8 (x t , \u03f5 y , t, T ) represent the conditional and unconditional models respectively, and s is the guidance scale. In contrast to the original CFG, UniDiffuser does not need to specify a null token for parameter sharing. \n\nCFG is also applicable to joint sampling. By setting t x = t y = t, note that the joint score model can be equivalently  expressed in the form of conditional models as follows: \n\nwhere q(x t , y t ) is the joint distribution of perturbed data at the same noisy level t. Inspired by the above relationship between score functions, \u03f5 \u03b8 (x t , y t , t, t) can be viewed as approximating a pair conditional scores \u2207 xt log q(x t |y t ) and \u2207 yt log q(y t |x t ). In the same spirit of CFG, we can replace each conditional score by interpolating the joint model with the corresponding unconditional model as follows: \n\nwhere \u03f5 x \u03b8 (x t , \u03f5 y , t, T ) and \u03f5 y \u03b8 (\u03f5 x , y t , T, t) represent unconditional models. We summarize the formulation of CFG in UniDiffuser for all tasks in Appendix C.",
            "score": 0.4956938016238076,
            "section_title": "Classifier-Free Guidance for Free",
            "char_start_offset": 10838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1797
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "274281409",
            "title": "Contrastive CFG: Improving CFG in Diffusion Models by Contrasting Positive and Negative Concepts",
            "text": "As Classifier-Free Guidance (CFG) has proven effective in conditional diffusion model sampling for improved condition alignment, many applications use a negated CFG term to filter out unwanted features from samples. However, simply negating CFG guidance creates an inverted probability distribution, often distorting samples away from the marginal distribution. Inspired by recent advances in conditional diffusion models for inverse problems, here we present a novel method to enhance negative CFG guidance using contrastive loss. Specifically, our guidance term aligns or repels the denoising direction based on the given condition through contrastive loss, achieving a nearly identical guiding direction to traditional CFG for positive guidance while overcoming the limitations of existing negative guidance methods. Experimental results demonstrate that our approach effectively removes undesirable concepts while maintaining sample quality across diverse scenarios, from simple class conditions to complex and overlapping text prompts.",
            "score": 0.4956761474045821,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "270869763",
            "title": "Diffusion Models and Representation Learning: A Survey",
            "text": "To address this limitation, Classifier-free guidance (CFG) [67] eliminates the need for a pre-trained classifier.CFG works by training an unconditional diffusion model parametrized by \u03f5 \u03b8 (x t , t, \u03d5) together with a conditional model parametrized by \u03f5 \u03b8 (x t , t, c).For the unconditional model, a null input token \u03d5 is used as a conditioning signal c.The network is trained by randomly dropping out the conditioning signal with probability p uncond .Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network.Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73,100].These methods do not need annotated data, allowing the use of larger unlabelled datasets.\n\nTable 1 shows the requirements of current guidance methods.While classifier and classifier-free guidance improve generation results, they require annotated training data.Self-guidance and online guidance are fully selfsupervised alternatives that achieve competitive performance without annotations.\n\nClassifier and classifier-free guidance are controlled generation methods that rely on conditional training.Trainingfree approaches modify the generation process of a pretrained model by binding multiple diffusion processes [14] or using time-independent energy functions [179].Other controlled generation methods take a variational perspective [54,119,146,164], treating controlled generation as a source point optimization problem [17].The goal is to find samples x that minimize a loss function L(x) and are likely under the model's distribution p.The optimization is formulated as min x0 L(x), where x 0 is the source noise point.The loss function L(x) can be modified for conditional sampling to generate a sample belonging to a particular class y.",
            "score": 0.49409342390201094,
            "section_title": "Diffusion Model Guidance",
            "char_start_offset": 17610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 557
                },
                {
                    "start": 559,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 995
                },
                {
                    "start": 997,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1296
                },
                {
                    "start": 1298,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1736
                },
                {
                    "start": 1736,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 1932
                },
                {
                    "start": 1932,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 63,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1522,
                    "end": 1526,
                    "matchedPaperCorpusId": "256900756"
                },
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "258418154"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "257757144"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "273228630",
            "title": "Diversity-Rewarded CFG Distillation",
            "text": "Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at https://google-research.github.io/seanet/musiclm/diverse_music/.",
            "score": 0.4913137686636637,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "266693789",
            "title": "Diffusion Model with Perceptual Loss",
            "text": "Guidance methods alter the model prediction and guide the sample toward desired regions during the generation process. Classifier Guidance [7] adds classifier gradients to the predicted score to guide the sample generation to maximize the classification. It can turn an unconditional diffusion model conditional. However, it is not evident why applying classifier guidance on an already conditional diffusion model can significantly improve sample quality. Previous research has attributed it to low-temperature sampling [15,24]. Classifier-Free Guidance (CFG) [15] uses Bayes' rule and finds that the diffusion model itself can be inverted as an implicit classifier. Specifically, the model is queried both conditionally and unconditionally at every inference step and the difference is amplified toward the conditional direction. Both methods only work for conditional generation and entangle sample quality with conditional alignment [24]. Self-Supervised Guidance [20] uses self-supervised networks to generate synthetic clustering labels for unconditional data. This allows unconditional data to use CFG for improving quality. Guidance-Free Training [4] shows that CFG can be applied at training. This bakes the guided flow into the model and saves computation during inference. More recently, Discriminator Guidance [25] proposes to train a discriminator network to classify real and generated samples and use it as guidance during diffusion generation. Self-Attention Guidance [18] finds that the self-attention map of the diffusion model can be exploited to enhance quality. Autoguidance [24] finds a smaller or less-trained model can be used as negative guidance to improve quality. Although these methods are effective in improving quality, they also present issues such as increased complexity and reduced diversity [15,24], etc. On the other hand, our research aims to explore the underlying issue: why diffusion models without guidance fail in the first place. \n\nThe loss objective of diffusion models has been studied by prior works. Loss weighting is found to influence perceptual quality and likelihood evaluation [6,13,50] but still cannot produce good samples without guidance. Multiscale loss [19] is proposed to improve high-resolution generation. Smoothness penalty [11] is proposed to enforce smoother latent traversal. l1 distance is explored for colorization and in-painting tasks [41].",
            "score": 0.49026550739628494,
            "section_title": "Related Work",
            "char_start_offset": 3312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1973
                },
                {
                    "start": 1976,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2410
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "254096299"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1827,
                    "end": 1831,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 2130,
                    "end": 2133,
                    "matchedPaperCorpusId": "247922317"
                },
                {
                    "start": 2133,
                    "end": 2136,
                    "matchedPaperCorpusId": "257557255"
                },
                {
                    "start": 2136,
                    "end": 2139,
                    "matchedPaperCorpusId": "235352469"
                },
                {
                    "start": 2212,
                    "end": 2216,
                    "matchedPaperCorpusId": "256274516"
                },
                {
                    "start": 2287,
                    "end": 2291,
                    "matchedPaperCorpusId": "266054322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "However, this method demands significant serial computation scaling (e.g., 64 times the looping) and invasive model modifications, necessitating training from scratch and complicating integration with existing trained LLMs. \n\nClassifier-Free Guidance Classifier-Free Guidance (CFG) stems from Classifier Guided Diffusion (Dhariwal & Nichol, 2021), which uses an additional classifier to guide image generation using diffusion models (Ho et al., 2020). By using the generation model itself as a classifier, CFG (Ho & Salimans, 2022) further eliminates dependency on the classifier and leverage two forward passes. Similar concepts have emerged in NLP, such as Coherence Boosting (Malkin et al., 2022), PREADD (Pei et al., 2023), Context-Aware Decoding (Shi et al., 2024), and Contrastive Decoding (Li et al., 2023). Recently, Sanchez et al. \n\n(2024) proposed transferring CFG to language models. However, due to constraints of human-designed heuristic rules, these techniques cannot leverage the power of training-time scaling (Kaplan et al., 2020) and the performance is limited. \n\nModel Ensemble Model ensemble is a classic research field in machine learning and is also employed in the context of LLMs (Chen et al., 2025). In traditional model ensembles, most ensemble components do not share parameters. Some recent work consider setups with partially shared parameters. For example, Monte Carlo dropout (Gal & Ghahramani, 2016) employs multiple different random dropouts during the inference phase, while BatchEnsemble (Wen et al., 2020;Tran et al., 2022) and LoRA ensemble (Wang et al., 2023a) use distinct low-rank matrix factorizations for model weights to differentiate different streams (we also experimented with this technique as input transformation in Appendix A). Weight sharing (Yang et al., 2021;Lan et al., 2019) is another line of work, where some weights of a model are shared across different components and participate in multiple computations. However, these works have not explored the scaling law of parallel computation from the perspective of model capacity.",
            "score": 0.490161307119962,
            "section_title": "Inference-Time Scaling",
            "char_start_offset": 29225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 346,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 433,
                    "end": 450,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 678,
                    "end": 699,
                    "matchedPaperCorpusId": "247476407"
                },
                {
                    "start": 708,
                    "end": 726,
                    "matchedPaperCorpusId": "260068722"
                },
                {
                    "start": 751,
                    "end": 769,
                    "matchedPaperCorpusId": "258866080"
                },
                {
                    "start": 1407,
                    "end": 1431,
                    "matchedPaperCorpusId": "160705"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "248097655",
            "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
            "text": "[19,36,32,18] guide GANs using gradients from a CLIP model. For diffusion models, Dhariwal and Nichol [11] introduced classifier guidance as a way to use gradients from a classifier trained on noised images to steer the model towards higher quality generations. Nichol et al. [35] train a CLIP model on noised images and guide a text-conditional diffusion model, while Crowson, Crowson [7,8] use an unnoised CLIP model to guide unconditional or class-conditional diffusion models. Ho and Salimans [24] introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. [35] showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation. \n\nSeveral previous works have trained generative image models that are directly conditioned on CLIP embeddings. Zhou et al. [61] condition GAN models on randomly perturbed CLIP image embeddings, finding that these models can generalize to CLIP text embeddings to produce text-conditional images. Crowson [9] trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional image generation. Wang et al. [54] train an autoregressive generative model conditioned on CLIP image embeddings, finding that it generalizes to CLIP text embeddings well enough to allow for text-conditional image synthesis. \n\nBordes et al. [3] train diffusion models conditioned on image representations from contrastive models. While the diffusion models themselves cannot generate images unconditionally, the authors experimented with a simple approach for two-stage image generation by employing Kernel Density Estimation to sample image representations. By feeding these generated representations to the diffusion model, they can generate images end-to-end in a way similar to our proposed technique. However, our work differs from this in two ways: first, we use multimodal contrastive representations rather than image-only representations; second, we employ much more powerful generative models for the first stage of the generation hierarchy, and these generative models are conditioned on text.",
            "score": 0.48996595004714166,
            "section_title": "Related Work",
            "char_start_offset": 25619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "269983384",
            "title": "A Versatile Diffusion Transformer with Mixture of Noise Levels for Audiovisual Generation",
            "text": "Classifier-free guidance (CFG) [14], a technique designed to enhance the quality of samples produced by conditional diffusion models using a linear combination of the conditional and unconditional outputs as follows: are obtained as illustrated in Fig. 4. Our vector formulation of the timestep allows us to apply varying levels of noise to different parts of the input.This opens up a number of possibilities for constructing various CFG forms by emphasizing different time segments or modalities, depending on the task at hand.MoNL supports classifier-free guidance (CFG) without requiring additional design.Unlike the original CFG (see Eq. 13), it does not need a null token either, hence gratis or free.This is achieved by injecting Gaussian noise to the conditional portions of the multimodal space and setting t pm,nq \" T for the output as illustrated in Fig. 10 for the case of cross-modal generation of audio-in, video-out.To illustrate mCFG, consider the conditional output of the network in the cross-modal task (see Eq. 7), denote term used in the gradient step as Z p1:M,1:N q t \" rz p1,1q t p1,1q , . . ., z pM,N q t pM,N q s and conditional portions as \u03f5 cond \u03b8 \" \u03f5 \u03b8 pZ p1:M,1:N q t , tq where Z p1:mc,1:N q t \" z p1:mc,1:N q 0 , Z pmc`1:M,1:N q t \" z pmc`1:M,1:N q t t p1:mc,1:N q \" 0, t pmc`1:M,1:N q \" t.\n\nThen, the output for the cross-modal generation task is:",
            "score": 0.4890204153451118,
            "section_title": "E Classifier-free guidance for free",
            "char_start_offset": 35207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1117
                },
                {
                    "start": 1117,
                    "end": 1322
                },
                {
                    "start": 1324,
                    "end": 1380
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 35,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "259982947",
            "title": "Can Diffusion Model Conditionally Generate Astrophysical Images?",
            "text": "Classifier-free guidance is introduced (Ho & Salimans 2021) for text-to-image generation and also used in popular models like Stable Diffusion (Rombach et al. 2021) and Imagen (Saharia et al. 2022). It is shown to achieve a better trade-off between sample quality and diversity. During training time, the label is discarded randomly with a probability (0.28 in this work). So the networks learn both the unconditional and conditional generative models. During the sampling time, the noise prediction \u03b5  is a combination of the unconditional prediction   (x  ) and conditional prediction   (x  |c) with a guidance scale  \n\nwhere  = 0 leads to a standard conditional generative model, and  = \u22121 leads to a standard unconditional generative model. Note that the above formula can be rewritten by \u03b5  (x  |c) =   (x  ) +  \u2032 (  (x  |c) \u2212   (x  )) (Nichol et al. 2022), where  \u2032 = 1 leads to a standard conditional generative model. From this form, the guidance scale can be interpreted by guiding the unconditional model to the direction where the output image meets the input condition (Dieleman 2022). When  \u2032 > 1, or equivalently,  > 0 in Equation 3, the diffusion models tend to conditionally generate higher-quality natural images, but at costs to image diversity.",
            "score": 0.4880389959360709,
            "section_title": "Classifier-free guidance",
            "char_start_offset": 7479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "259275061",
            "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
            "text": "Classifier guidance (CG) [Dhariwal and Nichol, 2021] is a technique used to trade off mode coverage and sample fidelity for diffusion models post training, similar to the effect of truncated or lowtemperature sampling for generative adversarial networks [Brock et al., 2018] and discrete flow models [Kingma and Dhariwal, 2018]. It modifies the score estimate of a diffusion model to include the gradient of the log likelihood of an auxiliary classifier. Ho and Salimans [2022] notes that CG approximates sampling from p(x | c)p(c | x) \u03b1 where c is the conditioner, and this can be simulated without a classifier by mixing the score estimate of a conditional model and an unconditional model. The unconditional model can be jointly trained by dropping the conditioner c with some probability, and the same model provides score estimates for both p(x) and p(x | c). \n\nWe extend the idea of classifier free guidance (CFG) to flow-matching models. The conditioner c is equivalent to (z, x ctx ) for audio models and (y, l ctx ) for duration models, which is dropped with p uncond during training. During inference, the modified vector field \u1e7dt for the audio model becomes \n\nwhere \u03b1 is the strength of the guidance, and v t (w; \u03b8) is obtained by dropping x ctx and z. We use \u03b1 and \u03b1 dur for the CFG strengths for audio and duration model, respectively, which are selected based on empirical results.5",
            "score": 0.48791574527806425,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 23124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1396
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 51,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 300,
                    "end": 327,
                    "matchedPaperCorpusId": "49657329"
                },
                {
                    "start": 455,
                    "end": 477,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "269005552",
            "title": "Resistive Memory-based Neural Differential Equation Solver for Score-based Diffusion Model",
            "text": "Classifier-free guidance 53 modifies the sampling process by adjusting the score function based on a trade-off between the data distribution and the conditional distribution of a specified condition.The method does not require a separate classifier model.The adjusted generation differential equation process can be expressed as: dx x x dt = f f f (x x x,t) \u2212 g 2 (t)s s s \u03b8 (x x x, c c c,t) + g(t) dw w w dt (6)   While the score function can be written by: s s s \u03b8 (x x x, c c c,t) = (1 + \u03bb )s s s \u03b8 (x x x, c c c,t) \u2212 \u03bb s s s \u03b8 (x x x,t)\n\nWhere \u03bb is a hyperparameter that controls the strength of the guidance.Classifier-free guidance helps in steering the generative process toward samples that are more likely under certain conditions.The encoded signals of the condition term c and time t can be simultaneously added to the network on the hardware, providing guidance to the generation process.",
            "score": 0.48710282039123437,
            "section_title": "Classifier-free guidance diffusion models",
            "char_start_offset": 29317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 540
                },
                {
                    "start": 542,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 900
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "263141271",
            "title": "Compositional Sculpting of Iterative Generative Processes",
            "text": "Generative model composition is a form of post-training control of the generation process, an established area of research in generative modeling. A simple approach to control is conditional generation, which can be achieved by training a conditional generative model   (|) on pairs (, ) of objects  and conditioning information . Types of conditioning information can include class labels [39] or more structured data such as text prompts [4,6,44], semantic maps, and other images for image-to-image translation [4]. This approach assumes that the generation control operations are specified at training time and the training data is annotated with conditioning attributes. Classifier guidance [32] provides a way to generate samples from conditional distributions that need not be specified at training time. The guidance is realized by a classifier that is trained on examples   (both clean and noisy) accompanied by conditioning labels . Dhariwal and Nichol [39] apply classifier guidance on top of unconditional or conditional diffusion models to improve the fidelity of generated images. Ho and Salimans [45] develop classifier-free guidance where the conditional and unconditional score functions are trained simultaneously and combined at inference time to guide the generation. In ControlNet [17], an additional network is trained to enable a pre-trained diffusion model to incorporate additional, previously unavailable, conditioning information. Meng et al. [46] and Couairon et al. [47] develop semantic image editing methods based on applying noise to the original image and then running the reverse denoising process to generate an edited image, possibly conditioned on a segmentation mask [47]. \n\nSimilar to conditional diffusion models, conditional GFlowNets have been used to condition generation on reward exponents [36] or combinations of multiple predefined reward functions [31]. \n\nNote that the methods developed in this work can be combined with conditional generative models, for example, conditional diffusion models (or GFlowNets) (| 1 ), \u2026 , (|  ) can act as base generative models to be composed.",
            "score": 0.4866085934763939,
            "section_title": "Related Work",
            "char_start_offset": 13017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 440,
                    "end": 443,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 443,
                    "end": 445,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 445,
                    "end": 448,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1469,
                    "end": 1473,
                    "matchedPaperCorpusId": "245704504"
                },
                {
                    "start": 1494,
                    "end": 1498,
                    "matchedPaperCorpusId": "253018768"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "253018768"
                },
                {
                    "start": 1834,
                    "end": 1838,
                    "matchedPaperCorpusId": "235367990"
                },
                {
                    "start": 1895,
                    "end": 1899,
                    "matchedPaperCorpusId": "253097761"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "240420063",
            "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
            "text": "We organize these works into the following three paradigms: \n\n\u2022 Pre-train then fine-tune ( \u00a7 2): perform generalpurpose pre-training with a large unlabeled corpus, and then perform a small amount of task-specific fine-tuning for the task of interest. \n\n\u2022 Prompt-based learning ( \u00a7 3): prompt a PLM such that solving an NLP task is reduced to a task similar to the PLM's pre-training task (e.g. predicting a missing word), or a simpler proxy task (e.g. textual entailment). Prompting can usually more effectively leverage the knowledge encoded in the PLMs, leading to few-shot approaches. \n\n\u2022 NLP as text generation ( \u00a7 4): Reformulate NLP tasks as text generation, to fully leverage knowledge encoded in a generative language model such as GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020). \n\nGenerative PLMs can be also used for text generation tasks. We refer readers to the excellent surveys on text generation such as Li et al. (2021b) and Yu et al. (2021b). This paper, unless otherwise specified, focuses on tasks that are not generative in nature (e.g. classification, sequence labeling and structure prediction) that still cover a broad range of NLP tasks including syntactic or semantic parsing of text, Information Extraction (IE), Question Answering (QA), Textual Entailment (TE), sentiment analysis, and so on. \n\nIn addition to the three paradigms, there is another, complementary method: to indirectly use any of the PLM paradigms above to improve results of target NLP tasks: \n\n\u2022 Data generation ( \u00a7 5): run PLMs to automatically generate data for NLP tasks. The generated data can be silver labeled data, where typically the generative PLM is fine-tuned for the task, or some auxiliary data, such as counterexamples, clarifications, contexts, or other. \n\nIn the first case, the silver labeled data can be added to existing labeled data. In the second case, the auxiliary data supports the target task in some way.",
            "score": 0.48563486083568763,
            "section_title": "Introduction",
            "char_start_offset": 1966,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 62,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 746,
                    "end": 768,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 776,
                    "end": 797,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1378173828125
        },
        {
            "corpus_id": "274233770",
            "title": "AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea",
            "text": "As discussed in Section G.2, we apply classifier-free guidance with respect to three conditionings: the input image c I , the text instruction c T and the visual prompt with task embedding c V . We introduce separate guidance scales s I , s T and s V that enable separately trading off the strength of each conditioning. \n\nWhen ignoring c V , we can have the modified score estimate as InstructPix2Pix [5]: \n\nBelow is the modified score estimate for our model with classifier-free guidance on three conditions (copied from Equation 8): \n\nOur generative model learns P (z|c I , c T ), the probability distribution of image latents z = E(x) conditioned on an input image c I , a text instruction c T and the visual prompt with task embedding c V . We arrive at our particular classifier-free guidance formulation by expressing the conditional probability as follows: \n\nDiffusion models estimate the score [26] of the data distribution, i.e., the derivative of the log probability. Taking the logarithm gives us the following expression: \n\nTaking the derivative and rearranging we attain: \n\nThis corresponds with the terms in our classifier-free guidance formulation in Equation 8.",
            "score": 0.48478276024152733,
            "section_title": "G.3. Classifier-free Guidance Details",
            "char_start_offset": 48136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1178
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 903,
                    "end": 907,
                    "matchedPaperCorpusId": "1152227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "274117064",
            "title": "The Unreasonable Effectiveness of Guidance for Diffusion Models",
            "text": "Diffusion models (DMs) have emerged as a powerful approach for generative tasks, achieving remarkable success in areas such as image synthesis and text-to-image generation [1,17,24,27,41,44,45]. DMs are a class of generative models that iteratively transform noise samples into samples that are close to a desired data distribution. Despite their success, DMs often fail to generate high-quality samples in the visual domain [3] and require guidance techniques to improve visual fidelity (Fig. 1). The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16]. \n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42]. \n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25].",
            "score": 0.4842213668257808,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "254854389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "265498623",
            "title": "IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers",
            "text": "Text-to-image Generative Classifiers. The classic approach [15,38] of using generative models to perform recognition tasks is the Bayes algorithm [29]. \n\nDuring training, this algorithm models the data distribution while during inference, it provides predictions by solving a maximum likelihood estimation (MLE) problem. Recently, there have been the methods [7,19] proposed to convert the text-to-image diffusion models, Stable Diffusion [40] or Imagen [41], into a zeroshot classifier using the Bayes algorithm. \n\nThe focus of IG captioner is different with these previous arts. First, we explore the possibility of achieving a good classifier solely through the generative training, instead of how to convert a generative model into a classifier. The Stable Diffusion used by the above diffusion model classifiers [7,19] is not suitable for our goal because it uses the CLIP text encoder pretrained with contrastive loss to provide the text guidance. Second, we identify the negative impact of the text priors inherited from the pretraining data and propose methods to reduce this impact for zero-shot classification tasks. Besides, IG captioner is an image-totext generative captioner. We also perform the comparisons in Tab. 8. \n\nClassifier-Free Guidance (CFG). CFG [17] is an important approach to improving the sample quality of text-to-image generative models [11,30,37,42,50]. During training, CFG requires the model to generate images both with and without text conditions. During inference, the output image is sampled according to a linear combination of both the conditional and unconditional predictions to improve text-image alignment, which relates CFG to IG captioner from a high level. In contrast, IG captioner is an image-to-text captioner that uses the probability gain from unconditionally to conditionally generating captions with the aim of reducing the bias from text priors for zero-shot classification tasks. \n\nContrast in Text Generation.",
            "score": 0.4838857125552483,
            "section_title": "Related Work",
            "char_start_offset": 6231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 151
                },
                {
                    "start": 154,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1935
                },
                {
                    "start": 1938,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 63,
                    "matchedPaperCorpusId": "10327263"
                },
                {
                    "start": 63,
                    "end": 66,
                    "matchedPaperCorpusId": "12196480"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "296750"
                },
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 1378,
                    "end": 1381,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7734375
        },
        {
            "corpus_id": "266162752",
            "title": "4M: Massively Multimodal Masked Modeling",
            "text": "Chained generation. Because any generated modality can be used as a conditioning too, we can perform chained generation of several modalities, one after another, with each fully generated one being added to the conditioning of the next (see Figure 3). Performing generation in this chained manner results in each additional modality being generated in a consistent manner, as shown in Figures 9 and 10. In addition, we found that for certain generative tasks, such as caption-to-RGB, generating intermediate modalities such as CLIP tokens can further improve image fidelity (see Figure 9). \n\nClassifier-free guidance. Classifier-free guidance is crucial for improving both image fidelity and how well the generation matches the conditioning. It is most commonly used in diffusion models [50], but can be applied in token-based models as well [40,123,18]. We perform classifier-free guidance by computing a weighted combinations of the logits of a forward pass with the conditioning and one without the conditioning: logits guided = logits uncond + w (logits cond \u2212 logits uncond ) . \n\nHere, w is the guidance scale. When performing chained generation, we add each fully generated modality to the set of guided modalities. \n\nMultimodal guidance. While guidance has been shown to significantly improve image quality, it can still happen that generative models ignore parts of the input, unpredictably focus on some parts more than others, or generate undesired concepts. Negative prompting [78] is a popular way of keeping the model from generating undesired concepts. Liu et al. [68] show that performing compositional guidance on multiple conditions can further improve text-image similarity. In a similar way, we can perform compositional generation by weighting different (parts of) modalities by different continous amounts -even negatively. We can do this by computing a weighted sum of the logits of an unconditional case and the logits of each conditional case: \n\nHere, w i are the guidance scales for the different conditions. For example, this allows 4M to generate semantically or geometrically similar variants of images by weakly conditioning on their extracted segmentation, normal, or depth maps (see Figure 13).",
            "score": 0.4828008726857025,
            "section_title": "A.3 Generation procedure details",
            "char_start_offset": 37472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 846,
                    "end": 850,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 850,
                    "end": 853,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "249375227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "275906852",
            "title": "TFG-Flow: Training-free Guidance in Multimodal Generative Flow",
            "text": "Recent advancements in generative foundation models have demonstrated their increasing power across a wide range of domains (Reid et al., 2024;Achiam et al., 2023;Abramson et al., 2024). In particular, diffusion-based foundation models, such as Stable Diffusion (Esser et al., 2024) and SORA (Brooks et al., 2024) have achieved significant success, catalyzing a new wave of applications in areas such as art and science. As these models become more prevalent, a critical question arises: how can we steer these foundation models to achieve specific properties during inference time? One promising direction is using classifier-based guidance (Dhariwal & Nichol, 2021) or classifierfree guidance (Ho & Salimans, 2022), which typically necessitate training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. Recently, there has been growing interest in training-free guidance for diffusion models, which allows users to steer the generation process using an off-the-shelf differentiable target predictor without requiring additional model training (Ye et al., 2024). A target predictor can be any classifier, loss, or energy function used to score the quality of the generated samples. Training-free guidance offers a flexible and efficient means of customizing generation, holding the potential to transform the field of generative AI. Despite significant advances in generative models, most existing training-free guidance techniques are tailored to diffusion models that operate on continuous data, such as images. However, extending generative models to jointly address both discrete and continuous data-referred to as multimodal data (Campbell et al., 2024)-remains a critical challenge for broader applications in scientific fields (Wang et al., 2023). One key reason this expansion is essential is that many real-world problems involve multimodal data, such as molecular design, where both discrete elements (e.g., atom types) and continuous attributes (e.g., 3D coordinates) must be modeled together.",
            "score": 0.48099865608475234,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 185,
                    "matchedPaperCorpusId": "269633210"
                },
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 642,
                    "end": 667,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1892,
                    "end": 1911,
                    "matchedPaperCorpusId": "260384616"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "273228630",
            "title": "Diversity-Rewarded CFG Distillation",
            "text": "Generative models for creative domains. Art and entertainment -domains historically driven by human creativity -are undergoing a profound transformation thanks to AI generative models. These models, often powered by Large Language Models (LLMs) or diffusion, can now generate texts (Gemini Team, 2023), images (Ramesh et al., 2022), videos (Ho et al., 2022), and audios (Agostinelli et al., 2023;Borsos et al., 2023;Cideron et al., 2024;Copet et al., 2023;D\u00e9fossez et al.;Kreuk et al., 2023). To further refine the quality, these models are often augmented with inference methods during real-world deployment, ranging from simple temperature scaling to more refined methods like Beam search (Freitag and Al-Onaizan, 2017), test-time augmentation (Shanmugam et al., 2021), or MCTS (Kocsis and Szepesv\u00e1ri, 2006). A particularly popular method for image and audio generation is classifier-free guidance (CFG) (Ho and Salimans, 2022), e.g., used in DALL-E (Ramesh et al., 2022) or AudioGen (Kreuk et al., 2023). CFG improves the model's fidelity to the prompt by combining the logits of conditional and unconditional generations. Despite its benefits, CFG has two main limitations: it doubles the computational cost during deployment and reduces the diversity of generated content (Dhariwal and Nichol, 2021;Ho and Salimans, 2022;Kreuk et al., 2023;Meng et al., 2023), hindering the exploration of novel and diverse ideas -a cornerstone of creativity. Ideally, these models should not only fulfill user intent but also surprise them with unexpected and innovative outputs: the model should not generate systematically the same content (Hamilton, 2024). \n\nQuality-diversity trade-off. Effectively controlling the quality-diversity trade-off is thus extremely important but challenging.",
            "score": 0.48092144662778563,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1780
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 437,
                    "matchedPaperCorpusId": "267499597"
                },
                {
                    "start": 437,
                    "end": 456,
                    "matchedPaperCorpusId": "259108357"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "252668761"
                },
                {
                    "start": 746,
                    "end": 770,
                    "matchedPaperCorpusId": "238634826"
                },
                {
                    "start": 780,
                    "end": 809,
                    "matchedPaperCorpusId": "15184765"
                },
                {
                    "start": 986,
                    "end": 1006,
                    "matchedPaperCorpusId": "252668761"
                },
                {
                    "start": 1277,
                    "end": 1304,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1326,
                    "end": 1345,
                    "matchedPaperCorpusId": "252668761"
                },
                {
                    "start": 1345,
                    "end": 1363,
                    "matchedPaperCorpusId": "252762155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "267334883",
            "title": "Diffusion Model Compression for Image-to-Image Translation",
            "text": "Fig. S2 presents additional experimental results on our time-step optimization for various iteration numbers, conducted on IP2P [6]. Our time-step optimization achieves superior results compared to uniform sampling in terms of both CLIP image similarity and CLIP text-image direction similarity for small iteration numbers, as demonstrated in Fig. S2 (a). The differences among the uniform sequence, our optimized sequence and 50-step sampling become marginal at 20 steps, as shown in Fig. S2 (d). This suggests that by transferring to simpler downstream tasks, the necessary number of iterations is naturally reduced compared to the conventional 50-step approach. Impact of CFG. Classifier Free Guidance (CFG) [23] is a method that is widely used in numerous conditional diffusion models to control the influence of conditions on image generation. CFG provides a single strength parameter for each condition to control its influence. To obtain a high-quality result that a user wants, they often run diffusion models multiple times with different values for the CFG strength parameters. IP2P [6], which is one of our target applications, also utilizes CFG to control the strength of prompt or image conditions. In this section, we additionally present an analysis on the impact of CFG on the time-step optimization. Specifically, we conduct the time-step optimization of IP2P [6] with different CFG strength parameter values for the image condition, and with different iteration numbers. Tab. S1 shows the result where CF G I indicates the CFG strength parameter for the image condition. As the result reveals, changing the CFG parameter leads to significantly different values for the time-step control parameter \u03b3, which indicate different optimal time step sequences, for all iteration numbers. This result also validates the practicality of our computationally-efficient time-step optimization method. To achieve high-quality results for different CFG parameter values requires to perform time-step optimization multiple times, which can be excessively time-consuming given the wide range of CFG parameter values.",
            "score": 0.4801577027507231,
            "section_title": "S3 Additional Analyses on Time-step Optimization",
            "char_start_offset": 30544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2118
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81396484375
        },
        {
            "corpus_id": "274581697",
            "title": "The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven Image Generation",
            "text": "We provide a comprehensive analysis of our method's performance across various classifier-free guidance (CFG) scales on 10k MSCOCO prompts in Fig. 7. Remarkably, even at low CFG scales, our approach achieves comparable or superior results compared to the baseline at much higher scales. This demonstrates that the selected noise samples effectively align with text prompts, reducing generation difficulty and eliminating the need for excessively high guidance scales that often cause over-saturation and instability. \n\nTo complement the quantitative analysis, Fig. S3 visualizes the generated images across different CFG scales. Our method consistently produces semantically accurate and visually coherent outputs even at low scales, while random noise frequently results in failed generations. Even at higher CFG scales, the baseline struggles to maintain stable semantic alignment, whereas our approach achieves robust and reliable performance across most scales.",
            "score": 0.4796746139440318,
            "section_title": "B.3. Different CFG Scale Analysis",
            "char_start_offset": 28391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 965
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "267027965",
            "title": "GATS: Gather-Attend-Scatter",
            "text": "Classifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4).",
            "score": 0.4778785999527738,
            "section_title": "Classifier-free guidance",
            "char_start_offset": 19616,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 225
                },
                {
                    "start": 228,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "260704453",
            "title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool",
            "text": "Generative-based Natural Language Processing (NLP) has witnessed significant progress (Brown et al., 2020) in recent years. With the introduction of Transformer (Vaswani et al., 2017) architecture, the possibility of developing high-accuracy language models that can perform tasks such as text generation, text summarization and language translation has become a reality. These models (Brown et al., 2020;Chowdhery et al., 2022), when scaled up to billions of parameters (Wei et al., 2022a), have shown remarkable improvements in text generation tasks such as zero-shot inference, popularized the term Generative AI. Instead of model fine-tuning, careful design of prompts has proven Figure 1: Retrieval-Centric Generation (RCG) approach presents an innovative concept that leverages the mutually beneficial interaction between LLMs and retrievers for more efficient context interpretation and knowledge memorization. Increased clarity in role-separation between context interpretation and knowledge memorization can potentially boost the performance of generative AI systems. \n\neffective in adapting these models to specific domains for various tasks (Brown et al., 2020). This has given rise to the field of prompt-engineering. Additionally, Chain-of-Thought (Wei et al., 2022b;Kojima et al., 2022) decomposes a complex task assigned into manageable steps, thereby expanding the capabilities of generative-based language models even further. \n\nTraining large language models (LLMs) requires immense computational resources, often involving thousands of high-end GPUs. Fine-tuning these models can also be challenging. Although prompt-engineering helped to reduce the need for fine-tuning, there was still noticeable instruction misalignment when interacting with a human user. To address this issue, techniques such as reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) have been explored to align the behavior of LLMs with human values (Ouyang et al., 2022;OpenAI, 2023).",
            "score": 0.4773883290462003,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 183,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 471,
                    "end": 490,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1280,
                    "end": 1300,
                    "matchedPaperCorpusId": "249017743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12103271484375
        },
        {
            "corpus_id": "273233268",
            "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with Backtracking",
            "text": "Large Language Models (LLMs) are increasingly used for various tasks, including natural language generation (Radford et al., 2019) and code generation (Chen et al., 2021). However, their outputs can suffer from issues such as hallucination (Xu et al., 2024), disclosure of private user information found in the training corpus (Wang et al., 2023), as well as incorrect code generation in programming tasks. When the output does not meet user expectations, users often have to restart the generation process with additional information in the prompt. Alternatively, decoding strategies like beam search can generate multiple potential outputs for a single prompt, allowing for the selection of the most suitable response. Both these approaches are computationally intensive and demand significant token generation, posing challenges in terms of efficiency and resource utilization. \n\nRecent techniques in context-free grammar (CFG) guided generation tried to address these issues by introducing constrained decoding techniques that ensure LLM outputs adhere to user-specified grammatical rules (Poesia et al., 2022;Willard and Louf, 2023;Lundberg et al., 2023;Geng et al., 2023;Ugare et al., 2024;Beurer-Kellner et al., 2024). These approaches typically involve various parsing techniques to analyze the LLM's partial outputs and determine the acceptable set of tokens based on the defined grammar. While effective in producing syntactically correct output, these techniques fall short of enforcing semantic properties that extend beyond syntax. For example, grammatical constraints alone cannot adequately ensure that a variable name in LLM-generated code is defined before its use or that the generated text avoids harmful language. \n\nIf an LLM generates a semantically incorrect output, the user typically must restart the generation from scratch. Current grammar-guided generation tools fail to address this problem effectively, as they cannot detect semantic violations, or pause the generation at intermediate points. Additionally, navigation through the generation by naively backtracking a certain number of tokens from the end of the output to the part that caused the violation is very difficult.",
            "score": 0.4768968020687692,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 1114,
                    "end": 1137,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 1159,
                    "end": 1177,
                    "matchedPaperCorpusId": "258841514"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.169189453125
        },
        {
            "corpus_id": "258865186",
            "title": "DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models",
            "text": "Classifier-free guidance (CFG, [10]) is a simple yet effective method for conditional image generation [25,37], modifying the predicted noise to include the gradient of the log-likelihood of p(C|x t ). Given that p(C|x t ) \u221d p(x t |C)/p(x t ), we can obtain \n\n, where w is the guidance scale. \n\nIn our setup, where the condition set is multimodal, the CFG pushes every condition to the same level of guidance. Unlike unimodal generation, our multimodal generation requires more subtle modulation of input conditions. In order to guide a modality c m \u2208 C while other modalities are also given, we need to control the estimate of \n\n. \n\nThus, the mode-specific guidance direction becomes \u03b5(x t , C)\u2212 \u03b5(x t , C\\c m ), where the second term is obatined by using the null input for c m while the other conditions are given. Consequently, the adjusted noise prediction for c m , in addition to the original CFG, is \n\nwhere \u03b3 \u2208 R is the controllable scale for mode-specific guidance. \u03b3 < 0 indicates reducing the impact of c m , \u03b3 > 0 indicates increasing, and \u03b3 = 0 recovers the original CFG.",
            "score": 0.4762957077514518,
            "section_title": "Mode-Specific Guidance for Controllability",
            "char_start_offset": 11361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1085
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 110,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "270214339",
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "text": "We introduced and empirically explored a principled and general approach to perform guidance on discrete state-spaces. Our approach, Discrete Guidance, is applicable to a broad class of generative models on discrete state-spaces realized through CTMCs, including continuous-time diffusion and flow models. We evaluated our approach empirically by applying it to guided conditional generation tasks in multiple domains, including small-molecules, DNA sequences and protein sequences. \n\nWhile our work demonstrated the effectiveness of Discrete Guidance in a variety of applications, it also leaves some avenues for further investigations and improvements. Although we have shown that Taylor-approximated guidance works well empirically, it lacks the theoretical guarantees offered by exact guidance, and further investigation into the potential trade-off between efficiency and accuracy would be of interest to the community. Also, guidance requires training predictors on noised samples, and it is unclear what training strategies would lead to the most effective guided generation (Klarner et al., 2024). Finally, it would also be interesting and potentially fruitful to explore Discrete Guidance for controllable text generation of language models. \n\nOur work illustrates that guided conditional generation in discrete state-spaces has the potential to be leveraged across the natural sciences. We expect that future work will more fully realize the potential of guided generation in these and other domains.",
            "score": 0.4762400584087696,
            "section_title": "Discussion",
            "char_start_offset": 36891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1510
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38134765625
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "Evaluation of Classifier-Free Guidance (CFG) To assess the effectiveness of the CFG strategy, we trained our model without CFG dropoff. During inference, the model utilized the original CFG denoising process, which utilized the empty caption feature from Stable Diffusion's text encoder as negative prompt features. The results in Table 5 demonstrate that all metrics are worse without CFG, indicating that the CFG training strategy improves the image generation quality. \n\nEvaluation of Different Loss Guidance As described in Sec. 3.3, we introduced an auxiliary loss, denoted as L CAP for CC3M training. To assess the impact of this loss and determine if the single caption loss alone can generate high-quality images like GILL, we trained our model without the caption loss L CAP (alignment between the mapped generative voken features and the caption features from stable diffusion text encoder) and the conditional latent diffusion loss L LDM (alignment between the mapped generative voken features and conditional features for latent diffusion process of ground truth images) separately. The results, as shown in Table 5, indicate that the caption loss significantly aids in generating better images, and the voken alignment loss further enhances coherence and image quality performance.",
            "score": 0.4751049179420588,
            "section_title": "Text-to-Image Generation Qualities on CC3M",
            "char_start_offset": 23433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1294
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6572265625
        },
        {
            "corpus_id": "267406729",
            "title": "Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis",
            "text": "In recent years, LLMs have shown promise in various SE applications [14]. LLMs are language models consisting of billions of parameters trained from a significant amount of data and have impressive performance in language processing tasks, including both natural languages and programming languages [15], [18]. LLMs designed for processing and generating human-like text, and GPT (Generative Pre-trained Transformer) is a prominent example within this category [12]. GPT models are particularly notable for their ability to perform a wide range of language tasks, from translation and summarizing to question-answering and creative writing, without needing task-specific training. As such, GPT stands as a flagship example of LLMs, showcasing the potential of these models in various applications that require nuanced language understanding and generation [41]. \n\nAs Treude et al. [39] mentioned, the GPT model and SE are related by applying NLP techniques to various tasks within the software development lifecycle. GPT's language generation capabilities offer valuable assistance and enhancements to SE processes [37], [21]. The integration of LLMs into SE has marked a significant paradigm shift in this field [1]. LLMs have demonstrated significant benefits compared to traditional methods like models guided by domain-specific languages, probabilistic grammars, and basic neural language models. Nowadays, LLMs have been applied to various field of SE. These include data analysis [31], text classification [6], software development [32], code search [17], unit test case generation [40], automated program repair [26], etc.",
            "score": 0.47461452992757003,
            "section_title": "Large Language Models in Software Engineering",
            "char_start_offset": 5792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1629
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "260386372"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "47021242"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11279296875
        },
        {
            "corpus_id": "261242901",
            "title": "InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models",
            "text": "For diffusion models, there exist two primary strategies for achieving controllable generation. One of these is classifier guidance (CG) (Dhariwal and Nichol 2021; Liu et al. 2023b), which utilizes a classifier during the sampling process and mixes its input gradient of the log probability with the score estimate of diffusion model. It is flexible and controllable, but tends to suffer a performance degradation (Ho and Salimans 2022). Another approach, named classifierfree guidance (CFG) (Ho and Salimans 2022;Nichol et al. 2021;Ramesh et al. 2022;Saharia et al. 2022), achieves the same effect through training a conditional diffusion model directly without a guidance classifier. This method performs better but requires a large amount of data with diverse text descriptions, which is difficult for our InstructME trained with source-target paired data. In this work, to attain a tradeoff between quality and controllability, we adopt both classifier and classifier-free guidance to achieve the controllable editing of Remix operations. \n\nWe specify instrument and genre tags with CFG by incorporating these tags into text commands to train the conditional diffusion models. During the training, we discard our text condition y randomly with a certain probability p CFG following (Liu et al. 2023a;Wang et al. 2023). Then, in the sampling, we can estimate the noise \u03b5\u03b8 (t, T (y), p s , z s , z t ) with a linear combination of the conditional and unconditional score estimates: \n\nwhere w can determine the strength of guidance. \n\nTo achieve finer-grained semantic control with weaklyassociated, free-form text annotations, we apply classifier guidance during sampling with a pre-trained MuLan (Huang et al. 2022), which can project the music audio and its corresponding text description into the same embedding space. The guidance function we use is: \n\nwhere E L (\u2022) and E M (\u2022) denote the language and music encoders respectively. Then, by adding the gradient on estimated x t , we can guide the generation \n\nwith factor s to control the guidance scale.",
            "score": 0.4740986194753394,
            "section_title": "Towards Advanced Music Editing -Remix",
            "char_start_offset": 13567,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2013
                },
                {
                    "start": 2016,
                    "end": 2060
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 181,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 514,
                    "end": 533,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 533,
                    "end": 552,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 552,
                    "end": 572,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "269982268",
            "title": "PaGoDA: Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher",
            "text": "In this section, we integrate Classifier-Free Guidance (CFG) [23,4] into PaGoDA for Text-to-Any generation, with a focus on Text-2-Image. Incorporating CFG alters the sample distribution, necessitating adjustments to the loss functions for Stages 2 and 3. Since previous GAN literature [24][25][26][27] has not addressed CFG integration, we introduce the classifier-free guided adversarial loss to accommodate this adaptation. \n\nCFG guides the denoising process by adjusting the conditional score gradient \u2207 log p t (x t |c) into a guided score \u2207 log p t (x t |c) + (\u03c9 \u2212 1)\u2207 log p(c|x t ). This adjustment leads our distillation learning target from p data (x|c) to p data (x|c, \u03c9), defined by \n\nreflecting the influence of guidance strength \u03c9.",
            "score": 0.4738522293274,
            "section_title": "PaGoDA with Classifier-Free Guidance",
            "char_start_offset": 12163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 744
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 65,
                    "end": 67,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "257427461"
                },
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "256105441"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "248006185",
            "title": "Video Diffusion Models",
            "text": "In the conditional generation setting, the data x is equipped with a conditioning signal c, which may represent a class label, text caption, or other type of conditioning. To train a diffusion model to fit p(x|c), the only modification that needs to be made is to provide c to the model as x\u03b8 (z t , c). \n\nImprovements to sample quality can be obtained in this setting by using classifier-free guidance [20]. This method samples using adjusted model predictions \u02dc \u03b8 , constructed via \n\nwhere w is the guidance strength, \n\nis the regular conditional model prediction, and \u03b8 (z t ) is a prediction from an unconditional model jointly trained with the conditional model (if c consists of embedding vectors, unconditional modeling can be represented as c = 0). For w > 0 this adjustment has the effect of over-emphasizing the effect of conditioning on the signal c, which tends to produce samples of lower diversity but higher quality compared to sampling from the regular conditional model [20]. The method can be interpreted as a way to guide the samples towards areas where an implicit classifier p(c|z t ) has high likelihood, and is an adaptation of the explicit classifier guidance method proposed by [16].",
            "score": 0.47369508584687414,
            "section_title": "Background",
            "char_start_offset": 4036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1208
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "267750410",
            "title": "Text2Data: Low-Resource Data Generation with Textual Control",
            "text": "2.1. Text-to-data diffusion-based generation Diffusion models, notably divided into classifierguided (Dhariwal & Nichol, 2021) and classifier-free (Ho & Salimans, 2022) categories, have significantly impacted data generation across various domains. (Hoogeboom et al., 2022;Yang et al., 2023;Ho et al., 2022;Voleti et al., 2022). The classifier-guided diffusion guides the model during inference phase by independently training a classifier and supervising the model with its gradient, which is inefficient when computing gradient at each time step and sometimes the generation quality is deficient as the guidance is not involved in the training. By contrast, classifier-free diffusion guidance blends score estimates from both a conditional diffusion model and an unconditional one with time step as a parameter, exemplified by E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al., 2022) and Motion Diffusion Model (MDM) (Tevet et al., 2023) for controllable molecule and motion generation, respectively. Furthermore, since natural languages are a prevalent medium for human to communicate with the world, the text-to-data generation paradigm has gained traction, with diffusion models being instrumental in generating high-quality data aligned with textual inputs. The extensive applications encompass text-to-image generation (Ruiz et al., 2023;Zhang & Agrawala, 2023), text-to-speech generation (Huang et al., 2022;Kim et al., 2022), text-to-shape generation (Li et al., 2023;Lin et al., 2023), and more, leveraging the abundant text descriptions for training potent generative models. Despite advancements in generating data from text across various modalities, many other modalities may not satisfy the stringent requirements for sufficient data-text pairs essential for attaining optimal controllability during the training of models.",
            "score": 0.4723537718958748,
            "section_title": "Related works",
            "char_start_offset": 5191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 4
                },
                {
                    "start": 5,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 126,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 249,
                    "end": 273,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 307,
                    "end": 327,
                    "matchedPaperCorpusId": "248965384"
                },
                {
                    "start": 868,
                    "end": 892,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 926,
                    "end": 946,
                    "matchedPaperCorpusId": "252595883"
                },
                {
                    "start": 1333,
                    "end": 1352,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1403,
                    "end": 1423,
                    "matchedPaperCorpusId": "250492984"
                },
                {
                    "start": 1423,
                    "end": 1439,
                    "matchedPaperCorpusId": "246430592"
                },
                {
                    "start": 1467,
                    "end": 1484,
                    "matchedPaperCorpusId": "254366593"
                },
                {
                    "start": 1484,
                    "end": 1501,
                    "matchedPaperCorpusId": "253708074"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "270924450",
            "title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents",
            "text": "Classifier-free guidance (Ho & Salimans, 2021) (cfg) is a mode-seeking technique commonly used in diffusion literature, such as class-conditioned genreation (Peebles & Xie, 2023) or text-to-image generation (Rombach et al., 2022).It generally guides the sampling trajectories toward higher-density regions.We can similarly apply classifier-free guidance in the DisCo-Diff, where we treat the discrete latent as conditional inputs.We follow the convention in (Saharia et al., 2022), and the classifier-free guidance at time step t is as follows:\n\nwhere D \u03b8 (x, \u03c3(t), z)/D \u03b8 (x, \u03c3(t), \u2205) is the conditional/unconditional models, sharing parameters.We drop the discrete latent with probability 0.1 during training, to train the unconditional model D \u03b8 (x, \u03c3(t), \u2205).A mild w would usually lead to improvement in sample diversity (Peebles & Xie, 2023).Table 3 demonstrates that using a moderate guidance scale w=1 (we use w = 1 and cfg=1 interchangeably in the paper) improves the FID score, suggesting that the learned discrete latent in the DisCo-Diff framework has strong indications of mode of data distribution.We further explore varying the guidance scale on ImageNet-128.As shown in Fig 9, increasing the classifier-free guidance scale w would strengthen the effect of guidance.",
            "score": 0.4720889903404129,
            "section_title": "B. Discrete Latent Variable Classifier-Free Guidance",
            "char_start_offset": 44691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 544
                },
                {
                    "start": 546,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 847
                },
                {
                    "start": 847,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1173
                },
                {
                    "start": 1173,
                    "end": 1280
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 178,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 825,
                    "end": 846,
                    "matchedPaperCorpusId": "254854389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "277272356",
            "title": "Guidance Free Image Editing via Explicit Conditioning",
            "text": "Diffusion models (Karras et al., 2022;2024) have made great progress in image generation abilities, demonstrating impressive results for conditional generation. Currently, all state-of-the-art (SOTA) conditional diffusion systems leverage Classifier Free Guidance (CFG) (Ho & Salimans, 2022;Zheng et al., 2023) to yield diverse high-quality conditional image generation, at the cost of the increased number of the denoising passes of the learned neural network models. Specifically, the image editing task is conditioned on a given image and an instruction prompt. Applying CFG for image editing requires three denoising passes to yield plausible results, see Fig. 1 (d), (e), and (f). \n\nInformally, the formulation of CFG is based on two premises (Ho & Salimans, 2022;Zheng et al., 2023;Brooks et al., 2022). First, there exists conditional information that is not natively supported by the underlying generative modeling. Second, the score, i.e., the gradient of the log of the density of conditional generative modeling may be aided by the score of unconditional generative modeling. More formally, CFG can be seen as predictor-corrector (Bradley & Nakkiran, 2024). CFG boosts conditional image generation significantly across diffusion and flow (Lipman et al., 2023;Tong et al., 2024) models alike. This has led to a surge of research on different ways to combine not just conditional and unconditional models but more generally two or more models (Sadat et al., 2024). \n\nThe main practical drawback of CFG lies in the need for multiple conditional and unconditional denoising passes, resulting in additional computation costs. To alleviate the computational cost, a student model is distilled to mimic in one pass the original teacher sampling mechanism requiring multiple passes. CFG distillation, however, imposes a cumbersome extra training stage and is prone to lag behind the teacher's performance (Meng et al., 2023).",
            "score": 0.47198880478546673,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1928
                }
            ],
            "ref_mentions": [
                {
                    "start": 1141,
                    "end": 1167,
                    "matchedPaperCorpusId": "271903235"
                },
                {
                    "start": 1270,
                    "end": 1288,
                    "matchedPaperCorpusId": "259847293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "278171688",
            "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
            "text": "In diffusion-based generative models, classifier-free guidance [25] is well-explored to strengthen the influence of conditioning signals during inference. This is achieved by using both conditional and unconditional predictions from the same model to guide the generation process as follows: \n\nwhere s is guidance scale. Since each modality exhibits different characteristics, we hypothesize that using a single guidance scale for all modalities may be sub-optimal. To allow better control over each modality during inference, we propose multimodal classifier-free guidance by assigning modalityspecific guidance scales: \n\nwhere s t is guidance scale for text modality and s v for remained modality, namely video. By adjusting s t and s v , we can adaptively control the focus between modalities. Higher s t encourages the model to follow the text more closely, improving intelligibility, while higher s v leads to better lip synchronizations. \n\nTo support CFG, we apply modality dropout during training by randomly dropping text, video, or both. This not only enables multimodal CFG but also improves robustness in cases where a modality may be missing.",
            "score": 0.47088703402374943,
            "section_title": "Multimodal Classifier-Free Guidance",
            "char_start_offset": 18966,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1154
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "277955619",
            "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
            "text": "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance mechanism based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. ERG results in significant improvements in various generation tasks such as text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further boosting generation performance.",
            "score": 0.47027686533328056,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.880859375
        },
        {
            "corpus_id": "271946872",
            "title": "Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion",
            "text": "The Classifier-Free Guidance (CFG) [14] guides an initial noise to the final image that depicts the semantics specified in the text condition. Specifically, with CFG, the predicted noise \u03b5\u03b8 can be written as \u03b5\u03b8 (z, c) \n\n, where z represents the unconditional embedding, c is the conditional embedding derived from the text prompt, and \u03b1 is the CFG scale. It is well-established that a larger value of \u03b1, representing stronger guidance, results in higher coherence between the generated image and the text prompt, but this comes at the expense of sample diversity [14]. \n\nIn this study, we examine how bias changes as the CFG scale varies, generating a total of 5,000 images for each profession (five CFG scales \u00d7 1,000 generations per profession). Figure 3 presents the ratio of the major attribute (y-axis) against the CLIP score (x-axis), with color intensity indicating the magnitude of the CFG scale. As the CFG scale decreases (represented by lighter colors), the ratio of the major attribute also declines. These findings support our hypothesis that weakening the text condition can reduce bias, though it comes at the cost of decreased alignment between the generated images and the text prompts.",
            "score": 0.4695223108941366,
            "section_title": "Impact of Classifier Free Guidance",
            "char_start_offset": 10664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 217
                },
                {
                    "start": 220,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1203
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 39,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "276928635",
            "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity",
            "text": "In order to generate samples following condition given by users, diffusion models are extended to conditional generative models [18,43] with additional inputs in the models: \n\nwhere x t , \u03f5 are sampled same as Eq. 1 and c denotes a specific condition that x has, in most cases the embedding of a class or text. However, since vanilla sampling often results in suboptimal performance for conditional generation, various guidance sampling methods have been extensively explored to enhance sample quality [1,7,10,18,20,21,24,46]. For clarity, let us shorten the notation as \u03f5 \u03b8 (x t , c) := \u03f5 \u03b8 (x t , t, c) and denote the unconditional model as \u03f5 \u03b8 (x t , \u2205), where \u2205 represents the null condition. Classifier-Free Guidance (CFG) adjusts the classconditioned probability relative to the unconditional one, becoming p( \n\n, resulting in an adjusted sampling process: \n\nwhere w is the guidance scale. Recently, \"weak model\" guidance has been introduced, which weakens the conditional model and computes the difference with the normal conditional output as follow: \n\nwhere s is the guidance weight, and \u03b5 represents a model that is intentionally weakened or perturbed, achieved through various heuristic methods. For instance, AG [24] uses a flawed model variant, PAG [1] replaces self-attention weights with an identity matrix, SEG [20] blurs attention weights, Time Step Gudiance (TSG) [46] perturbs timestep embeddings, and SelfGuidance [29] alters noise levels. While effective, these approaches lack a clear theoretical foundation and have limitations: 1) they require specific layer identification, 2) increase computational cost with added NFEs, and 3) are incompatible with step-distilled models. Our method overcomes all of these limitations.",
            "score": 0.4695127568386931,
            "section_title": "Guidance Sampling in Diffusion Models",
            "char_start_offset": 6334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 502,
                    "end": 505,
                    "matchedPaperCorpusId": "268692048"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1262,
                    "end": 1265,
                    "matchedPaperCorpusId": "268692048"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "267770589",
            "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
            "text": "Text-to-image diffusion model: \n\nRecent years have witnessed unprecedented progress in text-to-image synthesis, driven by large generative models such as diffusion models [6,17,50,51] (e.g., GLIDE [38], DALL\u2022E 2 [44], Imagen [47], and Stable Diffusion [45]) and VQ Transformers [9] (e.g., DALL\u2022E [43], CogView [7,8], and Parti [57]). This paper studies how to extract disentangled image factors from text-to-image diffusion models. \n\nGuidance for diffusion models. Guidance methods modify the output distribution of pre-trained diffusion models, based on additional inputs such as class labels [6], text [38], and corrupted images [24,32]. The first guidance method is classifier guidance [6], for which a class classifier is finetuned on noisy images. Similarly, CLIP guidance [30,38] finetunes a CLIP model [42] to support text input. To avoid finetuning classifiers or CLIP, classifier-free guidance (CFG) [16] jointly trains a conditional and an unconditional diffusion model and combines their score estimates, and CFG has become the default for text-to-image tasks [38,45]. To compose multiple texts, composable diffusion [29] combines score estimates with different text inputs. Besides user-specified conditions, several works showed that even guidance based on model outputs [3] or representations [18] can improve the quality of images. In this paper, we explore how to disentangle image factors with texts to gain fine-grained control. \n\nImage editing with diffusion models. Recent works have shown that diffusion models are capable of unpaired imageto-image translation [4,35,52,55]. A more recent trend of works have explored zero-shot image editing with text-to-image diffusion models [14,25,55]. One of the applications of our Contrastive Guidance is to improve the intended edit of some of these zero-shot image editors. \n\nGuidance for other generative models. Guidance has also been widely studied for GANs [12] and autoregressive language models (LMs).",
            "score": 0.46864229088021525,
            "section_title": "Related Work",
            "char_start_offset": 3897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 33,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 180,
                    "end": 182,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 296,
                    "end": 300,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 313,
                    "end": 315,
                    "matchedPaperCorpusId": "248476190"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 594,
                    "end": 597,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "246411364"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1075,
                    "end": 1078,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "251402961"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1582,
                    "end": 1585,
                    "matchedPaperCorpusId": "236950721"
                },
                {
                    "start": 1585,
                    "end": 1588,
                    "matchedPaperCorpusId": "245704504"
                },
                {
                    "start": 1588,
                    "end": 1591,
                    "matchedPaperCorpusId": "247476275"
                },
                {
                    "start": 1591,
                    "end": 1594,
                    "matchedPaperCorpusId": "252815928"
                },
                {
                    "start": 1699,
                    "end": 1703,
                    "matchedPaperCorpusId": "251252882"
                },
                {
                    "start": 1703,
                    "end": 1706,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 1706,
                    "end": 1709,
                    "matchedPaperCorpusId": "252815928"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8740234375
        },
        {
            "corpus_id": "270923987",
            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
            "text": "In this paper, we revisited the core aspects of classifier-free guidance and showed that by replacing the conditional vector in a trained conditional diffusion model with an independent condition, we can efficiently estimate the score of the unconditional distribution.We then introduced independent condition guidance (ICG), a novel method that simulates the same behavior as CFG without the need to learn an unconditional model during training.Inspired by this, we also proposed time-step guidance (TSG) and demonstrated that the time-step information learned by the diffusion model can be leveraged to enhance the quality of generations, even for unconditional models.Our experiments showed that ICG performs similarly to standard CFG and alleviates the need to consider the CFG objective during training.Thus, ICG streamlines the training of conditional models and potentially improves the training efficiency.Additionally, we verified that TSG also improves generation quality in a manner similar to CFG, without relying on any conditional information.As with CFG, challenges remain in accelerating the proposed methods so that the sampling cost approaches that of an unguided sampling process (i.e., removing the need to query the diffusion network twice at each sampling step); we consider this topic as a promising avenue for further research.",
            "score": 0.46497966682223446,
            "section_title": "Discussion and conclusion",
            "char_start_offset": 19091,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 914
                },
                {
                    "start": 914,
                    "end": 1057
                },
                {
                    "start": 1057,
                    "end": 1351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "273811150",
            "title": "ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.",
            "text": "Recent techniques for conditional generation have been developed to incorporate various modalities, ensuring that synthesized results align with the instructions provided by these modality signals [66]. Early approaches, such as conditional variational autoencoders (CVAEs) [66] and conditional generative adversarial networks [66], leverage class labels to distinguish image domains and guide generated image samples to possess domain-specific properties. For example, CycleGAN was proposed to translate images from a source domain to a target domain, while the StarGAN series [18], [67] further succeeded in translating images from a source domain to multiple target domains. Some techniques embed discrete class labels as conditional signals in the generation process, either by directly concatenating class labels with the input [18], [66] or by using conditional normalization techniques [67], [68]. Building on these successful practices, conditional diffusion models incorporate class information (e.g., class labels or text) into normalization layers and guide the generation process using classifier gradients. Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models [69], [70] that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals.",
            "score": 0.46395102660841986,
            "section_title": "Conditional Generative Models",
            "char_start_offset": 12660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1534
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "13936837"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "13936837"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "13936837"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "9417016"
                },
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "208617800"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "9417016"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "13936837"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "208617800"
                },
                {
                    "start": 899,
                    "end": 903,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1385,
                    "end": 1389,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91015625
        },
        {
            "corpus_id": "278713058",
            "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation",
            "text": "Motion-aware Classifier-free Guidance To further improve generation quality and generalization, we introduce motion-aware classifier-free guidance (CFG). Traditional CFG is typically used for text/image condition with well-defined unconditional input c \u2205 (e.g., empty text or zero image), following \u03b5\u03b8 = \u03f5 \u03b8 (z t , t, c \u2205 ) + w(\u03f5 \u03b8 (z t , t, c t ) \u2212 \u03f5 \u03b8 (z t , t, c \u2205 )) where \u03f5 \u03b8 denotes the denoising network, z t is the noisy latent at timestep t, and w is the CFG scale controlling the strength of conditioning. When w = 0, the generation is fully unconditional; when w = 1, it is fully conditional on c t . Since motion tokens lack natural unconditional forms, we use learnable unconditional motion tokens c \u2205 that match the feature dimension of z motion . During training, c t is randomly replaced by c \u2205 with a predefined probability p (i.e., c \u2205 is only updated when used). This enables joint learning of both conditioned and unconditioned generation, enhancing model robustness and controllability.",
            "score": 0.4636487376405095,
            "section_title": "4D Positional Encoding",
            "char_start_offset": 15320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1007
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "277321603",
            "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
            "text": "Guidance in Diffusion Models. Classifier-Free Guidance (CFG) [28] has become the de facto guidance technique for conditional generation with diffusion models, leading to notable improvements in both condition alignment and image quality. However, recent research has highlighted some of its limitations. Kynk\u00e4\u00e4nniemi et al. [41] have shown that the specific timesteps at which CFG is applied significantly impact image diversity, and proposed to restrict CFG to certain intervals. \n\nAnother line of work [1,31] addresses the limited applicability of CFG for text-based conditions when using off-the-shelf diffusion models like Stable Diffusion [52]. These approaches introduce a guidance technique that extends to a broader range of generation tasks, including unconditional generation, inverse problems, and conditional generation with non-text conditions (e.g., depth maps [67]). Recently, Karras et al. [38] propose Autoguidance which uses the noise estimate from an under-trained version of itself, instead of unconditional noise, to resolve inherent issues of the entangled guidance for condition alignment and image quality. A more detailed discussion on autoguidance can be found in the Appendix. However, previous works have not explored how the dynamics of CFG shift when a diffusion model is fine-tuned for a specific task [7,46,64]. In this work, we address the critical issue of unconditional noise degradation that occurs during fine-tuning and pro-pose a novel solution by combining noise predictions from multiple diffusion models. \n\nMerging Diffusion Models. Aligned with the mixtureof-experts [8] and model merging [65] literature on foundation models, there is growing research on methods for merging diffusion models to enable effective composition of multiple conditions. Diffusion Soup [3] directly merges weights of different diffusion models, Mix-of-Show [25] combines the weights of LoRA adapters [32], and Max-Fusion [48] merges intermediate model features. Notably, leveraging the iterative denoising process of diffusion models, merging their noise estimates has emerged as a simple yet powerful technique for composing conditions.",
            "score": 0.46351967847307,
            "section_title": "Related Works",
            "char_start_offset": 5639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 504,
                    "end": 507,
                    "matchedPaperCorpusId": "268692048"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1336,
                    "end": 1339,
                    "matchedPaperCorpusId": "257631738"
                },
                {
                    "start": 1339,
                    "end": 1342,
                    "matchedPaperCorpusId": "253523371"
                },
                {
                    "start": 1807,
                    "end": 1810,
                    "matchedPaperCorpusId": "270391477"
                },
                {
                    "start": 1942,
                    "end": 1946,
                    "matchedPaperCorpusId": "269148434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "274192248",
            "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models",
            "text": "Recent advancements in large language models (LLMs) have created new possibilities for complex applications such as code generation (Chen et al., 2021;Wang et al., 2021), debugging (Pearce et al., 2022;Mozannar et al., 2024), external tool invocation through function calling (OpenAI, 2024;LangChain, 2024), and robotic control (Liu et al., 2023). These applications bring great demand for LLM systems to perform structured generation and produce outputs that follow specific formats, such as JSON, SQL or other structures tailored to the task. The downstream applications can then organically consume the structured outputs to perform followup interactions with the system. Constrained decoding (Deutsch et al., 2019;Kuchnik et al., 2023) is a commonly adopted method for structured generation. At each decoding step, constrained decoding examines the vocabulary and filters out tokens that violate the specified structure by setting the probabilities of invalid tokens to zero. To support the rich structure formats arising in diverse applications, a flexible mechanism is needed to specify and check the constraints. Context-free grammar (CFG) (Chomsky, 1956;Poesia et al., 2022;Scholak et al., 2021) provides a general approach for defining structures through a set of rules. Each rule contains a sequence of characters or other rules, allowing recursive composition to represent complex structures. Compared to alternative formats such as regular expressions, CFGs offer greater flexibility by allowing recursive structures, making them suitable for describing common languages such as JSON, SQL, and domain-specific languages (DSLs). \n\nHowever, naively applying CFG to constrained decoding is not efficient because of its flexible nature. First, each decoding step needs to interpret CFG for every possible token in the vocabulary, which can be as large as 128k in Llama 3.1 (Dubey et al., 2024a). Additionally, CFG interpretation requires a stack state that tracks the recursive rules matched so far, making it impossible to precompute and cache all combinatorial combinations of stack patterns ahead of time.",
            "score": 0.46312238342031087,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 224,
                    "matchedPaperCorpusId": "253117056"
                },
                {
                    "start": 696,
                    "end": 718,
                    "matchedPaperCorpusId": "208613916"
                },
                {
                    "start": 718,
                    "end": 738,
                    "matchedPaperCorpusId": "254044477"
                },
                {
                    "start": 1147,
                    "end": 1162,
                    "matchedPaperCorpusId": "17432009"
                },
                {
                    "start": 1182,
                    "end": 1203,
                    "matchedPaperCorpusId": "237491759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1590576171875
        },
        {
            "corpus_id": "254564168",
            "title": "Towards Practical Plug-and-Play Diffusion Models",
            "text": "Diffusion models Diffusion models [8,18,26,38,49] and score-based models [51,53] are families of the generative model that generate samples from a given distribution by gradually removing noise. Unlike other likelihood-based methods such as VAEs [27] or flow-based models [9,10], diffusion models have shown superior generation capabilities comparable to GANs [3,12,23]. Although diffusion models suffer from slow generation, previous works such as DDIM [50], A-DDIM [2], PNDM [33], and DEIS [55] have achieved significant acceleration in the generation process.\n\nFor conditional generation in diffusion models, classifier guidance [8,53] and classifier-free guidance [19] are widely applied to various tasks [17,25,29,37,44]. Classifier guidance uses gradients of the external classifier, whereas classifier-free guidance interpolates between predictions from a diffusion model with and without labels. However, for classifier-free guidance, diffusion models should be learned as labeled data because it requires the prediction of labels. In this paper, we focus on the classifier guidance that freezes the unconditional diffusion model and guides it with the external model to conduct various conditional generations without labeled data in plug-and-play manner. Plug-and-play generation Following [36], we use the term plug-and-play to refer to the capability of generating images at test time based on a condition given by a replaceable condition network without training it and generative model jointly. There have been various attempts for plug-and-play conditional generation in both image generation [11,22,24,36,52] and text generation [6,34,48], by binding constraints to the unconditional models, such as GAN [12], VAE [27]. These methods allow the single unconditional generative model to perform various tasks by changing the constraint model.\n\nMost similar work to ours, Graikos et al. [13] attempted plug-and-play on diffusion models for various tasks by directly optimizing latent images with the off-the-shelf model. However, it fails to generate meaningful images in complex distribution as ImageNet. Contrary to this, our method successfully guidance",
            "score": 0.46200818977952696,
            "section_title": "Related Work",
            "char_start_offset": 4046,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 37,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 37,
                    "end": 40,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 43,
                    "end": 46,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 46,
                    "end": 49,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 73,
                    "end": 77,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 632,
                    "end": 635,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "246430592"
                },
                {
                    "start": 722,
                    "end": 725,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "36549760"
                },
                {
                    "start": 1612,
                    "end": 1615,
                    "matchedPaperCorpusId": "240225810"
                },
                {
                    "start": 1615,
                    "end": 1618,
                    "matchedPaperCorpusId": "235253954"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "244130146"
                },
                {
                    "start": 1645,
                    "end": 1648,
                    "matchedPaperCorpusId": "208617790"
                },
                {
                    "start": 1648,
                    "end": 1651,
                    "matchedPaperCorpusId": "222178257"
                },
                {
                    "start": 1651,
                    "end": 1654,
                    "matchedPaperCorpusId": "211069137"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "259165110",
            "title": "Fast Training of Diffusion Models with Masked Transformers",
            "text": "Classifier-free guidance For class-conditional generation, classifier-free guidance (CFG) (Ho & Salimans, 2022) is a widely used sampling method to improve the generation quality of diffusion models. In the EDM formulation, denote by D \u03b8 (x x x, t, c) the class-conditional denoising function, CFG defines a modified denoising function: \n\n, where w \u2265 1 is the guidance scale. To get the unconditional model for the CFG sampling, we can simply use a null token \u2205 (e.g., an all-zero vector) to replace the class label c, i.e., D \u03b8 (x x x, t) := D \u03b8 (x x x, t, \u2205). During training, we randomly set c to the null token \u2205 with some fixed probability p uncond .",
            "score": 0.46186137106523967,
            "section_title": "Preliminaries",
            "char_start_offset": 13006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 655
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "276741986",
            "title": "What Makes a Good Diffusion Planner for Decision Making?",
            "text": "For decision making tasks, guided sampling algorithms are used to generate desired plans or actions. In this work, we compare three types of different guided sampling methods: classifier guidance (Dhariwal & Nichol, 2021) (CG), classifier-free guidance (CFG) (Ho & Salimans, 2021), and Monte Carlo sampling from selections (MCSS). \n\nClassifier guidance: Classifier guidance (CG) is introduced to guide the unconditional diffusion models q t (x t ) to generate data over condition c. The conditioned score function is formulated as: \n\nwhere the second term is also know as a noised classifier that predict the condition using noised data x t . During sampling, the gradient of the classifier is then applied to the predicted noise \u03f5 \u03b8 (x t , t): \n\nwhere w is a weighting factor that controls the strength of the classifier guidance. For CG sampling, we tuned w in range [0.001, 10] on each task. \n\nClassifier-free guidance: To avoid training classifiers, classifier-free guidance (CFG) is proposed. The main idea of CFG is to train a diffusion model that can be used for both conditional noise predictor \u03f5 \u03b8 (x t , t, c) and unconditional noise predictor \u03f5 \u03b8 (x t , t): \n\nwhere \u03f5 \u03b8 (x t , t) = \u03f5 \u03b8 (x t , t, \u2205). Noise prediction of \u03f5 \u03b8 (x t , t, \u2205) and \u03f5 \u03b8 (x t , t, c) can be jointly learned by randomly discard conditioning with probability of p uncond . For decision making tasks, we can train diffusion models using condition of discounted returns, and using classifier-free guidance for better plan sampling. We can normalize the discounted return in the dataset for training, and use condition of 1 as target return for CFG sampling during inference (Ajay et al., 2022). However, experiments shows that fixing 1 as target may lead to unrealistic or unstable plans.",
            "score": 0.46144544806532717,
            "section_title": "A GUIDED SAMPLING ALGORITHMS",
            "char_start_offset": 27504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1769
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 280,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1655,
                    "end": 1674,
                    "matchedPaperCorpusId": "254044710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "273186941",
            "title": "AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models",
            "text": "Require: \n\npredict the conditioning label y from intermediate noisy samples x t , was used to steer the reverse process. This guidance was achieved by modifying the reverse sampling step as: \n\nwhere w is a scaling factor that adjusts the strength of the classifier's influence, and \u1fb1t = t i=1 1 \u2212 \u03b2 i . While effective, classifier-based guidance introduces several downsides, such as added complexity and potential inaccuracies due to classifier errors. \n\nClassifier-Free Guidance offers a simpler and more robust alternative by eliminating the need for an external classifier. Instead, the diffusion model itself is trained in two modes -conditional and unconditional: \n\n\u2022 conditional mode -the model is trained to predict the denoised data x 0 given noisy data x t and conditioning information y, learning the conditional distribution p \u03b8 (x t\u22121 |x t , y); \u2022 unconditional mode -the same model is also trained without any conditioning, learning the unconditional distribution p \u03b8 (x t\u22121 |x t ). \n\nDuring inference, CFG uses a combination of the conditional and unconditional predictions to guide the generation (see Algorithm 1). Specifically, for a given noisy sample x t , the guidance is achieved by interpolating between the conditional and unconditional predictions as follows: \n\nwhere \u03f5 \u03b8 (x t , \u00f8) is the model's prediction of the noise in x t when no conditioning is provided (unconditional), \u03f5 \u03b8 (x t , y) is the prediction of the noise in x t when conditioned on y, and w is the guidance scale, which controls how strongly the conditional information influences the generation. \n\nBy adjusting w, one can control the balance between sample diversity and adherence to the conditioning y. When w = 1, the process is equivalent to standard conditional generation. When w > 1, the conditional prediction is amplified, guiding the model to produce samples that more closely match the conditioning information, potentially at the cost of diversity. \n\n4 AUTOLORA Karras et al. (2024) introduced a novel technique called AutoGuidance to enhance the image generation capabilities of a diffusion model by guiding it with a bad version of itself -a smaller and less-trained variant. This method leads to more refined results while maintaining diversity in the outputs.",
            "score": 0.4613289446949011,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 14147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 11,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 996
                },
                {
                    "start": 999,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "268856926",
            "title": "Upsample Guidance: Scale Up Diffusion Models without Training",
            "text": "Techniques have been proposed for conditionally sampling images corresponding to specific classes or text prompts by adding a guidance term to the predicted noise.Ho & Salimans (2022) added the gradient of the log probability predicted by a classifier to \u03f5(x t , t), enabling an unconditional diffusion model to generate class-conditioned images.Subsequently, classifier-free guidance (CFG) was proposed.Instead of using a classifier, the noise predictor's architecture was modified to accept condition c as an input.The following formula is then used as the predicted noise:\n\nHere, w represents the guidance scale.It has been commonly observed that proper adjustment of the scale improves the alignment with the condition and the fidelity of the generated images.",
            "score": 0.4612512504759482,
            "section_title": "Guidances for Diffusion Models",
            "char_start_offset": 6286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 575
                },
                {
                    "start": 577,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 764
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "277510202",
            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
            "text": "Diffusion and Flow-based Models. Unlike generative ad-versarial methods [7] that rely on one-step generation, diffusion models [4] have demonstrated significantly improved performance in generating high-quality samples. Early diffusion models were primarily score-based generative models, including DDPM [10], DDIM [34], EDM [16], and Stable Diffusion [30], which focused on learning the SDEs governing the diffusion process. \n\nNext, Flow Matching [21] provides an alternative approach by directly modeling sample trajectories using ordinary differential equations (ODEs) instead of SDEs. This enables more stable and efficient generative processes by learning a continuous flow field that smoothly transports samples from a prior distribution to the target distribution. Several works, including Rectified Flow [24], SD3 [5], Lumina-Next [45], Flux [20], Vchitect-2.0 [6], Lumina-Video [23] HunyuanVideo [18], SkyReels-v1 [33], and Wan2.1 [39] have demonstrated that ODE-based methods achieve faster convergence and improved controllability in text-to-image and text-to-video generation. As a result, Flow Matching has become a compelling alternative to stochastic diffusion models, offering better interpretability and training stability. Thus, our analysis is based on Flow Matching models, which aim to provide more accurate classifier-free guidance. Guidance in Diffusion Models. Achieving better control over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance (CG) [4], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifierfree guidance (CFG) [9] was proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. \n\nDespite its effectiveness, CFG relies on an unbounded empirical parameter, known as the guidance scale, which determines how strongly the generated output is influenced by the conditional model.",
            "score": 0.4604507504722727,
            "section_title": "Related Work",
            "char_start_offset": 3527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 822,
                    "end": 825,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "240420063",
            "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
            "text": "The success of generative Transformer-based PLMs10 such as GPT, BART, and T5 has recently sparked interest in leveraging generative PLMs to solve various non-generative NLP tasks. These tasks include, but are not limited to, traditional discriminative tasks such as classification and structure prediction. For example, Figure 4 illustrates this \"text-to-text\" approach as described in Raffel et al. (2020). Instead of using traditional discriminative models for NLP tasks, these tasks are reformulated as text generation problems so that they can be directly solved with generative PLMs. The generated output sequences usually include the desired labels or other auxiliary information for the given task, enabling accurate reconstruction of the expected class labels (i.e. to avoid ambiguities in mapping) and facilitating the generation/decoding process (i.e. to provide sufficient context for predictions). \n\nIt is worth noting that some NLP tasks are already text generation tasks. Therefore, a straightforward strategy for those tasks is to fine-tune a generative PLM using task-specific training data to perform the specific tasks of interest. Examples include Machine Translation (Cooper Stickland et al., 2021), text summarization (Lewis et al., 2020), text style transfer (Lai et al., 2021), etc. We refer read-ers to Section 2 for more detailed discussion of this \"pre-train then fine-tune\" approach. In this section, we focus on tasks that are not traditionally text generation tasks.",
            "score": 0.45994715028874955,
            "section_title": "Paradigm 3: NLP as Text Generation",
            "char_start_offset": 69523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1495
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 406,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1187,
                    "end": 1218,
                    "matchedPaperCorpusId": "216914731"
                },
                {
                    "start": 1239,
                    "end": 1259,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.185791015625
        },
        {
            "corpus_id": "276618257",
            "title": "Recommendations Beyond Catalogs: Diffusion Models for Personalized Generation",
            "text": "We employ Classifier-Free Guidance (CFG) (Ho & Salimans, 2022), a widely used technique in diffusion-based generative models, to enhance user-conditioned image embedding generation. CFG modulates the sampling process by interpolating between the unconditional model output \u03b5 \u03b8 (I t e , t) and the user-rating conditional prediction \u03b5 \u03b8 (I t e , t, U, R): \n\nHere \u03c9 is the guidance scale that controls the trade-off between personalization and diversity. Higher values of guidance result in less diverse sets of images, but more attuned to their conditioning signals. We conduct extensive experiments to evaluate the influence of different guidance scaling in Section 4.5",
            "score": 0.4594994595626568,
            "section_title": "CLASSIFIER-FREE GUIDANCE",
            "char_start_offset": 13932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "267636873",
            "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
            "text": "Generative language models. Let p \u03b8 denotes an LLM parameterized by \u03b8. Consider a sequence x = [x 1 , . . . , x n ] as the input prompt, where each x i is a token from a predefined vocabulary. The LLM then generates the response sequence y = [y 1 , . . . , y m ] by sampling from the conditional probability distribution p \u03b8 (\u2022|x), where y t denotes individual token for 1 \u2264 t \u2264 m. The conditional distribution p \u03b8 (y|x) can therefore be expressed as p \u03b8 (y|x) = m t=1 p \u03b8 (y t |x, y <t ), where y <t = [y 1 , . . . , y t\u22121 ] for t > 1 and is empty for t = 1. In the case of LVLMs, visual tokens v = [v 1 , . . . , v k ] are additionally included. These tokens are generated from a pre-trained visual encoder and mapped into the token space through a linear projection. The conditional distribution of output y given the visual tokens v and textual prompt x is expressed as: \n\nwhere p \u03b8 is approximated by LVLMs. \n\nGuidance in generative models. The process of a guided generation involves getting the output y conditioned on input x, which encodes the desired properties of the output y. This guidance can be generally added to the model by two distinct approaches: classifier guidance (Dhariwal and Nichol, 2021) and classifier-free guidance (Ho and Salimans, 2021) As a result, the guided LLM p \u03b8 places more importance on the prompt x during generation with the increasing value of \u03b3, thereby producing texts that better align with the desired behavior from the prompt (Sanchez et al., 2023).",
            "score": 0.4587816395792135,
            "section_title": "Preliminaries",
            "char_start_offset": 9506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1496
                }
            ],
            "ref_mentions": [
                {
                    "start": 1187,
                    "end": 1214,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1244,
                    "end": 1267,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "266191061",
            "title": "Image is All You Need to Empower Large-scale Diffusion Models for In-Domain Generation",
            "text": "In-domain generation aims to accomplish various tasks within a specific domain, often requiring tailored approaches and labeled data. For instance, pix2pix [19] was developed for image-to-image translation (e.g., converting segmentation or edge images to realistic images), relying on paired images for training. Similarly, InterfaceGAN [45] enables attribute-based image editing but requires attributelabeled images to train editing vectors. Such traditional methods [6,12,19,45] depend heavily on specific generative tasks and data domains, which limits their generalization capability. \n\nThanks to billion-scale image-text datasets [2,37,43,44] and advances in large-scale natural language models [9,35,36], recent research on diffusion models has pushed the boundaries of AI-generated content, greatly enhancing both the diversity and controllability of generated outputs [39]. With various control mechanisms and approaches (e.g., ControlNet [61], SDEdit [29]), diffusion models can also perform diverse generative tasks beyond text-to-image synthesis, alleviating the data demands of traditional generative methods. However, the generated results often fail to fully align with specific data domains, as shown in orange in Fig. 1. \n\nWhile fine-tuning a pre-trained model on domainspecific data is an intuitive way to inject domain knowledge, it is challenging to balance fidelity with controllability, as illustrated in Fig. 2. For example, fine-tuning can improve the quality of face generation but may gradually reduce the effect of control prompts like wearing hat. In this work, we first identify this phenomenon caused by conditional guidance and unconditional guidance catastrophic forgetting during fine-tuning. These two guidances are integrated by classifier-free guidance for improved generative capability, which is employed in most text-to-image diffusion models. Then, we propose a guidance-decoupled prior preservation method to address these challenges. We decouple the conditional guidance in CFG into two parts: domain guidance, which is optimized in fine-tuning to enhance fi-delity, and control guidance, which is preserved for controllability.",
            "score": 0.45717529175436755,
            "section_title": "Introduction",
            "char_start_offset": 1334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1236
                },
                {
                    "start": 1239,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "6200260"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "218719151"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "4707877"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "250643915"
                },
                {
                    "start": 474,
                    "end": 477,
                    "matchedPaperCorpusId": "6200260"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "218719151"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "4396518"
                },
                {
                    "start": 700,
                    "end": 703,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 703,
                    "end": 706,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "265351587",
            "title": "Guided Flows for Generative Modeling and Decision Making",
            "text": "Conditional generative modeling paves the way to numerous machine learning applications such as conditional image generation (Dhariwal and Nichol, 2021;Rombach et al., 2022), text-to-speech synthesis (Wang et al., 2023;Le et al., 2023), and even solving decision making problems (Chen et al., 2021;Janner et al., 2021Janner et al., , 2022;;Ajay et al., 2022). Models that appear ubiquitously across a variety of application domains are diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020) and flow-based models (Song et al., 2020b;Lipman et al., 2023;Albergo and Vanden-Eijnden, 2022). Majority of this development has been focused around diffusion models, where multiple forms of conditional guidance (Dhariwal and Nichol, 2021;Ho and Preprint. Table 1.1: The data point x1 and the conditioning variables y on which the conditional guidance will be applied, for the three applications settings that we consider. Salimans, 2022) have been introduced to place larger emphasis on the conditional information. While flow models have been shown to be more efficient alternatives than diffusion models (Lipman et al., 2023;Pooladian et al., 2023) in unconditional generation, requiring less computation to sample, their behavior in conditional generation tasks has not been explored as much. It also remains unclear whether conditional guidance can be applied to and help the performance of flow-based models. \n\nIn this work, we study the behavior of Flow Matching models for conditional generation. We introduce Guided Flows, an adaptation of classifier-free guidance (Ho and Salimans, 2022) to Flow Matching models, showing that an analogous modification can be made to the velocity vector fields, including the optimal transport (Lipman et al., 2023) and cosine scheduling (Albergo and Vanden-Eijnden, 2022) flows used by prior works. \n\nWe experimentally validate Guided Flows on a variety of applications, ranging from generative modeling over multiple modalities to offline reinforcement learning (RL), see Table 1.1.",
            "score": 0.4564827473699155,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 2027
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 172,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 219,
                    "end": 235,
                    "matchedPaperCorpusId": "259275061"
                },
                {
                    "start": 279,
                    "end": 298,
                    "matchedPaperCorpusId": "235294299"
                },
                {
                    "start": 298,
                    "end": 317,
                    "matchedPaperCorpusId": "235313679"
                },
                {
                    "start": 453,
                    "end": 482,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 482,
                    "end": 498,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "256358592",
            "title": "PLay: Parametrically Conditioned Layout Generation using Latent Diffusion",
            "text": "Recent advances in Diffusion Models (DMs) (Sohl-Dickstein et al., 2015) have shown promising results in image generation (Song et al., 2020;Ho et al., 2020;Dhariwal & Nichol, 2021), where classifier guidance (Dhariwal & Nichol, 2021) and classifier-free guidance (CFG) (Ho & Salimans, 2022) methods not only improve the generation quality but also enable the possibility of developing conditional diffusion models. Most recent works for textto-image generation use CFG, including GLIDE (Nichol et al., 2021), DALL\u2022E2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022b), and LDMs (Rombach et al., 2022). In addition to text, LDMs explore other conditions such as bounding boxes and semantic maps. We also adopt CFG for the guideline conditions in PLay, as it does not require an extra classifier and leads to the generation of better results. \n\nApplying diffusion models outside of the image domain has also drawn attention from researchers, such as 3D-model (Lin et al., 2022), video (Ho et al., 2022), andmusic (Mittal et al., 2021) generation. (Mittal et al., 2021) converts discrete melody tokens into a continuous latent space, and trains the diffusion model in the latent space. This work inspires us to convert the discrete layout elements, composed by concatenating different types of tokens including their class and coordinates, to a continuous domain for the diffusion process. A concurrent work, (Strudel et al., 2022), applies the same idea for language generation. We also follow LDMs (Rombach et al., 2022) to add a small KLpenalty to ensure high layout reconstruction quality and avoid arbitrarily large variance in the latent space. \n\nSeveral recent works explore methods to further control the diffusion process.",
            "score": 0.45553308443542206,
            "section_title": "Diffusion Models",
            "char_start_offset": 11323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 71,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 140,
                    "end": 156,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 581,
                    "end": 603,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1500,
                    "end": 1522,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "226965131",
            "title": "Conditioned Natural Language Generation using only Unconditioned Language Model: An Exploration",
            "text": "We have witnessed tremendous progresses in natural language generation (NLG) with large-scale Transformer (Vaswani et al., 2017) based language models, such as the GPT-2 (Radford et al., 2019). A natural question to raise beyond language modeling is, how can we have more fine-grained control over these powerful models? Specifically, we would desire a language model to generate text centers on some user-defined conditions 1 . We call this kind of language model a conditioned NLG. One could easily imagine the commercial benefits of employing a conditioned NLG in our every day products, such as search. To obtain a conditioned NLG, a naiive approach would be reformulate the original language modeling objectives and train the whole model from scratch (Keskar et al., 2019). However, doing so requires tons of labeled text with conditions, which is not always available in most applications. Recently, PPLM (Dathathri et al., 2020) proposed to take advantage of the pretrained GPT-2 without any further retraining of the language model itself. 1 Interchangeable with attributes This is done by having another pretrained 'guidance model', such as a Bag of Word (BOW) or an attribute classifier (AC), to guide the latent state of GPT-2 toward generating more relevant tokens. Witnessing the upside of this approach, we further explored simple, flexible, and effective approaches to conditioned NLG, that encompasses these desired qualities: \n\n\u2022 Simple: only an unconditioned language model is needed. Does not require any additional training data (Keskar et al., 2019) or additional pretrained 'guidance models' (Dathathri et al., 2020). \n\n\u2022 Flexible: the model should be able to model any combination of conditions with any weighting. This is not the case in previous work. \n\n\u2022 Effective: the generated text is fluent and highly-relevant to the given attribute, by objective and human evaluation. \n\nWe propose four methods in total. Three of them are direct modeling of p(x|c) by modification of token embedding or hidden states. The other one models p(x)p(c|x) by the consideration of next-token distribution.",
            "score": 0.45499152740212245,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 128,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 911,
                    "end": 935,
                    "matchedPaperCorpusId": "208617790"
                },
                {
                    "start": 1614,
                    "end": 1638,
                    "matchedPaperCorpusId": "208617790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273193359375
        },
        {
            "corpus_id": "271161847",
            "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees",
            "text": "These out-of-distribution properties generally involve extreme range of values. Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) is a technique to improve conditioning fidelity; we found CFG useful for in-distribution properties, but problematic for some out-of-distribution conditioning values, especially for extreme values, resulting in poor generative efficiency (% of valid, unique, and novel molecules) and conditioning fidelity. Since guidance can still be beneficial to some extreme conditioning values, we propose a solution: random guidance with best-of-k self-filtering (described further below). \n\nIn this work , we tackle any-property-conditional molecule generation with self-criticism using STGG. In doing so, we make the following contributions: \n\n1. Mixed-data Property-Conditioning: We use an MLP on standardized continuous features and embeddings on categorical features while randomly masking some properties during training, allowing conditional generation on any number of properties (0, 1, 2, or all) and the use of Classifier-Free Guidance (CFG) for improved performance. 2. Improved Transformer Architecture: We use Flash-Attention, no bias terms, RMSProp, rotary embeddings, the SwiGLU activation, and better hyperparameters. 3. Improved Spanning-Tree: We extend STGG to 1) allow compound structures with a new token and masking conditions, 2) prevent incomplete samples through special masking when there are too many opened branches, 3) prevent ring overflow, 4) randomize the order of the graph during training for better generalization, and 5) automatically calculate valency and adapt the token vocabulary based on the dataset. 4. Auxiliary Property Prediction objective: The objective improves conditioning fidelity and enables out-of-the-box self-filtering of molecules with incorrect properties. 5. Random Guidance for Extreme Value Conditioning: Classifier-free guidance uses guidance w > 1 to improve performance, but this can fail when conditioning on extreme values (which are needed to generate molecules with out-of-distribution properties). We propose using random guidance (w \u223c U(0.5, 2)) with best-of-k filtering as a solution. 6.",
            "score": 0.45489428232248447,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2177
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "276422090",
            "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation",
            "text": "A major challenge in text-to-image generation using diffusion models (Yu et al., 2024;Liu et al., 2024;Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022). Models like DALL\u2022E 2 and Stable Diffusion (Rombach et al., 2022) have demonstrated exceptional capabilities in producing diverse and complex images, with promising extensions into text-to-3D generation. \n\nA.2 TEXT-TO-3D GENERATION \n\nRecent advances in text-to-3D generation can be broadly divided into two main approaches. The first approach focuses on directly learning 3D asset distributions from large-scale datasets such as Objaverse (Deitke et al., 2023). Notable models within this category include GET3D (Gao et al., 2022), Point-E (Nichol et al., 2022), Shap-E (Jun & Nichol, 2023), CLAY (Zhang et al., 2024c), and MeshGPT (Siddiqui et al., 2024), all of which leverage extensive 3D data to generate accurate 3D models. \n\nThe second approach relies on 2D priors (Jiang et al., 2023) for generating 3D models. Techniques like score distillation are foundational here, as exemplified by DreamFusion/SJC (Poole et al., 2022;Wang et al., 2023a) and ProlificDreamer (Wang et al., 2024b). \n\nBuilding on these baselines, researchers continue to improve visual quality.",
            "score": 0.4541271362308779,
            "section_title": "A.1 DIFFUSION MODELS FOR TEXT-TO-IMAGE GENERATION",
            "char_start_offset": 23503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 86,
                    "matchedPaperCorpusId": "272722848"
                },
                {
                    "start": 86,
                    "end": 103,
                    "matchedPaperCorpusId": "268819764"
                },
                {
                    "start": 103,
                    "end": 119,
                    "matchedPaperCorpusId": "269625321"
                },
                {
                    "start": 1161,
                    "end": 1182,
                    "matchedPaperCorpusId": "254685588"
                },
                {
                    "start": 1234,
                    "end": 1252,
                    "matchedPaperCorpusId": "252438648"
                },
                {
                    "start": 1319,
                    "end": 1340,
                    "matchedPaperCorpusId": "270619933"
                },
                {
                    "start": 1354,
                    "end": 1377,
                    "matchedPaperCorpusId": "265457242"
                },
                {
                    "start": 1493,
                    "end": 1513,
                    "matchedPaperCorpusId": "265020797"
                },
                {
                    "start": 1652,
                    "end": 1671,
                    "matchedPaperCorpusId": "254125253"
                },
                {
                    "start": 1692,
                    "end": 1712,
                    "matchedPaperCorpusId": "258887357"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "274446026",
            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
            "text": "Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions: \n\nNegative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows: \n\nVariational Score Distillation (VSD) is a powerful framework that utilizes pretrained diffusion-based text-to-image models to enhance text-based generation across both 3D and 2D domains. Originally proposed for text-to-3D tasks [48], VSD leverages diffusion-based score matching to align Neural Radiance Fields (NeRFs) with text prompt, enabling the generation of intricate 3D objects [48]. This approach extends effectively to 2D image synthesis as well, where VSD enables rapid, high-quality text-to-image generation in a single step with models such as SwiftBrush and DMD [7,33,54,55], sidestepping the computational burden of multi-step diffusion methods. The central aim of VSD is to ensures that the renderings of a differentiable generator align with the probability density of plausible images as guided by the 2D diffusion model. To accomplish this, VSD employs a two-teacher approach that uses a fixed pretrained diffusion model \u03f5 \u03c8 and an adaptive LoRA-based teacher \u03f5 \u03d5 . While training, the student model f \u03b8 produces 2D images x0 = f \u03b8 (z, y) using an input noise z \u223c N (0, I) and a text prompt y. Noisy images xt = \u03b1 t x0 + \u03c3 t \u03f5 is then fed into both teacher models. The LoRA teacher model \u03f5 \u03d5 aligns with the student distribution by minimizing a denoising L2 loss on singlestep samples. This arrangement supports robust and adaptive guidance suitable for a variety of generative architectures.",
            "score": 0.45188457308082586,
            "section_title": "Background",
            "char_start_offset": 9869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "258887357"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "258887357"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "271956822"
                },
                {
                    "start": 1186,
                    "end": 1189,
                    "matchedPaperCorpusId": "269982921"
                },
                {
                    "start": 1189,
                    "end": 1192,
                    "matchedPaperCorpusId": "265506768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "262054649",
            "title": "ConsistencyTTA: Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation",
            "text": "Since CFG is crucial to conditional generation quality, we consider three methods for incorporating it into the distilled model. Direct Guidance directly performs CFG on the consistency model output z0 by applying (1). Since this method na\u00efvely extrapolates/interpolates the guided and unguided z0 predictions, it may move the prediction outside the manifold of realistic audio embeddings, resulting in poor generation quality. Fixed Guidance Distillation aims to distill from the diffusion model coupled with CFG using a fixed guidance strength w. \n\nThe training risk function is still (2), but \u1e91n\u22121 is replaced with the estimation after CFG. Specifically, \u1e91n\u22121 becomes solve \u2022 f cfg T (zn, n, ete, w), where the guided teacher output \n\nwith \u2205 denoting the masked language token. Here, w is fixed to the value that optimizes teacher generation (3 for TANGO [1]). \n\nVariable Guidance Distillation mirrors fixed guidance distillation, except that the student U-Net fS takes the CFG strength w as an additional input so that w can be adjusted internally during inference. To add a w-encoding condition branch to fS, we use Fourier encoding for w following [30] and merge the w embedding into fS similarly as the time step embedding. During distillation, each training iteration samples a random guidance strength w via the uniform distribution supported on [0, 6). The latter two methods are related to yet distinct from twostage PD [30], with more details discussed in Appendix B.2.",
            "score": 0.45184336357244337,
            "section_title": "CFG-Aware Consistency Distillation",
            "char_start_offset": 12008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1481
                }
            ],
            "ref_mentions": [
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "252762155"
                },
                {
                    "start": 1431,
                    "end": 1435,
                    "matchedPaperCorpusId": "252762155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42724609375
        },
        {
            "corpus_id": "276647173",
            "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
            "text": "For conditional generation, classifier-free guidance (CFG) is typically employed [29,39] to enhance sample quality. This entails performing two neural function evaluations (NFEs) (or one NFE with twice the batch size) to compute predictions with and without the conditioning c, i.e. \u03f5 \u03b8 (x t\u22121 |x t , c) and \u03f5 \u03b8 (x t\u22121 |x t , \u2205). Sampling can Figure 6. Left: As the weak model is used more extensively during generation, compute benefits increase, but at the cost of some performance degradation. Middle: The optimal CFG scale varies depending on the extent to which the weak model is used. Each line corresponds to an inference scheduler that applies the weak model for a different proportion of denoising steps. Right: Benefits from our inference scheduler are orthogonal to performing a smaller overall number of diffusion steps. We plot FID for different overall number of steps T and different number of weak steps Tweak, using in every case the DDPM scheduler. \n\nthen take place as \u03f5 \n\n, where s cfg is the guidance scale. Recent work [53] has shown that using a smaller or less welltrained version of the model rather than an unconditional model can lead to better guidance signal [1,83]. We adapt these findings in our setting, leading to better generation quality without the need to train or deploy additional models. For each denoising step, given a patch size used for the conditional p cond and a patch size used for guidance p uncond , we compute: \n\nIn this setup, we use the powerful model for the conditional prediction and leverage the weak model's output as guidance. Unlike traditional approaches, our method applies guidance based on the conditional prediction from the weak model. Our guidance scheme requires performing inference using both the weak (for the unconditional) and the powerful (for the conditional) model, for some denoising steps. We show in Fig. 12 (appendix) how this can be efficiently implemented, making use of packing [23]. Depending on the guidance signal used, optimum generation quality can vary with respect to the guidance scale s cfg . This will become more apparent in the following experiments.",
            "score": 0.4517876530520084,
            "section_title": "Generation Guidance",
            "char_start_offset": 17417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 85,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1961,
                    "end": 1965,
                    "matchedPaperCorpusId": "259837358"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "270923987",
            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
            "text": "Classifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null condition. There is also no clear extension of CFG to unconditional models. In this paper, we revisit the core principles of CFG and introduce a new method, independent condition guidance (ICG), which provides the benefits of CFG without the need for any special training procedures. Our approach streamlines the training process of conditional diffusion models and can also be applied during inference on any pre-trained conditional model. Additionally, by leveraging the time-step information encoded in all diffusion networks, we propose an extension of CFG, called time-step guidance (TSG), which can be applied to any diffusion model, including unconditional ones. Our guidance techniques are easy to implement and have the same sampling cost as CFG. Through extensive experiments, we demonstrate that ICG matches the performance of standard CFG across various conditional diffusion models. Moreover, we show that TSG improves generation quality in a manner similar to CFG, without relying on any conditional information.",
            "score": 0.4511391266451122,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "258823370",
            "title": "DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion",
            "text": "Diffusion-LM (Li et al., 2022) proposes continuous diffusion on the embedding space for text generation. In the forward process, an embedding step is designed to introduce a Markov transition from discrete words w to x 0 that is parametrized by q \u03c6 (x 0 |w) = N (EMB(w), \u03c3 0 I). In the reverse process, a trainable rounding step is added and parametrized by p \u03b8 (w 1), the training objective is modified as: \n\n(2) \n\nClassifier-free Guidance Extending the guidance method proposed by Dhariwal and Nichol (2021), semantic diffusion guidance (SDG) (Liu et al., 2021) allows fine-grained and continuous control of model class, including either language or image guidance, or both. Furthermore, a classifierfree guidance method is proposed that is more effective at controlling generation (Ho and Salimans, 2022;Ramesh et al., 2022). Let unconditional denoising diffusion model p \u03b8 (x) be parameterized through a score estimator \u03b8 (x t , t) and the conditional model p \u03b8 (x|c) be parameterized through \u03b8 (x t , t, c). These two models can be learned via a single neural network. Precisely, a conditional diffusion model p \u03b8 (x|c) is trained on paired data (x, c), where the conditioning information c is discarded periodically and randomly, so that the model knows how to generate unconditionally as well, i.e. \u03b8 (x t , t) = \u03b8 (x t , t, c = \u2205). \n\nIn this paper, we focus on the sequence-tosequence text generation tasks which produce a target sequence w x = {w x 1 , ..., w x n } conditioning on the source sequence w c = {w c 1 , ..., w c m }. Different from Ho and Salimans (2022), conditional information is involved all the time and not discarded, which has been proved effective in Gong et al. (2022). Thus the training objective becomes: \n\n(3)",
            "score": 0.4501122630735946,
            "section_title": "Continuous Diffusion on Embedding Space",
            "char_start_offset": 9408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1744
                }
            ],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 509,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "268032561",
            "title": "Label-Noise Robust Diffusion Models",
            "text": "Conditional diffusion models have also been developed in order to generate samples that match a desired condition y. For the conditional models, we need to approximate the conditional score \u2207 xt log p t (x t |y), so we additionally take a condition input to a score network (Dhariwal & Nichol, 2021;Karras et al., 2022). Additionally, there are some approaches to guide conditional generation. One approach decomposes the conditional score using Bayes' theorem, and estimates the gradient of the log posterior probability of label using an auxiliary classifier: \n\nThis is known as the classifier guidance method (Dhariwal & Nichol, 2021;Chao et al., 2022;Kim et al., 2023). To amplify the effect of the condition, a positive scaling factor can be multiplied to the classifier gradient term. \n\nOn the other hand, the classifier-free guidance method (Ho & Salimans, 2021) is also proposed, which is a way to achieve the same effect as the classifier guidance without using a classifier. This method utilizes the score network, which produces both unconditional and conditional scores by introducing an auxiliary class for the unconditional score. Then, they use a linear combination of the unconditional score s \u03b8 (x t , t) and the conditional score s \u03b8 (x t , y, t) with \u03b1 > 0 for sampling. \n\ns CFG (x t , y, t) := (1 + \u03b1)s \u03b8 (x t , y, t) \u2212 \u03b1s \u03b8 (x t , t). \n\n(43) \n\nConditional diffusion models have been used in various applications by using conditions as labels (Kong et al., 2021;Meng et al., 2022a), text (Nichol et al., 2022;Rombach et al., 2022), or latent representations (Kim et al., 2022b;Preechakul et al., 2022).",
            "score": 0.4484925388779022,
            "section_title": "B.2 CONDITIONAL DIFFUSION MODELS",
            "char_start_offset": 34947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 299,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 299,
                    "end": 319,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 612,
                    "end": 637,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "247763065"
                },
                {
                    "start": 655,
                    "end": 672,
                    "matchedPaperCorpusId": "254096299"
                },
                {
                    "start": 1529,
                    "end": 1550,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1597,
                    "end": 1621,
                    "matchedPaperCorpusId": "244729224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "274422874",
            "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
            "text": "Classifier-Free Guidance (CFG) [10] uses Bayes' rule to replace a classifier-guided score with a linear combination of conditional and unconditional score estimates: \n\nCFG jointly trains the unconditional model \u03f5 \u03b8 (x t |\u03d5) and the conditional model \u03f5 \u03b8 (x t |c) (= \u03f5 \u03b8 (x t )) within a single model by setting the condition c to a null token \u03d5. Using this guided denoising process, the model better captures the conditions and often generates high-fidelity outputs.",
            "score": 0.4483924175418692,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 7688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "261064777",
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "text": "CLMs are designed to predict language sequence probabilities in a causal manner, while PLMs can be fine-tuned for causal sequence modeling and downstream applications. While the article does not delve into the application of large models in the field of software engineering, it offers a brilliant overview of the current state and development progress of large models, and clearly points out future research directions for these models. \n\nAdditionally, some research work has reviewed and empirically analyzed the application of LLM in a specific field or particular tasks. \n\nLi et al. (Li et al., 2022a) provides an overview of representative research achievements in text generation based on PLMs, reviews various evaluation metrics, open-source libraries, and common applications, with the aim to assist practitioners in assessing, selecting, and utilizing appropriate PLMs, and proposes some future research directions. Yang et al. (Yang et al., 2023b) investigates the application of Transformer-based PLMs for the Controllable Text Generation (CTG) task, summarizing typical applications, key methodologies, and evaluation systems of PLMs in CTG. Min et al. (Min et al., 2023) explores three trending paradigms of using pre-trained language models for NLP, They are Pre-train then Fine-tune, Prompt-based Learning, and NLP as Text Generation. Simultaneously, the paper mentions that their theoretical understanding of these paradigms is preliminary. \n\nWang et al. (Wang et al., 2023a) primarily summarizes the latest progress of PLMs in the biomedical field and their applications in downstream biomedical tasks, and discusses the development trends and directions. In response to the existing LLM-based recommendation systems, Wu et al. (Wu et al., 2023) proposes a classification that divides these models into two major paradigms, namely, Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec). The paper systematically reviews and analyzes the existing LLM-based recommendation systems in these two paradigms and discusses their methods and performance. \n\nOther research mainly investigates and analyzes the shortcomings of LLMs in their applications.",
            "score": 0.44753822186453807,
            "section_title": "Other works on reviewing LLM",
            "char_start_offset": 53063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2107
                },
                {
                    "start": 2110,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 1154,
                    "end": 1183,
                    "matchedPaperCorpusId": "240420063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10797119140625
        },
        {
            "corpus_id": "267938436",
            "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2022) is a commonly used conditional generation method in DPMs. Given a condition y and a pre-trained text-to-image DPM with the noise prediction neural network \u03d5, CFG generates images via \u03b5(x t ; y, t) = \u03f5 \u03d5 (x t ; y, t) + s(\u03f5 \u03d5 (x t ; y, t) \u2212 \u03f5 \u03d5 (x t ; t)), where s > 0 is the guidance scale. \n\nScore Distillation Score distillation sampling (SDS) is an optimization mechanism to distill the rich knowledge from pre-trained text-to-image generation diffusion models (Poole et al., 2022;Luo et al., 2023). SDS allows optimizing differentiable generators, and it has been widely explored in text-to-3D generation (Wang et al., 2023c;a), and image editing tasks (Hertz et al., 2023;Kim et al., 2023a). Given a pre-trained text-to-image LDM with the noise prediction neural network \u03d5 with noise \u03f5 \u223c N (0, 1), SDS optimizes a group of parameters \u03b8 by: \n\nwhere g(\u03b8) can be any differentiable function.",
            "score": 0.44752868566789483,
            "section_title": "Preliminary",
            "char_start_offset": 8223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 945
                }
            ],
            "ref_mentions": [
                {
                    "start": 709,
                    "end": 729,
                    "matchedPaperCorpusId": "258170014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "264406056",
            "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
            "text": "While some research focuses on selfconsistent prompts by regularizing LMs to generate consistent outputs across different prompts (Zhou et al., 2022;Zeng and Gao, 2023;Wan et al., 2023), the focus is different and orthogonal to ours which proposes to self-adapt smaller LMs for specific target tasks associated with external knowledge. \n\nSelf-adaption for Extractive QAs Several previous studies explored the self-adaptive capabilities of the traditional pre-trained language models, but they mainly addressed classification problems under the extractive setting (Li et al., 2020;Shakeri et al., 2020;Banerjee et al., 2021;Wang et al., 2021b;Ye et al., 2022). However, we focus on the generative setting, which makes a large difference due to fundamentally different objectives. To be specific, in an extractive setting, self-adaptation is based on probabilities, while in a generative setting, self-adaptation is done using generated text. \n\nIn situations where filtering is further applied, filtering is based on probabilities in the extractive setting, whereas, it is done using the generated text in the generative setting. Furthermore, Li et al. (2020) and Shakeri et al. (2020) assume an unsupervised QA setting, where the context-questionanswer triplets are not available, thus requiring an additional query-generation module. Such a pair generation approach is different and orthogonal to ours, since we aim to enhance answer generation directly from the provided context and question. \n\n3 Method",
            "score": 0.4468859671241331,
            "section_title": "Related Work",
            "char_start_offset": 5486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1504
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 149,
                    "matchedPaperCorpusId": "248496641"
                },
                {
                    "start": 149,
                    "end": 168,
                    "matchedPaperCorpusId": "259076388"
                },
                {
                    "start": 563,
                    "end": 580,
                    "matchedPaperCorpusId": "218516945"
                },
                {
                    "start": 580,
                    "end": 601,
                    "matchedPaperCorpusId": "222310116"
                },
                {
                    "start": 601,
                    "end": 623,
                    "matchedPaperCorpusId": "232307160"
                },
                {
                    "start": 623,
                    "end": 642,
                    "matchedPaperCorpusId": "237485466"
                },
                {
                    "start": 642,
                    "end": 658,
                    "matchedPaperCorpusId": "256631130"
                },
                {
                    "start": 1141,
                    "end": 1157,
                    "matchedPaperCorpusId": "218516945"
                },
                {
                    "start": 1162,
                    "end": 1183,
                    "matchedPaperCorpusId": "222310116"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.150634765625
        },
        {
            "corpus_id": "267750521",
            "title": "Enhancing Large Language Models for Text-to-Testcase Generation",
            "text": "A language model (LM) is a statistical model trained to predict the next word in a sequence [34], e.g., for the sequence \"Paris is the capital of\", it will predict with a very high likelihood that \"France\" is the next word in the sequence. Large language models are LMs with a substantially large number of weights and parameters and a complex training architecture trained to perform various downstream NLP tasks, e.g., text generation, question answering, and text classification [35]. The recent advancements in LLMs, most notably with chatbot platforms built on LLMs, e.g., OpenAI's Chat-GPT, have led to several applications in software engineering (SE) [36,37]. These include, among others, requirements engineering automation tasks [38], code generation and completion [39], software vulnerability [40], test generation [41], and software maintenance [36]. \n\nThe use of LLMs for generating test cases from text (text-totestcase) is a relatively new application. It involves LLMs understanding a natural language (NL) description of the software method and then generating appropriate test cases that can be used to verify that the software meets its specifications. While there is interest in this area, it still remains underexplored due to the complexity of accurately interpreting descriptions and generating meaningful and comprehensive test cases. However, as LLMs become more advanced, their ability to perform such specialized tasks will likely improve, making text-to-testcase generation a promising area for future research and application in software engineering. Prompting. In generative AI models (built on LLMs), the input is provided as a natural language instruction to elicit a desired response or output [42]. The input is known as a \"prompt\". How well a prompt is crafted determines the quality and relevance of the AI's output. There are different prompting strategies, and one typically needs to experiment with various prompts to elicit accurate responses or outputs from the models [36]. Fine-tuning In the context of Generative AI and ML refers to taking a pre-trained model and further training it on a specific, usually smaller, dataset to adapt it to a particular task or set of tasks.",
            "score": 0.44638338445331527,
            "section_title": "Large Language Models (LLMs)",
            "char_start_offset": 9474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 805,
                    "end": 809,
                    "matchedPaperCorpusId": "264146048"
                },
                {
                    "start": 1728,
                    "end": 1732,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134033203125
        },
        {
            "corpus_id": "277435154",
            "title": "Semantix: An Energy Guided Sampler for Semantic Style Transfer",
            "text": "Energy Function Diffusion models can be viewed as score-based generative models (Song et al., 2020b). In classifier guidance (Dhariwal and Nichol, 2021;Ho and Salimans, 2022), the gradient of the classifier \u2207 xt log p \u03d5 (y|x t ) is used to influence generation. From the perspective of score functions, the condition y can be integrated within a conditional probability q(x t |y) via an auxiliary score function and expressed as such: \n\nwhere the first term is viewed as the unconditional denoiser \u03f5 \u03b8 (x t ; t, \u2205), and the second term can be interpreted as the gradient of the energy function: E(x t ; t, y) = log q(y|x t ). Alternatively, classifier-free guidance (CFG) (Ho and Salimans, 2022) can also be used, expressed as: \n\nwhere \u03c9 is the classifier-free guidance strength. In fact, the diffusion model can also be interpreted as an energy-based model (Liu et al., 2022), guided by any energy function. One can design an energy function beyond class-based conditioning and use it for guidance (Zhao et al., 2022;Epstein et al., 2023;Bansal et al., 2023;Chen et al., 2024;Yu et al., 2023;Voynov et al., 2023a;Kwon and Ye, 2022).",
            "score": 0.4454655120708188,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 11113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1133
                }
            ],
            "ref_mentions": [
                {
                    "start": 858,
                    "end": 876,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 999,
                    "end": 1018,
                    "matchedPaperCorpusId": "250526607"
                },
                {
                    "start": 1018,
                    "end": 1039,
                    "matchedPaperCorpusId": "258999106"
                },
                {
                    "start": 1039,
                    "end": 1059,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1059,
                    "end": 1077,
                    "matchedPaperCorpusId": "258041377"
                },
                {
                    "start": 1093,
                    "end": 1114,
                    "matchedPaperCorpusId": "254018130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "266690861",
            "title": "Building Efficient Universal Classifiers with Natural Language Inference",
            "text": "Over the past year, generative models have taken both academia and public attention by storm. The main appeal of text generation is that it is so universal, that almost any other text-related task can be reformulated as a text generation task (Radford et al., 2019;Raffel et al., 2020). Especially when text generators are massively scaled up and tuned on human instructions, they acquire impressive capabilities to generalise to new tasks without requiring task-specific fine-tuning (Sanh et al., 2022;Ouyang et al., 2022;Chung et al., 2022;OpenAI, 2023;Touvron et al., 2023). Since the utility of these generative Large Language Models (LLMs) has become evident, large amounts of intellectual, financial and energy resources are being invested in improving and scaling generative LLMs. \n\nGiven that the resource requirements for training and deploying generative LLMs are prohibitive for many researchers and practitioners, this paper investigates other types of universal models, that make a different trade-off between resource requirements and universality. The literature has developed several other universal tasks that cannot solve generative tasks (summarization, translation etc.), but can solve any classification task with smaller size and performance competitive with generative LLMs (Xu et al., 2023;Schick and Sch\u00fctze, 2021b). \n\nThe principle of universal classifiers is similar to generative models: A model is trained on a universal task, and a form of instruction or prompt enable it to generalize to unseen classification tasks. While several efficient approaches to universal classification exist (Schick and Sch\u00fctze, 2021a;Xia et al., 2022;Yao et al., 2022;Xu et al., 2023;Bragg et al., 2021;Ma et al., 2021;Sun et al., 2022), this paper focuses on guidance for one approach: Natural Language Inference.",
            "score": 0.4449046133247563,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1824
                }
            ],
            "ref_mentions": [
                {
                    "start": 1297,
                    "end": 1314,
                    "matchedPaperCorpusId": "253523108"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "266362385",
            "title": "Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method",
            "text": "Diffusion models are powerful generative models that exhibit impressive performances across different modality generation, including image [5,11,12], video [22,34,39] and audio generation [16]. Guided sampling, including classifier guidance [5] and classifier-free guidance [11], has been widely used in diffusion models to realize controllable generation, such as text-to-image generation [29], imageto-image generation [24,28], and ControlNet [37]. Guided sampling controls the outputs of generative models by conditioning on various types of signals, such as descriptive text, class labels, and images. \n\nA line of guidance methods involves task-specific training of diffusion models using paired data, i.e., targets and conditions. For instance, classifier guidance [5] combines the score estimation of diffusion models with the gradients of the image classifiers to direct the generation process to produce images corresponding to a particular class. In this way, several image classifiers need to be trained on the noisy states of intermediate generation steps of diffusion models. Alternatively, classifier-free guidance [11] directly trains a new score estimator with conditions and uses a linear combination of conditional and unconditional score estimators for sampling. Although this line of methods can effectively guide diffusion models to generate data satisfying certain properties, they are not sufficiently flexible to adapt to any type of guiding due to the cost of training and the feasibility of collecting paired data. \n\nTo this end, another line of training-free guidance methods has been explored [2,14,36]. In training-free guided sampling, at a certain sampling step t, the guidance function is usually constructed as the gradients of the loss function obtained by the off-the-shelf pre-trained models, such as face-ID detection or aesthetic evaluation models. More specifically, the guidance gradients are computed based on the one-step approximation of denoised images from the noisy samples at certain steps t. Then, gradients are added to corresponding sampling steps as guidance to direct the generation process to the desired results. This line of methods offers greater flexibility by allowing the diffusion models to adapt to a broad spectrum of guidance.",
            "score": 0.4442564303458597,
            "section_title": "Introduction",
            "char_start_offset": 1371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "256390486"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "256616002"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1626,
                    "end": 1629,
                    "matchedPaperCorpusId": "257622962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8857421875
        },
        {
            "corpus_id": "271334192",
            "title": "Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning",
            "text": "Classifier-free guidance (CFG) [28] is a simple yet effective inference-time method to enhance the input text condition's influence, which is directly linked to our transferability goal.As mentioned in Sec.3.1, diffusion models can predict both the unconditioned score \u2207 x log p(x) and the conditioned score \u2207 x log p(x | y).In addition, by Bayes' rule, we know that p(x | y) \u221d p(x)p(y | x).As the goal is the amplify y's influence, we define:\n\nwhere \u03bb is a knob, named CFG scale, that controls the strength of y.Taking (\u2207 x log) on both sides gives us:\n\n(3) Meanwhile, we can rearrange the Bayes' rule terms to get:\n\nNote that a diffusion model can predict both RHS terms.Plugging Eqn.(4) into Eqn.(3), CFG performs\n\nat every inference iteration, where \u2207 x log p(x) is obtained by inputting an empty string as y.",
            "score": 0.4439982908604859,
            "section_title": "Classifier-free Guidance",
            "char_start_offset": 8757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 443
                },
                {
                    "start": 445,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 553
                },
                {
                    "start": 555,
                    "end": 616
                },
                {
                    "start": 618,
                    "end": 673
                },
                {
                    "start": 673,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 699
                },
                {
                    "start": 699,
                    "end": 716
                },
                {
                    "start": 718,
                    "end": 813
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "270123253",
            "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting",
            "text": "The primary goal of our method is to forget specific concepts from diffusion models using only the model's existing knowledge in the inference stage, without requiring any additional external data.Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2024).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process.",
            "score": 0.4431328180252117,
            "section_title": "Forgetting Concept for Conditionings",
            "char_start_offset": 14821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 470
                },
                {
                    "start": 470,
                    "end": 659
                },
                {
                    "start": 661,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 968
                },
                {
                    "start": 970,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1308
                }
            ],
            "ref_mentions": [
                {
                    "start": 634,
                    "end": 658,
                    "matchedPaperCorpusId": "261276613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "262045187",
            "title": "Enhance audio generation controllability through representation similarity regularization",
            "text": "However, the cross entropy loss in language model lacks explicit mechanism to enforce the audio token prediction align with the provided text conditions. Furthermore, the correlation between text and audio gets even loosen as the classifierfree guidance (CFG) method [26,6,8] is used in the training to regulate the balance between sample quality and diversity. Employing CFG involves training the language model both conditionally and unconditionally. Similar to AudioGen [6], 10% of the training samples have their accompanying text omitted during language model training. In unconditional situation, the loss is simply \n\nIn this work, the proposed representation regularization strengthens the correlation between audio representation and text representation while still maintains the effects of CFG method to train the language model unconditionally on text. Rather than directly mapping the text and audio representations to the same space and maximizing the similarity between audio and text as CLAP [29], we propose to minimize discrepancies in audio and text similarity compared to other samples within the same training batch as follows: \n\nHere In this study, the proposed representation regularization is exclusively applied during the CFG phase. The complete model training loss is defined as follows: \n\nHere, \u03bb represents the weighting factor for the representation regularization. Note that representation regularization is only employed during regular training steps when CFG is in use. We also conducted experiments involving representation regularization in non-CFG scenarios; however, these experiments did not yield improvements in objective metrics. We believe the degradation may be attributed to the fact that representation regularization has the potential to hinder language model learning by copying the text representation from crossattention as the audio representation in non-CFG.",
            "score": 0.4427464481973263,
            "section_title": "Representation regularization",
            "char_start_offset": 6333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1907
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "258947469",
            "title": "Prompt Evolution for Generative AI: A Classifier-Guided Approach",
            "text": "There exists alternative techniques in the literature that aims to satisfy user preferences in generative AI, as described in what follows.\n\n1) Fine Tuning [20]: updates pre-trained model weights at selective layers with respect to user preferences.\n\n2) Prompt Tuning [18]: uses learnable \"soft prompts\" to condition frozen pre-trained language model for text-to-image generation task.\n\n3) Prompt Engineering [5]: is a practice where users modify the prompts and generative model hyperparameters.\n\n4) In-Context Learning [17]: is an emergent behavior in large language models where a frozen language model performs a new task by conditioning on a few examples of the task.\n\nThe aforementioned techniques often generate a single output that satisfies user preferences to some extent. However, such techniques involve implicit trade-offs between producing outputs of high-fidelity, remaining faithful to the prompt, and generating diverse outputs. In contrast, prompt evolution generates multiple outputs in a single run to provide a wellbalanced approach that addresses all these trade-offs simultaneously (as per Definition 1).",
            "score": 0.44273575028035994,
            "section_title": "A. Relationship to Background Work",
            "char_start_offset": 2891,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17431640625
        },
        {
            "corpus_id": "273653942",
            "title": "TabDiff: a Mixed-type Diffusion Model for Tabular Data Generation",
            "text": "TABDIFF can also be extended as a conditional generative model, which is important in many tasks such as missing value imputation. Let y = {[y num , y cat ]} be the collection of provided properties in tabular data, containing both categorical and numerical features, and let x denote the missing interest features in this section. Imputation means we want to predict x = {[x num , x cat ]} conditioned on y. \n\nTABDIFF can be freely extended to conditional generation by only conducting denoising sampling for x t , while keeping other given features y t fixed as y. \n\nPrevious works on diffusion models (Dhariwal & Nichol, 2021) show that conditional generation quality can be further improved with a guidance classifier/regressor p(y | x). However, training the guidance classifier becomes challenging when x is a high-dimensional discrete object, and existing methods typically handle this by relaxing x as continuous (Vignac et al., 2023). Inspired by the classifier-free guidance (CFG) framework (Ho & Salimans, 2022) developed for continuous diffusion, we propose a unified CFG framework that eliminates the need for a classifier and handles multi-modal x and y effectively. The guided conditional sample distribution is given by p\u03b8 (x t |y) \u221d p \u03b8 (x t |y)p \u03b8 (y|x t ) \u03c9 , where \u03c9 > 0 controls strength of the guidance. Applying Bayes' Rule, we get \n\nWe drop p(y) for it does no depend on \u03b8. Taking the logarithm of the probabilities, we obtain, log p\u03b8 ( \n\nwhich implies the following changes in the sampling steps. For the numerical features, \u00b5 num \u03b8 (x t , t) is replaced by the interpolation of the conditional and unconditional estimates (Ho & Salimans, 2022):",
            "score": 0.44223173137486427,
            "section_title": "CLASSIFIER-FREE GUIDANCE CONDITIONAL GENERATION",
            "char_start_offset": 15544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1670
                }
            ],
            "ref_mentions": [
                {
                    "start": 604,
                    "end": 629,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 921,
                    "end": 942,
                    "matchedPaperCorpusId": "252595881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "232185408",
            "title": "Topical Language Generation using Transformers",
            "text": "The advent of transformer language models (Vaswani et al. (2017)) has greatly improved the performance of natural language processing (NLP) tasks such as text generation, which is an essential component in many downstream NLP applications. The use of transformer models like GPT-2 (Radford et al. (2019)) and GPT-3 (Brown et al. (2020)) to generate and continue text primed with an arbitrary input prompt has led to coherent and realistic texts. More strongly grounded applications such as translation, image captioning, and summarization that have input/outputs are less problematic in text generation with current decoding algorithms (Li et al. (2020)). However, in open-ended tasks (e.g., dialog generation or language modeling (LM)), failures such as repetitive text, unnatural topic switching, and contradictions are often observed (Holtzman et al. (2020)). Exploring new ways to confront these weaknesses is an active area of research in natural language processing. In this paper, we address the problem of topical language generation which plays a key role in generating long, coherent, and realistic texts. The results can also be used to improve the downstream open-ended text generation tasks such as dialog generation or predictive response suggestion (Kannan et al. (2016)). \n\nDespite the fact that pre-trained LMs store a vast amount of knowledge about the world (Petroni et al. (2019)), the real potential of them has not been harnessed yet for controlled text generation. This means that even though there is currently a lot of knowledge about the world in our pretrained LMs, we are still unable to control the topical attributes of generated texts. Controlling the text generation to incorporate knowledge about specific topics usually requires major changes to the LM architecture, loss function, or retraining the whole models with annotated data. \n\nLanguage modeling calculates the probability distribution P(x) of a sequence of tokens x for a given input text. On the other hand, topical language generation which is a type of controlled \u2022 Introduction of topical language generation (TLG) which consists of generating text conditioned on a specific chosen topic.",
            "score": 0.44222976461771446,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 64,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 315,
                    "end": 335,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 636,
                    "end": 653,
                    "matchedPaperCorpusId": "207853191"
                },
                {
                    "start": 837,
                    "end": 860,
                    "matchedPaperCorpusId": "127986954"
                },
                {
                    "start": 1264,
                    "end": 1285,
                    "matchedPaperCorpusId": "5759934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14453125
        },
        {
            "corpus_id": "270869763",
            "title": "Diffusion Models and Representation Learning: A Survey",
            "text": "Recent improvements in image generation results have largely been driven by improved guidance approaches.The ability to control generation by passing user-defined conditions is an important property of generative models, and guidance describes the modulation of the strength of the conditioning signal within the model.Conditioning signals can have a wide range of modalities, ranging from class labels, to text embeddings to other images.A simple method to pass spatial conditioning signals to diffusion models is to simply concatenate the conditioning signal with the denoising targets and then pass the signal through the denoising network [12,75].Another effective approach uses cross-attention mechanisms, where a conditioning signal c is preprocessed by an encoder to an intermediate projection E(c), and then injected into the intermediate layer of the denoising network using cross-attention [76,142].These conditioning approaches alone do not leave the possibility to regulate the strength of the conditioning signal within the model.Diffusion model guidance has recently emerged as an approach to more precisely trade-off generation quality and diversity.\n\nDhariwal and Nichol [42] use classifier guidance, a compute-efficient method leveraging a pre-trained noiserobust classifier to improve sample quality.Classifier guidance is based on the observation that a pre-trained diffusion model can be conditioned using the gradients of a classifier parametrized by \u03d5 outputting p \u03d5 (c|x t , t).The gradients of the log-likelihood of this classifier \u2207 xt log p \u03d5 (c|x t , t) can be used to guide the diffusion process towards generating an image belonging to class label y.The score estimator for p(x|c) can be written as\n\n(11) By using Bayes' theorem, the noise prediction network can then be rewritten to estimate:\n\nwhere the parameter w modulates the strength of the conditioning signal.Classifier guidance is a versatile approach that increases sample quality, but it is heavily reliant on the availability of a noise-robust pre-trained classifier, which in turn relies on the availability of annotated data, which is not available in many applications.\n\nTo address this limitation, Classifier-free guidance (CFG) [67] eliminates the need for a pre-trained classifier.",
            "score": 0.44163850041080344,
            "section_title": "Diffusion Model Guidance",
            "char_start_offset": 15445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1043
                },
                {
                    "start": 1043,
                    "end": 1165
                },
                {
                    "start": 1167,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1501
                },
                {
                    "start": 1501,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1727
                },
                {
                    "start": 1729,
                    "end": 1822
                },
                {
                    "start": 1824,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2163
                },
                {
                    "start": 2165,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "253581703"
                },
                {
                    "start": 647,
                    "end": 650,
                    "matchedPaperCorpusId": "252846258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "272832009",
            "title": "VoiceGuider: Enhancing Out-of-Domain Performance in Parameter-Efficient Speaker-Adaptive Text-to-Speech via Autoguidance",
            "text": "Denoising Diffusion. Diffusion models [2] are generative models that add Gaussian noise to data in multiple steps and learn a denoising process to generate data. In the case of speaker-adaptive TTS, given a text embedding c and a speaker embedding S, the diffusion models are trained to recover the noise \u03f5 t added to the noisy mel-spectrogram X t using the following objective function: \n\nwhere s \u03b8 is a diffusion model and \u03bb t is a predefined noise schedule of GradTTS [7] and t \u223c [0, 1] indicates noise level. Diffusion Guidance. Recently, diffusion models employ classifier-free guidance (CFG) [5] to improve sample quality and the likelihood of given conditions. At each generation step, CFG modifies the model's prediction with an extrapolation between two predictions: \n\nwhere \u03b3 is a guidance scale. CFG pushes away the unconditional distribution to avoid undesired speaker, thereby increasing the likelihood of speaker S.",
            "score": 0.4411578308677975,
            "section_title": "A. Background",
            "char_start_offset": 4049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 929
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "234483016"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "266374764",
            "title": "Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models",
            "text": "Diffusion Models (DMs) [14] exhibit outstanding generative capacities across domains such as images [45], video [15], audio [20], human pose estimation [5], and even cosmological simulations [48]. DMs generate data by sampling a noise instance and iteratively denoising the instance with a neural network. The sequential nature of this denoising operation makes sampling from DMs a slow and expensive process. In particular, the time required to sample from a given DM is a function of (i) the latency of each denoising iteration, and (ii) the total number of denoising steps. \n\nMany practical applications entail \"conditional generation\", where DMs create samples conditioned on specific criteria such as a class, a text, or an image [37]. DMs achieve conditional generation by replacing regular (i.e., unconditional) denoising steps with conditional ones, in which the neural network processes both the input and the condition. While conditional denoising steps provide competitive results, Ho et al. proposed the technique of Classifier-Free Guidance (CFG) [13] to enhance sample quality. CFG enriches the conditional denoising process by leveraging implicit priors of the diffusion model itself. Despite its simplicity, CFG significantly improves sample quality in tasks such as textto-image [8,37,40], image editing [4,35,50], and textto-3D [26,43]. Yet, the benefits of CFG come at the cost of duplicating the Number of Function Evaluations (NFEs), since each denoising iteration requires evaluating the neural network both conditionally and unconditionally. Adding to the problem, neural networks used in practice for DMs max out the parallelization capacity of production-grade GPUs 1 , preventing simultaneous computation of the conditional and unconditional function evaluations. \n\nIn this paper, we improve the efficiency of text-to-image diffusion models that use Classifier-Free Guidance (CFG). Our analysis reveals that not all denoising steps contribute equally to image quality, suggesting that the traditional policy of applying CFG in all steps is sub-optimal.",
            "score": 0.4411440716981926,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1302,
                    "end": 1305,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1346,
                    "end": 1350,
                    "matchedPaperCorpusId": "253708074"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "270923987",
            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
            "text": "The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.This insight leads us to propose independent condition guidance (ICG), a method that replicates the behavior of CFG at inference time without requiring separate training of an unconditional model.Inspired by the above, we also introduce a novel technique to extend classifier-free guidance to a more general setting that includes unconditional generation.This method, which we call timestep guidance (TSG), employs a perturbed version of the time-step embedding in diffusion models to create a guidance signal similar to CFG.Time-step guidance aims to improve the accuracy of denoising at each sampling step by leveraging the time-step information learned by the diffusion model to steer sampling trajectories toward better noise-removal paths.\n\nOur guidance techniques are easy to implement, do not require additional fine-tuning of the underlying diffusion models, and have the same sampling cost as CFG.Through extensive experiments, we empirically verify that: 1) ICG offers performance similar to CFG and can be readily applied to models that are not trained with the CFG objective in mind, such as EDM [18]; and 2) TSG improves output quality in a manner similar to CFG for both conditional and unconditional generation.\n\nThe core contributions of our work are as follows: (i) We revisit the principles of classifier-free guidance and offer an efficient, theoretically motivated method to employ CFG without the need to train an unconditional module, greatly simplifying the training process of conditional diffusion models.(ii) We offer an extension of CFG that is generally applicable to all diffusion models, whether conditional or unconditional.(iii) We demonstrate empirically that our guidance techniques achieve the quality-boosting benefits of CFG across various setups and network architectures.",
            "score": 0.4402696595546802,
            "section_title": "Introduction",
            "char_start_offset": 2195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1387
                },
                {
                    "start": 1389,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1816
                },
                {
                    "start": 1816,
                    "end": 1971
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "261064777",
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "text": "The tremendous potential of LLM has attracted numerous investigations regarding its applications, either within the field of LLM itself or in specific domains. In this section, we will showcase these research efforts and explain their distinctions from our work. Our work focus on systematically investigate, analyze, and compile the research progress of LLM in the context of software engineering tasks. \n\nZhao et al. (Zhao et al., 2023b) is a detailed article that introduces the background, development history, technical roadmap, and latest advancements  Gozalo-Brizuela et al. (Gozalo-Brizuela and Garrido-Merchan, 2023), on the other hand, classifies generative AI models based on their input and output formats, dividing them into nine categories as shown in Table 11. The paper demonstrates the capabilities of generative AI models through these classifications. While the author believes these models possess significant creativity and potential, he also acknowledges the numerous constraints they face, particularly in terms of data acquisition. Additionally, concerns were raised over the significant computational resource consumption during their training and the time cost of model construction. Despite summarizing the capabilities of some generative AI models, the paper does not focus on LLM, nor does it discuss the performance of LLM in specific tasks, including software engineering tasks. \n\nLiu et al. (Liu et al., 2023e) provides a comprehensive review of ChatGPT and GPT4, highlighting their potential applications and contributions in the field of Natural Language Processing (NLP). Meanwhile, it outlines several potential ethical issues related to the development and use of LLMs, advocating for a focus on addressing these ethical concerns, exploring new applications, and ensuring the responsible use of ChatGPT and GPT-4. Fan et al. (Fan et al., 2023a) claims that they conducted a bibliometric analysis of over 5,000 LLM research papers from 2017 to early 2023, investigating their  Wei et al. (Wei et al., 2023) provides a comprehensive overview of traditional language models (CLMs) and their successors, pre-trained language models (PLMs). CLMs are designed to predict language sequence probabilities in a causal manner, while PLMs can be fine-tuned for causal sequence modeling and downstream applications.",
            "score": 0.4402030243040917,
            "section_title": "Other works on reviewing LLM",
            "char_start_offset": 50890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2340
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046478271484375
        },
        {
            "corpus_id": "259308807",
            "title": "Stay on topic with Classifier-Free Guidance",
            "text": "Controlling the generation usually involves guiding or adding constraints to that semantic representation. In Classifier Guidance [28], an auxiliary classifier P \u03d5 (c|x) is introduced, which guides the sampling from P \u03b8 (x) with the gradients \u03b3\u2207 z P \u03d5 (c|x) to increase the likelihood of c for generation x. This modification results in approximate samples from the distribution: \n\nwhere \u03b3 is called the guidance strength. This guidance results in a reweighting of the density according to the classifier likelihood. For \u03b3 = 0, it reduces to the unconditional generation, while \u03b3 = 1 reduces to the conditional generation. When \u03b3 > 1 then P overemphasizes the conditioning, which as noticed by [28] results in a better inception score at the cost of diversity. This approach has been successfully used in a variety of works [32,41,22] P \u03b8 (w|c) \u03b3 P \u03b8 (w) \u03b3\u22121 (6) \n\nWhile conditioned diffusion models cannot predict unconditioned distributions without extra training, language models handle both P \u03b8 (w|c) and P \u03b8 (w) naturally due to being trained on finite context windows. Being able to drop the prefix c is a natural feature. We can thus sample the next i-th token w i in the logits space: log P \u03b8 (w i |w j<i , c) = log P \u03b8 (w i |w j<i ) + \u03b3 log P \u03b8 (w i |w i<j , c) \u2212 log P \u03b8 (w i |w j<i ) \n\nThis formulation can be extended to accomodate \"negative prompting\", as in Equation 5. Negative prompting as applied in autoregressive LMs will be further addressed in Section 3.4. Now, we will continue on to the next section, where we introduce our experiments. In this section, we will explore the effects of CFG on different variations of prompting.",
            "score": 0.4401356556121656,
            "section_title": "Introduction",
            "char_start_offset": 2079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1649
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "268876460",
            "title": "Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data",
            "text": "In this section, we briefly introduce the DDPMs presented in (Ho et al. 2020) and their extensions to conditional generation with the classifier-free guidance (Ho & Salimans 2022, CFG) on which our study is based.",
            "score": 0.43988572635201406,
            "section_title": "Background",
            "char_start_offset": 13102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "267782902",
            "title": "Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation",
            "text": "Controlled Text Generation Methods for controlled generation can generally be grouped into three main categories: fine-tuning, retraining, and post-processing (Zhang et al., 2023). Postprocessing approaches are the most lightweight and involve applying transformations during decoding, rather than making any adjustments to model weights themselves. Examples of such methods include plug and play, or PPLM (Dathathri et al., 2020)), which uses gradients from an attribute classifier to guide the language model's hidden state; generative discriminators (GeDI) which compute control codes and anti-control codes for all possible next tokens (Krause et al., 2020); and Attribute Alignment, which learns an alignment function (Yu et al., 2021) infuse attribute representations into a pre-trained language model to guide generation. Prefix-tuning (Li and Liang, 2021;Qian et al., 2022) can also guide generation by prepending taskor style-specific \"prefix\" vectors. Retraining (or refactoring) methods involve retraining language models from the ground up on the control task; for example, CTRL (Keskar et al., 2019) retrains a class-conditional language model conditioned on many control codes to guide generations. Another retraining approach is Cev-LM (Moorjani et al., 2024), a prototype-then-edit semi-autoregressive language model that applies edit vectors in the latent space. \n\nOur work falls under the fine-tuning category. Fine-tuning methods adjust parameters of a pretrained LLM toward fulfilling the desired controls. Reinforcement learning (RL) is a common fine-tuning approach for controlled text generation (Zhang et al., 2023), e.g., Gong et al. (2019) use a style classifier model to provide a target style reward, and Upadhyay et al. (2022) use token-level dense rewards and taking the weighted sum of these rewards that were heuristically determined to update the policy.",
            "score": 0.43967030801710544,
            "section_title": "Related Work",
            "char_start_offset": 3467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 179,
                    "matchedPaperCorpusId": "245986550"
                },
                {
                    "start": 406,
                    "end": 430,
                    "matchedPaperCorpusId": "208617790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20751953125
        },
        {
            "corpus_id": "249145348",
            "title": "Classifier-Free Diffusion Guidance",
            "text": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
            "score": 0.43911600783869886,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "269982343",
            "title": "Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation",
            "text": "We use the same unmask policy for image-audio co-generation, where the model generates image and audio token simultaneously in every iteration.\n\nClassifier-free Guidance Classifier-free guidance (CFG) [22] is proved to be a strong tool to achieve trade-off between generation fidelity and (conditional) alignment in diffusion-based conditional generation methods.In our paper, we also deploy CFG to improve the performance.However, unlike existing works that need to do extra unconditional training (typically remove the condition during training of a low ratio), CFG could be deployed to our trained model off-the-shelf, it is similar to UniDiffuser [1] which also uses concatenated embeddings as the input.We posit the reasons why CFG could directly work without extra training are 2-fold.First, the mask ratios (usually larger than 0.5) of two modalities are sampled independently, there is a chance that one modality is almost fully masked.Second, one necessary reason for extra CFG unconditional training in diffusion model is to learn denoising under the null condition (since most methods adopt text as condition) which is not seen during conditional training.While in our method, we have a learnable mask embedding which is put before the transformer decoder during training, which is used as unconditional input.\n\nIn other words, the model already saw the unconditional input (the learnable mask embedding) during training.The off-the-shelf CFG (for image2audio generation) is achieved as below for each unmasking iteration, which operates in the prediction space output by the transformer.\n\nwhere the \u00e3i means the input audio tokens in the current iteration, v i is the guided image token embedding without being masked, M is a full mask leading to unconditional input, and hyperparameter s is the CFG guidance factor to control the balance.Note there are actually 2 forward passes to transformer every iteration, where the two inputs are audio tokens concatenated with guided visual tokens and fully masked visual tokens, illustration of CFG during inference could be found in the appendix A.3.",
            "score": 0.43857853010057224,
            "section_title": "Inference",
            "char_start_offset": 16270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 145,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1321
                },
                {
                    "start": 1323,
                    "end": 1432
                },
                {
                    "start": 1432,
                    "end": 1599
                },
                {
                    "start": 1601,
                    "end": 1851
                },
                {
                    "start": 1851,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 651,
                    "end": 654,
                    "matchedPaperCorpusId": "257496235"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "260899968",
            "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation",
            "text": "Recent years have witnessed a remarkable advancement in generative artificial intelligence models, which typically ingest context, frequently presented as prompts, and subsequently generate text, images, videos, or audio conditioned upon this provided context (Jo 2023;Epstein et al. 2023). \n\nThe extent to which a model attends to such context is determined during the training phase, thereby granting users minimal control (Chung et al. 2022). This issue first garnered attention with the emergence of generative adversarial models (GANs) and then diffusion models (Dhariwal and Nichol 2021a). A series of techniques and methodologies have been proposed to better control this conditioning, forming the basis of the following sections. \n\nTraditional efforts to guide language models, such as PPLM (Dathathri et al. 2020) and GeDI (Krause et al. 2020), relied on external classifiers for specific attributes (Sitdikov et al. 2022). These methods utilise the gradient of a classifier, trained to recognise certain attributes, to adjust the logits during the generation process. By computing the gradient with respect to the desired attribute and applying it to the model's hidden states, they steer the generative process towards or away from particular characteristics. This proved expensive and complex, requiring additional models and continuous adjustments during implementation. \n\nLogit guidance emerged as an alternative approach for governing aspects of text generation, focusing on the manipulation of token distributions rather than semantic control, thereby signaling a progression towards architecture-agnostic guidance without additional classifiers. In the context of autoregressive language models, the token generation process unfolds sequentially, where the probability of each token is conditioned on its preceding tokens. Mathematically, the joint probability of a token sequence w is expressed as: \n\n(1) \n\nHere, P \u03b8 (w i |w j<i ) denotes the conditional probability of the i-th token given its predecessors, and is modeled as a distribution over the entire vocabulary. This distribution, represented as logits, is a function of the model's current state and can be directly manipulated. By selectively adjusting the logits' values, it's possible to exert influence over the generated text, steering it towards specific characteristics or themes.",
            "score": 0.4378340800390712,
            "section_title": "Related work Evolution of Generative Models and Control Over Conditional Generation",
            "char_start_offset": 3521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1916
                },
                {
                    "start": 1919,
                    "end": 1922
                },
                {
                    "start": 1925,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2364
                }
            ],
            "ref_mentions": [
                {
                    "start": 567,
                    "end": 594,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 799,
                    "end": 822,
                    "matchedPaperCorpusId": "208617790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68994140625
        },
        {
            "corpus_id": "272022133",
            "title": "Synthetic data generation by diffusion models",
            "text": "GANs) when scaling to higher resolutions or more complex data types. Many efforts have been devoted to improving the sample efficiency, including the training-free ODE solvers [10 ] and the disti l lation methods [11 ] with some extra training. In many generation tasks, e.g. textto-image generation [7 ], we would have some input (e.g. text prompts). One key technique for such applications is classifier-free guidance [12 ], which trains two weight-sharing models \u03b8 (x t , t, y ) and \u03b8 (x t , t, \u2205 ) for the noise prediction models, where y denotes the input (e.g. text prompt) and \u03b8 (x t , t, y ) is the conditional model. Here, we use \u2205 as a special 'empty' token for the unconditional model. In a practical implementation, Ho and Salimans [12 ] chose to randomly set y to the unconditional identifier \u2205 with some prespecified probability. Classifier-free guidance then combines these two models as \u02c6 \u03b8 (x t , t, y ) := (1 + s ) \u03b8 (x t , t, y ) \u2212 s \u03b8 (x t , t, \u2205 ) to trade off the text-image alignment and the sample diversity. The hyperparameter s is known as a 'guidance scale' , where a larger s usually improves the text-image alignment, but reduces the sample diversity. By choosing a proper guidance scale, pre-trained largescale text-to-image diffusion models can generate images with comparable quality to human artists. \n\nBesides images, diffusion models (often with proper extension of the guidance) have been adopted for generating high-quality data across various domains, including speech, three-dimensional (3D) contents, human motions, videos and molecules.",
            "score": 0.4373467127821302,
            "section_title": "Jun Zhu",
            "char_start_offset": 3457,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 181,
                    "matchedPaperCorpusId": "249282317"
                },
                {
                    "start": 213,
                    "end": 218,
                    "matchedPaperCorpusId": "257280191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "258967549",
            "title": "Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models",
            "text": "We can condition the score function on a label y to learn conditional generative models. This objective can be done by leveraging the Bayes' rule to decompose the conditional score function [3]: \n\nWhen y is independent of x during the generation process, the third term is zero. \n\nClassifier guidance: By providing a weight term \u03c9 to adjust the balance between the unconditional score function and the classifier guidance during the sampling phase, we get: \n\nunconditional score function (8) Classifier-free guidance: We can also model the unconditional score function \u2207 x log p(x) and the joint score function \u2207 x log p(x, y) at the same time, to substitute the classifier guidance [9]:",
            "score": 0.4372002135136316,
            "section_title": "Guided diffusion models",
            "char_start_offset": 7988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 687
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 488,
                    "end": 491,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 683,
                    "end": 686,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "274598164",
            "title": "Self-Guidance: Boosting Flow and Diffusion Generation on Their Own",
            "text": "Over the past decade, deep generative models have achieved remarkable advancements across various applications [5, 6, 9, 14, 17, 21, 22, 25, 26, 36-38, 40, 42, 44, 45, 48]. Among them, diffusion models [12,58,60] and flow-based models [2,8,29] have notably excelled in producing high-resolution, text-driven data such as images [50,51,54], videos [4,14,67], and others [7,20,28,39,45,56,62,68], pushing the boundaries of Artificial Intelligence Generated Contents. \n\nIn simple terms, diffusion models learn a multi-step transition from a prior distribution p T (x T ) to a real data distribution p 0 (x 0 ). However, default sampling methods for diffusion and flow-based models often lead to unsatisfactory generation quality, such as broken human hands and faces, and images with bad foreground and background. To address these issues, various guidance strategies have emerged as cheap yet effective ways to guide the generation process for better generation quality. For instance, classifier-free guidance (CFG) [11] modifies the velocity of diffusion and flow-based generative models by adding a delta term between class-conditional and unconditional velocities, which pushes generated samples to have high class probabilities. \n\nThough these existing guidance have shown impressive performance improvements, they have various individual restrictions. For instance, the CFG relies on computing an additional unconditional velocity, which requires training the diffusion model under both conditional and unconditional settings, therefore, harms the modeling performances [1]. Auto-Guidance (AG) pays a significant price that requires training an additional bad-version model, which is tricky as well as requiring more memory costs. Other guidance, such as Perturbed-attention Guidance (PAG [1]), and selfattention guidance (SAG [16]), do not rely on additional training. However, as the PAG paper described, the effectiveness of PAG is highly sensitive to the selection of perturbed attention layers inside the neural network, making it less flexible to enhance models in general applications.",
            "score": 0.4370241614014738,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "235352469"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "221818900"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "259108195"
                },
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "259501305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "253523104",
            "title": "An Overview on Controllable Text Generation via Variational Auto-Encoders",
            "text": "We define the task of controllable text generation as finding a function f to generate sentences that obey certain generation rules or conditions. This can be formally defined as: given a set of n conditions C = {c i } n 1 \u2208 C, where C denotes the condition space. The goal of controllable generation is formalized as learning a function f :\n\nwhich aims at generating sentences Z from space Z that fulfill desired conditions C. In general, the controlled sentence generation task can be divided into two categories according to the category in which restrictions are imposed: generation with soft constraint and hard constraint.\n\nTo be more precise, (1) soft constraint text generation requires the generated sentences to be semantically similar to the given constraints (e.g., topic or style), rather than explicitly enforcing certain concepts or rules (e.g., keywords) to appear in the content. The mapping function f mentioned above serves as a measurement to find sentences with the highest semantic similarity with given constraints. For example, given a corpus of (style, text) pairs as training data, followed by training a conditional language model to learn the linguistic relevance between (style, text) pairs and generate texts with such style, we achieve controllable text generation with soft constraints. (2) Hard constraint focuses on controlling specific tokens or textual structures (e.g., keywords, sentence length) during generation, thus being more fine-grained compared with the soft one. It indicates the compulsive inclusion of given constraints in the output texts. Hence, the function f here is regarded as a binary sign on a specified controlling level (e.g., token, syntax) to eliminate the possibility of producing unqualified features on such a level. Typically, the condition of hard constraint is keywords and the controllable generation process requires the model to generate sentences with provided exact keywords embedded in generated contexts.\n\nHowever, generating text under specific lexical constraints is challenging . As for the comparison of two types of controllable generation, soft constraint generation, on the other way around, is not capable of handling the explicit appearance of conditions on the token level but can produce authentic texts with particular styles or topics and more straightforward network designs as trade-offs. Hard constraint generative models process given conditions with higher proficiency by placing explicit restrictions on independent attribute controls, but",
            "score": 0.43692126590466884,
            "section_title": "Controllable Text Generation",
            "char_start_offset": 3895,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "272832564",
            "title": "TFG: Unified Training-Free Guidance for Diffusion Models",
            "text": "Recent advancements in generative models, particularly diffusion models [61,21,62,66], have demonstrated remarkable effectiveness across vision [65,48,52], small molecules [74,73,24], proteins [1,72], audio [35,29], 3D objects [40,41], and many more. Diffusion models estimate the gradient of log density (i.e., Stein score, [67]) of the data distribution [65] via denoising learning objectives, and can generate new samples via an iterative denoising process. With impressive scalability to billions of data [58], future diffusion models have the potential to serve as foundational generative models across a wide range of applications. Consequently, the problem of conditional generation based on these models, i.e., tailoring outputs to satisfy user-defined criteria such as labels, attributes, energies, and spatial-temporal information, is becoming increasingly important [63,2]. \n\nConditional generation methods like classifier-based guidance [66,7] and classifier-free guidance [23] typically require training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. In contrast, training-free guidance aims to generate samples that align with certain targets specified through an off-the-shelf differentiable target predictor without involving any additional training. Here, a target predictor can be any classifier, loss function, probability function, or energy function used to score the quality of the generated samples. \n\nIn classifier-based guidance [66,7], where a noise-conditional classifier is specifically trained to predict the target property on both clean and noisy samples, incorporating guidance in the diffusion Figure 1: (a) Illustration of the unified search space of our proposed TFG, where the height (color) stands for performance. Existing algorithms search along sub-manifolds, while TFG results in improved guidance thanks to its extended search space. (b) The label accuracy (higher the better) and Fr\u00e9chet inception distance (FID, lower the better) of different methods for the label guidance task on CIFAR10 [30], averaged across ten labels.",
            "score": 0.4367837921309244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 148,
                    "end": 151,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "258436871"
                },
                {
                    "start": 179,
                    "end": 182,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "269633210"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "271161349"
                },
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "232092778"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "53492374"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "260957043"
                },
                {
                    "start": 881,
                    "end": 883,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "268857323",
            "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion Models",
            "text": "Analyzing the effect of CFG rate.Text-to-image generative models require nuanced evaluation beyond single metrics.Sampling parameters are vital for customization, with the Classifier-Free Guidance (CFG) rate [17] directly influencing the balance between visual fidelity and semantic alignment with text prompt.Rombach et al. [55] experimentally demonstrate that different CFG rates result in different CLIP and FID scores.\n\nIn this study, we find that CFG rate as a sampling parameter yields inconsistent results across different model sizes.Hence, it is interesting to quantitatively determine the optimal CFG rate for each model size and sampling steps using either FID or CLIP score.We demonstrate this by sampling the scaled models using different CFG rates, i.e., (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) As demonstrated in the left and center panels, the optimal CFG rate changes as the sampling steps increased.To determine the optimal performance (according to the FID score) of each model and each sampling steps, we systematically sample the model at various CFG rates and identify the best one.As a reference of the optimal performance, the right panel shows the CFG rate corresponding to the optimal performance of each model for a given number of sampling steps.Smaller models achieve better FID scores than larger models for a fixed sampling cost.\n\nFor instance, at a cost of 3, the 83M model achieves the best FID compared to the larger models.This suggests that smaller models might be more efficient in achieving good visual quality with lower costs.We highlight the best performing model at different sampling costs .\n\nand comparing their quantitative and qualitative results.In Fig. 7, we present visual results of two models under varying CFG rates, highlighting the impact on the visual quality.We observed that changes in CFG rates impact visual quality more significantly than prompt semantic accuracy and therefore opted to use the FID score for quantitative determination of the optimal CFG rate.performance.Fig. 8 shows how different classifier-free guidance rates affect the FID scores in text-to-image generation (see figure caption for more details).\n\nScaling efficiency trends.",
            "score": 0.4357113344649695,
            "section_title": "Scaling sampling-efficiency",
            "char_start_offset": 12641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 33,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 422
                },
                {
                    "start": 424,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1361
                },
                {
                    "start": 1363,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1567
                },
                {
                    "start": 1567,
                    "end": 1635
                },
                {
                    "start": 1637,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1816
                },
                {
                    "start": 1816,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2033
                },
                {
                    "start": 2033,
                    "end": 2179
                },
                {
                    "start": 2181,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "268231006",
            "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models",
            "text": "We evaluate BNS solvers on: (i) Class conditional image generation, (ii) Text-to-Image generation, and (iii) Text-to-Audio generation.Additionally, we compare our method with model distillation.Unless stated otherwise, conditional sampling is done using classifier-free guidance (CFG) (Ho & Salimans, 2022;Zheng et al., 2023).All BNS solvers are trained on 520 pairs (x 0 , x(1)) of noise and generated image using adaptive RK45 (Shampine, 1986) Vanden-Eijnden, 2022).Details of the pre-trained models are in Appendix E.",
            "score": 0.43481453570612677,
            "section_title": "Experiments",
            "char_start_offset": 17954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 520
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 445,
                    "matchedPaperCorpusId": "26665726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "266191061",
            "title": "Image is All You Need to Empower Large-scale Diffusion Models for In-Domain Generation",
            "text": "We decouple the conditional guidance in CFG into two parts: domain guidance, which is optimized in fine-tuning to enhance fi-delity, and control guidance, which is preserved for controllability. With introducing multiple guidances in the synthesis process, the control guidance and unconditional guidance, which we want to preserve during personalization, are directly predicted by diffusion priors, while the domain guidance is predicted by an independent diffusion model. Additionally, we introduce an efficient domain knowledge learning mechanism, designing a null-text Diffusion Model to learn domain knowledge in a concise way. \n\nWe conduct extensive experiments to demonstrate the effects of our proposed task and validate the superiority of our approach. To summarize, our contributions are as follows: \n\n\u2022 We present the task of aligning large-scale diffusion models with specific domains using only image data to perform a variety of generative tasks. \u2022 We propose a guidance-decoupled prior preservation mechanism to address guidance forgetting during finetuning, which decouples domain guidance to learn domain knowledge with other guidance preserved. Besides, we propose an efficient learning mechanism to learn domain knowledge. \u2022 We present the pipeline of our proposed method for in-domain generation tasks and conduct experiments and comparisons across multiple domains and tasks to demonstrate the effectiveness and advancement of our approach.",
            "score": 0.43415450513435183,
            "section_title": "Introduction",
            "char_start_offset": 3309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1461
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "263334142",
            "title": "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation",
            "text": "The stellar capabilities of LLMs on NLP tasks have propelled their adoption in evaluating generated text quality. Techniques range from harnessing conditional generative probabilities (Fu et al., 2023) to leveraging the prompts tailored for specific evaluation needs (Liu et al., 2023;Lu et al., 2023). Such methods have been deployed across diverse NLG domains, including summarization (Gao et al., 2023), machine translation (Kocmi & Federmann, 2023), and more (Wang et al., 2023), with evaluations being both individual and comparative (Chen et al., 2023;Zheng et al., 2023). In contrast to these endeavors, our research uses LLMs with the Socratic method to realize reference-free reasoning evaluation, whereas the aforementioned works target other text-generation tasks.",
            "score": 0.43312526484696834,
            "section_title": "Related Work",
            "char_start_offset": 6336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 775
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "202573071",
            "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
            "text": "With enough data, model capacity, and compute, generative models can learn distributions powerful enough to produce high-quality samples from complex domains. In computer vision, the advent of generative adversarial networks (Goodfellow et al., 2014) improved image generation. Much research then focused on methods for controlling the generation process and improving estimation of generative distributions (Arjovsky et al., 2017;Chen et al., 2016;Kingma & Welling, 2013). \n\nIn natural language processing, language models are often trained as conditional language models for specific tasks that require text generation (Brants et al., 2007;Sutskever et al., 2014;Rush et al., 2015). They are also used as a means of learning word vectors (Mikolov et al., 2013), document vectors (Kiros et al., 2015), or contextualized word vectors (McCann et al., 2017;Peters et al., 2018;Devlin et al., 2018) for transfer learning. The language models themselves have been transferred to new tasks through fine-tuning as well (Dai & Le, 2015;Radford et al., 2018;Howard & Ruder, 2018). Less is understood about generation that is not constrained to any specific task. Typically prompts generated by models (Fan et al., 2018) or written by humans can only be used to provide a rough guide or starting point for the generated text. This raises the question of how text generation can be controlled more explicitly. \n\nInspired by the degree of control available in image generation as well as the recent progress in text generation (Radford et al., 2019) and multitask learning McCann et al. (2018), we train a language model that is conditioned on a variety of control codes (Pfaff, 1979;Poplack, 1980) that make desired features of generated text more explicit. With 1.63 billion parameters, our Conditional Transformer Language (CTRL) model can generate text conditioned on control codes that specify domain, style, topics, dates, entities, relationships between entities, plot points, and task-related behavior.",
            "score": 0.43308197502601287,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 250,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 408,
                    "end": 431,
                    "matchedPaperCorpusId": "2057420"
                },
                {
                    "start": 431,
                    "end": 449,
                    "matchedPaperCorpusId": "5002792"
                },
                {
                    "start": 621,
                    "end": 642,
                    "matchedPaperCorpusId": "633992"
                },
                {
                    "start": 642,
                    "end": 665,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 740,
                    "end": 762,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 781,
                    "end": 801,
                    "matchedPaperCorpusId": "9126867"
                },
                {
                    "start": 834,
                    "end": 855,
                    "matchedPaperCorpusId": "9447219"
                },
                {
                    "start": 1013,
                    "end": 1029,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 1673,
                    "end": 1687,
                    "matchedPaperCorpusId": "201699959"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12451171875
        },
        {
            "corpus_id": "270562277",
            "title": "Synergizing Foundation Models and Federated Learning: A Survey",
            "text": "An FM is a model that can be adapted to a wide array of tasks through fine-tuning after initial pretraining (Bommasani et al., 2021).The lifecycle of FMs typically involves pre-training on extensive generic data to establish the basis of their abilities (Bubeck et al., 2023), followed by adaptation to downstream tasks such as domain-specific question answering (Zhang et al., 2023e), and ultimately application in various domains.\n\nFMs have sparked a significant paradigm shift in various fields of AI such as NLP, CV, speech and acoustics, and beyond.In the realm of NLP, the most prominent example is Large Language Models (LLMs) with substantial parameter sizes (Zhao et al., 2023).These models, such as ChatGPT and GPT-4 (OpenAI, 2022, 2024), demonstrate exceptional abilities in natural language understanding and generation, enabling them to comprehend and respond to user inputs with remarkable contextual relevance.This capability proves invaluable in applications like customer service, virtual assistants, and chatbots, where effective communication is paramount.Moreover, LLMs eliminate the need for training models from scratch for specific tasks, be it machine translation, document summarization, text generation, or other language-related tasks.\n\nIn the realm of CV and other modalities, FMs have also made remarkable progress.Vision Transformers (ViTs) (Dosovitskiy et al., 2021) segment images into distinct patches, which serve as inputs for transformer architectures.SAM (Kirillov et al., 2023) can segment anything in images according to the input prompts.CLIP (Radford et al., 2021) bridges the gap between text and images through contrastive learning.DALL\u2022E, proposed by Ramesh et al. (2021), generates images from textual descriptions, expanding the possibilities of creative image generation.Additionally, models like GAto (Reed et al., 2022), exhibit versatility by being applicable across various tasks such as conversational agents, robotic control, and gaming.",
            "score": 0.4330814276942393,
            "section_title": "Background 2.1 Foundation Models",
            "char_start_offset": 3381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 432
                },
                {
                    "start": 434,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 687
                },
                {
                    "start": 687,
                    "end": 925
                },
                {
                    "start": 925,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1262
                },
                {
                    "start": 1264,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1675
                },
                {
                    "start": 1675,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 384,
                    "matchedPaperCorpusId": "259370572"
                },
                {
                    "start": 1371,
                    "end": 1397,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1583,
                    "end": 1605,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1695,
                    "end": 1715,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1849,
                    "end": 1868,
                    "matchedPaperCorpusId": "248722148"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "270710929",
            "title": "SyncNoise: Geometrically Consistent Noise Prediction for Text-based 3D Scene Editing",
            "text": "Many prior works have demonstrated the effectiveness of classifier-free guidance (CFG) for integrating additional conditions during the inference process.The objective of CFG is to shift the predicted scores towards locations that align more tightly with the conditions via extrapolation between unconditional scores and conditional scores.In this paper, we follow InstructPix2Pix [2] and employ a two-condition CFG strategy to ensure that the edited results are more faithful to the instructions and the original image.The final extrapolated scores can be estimated as the follows:\n\nwhere \u2205 is a fixed null value that represents the unconditional inputs, with c I and c T denoting the text instruction and input image, respectively.g I and g T are guidance weights designed to control the strength of the corresponding conditions.During training, the conditions are randomly dropped to allow the denoising network \u03f5 \u03b8 to conduct denoising in a conditional or unconditional context for both or either conditional inputs.",
            "score": 0.4323222677288324,
            "section_title": "A.2 Classifier-Free Guidance",
            "char_start_offset": 25327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 520
                },
                {
                    "start": 520,
                    "end": 582
                },
                {
                    "start": 584,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 1020
                }
            ],
            "ref_mentions": [
                {
                    "start": 381,
                    "end": 384,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "252781022",
            "title": "Visualize Before You Write: Imagination-Guided Open-Ended Text Generation",
            "text": "Open-ended Conditional Text Generation is the task of generating a coherent portion of the text based on the given context. Recent advances in pre-trained models have pushed frontier in the open-ended conditional text generation, such as text completion (See et al., 2019;Ippolito et al., 2020), story generation (Guan et al., 2020;Fan et al., 2018;Yao et al., 2019) and concept-to-text generation (Zhou et al., 2021;Liu et al., 2021 (Yang et al., 2020;Cho et al., 2021;Su et al., 2022a). However, the retrieved images may fail to fully incorporate the context, which will misguide the LM from yielding contextually consistent predictions. 2 Unlike prior work, our approach leverages images generated conditioning on the context to assist the text generation process.\n\nVisually-aided NLP Recent work show the power of visual guidance in natural language processing, spanning from the language representation learning (Lu et al., 2019;Li et al., 2019;Sun et al., 2019;Luo et al., 2020;Tan and Bansal, 2020;Lu et al., 2022), the downstream tasks (Grubinger et al., 2006;Elliott et al., 2016;Xie et al., 2019;Christie et al., 2016;Shi et al., 2019;Lu et al., 2022) and evaluation (Zhu et al., 2021). They either leverage visual information from an external vision-and-language corpus or obtain such visual knowledge from the large pretrained model. In this line of work, imagination achieves promising performance in various NLP domains (Long et al., 2021;Zhu et al., 2021;Wang et al., 2022a;Lu et al., 2022). Previous imaginationbased work in NLP either study non-generation problems (Zhu et al., 2021;Lu et al., 2022) or utilize non-visual information (Long et al., 2021;Wang et al., 2022a). Our work explores the potential of generating visual imagination to improve open-ended text generation tasks.",
            "score": 0.43222299928670216,
            "section_title": "Related Work",
            "char_start_offset": 4837,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 272,
                    "matchedPaperCorpusId": "202734604"
                },
                {
                    "start": 272,
                    "end": 294,
                    "matchedPaperCorpusId": "218551245"
                },
                {
                    "start": 349,
                    "end": 366,
                    "matchedPaperCorpusId": "53306064"
                },
                {
                    "start": 417,
                    "end": 433,
                    "matchedPaperCorpusId": "221970785"
                },
                {
                    "start": 434,
                    "end": 453,
                    "matchedPaperCorpusId": "209501155"
                },
                {
                    "start": 470,
                    "end": 487,
                    "matchedPaperCorpusId": "248525064"
                },
                {
                    "start": 934,
                    "end": 950,
                    "matchedPaperCorpusId": "199528533"
                },
                {
                    "start": 950,
                    "end": 967,
                    "matchedPaperCorpusId": "102483628"
                },
                {
                    "start": 967,
                    "end": 984,
                    "matchedPaperCorpusId": "211132410"
                },
                {
                    "start": 984,
                    "end": 1005,
                    "matchedPaperCorpusId": "222341606"
                },
                {
                    "start": 1128,
                    "end": 1145,
                    "matchedPaperCorpusId": "174801519"
                },
                {
                    "start": 1177,
                    "end": 1195,
                    "matchedPaperCorpusId": "235390670"
                },
                {
                    "start": 1434,
                    "end": 1453,
                    "matchedPaperCorpusId": "221819393"
                },
                {
                    "start": 1453,
                    "end": 1470,
                    "matchedPaperCorpusId": "235390670"
                },
                {
                    "start": 1470,
                    "end": 1489,
                    "matchedPaperCorpusId": "248525064"
                },
                {
                    "start": 1582,
                    "end": 1600,
                    "matchedPaperCorpusId": "235390670"
                },
                {
                    "start": 1651,
                    "end": 1670,
                    "matchedPaperCorpusId": "221819393"
                },
                {
                    "start": 1670,
                    "end": 1689,
                    "matchedPaperCorpusId": "248525064"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11578369140625
        },
        {
            "corpus_id": "249626186",
            "title": "Why is constrained neural language generation particularly challenging?",
            "text": "Typical attributes used to generate constrained natural language are the tense and the length of the summaries in text summarization [24], the sentiment of the generated content in review generation [107], language complexity in text simplification or the style in text style transfer applications.In addition, constrained text generation is used to overcome limitations of neural text generation models for dialogue such as genericness and repetitiveness of responses [131], [134].\n\nNevertheless, generating text under specific lexical constraints is challenging.Common models and architectures employed for natural language generation are autoregressive in nature, generating tokens one by one in a sequential manner from left to right; by design, these models lack fine control over the generated sequence and cannot easily support constraints at arbitrary positions in the output or constraints involving multiple input objects [168], [53].While for humans it is straightforward to generate sentences that cover a given set of concepts or abide to pre-defined rules by making use of their commonsense reasoning ability, generative commonsense reasoning with a constrained text generation task is more challenging for machine learning models [86].",
            "score": 0.4318399576550931,
            "section_title": "C. Constrained Text Generation",
            "char_start_offset": 16773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 482
                },
                {
                    "start": 484,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1250
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "22716243"
                },
                {
                    "start": 199,
                    "end": 204,
                    "matchedPaperCorpusId": "2148537"
                },
                {
                    "start": 469,
                    "end": 474,
                    "matchedPaperCorpusId": "67855999"
                },
                {
                    "start": 476,
                    "end": 481,
                    "matchedPaperCorpusId": "6126582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058135986328125
        },
        {
            "corpus_id": "273653942",
            "title": "TabDiff: a Mixed-type Diffusion Model for Tabular Data Generation",
            "text": "Implementing classifier-free guidance (CFG) for this task is straightforward. We approximate the conditional model using the unconditioned TABDIFF trained on all columns from the previous unconditional generation tasks. For the unconditional model, we train TABDIFF on the target column with a significantly smaller denoising network. Detailed implementation is provided in Appendix C, and results are presented in Table 4. As demonstrated, TABDIFF achieves higher imputation accuracy than TABSYN on five out of seven datasets, with an average improvement of 5.60% over the non-generative XGBoost classifier. This indicates TABDIFF's superior capacity for conditional tabular data generation. Moreover, we empirically demonstrate the efficacy of our CFG framework by showing that the model consistently performs better with \u03c9 = 0.6 compared to \u03c9 = 0.0 (which is equivalent to TABDIFF without CFG).",
            "score": 0.4317236110502773,
            "section_title": "Machine Learning Efficiency.",
            "char_start_offset": 25713,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 897
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7724609375
        },
        {
            "corpus_id": "264946463",
            "title": "Evaluating the Performance of ChatGPT in the Automation of Maintenance Recommendations for Prognostics and Health Management",
            "text": "The rapid advancements by the artificial intelligence (AI) community in the development in the area of natural language processing (NLP) are creating a widening gap between their transformative impact on everyday life and their abilities to address the technical language needs in industry. The engineering approach which leverages NLP tools intelligently and selectively for technical language data has been dubbed Technical Language Processing (TLP) (Brundage, Sexton, Hodkiewicz, Dima, & Lukens, 2021). Although out-of-the-box NLP models trained on general language may not directly apply to industrial text, promising advancements have been made in adapting NLP techniques to technical domains, generating substantial interest within the Prognostics and Health Management (PHM) community (Dima, Lukens, Hodkiewicz, Sexton, & Brundage, 2021;Nandyala, Lukens, Rathod, & Agarwal, 2021). \n\nOne such area of promising NLP development which has emerged in the past year are large language models (LLMs) such as ChatGPT, which are a subset of the broader domain of Generative AI. Generative AI encompasses AI algorithms that generate new data or content, distinct from discriminative AI that categorizes and classifies existing data. Through deep learning, NLP, and computer vision, generative AI models analyze patterns in existing data to produce statistically similar output. Applications span from generating realistic images to composing music and creating art, showcasing the vast possibilities enabled by generative AI technologies. LLMs specifically employ massive amounts of text data to generate human-like language, enabling tasks such as conversational responses, text completion, and document generation. \n\nPHM involves a strategic decision-making process based on diagnostics or prognostics information, resource availability, and operational demands. It encompasses crucial elements such as data collection, predictive modeling and the critical task of initiating appropriate actions based on extracted information, while continuously validating the accuracy of predictions. One specific task in PHM, which may be well-suited for LLMs, is generating recommendations for troubleshooting actions in response to alerts from PHM models. Currently, this process involves multiple stakeholders, where an analyst or reliability engineer identifies an alert and collaborates with others for troubleshooting.",
            "score": 0.43168753996375675,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 844,
                    "end": 886,
                    "matchedPaperCorpusId": "237855146"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06671142578125
        },
        {
            "corpus_id": "269005366",
            "title": "Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance",
            "text": "Recently, text-to-image generation has witnessed rapid development and various applications [30,31,33,34,48], where visually stunning images can be created by simply typing in a text prompt.In particular, after DDPM [7,12] succeeded GANs [3,8], diffusion models [40], such as Stable Diffusion [34] and DallE-3 [2], have emerged as the new state-of-the-art family for image-generative models.\n\nThe key feature of diffusion models is to approximate the true data distribution p(x) by reversing the process of perturbing the data with noise progressively in a long iterative * the corresponding author: liuyuisanai@gmail.com Figure 1.A motivation example.The first line shows images generated by Stable Diffusion with CFG and S-CFG, where the prompt is \"a photo of an astronaut riding a horse\" and the segmentation maps are manually labeled (Ground, Sky, Horse, Astronaut).The below line shows the average norm curves of the estimated classifier score \u2207x t log p(c|xt) (solid line) and diffusion score \u2207x t log p(xt) (dashed line) in each semantic region.The Y-axis scale unit is set as the dynamic variance parameter \u03c3t for better illustrations without damaging the conclusion.chain.To incorporate the text prompt c into the final generation, it is necessary to enhance the likelihood of c given the current latent image x t at each reversed diffusion step t.Instead of training extra classifiers to model p(c|x t ) at each diffusion step t [7], classifier-free guidance (CFG) [11] has recently been proposed to estimate both the classifier score \u2207 xt log p(c|x t ) and the diffusion score \u2207 xt p(x t ) with the same neural models, such as U-net [35].In particular, an empirical CFG scale is introduced to control the strength of the text guidance on the whole image space.However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths during the denoising process and suboptimal quality of the final image.",
            "score": 0.43159565864339616,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 391
                },
                {
                    "start": 393,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 870
                },
                {
                    "start": 870,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "277621927",
            "title": "Your Image Generator Is Your New Private Dataset",
            "text": "Shortly after the introduction of the GAN, a conditional formulation was proposed, allowing the generation process to be guided by additional information provided to both generator and discriminator [10]. In particular, conditioning on class labels has demonstrated benefits through scaling the training set and through the truncation of the variance in the noise input, an approach that can help enhance image quality [11]. \n\nMore recently, the possibility of conditioning generation on text prompts has been widely explored. One line of research focuses on improving the quality of the textual representation used for generation. For instance, Ku et al. proposed training a regressor that produces more precise text-conditioned vectors, facilitating fine control over minor features in the generated images [12]. Another work by Tao et al. exploited CLIP's broad understanding of visual scenes, combining a CLIP-based discriminator with a CLIP-enhanced generator to reduce training time while improving the synthesized output [13,14]. \n\nDiffusion-based solutions also employ textual prompts and often adopt guidance strategies to balance fidelity and diversity. Nichol et al. demonstrated that classifier-free guidance, achieved by blending the model's predictions with and without text conditioning, can outperform approaches relying on CLIP guidance [15]. Stable Diffusion itself incorporates a parameter named Guidance Scale to govern the adherence of generated images to the text prompt [4]. Furthermore, several works explored rewriting prompts with large language models to enhance semantic alignment [16,17,18], while other approaches leveraged newly introduced tokens in the text embedding space to teach the model novel concepts or styles, leading to a higher variety of generated outcomes [19,20].",
            "score": 0.4311218062945471,
            "section_title": "Conditioning Methods",
            "char_start_offset": 4652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1809
                }
            ],
            "ref_mentions": [
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "258274991"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "256389964"
                },
                {
                    "start": 1032,
                    "end": 1035,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1493,
                    "end": 1496,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "267068823"
                },
                {
                    "start": 1613,
                    "end": 1616,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1616,
                    "end": 1619,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 1801,
                    "end": 1805,
                    "matchedPaperCorpusId": "251800180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "247447633",
            "title": "On Information Hiding in Natural Language Systems",
            "text": "Carrier Generation techniques aim to generate natural language carrier texts that encode secret messages. These techniques range from carrier generation using Context-Free-Grammars (CFGs) (Wayner 1992;Chapman and Davida 1997), Markov chains (Shniperov and Nikitina 2016) and neural networks (Ziegler, Deng, and Rush 2019;Yang et al. 2021a). The increased computational prowess and improved language modeling capabilities have resulted in major improvements in carrier generation models of NLS. Here, we divide carrier generation based NLS approaches into three categories, i.e. (CFG) based methods, Conditional Proba-bility (ConProc) based methods and latent space semantics based methods.\n\nContext-Free Grammars One of the earliest attempts towards generating natural language texts for NLS proposed the use of context-free-grammars (CFGs) (Wayner 1992). A CFG is a set of production rules, forming the formal grammar for a given language, which can universally describe any combination of valid text sequences in that language. Thus, assuming that the CFG encompasses all possible text sequence permutations present within a given natural language, it can be used to generate syntactically legitimate text sequences. In order to hide information within the text generated using CFGs, Wayner developed a custom-made CFG where the choice of the CFG branch portrayed the bit(s) encoded (Wayner 1992). While these grammars can yield syntactically correct outputs, they can lead to repeated sentences, unless huge grammars are designed (Chapman and Davida 1997). Towards generating semantically correct outputs, (Chapman and Davida 1997) combined a dictionary table containing a large list of POS tags, word synset and word pairs, and style templates to generate cover texts. But these methods have mostly been overtaken by conditional probability based frameworks.",
            "score": 0.43102855962101916,
            "section_title": "Carrier Generation",
            "char_start_offset": 11753,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 201,
                    "matchedPaperCorpusId": "205487962"
                },
                {
                    "start": 201,
                    "end": 225,
                    "matchedPaperCorpusId": "28929439"
                },
                {
                    "start": 241,
                    "end": 270,
                    "matchedPaperCorpusId": "1568753"
                },
                {
                    "start": 291,
                    "end": 321,
                    "matchedPaperCorpusId": "202537031"
                },
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "222223657"
                },
                {
                    "start": 841,
                    "end": 854,
                    "matchedPaperCorpusId": "205487962"
                },
                {
                    "start": 1385,
                    "end": 1398,
                    "matchedPaperCorpusId": "205487962"
                },
                {
                    "start": 1533,
                    "end": 1558,
                    "matchedPaperCorpusId": "28929439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1571044921875
        },
        {
            "corpus_id": "277043912",
            "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
            "text": "The selection of an appropriate classifier-free guidance (CFG) scale is a crucial factor influencing the quality of generated videos. Originally proposed in the context of diffusion models, the CFG scale determines the tradeoff between fidelity to the textual prompt and the diversity of generated visual content. Classifier-free guidance integrates conditional (prompt-based) and unconditional (prompt-free) signals during model training, thereby enhancing the model's capacity to adhere to the provided textual descriptions without sacrificing model's generative flexibility. Specifically, the CFG scale directly modulates this balance. Higher CFG values enforce strict adherence to the input prompt, yielding videos that are semantically precise and visually coherent. However, an excessively high CFG scale can limit the output diversity, resulting in repetitive or overly rigid generations. Conversely, lower CFG scales offer greater creative freedom, allowing the model to explore diverse interpretations beyond the prompt, thus enhancing the variability and naturalness of generated videos, albeit potentially reducing semantic accuracy. \n\nInspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity. \n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale \n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10.",
            "score": 0.43090456154089674,
            "section_title": "Appendix A. Effect of CFG Scale in Text-to-Video Generation",
            "char_start_offset": 26399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2277
                },
                {
                    "start": 2280,
                    "end": 2470
                },
                {
                    "start": 2471,
                    "end": 2474
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "258999426",
            "title": "FigGen: Text to Scientific Figure Generation",
            "text": "Figures 2 show additional generated samples of FigGen when tuning the classifier-free guidance (CFG) (Ho & Salimans, 2022) parameter. We observe improvement in figure quality when increasing the CFG scale which is also shown quantitatively. Figure 3 presents more generations of FigGen. Note the variability in text length between samples, as well as the technical level of the captions, which makes it difficult for the model to properly generate understandable figures. However, highlevel concepts are correctly captured, such as captions that describe charts and plots, algorithms, or cases where we aim to display neural network architectures. \n\nFigure 3: Generated samples from the test set prompts above as input. Samples generated using FigGen Base .",
            "score": 0.43073724721278944,
            "section_title": "A.3 ADDITIONAL GENERATION RESULTS",
            "char_start_offset": 6712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 757
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "268248478",
            "title": "Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models",
            "text": "To uncover the unreasonable power of these guided approaches and better assist practice, this paper takes the first step towards this goal in the context of diffusion models. Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling (Ho et al., 2020;Song et al., 2020b;Yang et al., 2023). Compared to alternative generative models, such as variational autoencoder or generative adversarial network, diffusion models are known to be more stable, and generate high-quality samples based on learning the gradient of the log-density function (also known as the score function). When data is multi-modal, namely, it potentially comes from multiple classes, a natural question is how to make use of these class labels for conditional synthesis. Towards this direction, Dhariwal and Nichol (2021) put forward the idea of classifier guidancean approach to enhance the sample quality with the aid of an extra trained classifier. The classifier guidance approach combines an unconditional diffusion model's score estimate with the gradient of the log probability of a classifier. Subsequently, Ho and Salimans (2022) presented the so-called classifier-free guidance, which \n\nFigure 1: The effect of guidance on a three-component GMM in R 2 . Each component has weight 1/3 and identity covariance, and the component centers are ( \u221a 3/2, 1/2), (\u2212 \u221a 3/2, 1/2) and (0, \u22121). The leftmost panel displays the unguided density. We increase the guidance strength from left to right. This plot imitates Figures 2 of Ho and Salimans (2022). \n\ninstead mixes the score estimates of an unconditional diffusion model with that of a conditional diffusion model jointly trained over the data and the label. For both guidance methods, adjusting the mixing weights of the unconditional score estimate and the other component controls the trade-off between the Fr\u00e9chet Inception Distance (FID) and the Inception Score (IS) in the context of image synthesis.",
            "score": 0.4306156192457662,
            "section_title": "Training with guidance for diffusion models",
            "char_start_offset": 1407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 367,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "269929762",
            "title": "Generative AI in Cybersecurity: A Comprehensive Review of LLM Applications and Vulnerabilities",
            "text": "Min et al. [28] surveys the latest advancements in leveraging PLMs for NLP, organizing the approaches into three main paradigms.Firstly, the \"Pre-train then Fine-tune\" method involves general pre-training on large unlabeled datasets followed by specific fine-tuning for targeted NLP tasks.Secondly, \"Prompt-based Learning\" uses tailored prompts to transform NLP tasks into formats akin to a PLM's pre-training, enhancing the model's performance, especially in few-shot learning scenarios.Lastly, the \"NLP as Text Generation\" paradigm reimagines NLP tasks as text generation problems, fully capitalizing on the strengths of generative models like GPT-2 and T5.These paradigms represent the cutting-edge methods in utilizing PLMs for various NLP applications.",
            "score": 0.43040773220885176,
            "section_title": "D. Advancements in PLMs for NLP",
            "char_start_offset": 11262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 289
                },
                {
                    "start": 289,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 757
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 15,
                    "matchedPaperCorpusId": "240420063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08154296875
        },
        {
            "corpus_id": "276094842",
            "title": "Compressed Image Generation with Denoising Diffusion Codebook Models",
            "text": "The task of text-conditional image generation can be solved using a conditional diffusion model, which, theoretically speaking, learns to sample from the posterior distribution p 0 (x 0 |y). In practice, however, using a conditional model directly typically yields low fidelity to the inputs. To address this limitation, CG can be used to improve this fidelity at the expense of sample quality and diversity (Dhariwal & Nichol, 2021). Classifier-Free Guidance (CFG) is used more often in practice, as it achieves the same tradeoff by mixing the conditional and unconditional scores during sampling (Ho & Salimans, 2021), thus eliminating the need for a classifier. Particularly, assuming we have access to both the conditional score s i (x i , y) := \u2207 xi log p i (x i |y) and the unconditional one s i (x i ), CFG proposes to modify the conditional score by \n\nwhere w, the CFG scale, is a hyper-parameter controlling the tradeoff between sample quality and diversity. \n\nHere, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs. \n\nNote that optimizing Eq. ( 51) is roughly equivalent to optimizing L P when x i is high dimensional (see App. C.2). As in App. C.5, we promote sample diversity by choosing k i from a randomly sampled subset of K < K indices at each step during the generation. We coin our method Compressed CFG (CCFG). \n\nWe implement our method using SD 2.1 trained on 768 \u00d7 768 images, adopting a DDPM noise schedule with T = 1000 diffusion steps, K = 64 fixed vectors in each codebook and K \u2208 {2, 3, 4, 6, 9}. We compare against the same diffusion model with standard DDPM sampling, using T = 1000 steps and w \u2208 {2, 5, 8, 11}.",
            "score": 0.4302877373228693,
            "section_title": "C.6. Compressed Classifier-Free Guidance",
            "char_start_offset": 38692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 433,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 598,
                    "end": 619,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "235422416",
            "title": "Toward Diverse Precondition Generation",
            "text": "Generating preconditions is a difficult task even for a single output setting (Kwon et al., 2020). With the training data derived from existing news articles, generative models only get to see one possible precondition for each target event. Not surprisingly the top candidates in beam search tend to be focused towards a specific type of precondition event with minor variations. This suggests that we need to provide explicit guidance to the model to explore diverse candidates.\n\nHow can we get such diverse guidance? A main strength of large generative language models is that they learn to generate text that fits with the input context. If we can get the input context to be less specific then we can aim to get more general outputs. We can exploit this behavior by training a separate event sampler that is fed a reduced version of the target event description. For example, we can denote the target event by just the event trigger and its arguments. The event sampler learns to predict possible precondition event triggers based on this reduced context. This task forces the sampler to learn a more general mapping between target and precondition events that can produce a diverse set of starting points for generating the precondition events. We can then train another generative model to condition on the precondition trigger in addition to the input sentence. This gives us a model whose outputs we can control by providing different possible precondition triggers. Not all precondition triggers may yield high quality preconditions. To further assist the model, we also devise a precondition re-ranker.\n\nOur overall system, shown in Figure 1, consists of three components -an event sampler, a candidate generator, and a post processor (Precondition re-ranker and Similarity filter). The first two stages are used for generation -they use two separate generation models, and the last is employed to improve the quality of generated preconditions. We refer to this system as DiP short for Diverse Preconditions.",
            "score": 0.4301365143282523,
            "section_title": "Diverse Precondition Generator",
            "char_start_offset": 7660,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22119140625
        },
        {
            "corpus_id": "276421312",
            "title": "Diffusion Models without Classifier-free Guidance",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;Ho et al., 2020;Song et al., 2021a;b) have become the cornerstone of many successful generative models, e.g. image generation (Dhariwal & Nichol, 2021;Nichol et al., 2022;Rombach et al., 2022;Podell et al., 2024;Chen et al., 2024) and video generation (Ho et al., 2022;Blattmann et al., 2023;Gupta et al., 2025;Polyak et al., 2024;Wang et al., 2024) tasks. However, diffusion models also struggle to generate \"low temperature\" samples (Ho & Salimans, 2021;Karras et al., 2024) due to the nature of training objectives, and techniques such as Classifier guidance (Dhariwal & Nichol, 2021) and Classifier-free guidance (CFG) (Ho & Salimans, 2021) are proposed to improve performances. \n\nDespite its advantage and ubiquity, CFG has several drawbacks (Karras et al., 2024) and poses challenges to effective implementations (Kynk\u00e4\u00e4nniemi et al., 2024)  models. One critical limitation is the simultaneous training of unconditional model apart from the main diffusion model. The unconditional model is typically implemented by randomly dropping the condition of training pairs and replacing with an manually defined empty label. The introduction of additional tasks may reduce network capabilities and lead to skewed sampling distributions (Karras et al., 2024;Kynk\u00e4\u00e4nniemi et al., 2024). Furthermore, CFG requires two forward passes per denoising step during inference, one for the conditioned and another for the unconditioned model, thereby significantly escalating the computational costs.",
            "score": 0.43009212067463504,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1552
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 46,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 46,
                    "end": 65,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 81,
                    "end": 100,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 216,
                    "end": 236,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 236,
                    "end": 257,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 257,
                    "end": 277,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 277,
                    "end": 295,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 317,
                    "end": 334,
                    "matchedPaperCorpusId": "248006185"
                },
                {
                    "start": 334,
                    "end": 357,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 357,
                    "end": 376,
                    "matchedPaperCorpusId": "266163109"
                },
                {
                    "start": 500,
                    "end": 521,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 521,
                    "end": 541,
                    "matchedPaperCorpusId": "270226598"
                },
                {
                    "start": 688,
                    "end": 709,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 812,
                    "end": 833,
                    "matchedPaperCorpusId": "270226598"
                },
                {
                    "start": 884,
                    "end": 911,
                    "matchedPaperCorpusId": "269043032"
                },
                {
                    "start": 1299,
                    "end": 1320,
                    "matchedPaperCorpusId": "270226598"
                },
                {
                    "start": 1320,
                    "end": 1346,
                    "matchedPaperCorpusId": "269043032"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81298828125
        },
        {
            "corpus_id": "276160953",
            "title": "DICE: Distilling Classifier-Free Guidance into Text Embeddings",
            "text": "Text-to-image diffusion models are capable of generating high-quality images, but these images often fail to align closely with the given text prompts. Classifier-free guidance (CFG) is a popular and effective technique for improving text-image alignment in the generative process. However, using CFG introduces significant computational overhead and deviates from the established theoretical foundations of diffusion models. In this paper, we present DIstilling CFG by enhancing text Embeddings (DICE), a novel approach that removes the reliance on CFG in the generative process while maintaining the benefits it provides. DICE distills a CFG-based text-to-image diffusion model into a CFG-free version by refining text embeddings to replicate CFG-based directions. In this way, we avoid the computational and theoretical drawbacks of CFG, enabling high-quality, well-aligned image generation at a fast sampling speed. Extensive experiments on multiple Stable Diffusion v1.5 variants, SDXL and PixArt-$\\alpha$ demonstrate the effectiveness of our method. Furthermore, DICE supports negative prompts for image editing to improve image quality further. Code will be available soon.",
            "score": 0.4299492114921069,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "259316476",
            "title": "PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation",
            "text": "Selected patterns can be fed back to the LLM as samples or references for finetuning, which can provide more targeted and personalized guidance information to help the language model better adapt to a specific task or domain. As shown in Figure 3, by using patterns for finetuning, more targeted and personalized guidance information can be provided by generating diverse contextual instructions [45] or prompts [46] to help the language model better adapt to a specific task or domain during the finetuning process to a specific task or domain. Such a fine-tuning approach can improve the performance and generative power of the model, making it more suitable for specific application scenarios and task requirements. By generating diverse contextual instructions, the language model can be guided to focus on specific task requirements or domain knowledge during the fine-tuning process. These instructions can cover different semantic aspects, conditional constraints, and operational requirements to motivate the model to produce more accurate and coherent output. For example, in a text generation task, a series of contextual instructions, including input samples, desired outputs, etc., can be generated to guide the model in fine-tuning the task according to the characteristics of the task type and the target output.",
            "score": 0.428866373976102,
            "section_title": "Model Fine-tuning",
            "char_start_offset": 22287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443359375
        },
        {
            "corpus_id": "260886956",
            "title": "LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts",
            "text": "Conditional Diffusion Models Classifier-guidance [4] provides a way for diffusion model to achieve conditional generation by using the gradient of a separately trained classifier p(y|x t ) during sampling. As a more efficient technique, classifier-free guidance [14, 24] replaces the noise estimator by a combination of conditional and unconditional model, without requirement of p(y|x t ): \n\nwhere y is the class label or text embedding from language model [24], \u03c9 \u2265 1 denotes the guidance scale and trivially increasing \u03c9 will amplify the effect of conditional input. \n\nWith the help of large-scale pre-trained CLIP [25] and other language models [31], diffusion models produce impressive results on text-to-image generation. However, their performance of complex scene generation are always unsatisfactory because the text embeddings from the linguistic models can not accurately capture the spatial properties, e.g., objects' locations, sizes and their implicit spatial associations. Distinct from text prompts, we focus on the task of generating complex scene images from the structured layout configurations (L2I) and further propose a diffusion modelbased method with flexibility and compositionality.",
            "score": 0.4287850375107035,
            "section_title": "Preliminaries",
            "char_start_offset": 11621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1208
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "259501046",
            "title": "DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer",
            "text": "A final consideration is that we aim to perform prompt-less execution of LDMs, given our use of exemplar images for both content and style. As such, we only need to use the model's unconditional capabilities. Latent Diffusion Models execute two iterations of their model: one with no prompt conditioning and one with prompt conditioning. The output of both branches is joined at every time step via the classifier free guidance (CFG). This exposes prompt control via this adjustable strength. Given that we aim not to use any text prompts anywhere in the process, we, therefore, altogether disable the prompt-conditioned branch of the model execution and use only the un-conditional branch for both inversion and reverse diffusion. The process would function the same if the text prompt were fixed to a generic prompt throughout or if CFG was zero, but this approach saves on compute.",
            "score": 0.4285161455282438,
            "section_title": "DIFF-NST real image inversion",
            "char_start_offset": 10925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 884
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "276977903",
            "title": "TA-V2A: Textually Assisted Video-to-Audio Generation",
            "text": "During inference, guidance techniques such as Classifier Guidance (CG) [22] and Classifier-Free Guidance (CFG) [23] are employed to control the generation process. CG relies on an additional classifier P \u03d5 to guide the reverse process at each timestep via the gradient of the class label log-likelihood \u2207 log P \u03d5 (y|xt). CFG, on the other hand, combines conditional and unconditional score estimates to steer the reverse process. As suggested in [12], double guidance can be applied for enhanced alignment: \n\nwhere \u03b3 and \u03c9 represent the scales for CG and CFG, respectively. Notably, CFG employs Emix, while CG uses Ev due to the aligned classifier P \u03d5 (y|zt, Ev) trained for the alignment of audio-visual pairs as discussed in [12]. \n\nFrom the perspective of Energy Based Models (EBMs) [24], multiple conditions can also influence the inference process independently without combination. The conditional probability can be estimated by the following formula: \n\nHere, we define g to represent the difference between the unconditional and conditional score estimates: \n\nwhere c is the condition and \u03c9 is a hyperparameter. Then the multiconditioned inference steps could be represented as \n\nwhere Ev represents video features, E l represents prompt features, and E nl represents negative prompt features. This approach allows for a more flexible inference process by independently considering the effects of multiple conditions. The workflow is designed to efficiently process the video and textual descriptions through CVALP, align and mix features using LDM, and apply inference techniques to produce the final highquality audio output that matches the given video content. Speicfically, in the inference process, the positive prompt input by the human will undergo the Portable Plug-in Prompt Refiner (PPPR) [25] for standardization, ensuring that it corresponds with the AIgenerated text involved in the training. If a human-provided prompt is not available, we will use the Video-LlaMA2 [18] model to generate a description of the video content automatically.",
            "score": 0.42734835668067894,
            "section_title": "D. Inference with Guidance",
            "char_start_offset": 9866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 75,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "259309037"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "259309037"
                },
                {
                    "start": 786,
                    "end": 790,
                    "matchedPaperCorpusId": "249375227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "257505012",
            "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
            "text": "Labels improve image synthesis. Early works on generative adversarial models (GAN) have shown that class labels improve the image synthesis quality [35], [36], [37], [38], [39]. As a pioneering work, Conditional GAN [35] feeds the class label as an additional input layer to the model. Moreover, [40] applies class-conditional normalization statistics in image generation. In addition, AC-GAN [38] explicitly adds an auxiliary classifier loss. In other words, labels can help improve the GAN image synthesis quality by providing a conditional input or guiding the image synthesis via an auxiliary classifier. Following these success practices, [41] introduces class-conditional normalization and an auxiliary classifier into diffusion models. In order to distinguish whether the label information is added as a conditional input or an auxiliary loss with gradients, we follow [41] to define the conditional diffusion model and guided diffusion model as follows. \n\nConditional diffusion model: A conditional diffusion model learns from additional information (e.g., class and text) by taking them as model input. \n\nGuided diffusion model: During the training of a guided diffusion model, the class-induced gradients (e.g. through an auxiliary classfier) are involved in the sampling process. \n\nClassifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.",
            "score": 0.4270921508980777,
            "section_title": "Guidance in diffusion-based image synthesis",
            "char_start_offset": 9118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "1099052"
                },
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "2023211"
                },
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "52889459"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "273662413",
            "title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges",
            "text": "This technique improves model robustness and efficiency, particularly in data-scarce legal NLP applications [24]. \u2022 Parameter-Efficient Fine-Tuning (PEFT): PEFT is a method for adapting PLMs that involves freezing the majority of the model's parameters and only updating a small subset. This approach significantly reduces the computational resources and time required for fine-tuning, making it particularly effective in resource-limited scenarios, while still achieving competitive performance in tasks, such as text generation [73]. These machine learning paradigms enhance the performance of core NLP methods, particularly PLMs, by tailoring models to specific tasks and optimising their training efficiency. \n\n(3) Text retrieval technique: \n\n\u2022 Retrieval-Augmented Generation (RAG): RAG combines traditional Information Retrieval (IR) methods with generative NLP models, allowing systems to retrieve external knowledge before generating responses. Text retrieval techniques, such as RAG complement the core NLP methods by providing additional context and external information that enhances the generation and refinement of texts.",
            "score": 0.4269634288382177,
            "section_title": "Basic foundations and concepts of NLP.",
            "char_start_offset": 17820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1133
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "237571793"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "260435365"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09588623046875
        },
        {
            "corpus_id": "275920706",
            "title": "Visual Generation Without Guidance",
            "text": "Low-temperature sampling is a critical technique for enhancing generation quality by focusing only on the model's high-likelihood areas. Visual models mainly achieved this through Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). CFG jointly optimizes both conditional and unconditional models during training and combines them to define the sampling process. By altering the guidance scale, it can flexibly trade off image fidelity and diversity at inference time, while significantly improving sample quality. Due to its effectiveness, CFG has been a default technique for a wide spectrum of visual generative models, including diffusion (Ho et al., 2020), autoregressive (AR) (Chen et al., 2020;Tian et al., 2024), and masked-prediction models (Chang et al., 2022;Li et al., 2023). \n\nDespite the wide application, CFG requires inferencing both conditional and unconditional models to achieve the sampling distribution. This not only doubles the sampling cost but also complicates the post-training of visual models: When distilling pretrained diffusion models (Meng et al., 2023;Luo et al., 2023;Yin et al., 2024b) for fast inference or applying RLHF techniques (Black et al., 2023;Chen et al., 2024b), CFG requires the extra unconditional model to be additionally considered in the algorithm design. \n\nIt's also worth noting that for language models (LMs), a single model is sufficient to represent sampling distributions across various temperatures. However, similarly following LMs' approach to divide model output by a constant temperature value has largely been found ineffective in visual sampling (Dhariwal & Nichol, 2021), even for visual AR models with similar architecture to LMs (Sun et al., 2024). This leaves us wondering, how can we control the sampling temperature for visual models using only one model? \n\nExisting attempts mainly include distillation methods for diffusion models (Meng et al., 2023;Luo et al., 2023;Yin et al., 2024b) and alignment methods for AR models (Chen et al., 2024b).",
            "score": 0.4257412716580212,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1826
                },
                {
                    "start": 1829,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 683,
                    "end": 702,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 771,
                    "end": 787,
                    "matchedPaperCorpusId": "253553243"
                },
                {
                    "start": 1067,
                    "end": 1086,
                    "matchedPaperCorpusId": "252762155"
                },
                {
                    "start": 1103,
                    "end": 1121,
                    "matchedPaperCorpusId": "265506768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "273963030",
            "title": "Concept Bottleneck Language Models For protein design",
            "text": "Related Work Language models are increasingly used across various fields, from natural language processing (NLP) to specialized areas like chemistry and biology, demonstrating exceptional  Concept Bottleneck Models enhance neural network interpretability by incorporating a \"concept bottleneck\" layer that maps inputs to human-understandable concepts for final predictions (Koh et al., 2020). Recently, Ismail et al. (2023) extended this approach to generative models (CBGMs), enabling controllable generation and concept-level explanations, particularly in image generation tasks. In this paper, we adapt this approach to generative language models, achieving global control over the entire input while retaining token-level generation. A few very recent works have proposed CBMs for text classification tasks Tan et al. ( 2024); Sun et al. ( 2024), but this is a much easier and less interesting setting than generative language modeling which we address. \n\nProtein language models are extensively used in biological machine-learning research, trained on vast protein sequences across the evolutionary tree of life. Despite their critical applications in healthcare and drug discovery (Hie et al., 2024), pLMs currently lack interpretability. Historically, pLM architectures, loss functions, and training setups have closely mirrored the original masked language model setup (Devlin et al., 2018), differing mainly in vocabulary and dataset. While some protein language models support conditional generation by concatenating different protein functions and properties to the inputs (Shuai et al., 2021;Madani et al., 2020;Hayes et al., 2024), they do not provide mechanisms for interpretability, debuggability, or insights into what the model has learned. \n\nSee Appendix E.1 for extended related work.",
            "score": 0.42561563289284265,
            "section_title": "DISCUSSION AND CONCLUSION",
            "char_start_offset": 26863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 391,
                    "matchedPaperCorpusId": "220424448"
                },
                {
                    "start": 403,
                    "end": 423,
                    "matchedPaperCorpusId": "271745630"
                },
                {
                    "start": 1187,
                    "end": 1205,
                    "matchedPaperCorpusId": "263364541"
                },
                {
                    "start": 1584,
                    "end": 1604,
                    "matchedPaperCorpusId": "245224243"
                },
                {
                    "start": 1624,
                    "end": 1643,
                    "matchedPaperCorpusId": "270963342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.095703125
        },
        {
            "corpus_id": "273098845",
            "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2021b) are a class of generative models that learn the data distribution by reversing a forward process that adds noise to the data until the samples are indistinguishable from pure noise. Although the theory suggests that simulating the backward process in diffusion models should result in correct sampling from the data distribution, unguided sampling from diffusion models often results in low-quality images that do not align well with the input condition. Accordingly, classifier-free guidance (Ho & Salimans, 2022) has been established as an essential tool in modern diffusion models for increasing the quality of generations and the alignment between the condition and the generated image, albeit at the cost of reduced diversity (Ho & Salimans, 2022;Sadat et al., 2024a). \n\nModern text-to-image models, such as Stable Diffusion (Rombach et al., 2022), generally require high guidance scales in order for the generations to have better quality and align well with the input prompt. However, high guidance scales often result in oversaturated colors and simplified image compositions (Saharia et al., 2022b;Kynk\u00e4\u00e4nniemi et al., 2024). Despite these disadvantages, high CFG scales are still used in practice due to their superior image quality compared to alternatives. \n\nIn this paper, we analyze the update rule of CFG and show that with a few modifications to how the CFG update is applied at inference, we can vastly mitigate the oversaturation and artifacts of high guidance scales. First, we show that the CFG update rule can be decomposed into two components, one that is parallel to the conditional model prediction, and one that is orthogonal to this prediction. We show that the orthogonal element is mainly responsible for improving image quality, while the parallel part primarily adds contrast and saturation to the output. To the best of our knowledge, this is the first study that disentangles these two effects in CFG. \n\nAdditionally, we establish a connection between the CFG update rule and stochastic gradient ascent.",
            "score": 0.42523057679515336,
            "section_title": "INTRODUCTION",
            "char_start_offset": 386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 62,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 62,
                    "end": 81,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 834,
                    "end": 854,
                    "matchedPaperCorpusId": "264490969"
                },
                {
                    "start": 912,
                    "end": 934,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "276249479",
            "title": "History-Guided Video Diffusion",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2022) is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models s \u03b8 (x, c, k) \u2248 \u2207 log p k (x k |c) and s \u03b8 (x, \u2205, k) \u2248 \u2207 log p k (x k ) by randomly dropping out the conditioning c. During sampling, the true conditional score \u2207 log p k (x k |c) is replaced with the weighted score \n\nwhere \u03c9 \u2265 1 is the guidance scale that pushes the sample towards the conditioning. In VDMs, CFG is predominantly used for text guidance (Ho et al., 2022b;Wang et al., 2023). For frame conditioning, \"first frame\" guidance is commonplace in image-to-video models (Blattmann et al., 2023a;  Yang et al., 2024), or \"fixed set of few frames\" (Blattmann et al., 2023b;Gupta et al., 2023;Watson et al., 2024), likewise in multi-view diffusion models (Gao et al., 2024). \n\nOur work generalizes CFG by enabling guidance with a variable number of conditioning frames and later extends beyond the conventional approach of subtracting an unconditioned score -similar to prior works in compositional generative models (Du & Kaelbling, 2024;Liu et al., 2022;Du et al., 2023), we compose score from multiple conditioning to combine their behaviors. Additionally, we eliminate the reliance on binary-dropout training, the default mechanism for enabling CFG, which we empirically show performs sub-optimally when extended to history guidance. Diffusion Forcing. Traditionally, diffusion models are trained using uniform noise levels across all tokens. Diffusion Forcing (DF) (Chen et al., 2024) proposes training sequence diffusion models with independently varied noise levels per frame. Although DF provides theoretical and empirical support for this approach, their work focuses on causal, state-space models. CausVid (Yin et al., 2024) builds on DF by scaling it to a causal transformer, creating an autoregressive video foundation model.",
            "score": 0.4251251040485666,
            "section_title": "Preliminaries and Related Work",
            "char_start_offset": 6172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 737,
                    "end": 762,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 1127,
                    "end": 1144,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1144,
                    "end": 1160,
                    "matchedPaperCorpusId": "257078922"
                },
                {
                    "start": 1558,
                    "end": 1577,
                    "matchedPaperCorpusId": "270869622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "272524935",
            "title": "A Survey on Diffusion Models for Recommender Systems",
            "text": "In the previous sections, we have introduced the DDPMs and SGMs from an unconditional perspective, where they generate the data samples based on the learned distribution of the source data without any explicit guidance or conditions. However, the ability to control the generation process by passing explicit guidance or conditions is an important characteristic of generative models. The diffusion models are able to generate the data samples not only from an unconditional distribution  0 , but also from a conditional distribution  0 (x|) given a condition . The conditioning signals can have a variety of modalities, ranging from class labels to features (e.g., text embeddings) related to the input data x [116]. More specifically, there are various sampling algorithms designed for conditional generation [158], e.g., label-based guidance [26], label-free guidance [48], text-based conditions [37,65], \n\ngraph-based conditions [120], etc. \n\nClassically, the sampling under the conditions of labels and classifiers involves using gradient guidance at each step, typically requiring an additional differential classifier   ( |x) (e.g., U-Net [118] and Transformer [132]) to generate condition gradients for specific labels [26]. These guidance labels are flexible, and can be textual, categorical, or taskspecific feature embeddings [26,50,109,162]. This is referred to as the classifier guidance, whose conditional reverse process can be written as: \n\nwhere  is the normalization factor. \n\nAlthough classifier guidance is a common and versatile approach to improve the sample quality, it heavily relies on the availability of a noise-robust pre-trained classifier   ( |x). This requirement largely depends on the existence of annotated data to well train the classifier network, which is impractical in many real-world data-hungry applications. To this end, the classifier-free guidance is proposed. Compared to the high accuracy of the labeled conditional diffusion model, the sampling under unlabeled conditions solely relies on self-information for guidance, and is better at generating innovative, creative and diverse data samples [11,17,31,62].",
            "score": 0.4250877836078691,
            "section_title": "Conditional Diffusion Models.",
            "char_start_offset": 20642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 711,
                    "end": 716,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 811,
                    "end": 816,
                    "matchedPaperCorpusId": "252070859"
                },
                {
                    "start": 845,
                    "end": 849,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 903,
                    "end": 906,
                    "matchedPaperCorpusId": "259275061"
                },
                {
                    "start": 1146,
                    "end": 1151,
                    "matchedPaperCorpusId": "3719281"
                },
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1337,
                    "end": 1341,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1341,
                    "end": 1344,
                    "matchedPaperCorpusId": "244896176"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "264818034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "257632081",
            "title": "Generative AI and the Digital Commons",
            "text": "We will use the phrase \"generative foundation models\" (GFMs) to refer to machine learning systems that are: 1) \"generative\" -they generate text, images, or other sequences of information based on some input prompt, and 2) \"foundation models\" -neural network models trained on a large dataset comprising diverse origins and content, and can be adapted to a wide range of tasks. (Machine learning, or ML, is sometimes also referred to as artificial intelligence, or AI). Examples of wellknown GFMs are: OpenAI's GPT family of language models (including ChatGPT) that take in text and generate text; DALLE-2, which takes in text/images and generates images; BERT, which takes in text and generates text, Stable Diffusion, which takes in text and generates images; Codex, which takes in code (a specific kind of text) and generates code. We speak of \"generative\" foundation models, rather than foundation models at large, per Bommasani et al. (2022), because we are concerned primarily with the applicability of these models for generating content, such as generating text, code or images. This may include tasks such as summarization (generating a summary of a text) or text continuation (continuing the text by iteratively predicting the next word) or creating images and videos. \n\nGenerative foundation models are a general technology, although they benefit from adaptation to the \"downstream\" tasks they are used for, e.g. by \"fine-tuning\" them by training on more specific datasets such that they can generate the appropriate material for the use context. The structure of the nascent industry is likely to greatly change, but at the moment there are a few key actors creating more general-purpose GFMs, such as OpenAI, Midjourney, EleutherAI, BigScience, and Stability. More-specialized companies often build off the technology released by the actors above, applying the technology for specific tasks such as: \n\n\u2022 Copywriting (e.g. Copy.ai, Jasper, NeuralText, Nichesss) \u2022 Website generation (e.g. The.com, Debuild) \u2022 Marketing and stock image generation (e.g.",
            "score": 0.42505210465111504,
            "section_title": "THE RISE OF GENERATIVE FOUNDATION MODELS (GFMS)",
            "char_start_offset": 50,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2062
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054901123046875
        },
        {
            "corpus_id": "277104845",
            "title": "Reward-Instruct: A Reward-Centric Approach to Fast Photo-Realistic Image Generation",
            "text": "Figure 3 demonstrates the gradient norms of reward loss and divergence loss in DI++. It clearly shows that updates in DI++ are overwhelmingly driven by the reward term, relegating the diffusion distillation objective to a secondary role. \n\nSimilar phenomena can be widely observed in contemporary text-to-image generation models where various guidance modules for aligning conditions (Ho & Salimans, 2022;Bansal et al., 2023;Ma et al., 2023) are disproportionately amplified compared with the seemingly more important diffusion generation component. For instance, large classifierfree guidance (CFG) coefficients are indispensable (7.5 by default in Stable Diffusion (Rombach et al., 2022) and 100 in DreamFusion (Poole et al., 2022)). When dealing with the strong condition of a multi-attribute object correspondence, the image produced by a higher CFG is more semantically compliant. As also illustrated in (Luo, 2024), CFG can be seen as an implicit reward on text-image alignment. From a high-level view, the concept of rewards can be very general. In this work, we extend the definition of reward to include any discriminative model that can judge the goodness of generated images, including both image classifiers, language-image alignment models, and Vision Language Models (VLMs). \n\nThese findings prompt a fundamental rethinking of strong conditional generation tasks such as text-to-image generation. In diffusion models, we model the conditional density P (x|y) through the decomposition P (x|y) \u221d P (y|x)P (x). \n\nHow to better learn the marginal density P (x) is usually the main focus of diffusion training while the condition part is usually handled by CFG or external guidance in diffusion models (Bansal et al., 2023;Ma et al., 2023). However, as conditions get stronger and more complicated, the conditional distribution may be ill-conditioned to estimate. In modern text-to-image generation scenarios, the conditions are so rich that there are no two images with the same text in the training dataset.",
            "score": 0.4246656151960547,
            "section_title": "Introduction",
            "char_start_offset": 1866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 425,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 667,
                    "end": 689,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1712,
                    "end": 1733,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "234316027",
            "title": "Split-Based Algorithm for Weighted Context-Free Grammar Induction",
            "text": "The task of grammar or automata induction is a part of symbolic artificial intelligence [1] and is called grammatical inference or grammar induction [2]. Among different subtasks of this scientific field, learning (stochastic or more general weighted) context-free grammars (CFGs) from input data has been growing in importance, due to its practical implications such as natural language and biological sequences modelling. \n\nLearning CFG is known to be a hard task and notable open questions are still open [2]. According to Gold's theorem [3], CFGs cannot be learned from positive examples only, but in 1969 Horning proved that for effective probabilistic/stochastic CFG (PCFG) induction no negative evidence is obligatory [4]. It is imperative to note that learning PCFG only from positive data leads to grammars, thereby making it difficult to discriminate negative sequences from the input data. To overcome these difficulties, we have recently proposed the novel algorithm for weighted CFG (WCFG) learning [5,6]. Weighted Grammar-based Classifier System (WGCS) is one of the few grammatical inference approaches learning both grammar structure (i.e., rules) and stochastic grammar parameters (i.e., weights of rules). Initially, the method was dedicated to learning crisp context-free grammar [7], and later, it was extended to weighted versions (including fuzzy one [8] or stochastic [9]). \n\nWGCS is learned in an unsupervised manner from unannotated data such as, a structured corpus or treebank. There are some other unsupervised grammatical inference methods like ABL [10], EMILE [11], ADIOS [12], or LS [13]. However, none of these methods induces both structure and parameters of grammar. \n\nThe main contribution of this paper is to define and test a new version of WGCS approach, in which the split concept has been employed to reveal the grammar structure. Although the split was used for the first time in [6], its verification was rudimentary and limited due to the unrepresentative bioinformatics dataset.",
            "score": 0.42343520951712504,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "12438987"
                },
                {
                    "start": 1012,
                    "end": 1015,
                    "matchedPaperCorpusId": "215756763"
                },
                {
                    "start": 1015,
                    "end": 1017,
                    "matchedPaperCorpusId": "221980942"
                },
                {
                    "start": 1299,
                    "end": 1302,
                    "matchedPaperCorpusId": "119007121"
                },
                {
                    "start": 1373,
                    "end": 1376,
                    "matchedPaperCorpusId": "15507800"
                },
                {
                    "start": 1391,
                    "end": 1394,
                    "matchedPaperCorpusId": "167221980"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "1645458"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "36266782"
                },
                {
                    "start": 1602,
                    "end": 1606,
                    "matchedPaperCorpusId": "656048"
                },
                {
                    "start": 1614,
                    "end": 1618,
                    "matchedPaperCorpusId": "11257985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.229248046875
        },
        {
            "corpus_id": "274234066",
            "title": "Gradient-Free Classifier Guidance for Diffusion Model Sampling",
            "text": "Image generation using diffusion models have demonstrated outstanding learning capabilities, effectively capturing the full distribution of the training dataset. They are known to generate wide variations in sampled images, albeit with a trade-off in image fidelity. Guided sampling methods, such as classifier guidance (CG) and classifier-free guidance (CFG), focus sampling in well-learned high-probability regions to generate images of high fidelity, but each has its limitations. CG is computationally expensive due to the use of back-propagation for classifier gradient descent, while CFG, being gradient-free, is more efficient but compromises class label alignment compared to CG. In this work, we propose an efficient guidance method that fully utilizes a pre-trained classifier without using gradient descent. By using the classifier solely in inference mode, a time-adaptive reference class label and corresponding guidance scale are determined at each time step for guided sampling. Experiments on both class-conditioned and text-to-image generation diffusion models demonstrate that the proposed Gradient-free Classifier Guidance (GFCG) method consistently improves class prediction accuracy. We also show GFCG to be complementary to other guided sampling methods like CFG. When combined with the state-of-the-art Autoguidance (ATG), without additional computational overhead, it enhances image fidelity while preserving diversity. For ImageNet 512$\\times$512, we achieve a record $\\text{FD}_{\\text{DINOv2}}$ of 23.09, while simultaneously attaining a higher classification Precision (94.3%) compared to ATG (90.2%)",
            "score": 0.42283124104333664,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87255859375
        },
        {
            "corpus_id": "271957385",
            "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
            "text": "We also explore the effectiveness of classifier-free guidance (CFG), as documented in Table IX, which presents the experimental results. These findings reveal that the CFC strategy significantly influences generation performance. For example, omitting the CFG strategy (by setting \u03bb = 1) results in decreased performance. Furthermore, different parameter configuration also influences the performance. In this study, we default set \u03bb = 5 for experiments.",
            "score": 0.4227106823263234,
            "section_title": "G. The influence of classifier free guidance",
            "char_start_offset": 46364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 454
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "253581213",
            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
            "text": "Classifier-free diffusion guidance [20] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifierfree guidance effectively shifts probability mass toward data where an implicit classifier p \u03b8 (c|z t ) assigns high likelihood to the conditioning c. Our model is therefore capable of conditional or unconditional denoising with respect to both or either conditional inputs. We introduce two guidance scales, s I and s T , which can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction. Our modified score estimate is as follows: \n\nIn Figure 4, we show the effects of these two parameters on generated samples. See Appendix B for details of our classifier-free guidance formulation.",
            "score": 0.42237397713631475,
            "section_title": "Classifier-free Guidance for Two Conditionings",
            "char_start_offset": 14318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "267547881",
            "title": "SPAD: Spatially Aware Multi-View Diffusers",
            "text": "Classifier-free diffusion guidance [33] is a technique used to balance the quality and diversity of images produced by diffusion models. This method is particularly effective in class-conditional and text-conditional image generation, enhancing both the visual quality of images and their alignment with given conditions. Inspired by [5] we explore the integration of classifier-free guidance with Epipolar Attention and Pl\u00fccker Embedding. Implementing classifierfree guidance involves simultaneous training of the diffusion model for both conditional and unconditional denoising tasks. During inference, these models' score estimates are merged. We have four different types of conditioning injected into our system: Outcome: As shown in Fig. 13, we find that classifier-free guidance beyond text conditioning does not provide additional benefits, and rather leads to over-saturated generations. This also aligns with our observations on MVDream.",
            "score": 0.4214885373002368,
            "section_title": "B.4. Classifier-free Guidance",
            "char_start_offset": 34445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 947
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 39,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 334,
                    "end": 337,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "266573122",
            "title": "SVGDreamer: Text Guided SVG Generation with Diffusion Model",
            "text": "In this section, we explore how Classifier-free Guidances (CFG) [7] affects the diversity of generated results. For VPSD, we set the number of particles as 6 and run experiments with different CFG values. For LSDS [12], we run 4 times of generation with different random seeds. The results are shown in Fig. 12. As shown in the figure, smaller CFG provides more diversity. We conjecture that this is because the distribution of smaller guidance weights has more diverse modes. However, when the CFG becomes too small (e.g., CFG= 2), it cannot provide enough guidance to generate reasonable results. Therefore, in our implementation, we set CFG to 7.5 as a trade-off between diversity and optimization stability. Note that SDS-based methods [12,48] do not work well in such small CFG weights. Instead, our   VPSD provides a trade-off option between CFG weight and diversity, and it can generate more diverse results by simply setting a smaller CFG.",
            "score": 0.4214686847878448,
            "section_title": "E.1. Ablation on CFG [7] Weights",
            "char_start_offset": 30415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 947
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 218,
                    "matchedPaperCorpusId": "253734791"
                },
                {
                    "start": 740,
                    "end": 744,
                    "matchedPaperCorpusId": "253734791"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "259252217"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "265506481",
            "title": "Autonomous Agents in Software Development: A Vision Paper",
            "text": "Generative AI refers to a category of AI models and algorithms that are designed to generate new content that is often similar to content created by humans [3]. This type of AI has experienced notable progress in recent times [8]. Nowadays, generative AI has been utilized in various fields, such as NLP, computer vision, and image and video generation [15]. In NLP, generative AI techniques are commonly used for various tasks, including text generation, machine translation, dialog systems, and code generation. According to Aydn et al. [2], researchers have leveraged generative AI models to improve various NLP tasks. Generative Adversarial Networks (GANs) and autoregressive language models, such as GPT, are subsets of generative AI that have been applied to tasks like text generation, machine translation, and dialogue systems. \n\nThe GPT model is a specific type of generative AI model that excels at generating human-like text due to its architecture, pretraining, and fine-tuning processes [25]. It shows the capabilities of generative models in the domain of NLP, and its success has spurred further research and development in text generation and language understanding [20]. The foundation of the GPT model can be traced back to the introduction of the transformer architecture proposed by Vaswani et al. [33]. This architectural innovation transformed the field of NLP by introducing the self-attention mechanism, enabling the model to capture contextual connections among words, irrespective of their position within a sequence. In 2018, OpenAI introduced the GPT-1 model to demonstrate the potential of large-scale language models for text generation tasks [25]. Progress in GPT models has increased efficiency, adaptability to various tasks, and the potential for practical applications in various industries. Several researchers and OpenAI have made significant contributions to improving the performance of GPT models by using a variety of techniques and approaches [25], [26], [23], [5]. For instance, adding more parameters to the GPT model allows it to capture more complex patterns and nuances in language.",
            "score": 0.4211075529345013,
            "section_title": "BACKGROUND 2.1 Generative AI",
            "char_start_offset": 3763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "256347543"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "257405349"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "256615207"
                },
                {
                    "start": 1991,
                    "end": 1995,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1997,
                    "end": 2001,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 2003,
                    "end": 2006,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11614990234375
        },
        {
            "corpus_id": "267069368",
            "title": "Make-A-Shape: a Ten-Million-scale 3D Shape Model",
            "text": "Classifier-free Guidance. As previously mentioned, we employ classifier-free guidance, as detailed in [23], to enhance the quality of conditioned samples. A crucial hyperparameter in this classifier-free guidance, during inference, Table 6. We quantitatively analyse the performance of our conditional generation models on different guidance weights.  is the scale parameter or guidance weight, denoted as w. This parameter plays a key role in managing the trade-off between the generation's fidelity to the input conditions and the diversity of the generated output. We experiment to explore the effect of the guidance weight parameter on the quality of samples generated by various conditional models. The guidance weight parameter was systematically adjusted in a linear progression from 1.0 to 5.0. It is important to note that, for efficient evaluation, an inference timestep of 100 was consistently employed across all experiments. The results of this study are presented in Table 6. \n\nEmpirically, we observe that a guidance weight of 2.0 is optimal for most conditional generation tasks. However, when the model is conditioned on point cloud data, a lower guidance weight of 1.0 yields better results. This contrasts with the text-to-image scenarios, which typically require a larger value for the guidance weight. We suspect this difference is attributable to the nature of the input conditions we use, such as images and point clouds, which contain more information and thus make it more challenging to generate diverse samples compared to text-based inputs. Note that we adopt these identified optimal values as fixed hyperparameters for all subsequent inferences in the remainder of our experiments, as well as for the generation of qualitative results. \n\nInference Time Step Analysis. Furthermore, we also pro-Figure 17. In comparison with meshes generated from interpolation using nearest neighbor upsampling and trilinear interpolation, our generation results display notably smoother surfaces. vide a detailed analysis of the inference timesteps for both our conditional and unconditional models. Specifically, we evaluate the generation models under the same settings as above but with varying timesteps, namely 10, 100, 500, and 1000. Table 7 presents the quantitative results for our different generative models using various time steps during inference.",
            "score": 0.420788325695462,
            "section_title": "Ablation Studies",
            "char_start_offset": 48620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2252
                },
                {
                    "start": 2253,
                    "end": 2373
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "276961040",
            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
            "text": "Trading diversity for fidelity in conditional generation is a long-standing problem that has been actively studied by the community. For probabilistic models trained with the maximum likelihood objective, Ackley et al. [2] propose low-temperature sampling to effectively focus on the mode of the learned distribution, borrowing ideas from statistical mechanics [43]. This technique has also been employed beneficially for high-quality image synthesis [33,45]. Recent large language models (LLMs) [1,19] also exploit this idea, keeping a balance between creativity and determinism via temperature control during next token prediction via the learned probability model [8]. For image synthesis with generative adversarial nets (GANs), the truncation trick [7,28] was developed to enforce sampling from a truncated normal distribution rather than the standard normal prior. This encourages conditional generations to remain close to the mode of the data distribution observed during training, preventing them from diverging too far. More recently, denoising diffusion models have demonstrated impressive generation capabilities in various domains [11,36,47,50]. Classifier-free guidance [24], built upon classifier guidance [16], has emerged as a standard for controlling conditional generations in the era of denoising diffusion models. Our work contributes to the understanding of the trade-off between diversity and fidelity in the field of denoising diffusion models. For this, we carefully study classifier(-free) guidance. The gained insights motivate a generic postprocessing step that improves the fidelity for both classifier guidance and classifier-free guidance. Generation with guidance is closely related to our study. Techniques discussed in the preceding paragraph, except classifier guidance, solely require trained generative mod-els, e.g., the generator in GANs, to control the diversity and fidelity trade-off. In contrast, guidance relies on a separate model to influence the conditional generation. Rejection sampling [9] is an active area of research in this direction. For GANs, prior works use the discriminator paired with the generator to reject generations for which the discriminator has high confidence [4,57]. Alternatively, Che et al. [10] utilize the discriminator to reject samples in the latent space.",
            "score": 0.42055637314527483,
            "section_title": "Related Works",
            "char_start_offset": 4330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 222,
                    "matchedPaperCorpusId": "12174018"
                },
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "1046577"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "49657329"
                },
                {
                    "start": 499,
                    "end": 502,
                    "matchedPaperCorpusId": "271571434"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 1151,
                    "end": 1154,
                    "matchedPaperCorpusId": "252596091"
                },
                {
                    "start": 1154,
                    "end": 1157,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1184,
                    "end": 1188,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 2229,
                    "end": 2232,
                    "matchedPaperCorpusId": "53018855"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90673828125
        },
        {
            "corpus_id": "257833743",
            "title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents",
            "text": "Large language models (LLMs; Brown et al. (2020); Lewis et al. (2020)) are deep-learning models that have been trained to predict natural language text conditioned on an input. The use of these models has led to advances in natural language performance far beyond just language mod-eling tasks. Within the realm of medicine, LLMpowered methods have shown improvements in medical tasks such as question answering (Singhal et al., 2022;Li\u00e9vin et al., 2022), information extraction (Agrawal et al., 2022), and summarization (Chintagunta et al., 2021). \n\nLLM-powered methods use natural language instructions called prompts. These instruction sets often include a task definition, rules the predictions must follow, and optionally some examples of the task input and output (Reynolds and McDonell, 2021;Brown et al., 2020). The ability of generative language models to create output based on natural language instructions (or prompts) removes the need for task-specific training (Min et al., 2022) and allows non-experts to build upon this technology. \n\nWhile many tasks can be formulated as a single prompt, later work has shown that breaking down single tasks into sub-tasks (called chaining) has benefits in terms of task performance and interpretability (Wu et al., 2022). Examples of chaining strategies include chain-of-thought (Wei et al., 2022) and other task-specific approaches (e.g, Agrawal et al. (2022)). Chain-of-thought strategies prompt the model to think through a problem as an expert might approach it, leading to improvements in some tasks (Li\u00e9vin et al., 2022;Wang et al., 2022;Tafjord et al., 2022). \n\nAll of these approaches attempt to coerce the correct generation from the base language model. However, one fundamental limitation of this strategy is that these prompting architectures are restricted to a fixed set of prompts designed for specific tasks in mind, such as writing explanations or resolving anomalies within the output.",
            "score": 0.42029341493182626,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 48,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 50,
                    "end": 69,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 521,
                    "end": 547,
                    "matchedPaperCorpusId": "235097485"
                },
                {
                    "start": 770,
                    "end": 799,
                    "matchedPaperCorpusId": "231925131"
                },
                {
                    "start": 799,
                    "end": 818,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1254,
                    "end": 1271,
                    "matchedPaperCorpusId": "238353829"
                },
                {
                    "start": 1577,
                    "end": 1595,
                    "matchedPaperCorpusId": "253098851"
                },
                {
                    "start": 1595,
                    "end": 1616,
                    "matchedPaperCorpusId": "253097865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1087646484375
        },
        {
            "corpus_id": "233168627",
            "title": "Creativity and Machine Learning: A Survey",
            "text": "In order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. A possibility is to use classifier guidance [63]: the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance [117]: to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) [198] demonstrates how classifier-free guidance can be effectively used to generate text-conditional images. In addition, it shows how diffusion models can be used for image editing by fine-tuning in order to reconstruct masked regions. Performance improvement can be obtained by means of a cascade of multiple diffusion models performing conditioning augmentation [116]. Notably, the diffusion model can operate on latent vectors instead of real images. Stable Diffusion [232] employs a diffusion model in the latent space of a pre-trained autoencoder. Similarly, DALL-E 2 [221] generates images by conditioning with image representations. At first, it learns a prior diffusion model to generate possible CLIP image embeddings from a given text caption, i.e., conditioned by its CLIP text embedding. Then, a diffusion decoder produces images conditioned by the image embedding. The generation quality can be further improved by means of generated captions for the images in the training set [19]. Imagen [235] uses instead a cascaded diffusion decoder, together with a frozen language model as a text encoder to increase the quality of output. \n\nAlthough the approach is particularly suitable for images, applications to other data sources have been developed as well. DiffWave [152] and WaveGrad [45] use diffusion models to generate audio. They overcome the continuousdiscrete dichotomy by working on waveform.",
            "score": 0.4199621459439761,
            "section_title": "Diffusion Models",
            "char_start_offset": 46859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1947
                },
                {
                    "start": 1950,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 365,
                    "end": 370,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 803,
                    "end": 808,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1168,
                    "end": 1173,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 1275,
                    "end": 1280,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1808,
                    "end": 1813,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 2082,
                    "end": 2087,
                    "matchedPaperCorpusId": "221818900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "266693667",
            "title": "Exploring the effectiveness of instruction tuning in biomedical language processing",
            "text": "Transformers have become the cornerstone of modern NLP, providing the backbone for a wide * Both authors contributed equally to this work. array of applications including machine translation, question-answering, and text summarisation (Vaswani et al., 2017). Their self-attention mechanisms and parallelised architecture have proven to be highly effective in capturing the nuances of human language (Devlin et al., 2019). \n\nAutoregressive language models, exemplified by the Generative Pre-trained Transformer series like GPT (Radford et al., 2018) and GPT-3 (Brown et al., 2020), have revolutionised the way NLP is approached. These models, operating as decoder-only transformers, excel at generating text in a sequential, token-by-token manner, leveraging their attention mechanisms to focus on relevant segments of input text. Models based on this architecture, such as GPT-4 have demonstrated a remarkable ability to perform a variety of language tasks without the need for task-specific fine-tuning, showcasing strong zero-shot and few-shot learning capabilities. This feature allows these models to effectively respond to text-based prompts, including those with a limited number of examples or instructions, thereby enabling a more interactive and dynamic text generation process. \n\nMedical language models, particularly encoderonly models like BioBERT and ClinicalBERT, have been instrumental in advancing tasks such as medical diagnosis, biomedical literature mining, and clinical information extraction (Clusmann et al., 2023;Kormilitzin et al., 2021). Excelling in areas like classification and Named Entity Recognition (NER), these models have significantly contributed to biomedical NLP. However, they often lack inherent capabilities in interpreting and executing natural language instructions or generating reports from arXiv:2401.00579v1 [cs.CL] 31 Dec 2023 medical Electronic Health Records (EHRs). This limitation has spurred research into developing generative Large Language Models (LLMs) capable of handling more dynamic tasks, aiming to parallel the performance of specialised encoder-only models in the biomedical domain.",
            "score": 0.4199434162662888,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 257,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 399,
                    "end": 420,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 559,
                    "end": 579,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1536,
                    "end": 1561,
                    "matchedPaperCorpusId": "211817856"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.107421875
        },
        {
            "corpus_id": "268733326",
            "title": "TextCraftor: Your Text Encoder can be Image Quality Controller",
            "text": "However, the text encoder from CLIP model is optimized through the contrastive objective between text and images.Therefore, it does not necessarily learn the semantic meaning of the prompt, resulting the generated image might not align well with the given prompt using such a text encoder.In Sec.3.2, we introduce the technique of improving the text encoder without using the text and image contrastive pre-training in CLIP [37].Denoising Scheduler -DDIM.After a text-to-image diffusion model is trained, we can sample Gaussian noises for the same text prompt using numerous samplers, such as DDIM [50], that iteratively samples from t to its previous step t \u2032 with the following denoising process, until t becomes 0:\n\nClassifier-Free Guidance.One effective approach to improving the generation quality during the sampling stage is the classifier-free guidance (CFG) [17].By adjusting the guidance scale w in CFG, we can further balance the trade-off between the fidelity and the text-image alignment of the synthesized image.Specifically, for the process of text-conditioned image generation, by letting \u2205 denote the null text input, classifier-free guidance can be defined as follows:",
            "score": 0.41977638466349526,
            "section_title": "Preliminaries of Latent Diffusion Models",
            "char_start_offset": 9758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 289
                },
                {
                    "start": 289,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 455
                },
                {
                    "start": 455,
                    "end": 717
                },
                {
                    "start": 719,
                    "end": 744
                },
                {
                    "start": 744,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 1026
                },
                {
                    "start": 1026,
                    "end": 1186
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "267938436",
            "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
            "text": "For most of the generative models, the generation process can be described as mapping (y, \u03f5) to x where \u03f5 is a random vector providing diversity. In practice, the controllability of the generative model is critically dependent on the label quality, i.e., y|x should be as detailed as possible. Beyond human labeling, SOTA DPMs such as DALLE-3 (Betker et al., 2023) utilize powerful vision language models (VLMs) to regenerate image captions during training. Then, given a new prompt y at the inference phase, the ideal image x|y should be deemed fit by the VLM. From this perspective, the text-to-image generation case can be seen as a model inversion task on the VLM, with explicit mappings parametrized by the score network in DPMs. On the other hand, the discriminative module also plays a more central role in aligning with the condition. As supporting evidence, consider the importance of guidance in current text-to-image DPMs (Ho & Salimans, 2022;Bansal et al., 2023;Ma et al., 2023). Although designed to work directly, a relatively large classifier-free guidance (CFG) coefficient is indispensable (7.5 by default in SD). When faced with the strong condition of a multi-attribute object correspondence, the image produced by a higher CFG is more semantically compliant (see results in Appendix A.1). \n\nThe utilization of VLM in DALLE-3 and a higher CFG in SD emphasize the importance of the discriminative com- Our method can effectively generate faithful images strictly following the prompt \"Two hot dogs sit on a white paper plate near a soda cup which is sitting on a green picnic table while a bike and a silver car are parked nearby\".",
            "score": 0.4194658358481832,
            "section_title": "Introduction",
            "char_start_offset": 1794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1649
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 364,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 954,
                    "end": 974,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "270560296",
            "title": "Consistency-diversity-realism Pareto fronts of conditional image generative models",
            "text": "Guidance scale.To control the strength of the conditioning, a guidance scale (g-scale) hyper-parameter can be used to bias the sampling of diffusion models like DDPM (Ho et al., 2020), see e.g., classifier (Dhariwal & Nichol, 2021) or classifier-free guidance (CFG) (Ho & Salimans, 2021).More precisely, rewriting Eq. ( 1) for diffusion models trained with CFG, we obtain:\n\nwhere \u03bb is the guidance scale, \u2205 is an empty conditioning prompt, and the first and second terms indicate conditional and unconditional samplings, respectively.Importantly, \u03bb can be arbitrarily increased in order to steer the model to generate samples more aligned with the conditioning p.\n\nPost-hoc filtering.To improve the generated images, e.g. in terms of realism or consistency, or to avoid certain undesirable generations, a set of images generated for the same prompt may be filtered to retain the top-m images based on a predefined criterion, which can be either based on human preferences or automatic metrics.Considering the latter case, a common choice of metric is the CLIPScore, resulting in:\n\nwhere decreasing m ensures higher consistency.\n\nRetrieval-augmented generation.Generation can be conditioned on additional information, e.g.via nearestneighbor search in a database given a query image.\n\nwhere \u2295 denotes the aggregation operator and K is the set of nearest neighbors of p. Existing retrievalaugmented image generative models adopt different aggregation operators.For instance, RDM (Blattmann et al., 2022), KNN-Diffusion (Sheynin et al., 2023), and Re-Imagen (Chen et al., 2022), concatenate the retrieved vectors, and use cross-attention to condition the generative process.Autoregressive models like RA-CM3 (Yasunaga et al., 2023) and CM3Leon (Yu et al., 2023), concatenate the retrieved vectors to the input before performing self-attention.Regardless of the type of aggregation, changing the value of k in retrieval-augmented models can affect the conditional diversity and consistency of the generations.",
            "score": 0.4194542323750936,
            "section_title": "Consistency-diversity-realism knobs",
            "char_start_offset": 11238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 15,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 372
                },
                {
                    "start": 374,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 663
                },
                {
                    "start": 665,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1079
                },
                {
                    "start": 1081,
                    "end": 1127
                },
                {
                    "start": 1129,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1282
                },
                {
                    "start": 1284,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1671
                },
                {
                    "start": 1671,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 231,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 266,
                    "end": 287,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1477,
                    "end": 1501,
                    "matchedPaperCorpusId": "253098838"
                },
                {
                    "start": 1517,
                    "end": 1539,
                    "matchedPaperCorpusId": "247996596"
                },
                {
                    "start": 1705,
                    "end": 1728,
                    "matchedPaperCorpusId": "253802096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "256416107",
            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
            "text": "We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second\"guidance\"model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance:\"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.",
            "score": 0.41941862551307807,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "266191061",
            "title": "Image is All You Need to Empower Large-scale Diffusion Models for In-Domain Generation",
            "text": "In-domain generation aims to perform a variety of tasks within a specific domain, such as unconditional generation, text-to-image, image editing, 3D generation, and more. Early research typically required training specialized generators for each unique task and domain, often relying on fully-labeled data. Motivated by the powerful generative capabilities and broad applications of diffusion models, we are driven to explore leveraging label-free data to empower these models for in-domain generation. Fine-tuning a pre-trained generative model on domain data is an intuitive but challenging way and often requires complex manual hyper-parameter adjustments since the limited diversity of the training data can easily disrupt the model's original generative capabilities. To address this challenge, we propose a guidance-decoupled prior preservation mechanism to achieve high generative quality and controllability by image-only data, inspired by preserving the pre-trained model from a denoising guidance perspective. We decouple domain-related guidance from the conditional guidance used in classifier-free guidance mechanisms to preserve open-world control guidance and unconditional guidance from the pre-trained model. We further propose an efficient domain knowledge learning technique to train an additional text-free UNet copy to predict domain guidance. Besides, we theoretically illustrate a multi-guidance in-domain generation pipeline for a variety of generative tasks, leveraging multiple guidances from distinct diffusion models and conditions. Extensive experiments demonstrate the superiority of our method in domain-specific synthesis and its compatibility with various diffusion-based control methods and applications.",
            "score": 0.4193204152399424,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492431640625
        },
        {
            "corpus_id": "257632090",
            "title": "Object-Centric Slot Diffusion",
            "text": "We further evaluate the image generation quality of LSSD. \n\nIn Figure 8, we show the image reconstruction results (denoted by cfg = 1.0). The results demonstrate that the model output suffers from visual artifacts which constrained both the image reconstruction and generation capabilities. To further improve the generation results, we explore the technique of classifier-free guidance which is widely employed in text-to-image diffusion models. \n\nClassifier-free guidance [34] uses a mixing weight (abbreviated as cfg) to control the weighted sum of noise predictions between a conditional and an unconditional diffusion model. With cf g = 1, it operates similarly to standard conditional generation. When cf g > 1, the model generates images that are more align to the text input, but comes with a cost of reduced image diversity. Practically speaking, an optimal cf g > 1 can yield images that not only better match the conditions but also exhibit enhanced quality. Since LSSD provides object slots as text tokens to the pre-trained DMs, classifier-free guidance can be applied directly to the input slot, akin to how it's applied to input text in standard DMs, without any model modifications. And our experiment shows that a cf g > 1 for slots can also significantly improve the image generation quality. \n\nIn Figure 8, we tested various cfg values with LSSD to optimize image generation quality. As we can see in the figure, from cfg = 1.0 to 1.3, the images appear cleaner with less visual artifacts, delivering conceptually more consistent images with the input slots. Nevertheless, we also observe that higher cfg values strip away intricate visual details, leading to overly simplified images. We settled on a cfg value of 1.3 for following experiments. \n\nIn Figure 9, we show image samples generated based on the same set of learned slots. The variance in these samples comes from differing the initial noise maps in the denoising process. These results demonstrate that with a proper cfg value, LSSD achieves slot-based conditional image generation with unconstrained real-world objects, which is for the first time in object-centric generative models.",
            "score": 0.41847987193055747,
            "section_title": "Real-World Image Generation Results",
            "char_start_offset": 43992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 60,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1764
                },
                {
                    "start": 1767,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "258714952",
            "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration",
            "text": "For conditional generation tasks given the condition y, the goal is to sample images from the posterior distribution p(x|y). In the work of Song et al. [53], (3) can be rewritten as follows for conditional generation with the help of Bayes' theorem \n\nwhere the posterior is divided into p t (x) and p t (y|x). In this way, the unconditional pre-trained diffusion models can be used for conditional generation with an additional classifier. \n\nHo et al. [25] introduced the classifier-free diffusion guidance with s \u03b8 (x, t, y) = \u2207 x log p t (x|y) the imageconditional diffusion models. With the same idea, Saharia et al. [48,49] trained image-conditional diffusion models for SR and image-to-image translation in concurrent work. Nichol et al. [40] proposed to use text-guided diffusion models to generate photo-realistic images with classifierfree guidance. The hyperparameter \u03bb in (1) can be interpreted as the guidance scale in classifier-free diffusion models. \n\nWhile the above methods need to train a diffusion model from scratch, conditional generation can also be done with unconditional pre-trained diffusion models [7,8,32,38]. Given (9), we can first update with one unconditional reverse diffusion step and then incorporate the conditional information.",
            "score": 0.4184588979139855,
            "section_title": "Conditional Diffusion Models",
            "char_start_offset": 9389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1263
                }
            ],
            "ref_mentions": [
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 1143,
                    "end": 1146,
                    "matchedPaperCorpusId": "245117332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "275920706",
            "title": "Visual Generation Without Guidance",
            "text": "Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to a single model, halving the computational cost. Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch. GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models. Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG. Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversity-fidelity trade-offs compared with CFG baselines, all while being guidance-free. Code will be available at https://github.com/thu-ml/GFT.",
            "score": 0.4183704940596237,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "267770122",
            "title": "Se2: Sequential Example Selection for In-Context Learning",
            "text": "As described in Section 2, LLM generates \u0177 conditioned on in-context prompts composed of examples. Prior work (Rubin et al., 2022;Cheng et al., 2023) indicates that selecting examples solely based on semantic similarity does not yield optimal performance. An intuitive and more generalized approach involves scoring by the LLM itself. Our method is adaptable to various NLP tasks, encompassing both Natural Language Understanding (NLU) and Natural Language Generation (NLG). We apply the following scoring functions for them. Given a data instance (x, y) and an example e, we measures the benefit of e for (x, y) by: NLU S NLU (x, y, e) = LH(y | e \u2295 x) \n\nwhere the label space is Y, LH(\u2022 | \u2022) is the pertoken conditional likelihood of the LLM. \n\nS NLG (x, y, e) = metric(y, \u0177), (4) where metric(\u2022) is the task-specific metric (e,g., Rouge (Lin, 2004)) to compare the prediction \u0177 and ground truth y.",
            "score": 0.41817769129977167,
            "section_title": "Example Scoring",
            "char_start_offset": 4840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 899
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 130,
                    "matchedPaperCorpusId": "245218561"
                },
                {
                    "start": 130,
                    "end": 149,
                    "matchedPaperCorpusId": "257532394"
                },
                {
                    "start": 839,
                    "end": 850,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1436767578125
        },
        {
            "corpus_id": "271843251",
            "title": "GPT-3 Powered Information Extraction for Building Robust Knowledge Bases",
            "text": "With the rapid increase in the generation of biomedical research and clinical text , it has become more and more essential for both researchers and practitioners to convert large amounts of biomedical text into structured data. Recently, pretrained language models (PLMs), which can be either generalpurpose or specialized for biomedicine, have significantly improved the ability to extract information from the biomedical text in various tasks [1]. \n\nThe development of Generative Pre-trained Transformer, GPT-3 [2], a new pre-trained language model, represents a significant advancement in the field of natural language processing. Unlike previous models which required extensive finetuning for specific tasks, GPT-3 can generalize unseen cases after being provided with just a few in-context examples. This opens up many new possibilities for NLP systems, including expanding emails, entity extraction from text, and generating code based on natural language instructions with only a few demonstration examples. \n\nNewly released pre-trained language models (PLMs), including GPT-3, Megatron-Turing NLG [3], and the Switch Transformer [4], have many thousands parameters and have demonstrated remarkable achievement in natural language processing (NLP) tasks using a new research paradigm called \"in-context learning.\" PLMs can utilize their natural language generation skills to complete prompts or pieces of text in a manner similar to how humans approach a given task. By utilizing in-context learning, these large models can tackle various NLP issues without requiring updates to their parameters. This approach results in significant savings in terms of data annotation and engineering costs compared to traditional model training methods. It is worth noting that GPT-3's incontext learning produces competitive outcomes in numerous NLP tasks, even when supplied with only a limited number of demonstrative examples in the prompt. \n\nAs there are multiple potential applications for biomedical information extraction and the cost of biomedical annotations is high, alongside the challenges in model training, in-context learning has become an appealing option for biomedical use cases.",
            "score": 0.41780414514621933,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1937
                },
                {
                    "start": 1940,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 445,
                    "end": 448,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1137,
                    "end": 1140,
                    "matchedPaperCorpusId": "231573431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.100341796875
        },
        {
            "corpus_id": "255372955",
            "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
            "text": "We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u .",
            "score": 0.4177404521233028,
            "section_title": "Classifier Free Guidance",
            "char_start_offset": 13103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "278394368",
            "title": "Diffusion Model Quantization: A Review",
            "text": "Text-conditional Guided Image Generation. We evaluate the performance of text-guided image generation using Stable Diffusion v1-4 on the MS-COCO 512x512 [91] dataset. The PLMS [29] sampler is employed with 50 sampling steps, and the classifier-free guidance (cfg) is fixed at the default value of 7.5 in Stable Diffusion, serving as a trade-off between sample quality and diversity. \n\nFor each evaluation round, we generate 30,000 images and compute the FID, sFID, and CLIP score [143] as quantitative evaluation metrics.",
            "score": 0.41768016625017,
            "section_title": "Experimental Setup",
            "char_start_offset": 69059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 521
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "14113767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "266844311",
            "title": "InFoBench: Evaluating Instruction Following Ability in Large Language Models",
            "text": "The rapidly evolving domain of Natural Language Processing (NLP) has witnessed the advent and rise of Large Language Models (LLMs) (Ouyang et al., 2022;Brown et al., 2020;OpenAI, 2023b;Touvron et al., 2023;Bai et al., 2022;Chowdhery et al., 2022;Zhang et al., 2022), which exhibit remarkable prowess in simulating human-like text generation. Their ability to follow instructions is of utmost importance for their practical application, as it customizes the outputs according to specific user needs. Despite this, evaluation of these models frequently emphasizes general natural language tasks (Wang et al., 2018(Wang et al., , 2019;;Hendrycks et al., 2020;Srivastava et al., 2022;Liang et al., 2022), or specific downstream tasks (Qin et al., 2023;Bang et al., 2023), while the critical element of instructionfollowing-the model's capacity to accurately understand and execute user instructions-has not been thoroughly explored. This evident gap in research and the absence of systematic evaluation methods dedicated to this crucial aspect serve as the impetus for our present study. We aim to establish a reliable protocol and benchmark for appraising the instruction-following aptitude of LLMs. \n\nCurrent research tends to utilize evaluation methodologies such as A/B Testing (Askell et al., 2021;Taori et al., 2023;Chiang et al., 2023;Li et al., 2023b;Wang et al., 2023c), overall scoring (Wang et al., 2022), and Elo rating (Zheng et al., 2023;Bai et al., 2022;Glaese et al., 2022)-a derivative of A/B testing. While these strategies have proven somewhat effective, they suffer from considerable drawbacks, including scalability issues and a lack of interpretability. A/B testing, for instance, necessitates N 2 comparisons for N systems. Moreover, the overall scoring system is opaque and difficult to interpret, failing to elucidate the reasoning behind the assignment of specific scores to particular instructions.",
            "score": 0.417319971553104,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 171,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 611,
                    "end": 633,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 656,
                    "end": 680,
                    "matchedPaperCorpusId": "263625818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058563232421875
        },
        {
            "corpus_id": "271051241",
            "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
            "text": "Diffusion models are powerful generative models for creating visual content from textual prompts.Their success stems from extensive training data and their ability to handle various modalities and signals, enabling diverse applications such as content editing, inpainting, and personalization.\n\nControlling a diffusion model can be achieved primarily in two ways -conditioning and guidance.When a diffusion model is conditioned, it is typically trained to accept a particular form of additional conditioning input, such as a text prompt, image edges, segmentation map, and class labels.However, adapting the model to a different condition often necessitates retraining from scratch.This reliance on expensive retraining poses challenges for end-users seeking to adopt and employ conditioning techniques to control diffusion models.\n\nAn alternative way to control a diffusion model is through a guidance mechanism.Unlike conditioning techniques, this approach does not rely on an external conditioning signal.Instead, it associates a guidance function with the diffusion model to fulfill a specific target criterion, which could be as simple as minimizing the CLIP distance between the generated image and the provided text description.When sampling an image, the reverse process iterations are steered in the direction of the guidance function's gradient, resulting in constrained image generation.\n\nWhen comparing control techniques for diffusion models, guidance emerges as a more versatile approach.It treats the diffusion network as a foundational model which can accommodate different use cases.An earlier method in this domain involved classifier guidance [8], where an explicit classifier functioned as the guidance mechanism.This method utilized the classifier's gradients to drive the image generation process.However, classifier guidance has transitioned to classifier-free guidance [12], eliminating the need for an explicit classifier.In classifier-free guidance approaches, the network is trained to adapt class-label information and conditioning signals without relying on a fixed network architecture.\n\nIn this paper, we propose enhancing image generation quality beyond classifier-free guidance by introducing a novel and universal segmentation-free guidance approach.This methodology aims to improve image quality of diffusion models without necessitating costly retraining, architectural changes, or additional computing during inference.\n\nImage generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.",
            "score": 0.41709503711511653,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 293
                },
                {
                    "start": 295,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 831
                },
                {
                    "start": 833,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1398
                },
                {
                    "start": 1400,
                    "end": 1502
                },
                {
                    "start": 1502,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1947
                },
                {
                    "start": 1947,
                    "end": 2116
                },
                {
                    "start": 2118,
                    "end": 2284
                },
                {
                    "start": 2284,
                    "end": 2456
                },
                {
                    "start": 2458,
                    "end": 2634
                }
            ],
            "ref_mentions": [
                {
                    "start": 1893,
                    "end": 1897,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "268264822",
            "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
            "text": "In this section, we spotlight several pivotal and widely utilized text-to-image foundational models.Detailed information regarding these models is systematically compiled and presented in   \u2022 GLIDE [21].To generate images aligned with free-form text prompts, GLIDE intuitively replace the class label in class-conditioned diffusion models (i.e.ADM [18]) with text, formalizing the first text-to-image diffusion model.The authors explore two different guidance for text-conditioning.\n\nFor classifier guidance, GLIDE trains a CLIP model in noisy image space to provide CLIP guidance.Following [20], GLIDE additionally investigates classifier-free guidance (CFG) for comparison, which yields more preferable results in both image photo-realism and textual alignment by human evaluators and is chosen as the fundamental mechanism for text-to-image generation.For text condition, GLIDE first transforms the input text c into a token sequence via a trainable transformer [36].Subsequently, they replace the class embedding with the pooled text features and further concatenate the projected sequence text features to the attention context at each attention layer in diffusion model.GLIDE trains the diffusion model and text transformer on the same dataset as DALL\u2022E [22].The diffusion model is trained to predict p(x t\u22121 |x t , c) and generate images with CFG.\n\n\u2022 Imagen [24].Following GLIDE, Imagen adopts classifierfree guidance (CFG) for text-to-image generation.Instead of training a task-specified text encoder from scratch in GLIDE, Imagen leverages a pre-trained and frozen large language model (LLM) as its text encoder, aiming to reduce computational demands.The authors conduct a comparative analysis of various LLMs, including those trained on imagetext datasets (e.g., CLIP [39]) and solely on text corpora (e.g., BERT [40], T5 [37]).Their findings suggest that increasing the scale of language models more effectively enhances the fidelity of samples and the congruence between image and text, compared to the enlargement of image diffusion models.Furthermore, Imagen's exploration into different text conditioning methods reveals cross-attention as the most effective technique.",
            "score": 0.4169471437077243,
            "section_title": "Text-to-Image Diffusion Models",
            "char_start_offset": 7043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 100,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 482
                },
                {
                    "start": 484,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1354
                },
                {
                    "start": 1356,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1460
                },
                {
                    "start": 1460,
                    "end": 1662
                },
                {
                    "start": 1662,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 2055
                },
                {
                    "start": 2055,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 965,
                    "end": 969,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1260,
                    "end": 1264,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1780,
                    "end": 1784,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1834,
                    "end": 1838,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "259212148",
            "title": "Exploring the Effectiveness of Dataset Synthesis: An application of Apple Detection in Orchards",
            "text": "A useful technique for obtaining realistic images in Stable Diffusion involves utilizing Prompt Engineering [40]. This entails creating input prompts that can control the image generation process, including initial images or masks, as well as textual or other cues that can be fed into the model as input. \n\nPrompt engineering enables the creation of highly specific and detailled images using Stable Diffusion. It can be employed to generate images of particular objects, scenes, or styles, as well as to manipulate the image generation process to achieve desired artistic effects. \n\nTo generate high-quality images using Stable Diffusion, carefully crafted positive and negative prompts can be employed. The inclusion of negative prompts in Stable Diffusion [41] was a refinement over Latent Diffusion [12] and involved examining the distinction between the image that is being generated, to steer the final image towards the positive prompt and steer away from the negative prompt. \n\nWhile prompt-based image generation can produce realistic images, using Classifier-free Guidance (CFG) [42] can provide even greater control over the generation process. CFG, in Stable Diffusion, amplifies the effect of the text prompt on the generated image. By default, Stable Diffusion applies a classifier to the text prompt to guide the generation of the image [12]. However, CFG allows for fine-tuning the influence of this classifier, resulting in more creative control over the generated image. Typically, CFG is defined to be in a range between 1 and 30, with lower values generating more creative images. \n\nMore specifically, CFG determines the trade-off between the coverage of modes and image fidelity [42]. When set to 1, the model generates samples based solely on the prior distribution, without any guidance. As the guidance scale increases, the model is instructed to produce samples that better match some given condition. The classifier-free aspect of this technique refers to the fact that it does not require training a classifier to incorporate guidance during generation [42]. Instead, the model is guided by adding a penalty term to the generation process, forcing the generated images to match the desired condition.",
            "score": 0.4166387137351548,
            "section_title": "Prompt Engineering",
            "char_start_offset": 11081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2228
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "259308807",
            "title": "Stay on topic with Classifier-Free Guidance",
            "text": "\"Today in France, citizens were celebrating Christmas\" \"Today in France, and chickens lay eggs\" \u03b3=0 \u03b3=1 \u03b3=1.5 \n\n\"Today in France, citizens were celebrating Thanksgiving\" \n\nx 0 \n\nx 1 \"Today in France, citizens were celebrating Bastille Day\" \u03b3=0.5 \n\nFigure 1: A notional 2D projection of a textual latent space showing how increasing the guidance weight \u03b3 increases the importance of the prompt \"Today in France,\". \n\nIn recent years large language models have exhibited strong generative capabilities to solve a diverse range of tasks [26,15,71]. \"Prompting\" is typically used to condition generation, with task instructions and context [64], or a small set of examples [15]. However, language generation, especially with smaller models, has been shown to struggle with issues such as hallucination [49], degradation [38] and meandering [76]. Various approaches have been proposed to address this, e.g.: instruction-finetuning [81,70] and reinforcement learning [56,4,6]. These techniques are expensive and their compute and data cost may not be accessible to all users. In this paper we propose an inference time methodology which, as shown in Figure 1, gives more importance to the user intent, expressed through the prompt. Our hypothesis in this paper is: focusing more on the prompt at inference-time will result in generations that better align with expected behavior. \n\nText-to-image-generation, too, has been shown to suffer from similar problems [28]. Standard inference approaches can ignore parts of the prompt-conditioning, especially with specific or uncommon prompts [53]. Classifier Guidance [28] 2.1 Guidance in Text-to-Image Models Let P \u03b8 (x) be the unconditional generative model for an image x with parameters \u03b8. During inference, we wish to condition the generation on a label or text description c in order to model P(x|c). Generative models usually generate data from an abstract representation z in semantic space that is decoded into an actual sample (e.g. the latent vectors in GANs or the intermediate sampling steps in diffusion models). Controlling the generation usually involves guiding or adding constraints to that semantic representation.",
            "score": 0.41628250998892274,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 112,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "231925131"
                },
                {
                    "start": 925,
                    "end": 929,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 960,
                    "end": 964,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "245335086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "259836938",
            "title": "Advancements in Scientific Controllable Text Generation Methods",
            "text": "Newly pre-trained conditional language models, including XLNet [45], GPT [46], GPT-2 [47], and GPT-3 [48], have been widely utilized in text generation tasks. Researchers have made efforts to enhance these pre-trained models for specific controlled text generation tasks in various studies [7,49,50]. However, a recent model called Galactica [22] has outperformed the more recent GPT-3 model in technical knowledge-related evaluations, achieving a higher score of 68.2% compared to 49.0%. \n\nWhile pre-trained models often generate fluent and grammatically correct content, adapting them for sequence-to-sequence applications like machine translation and abstractive summarization can be challenging. One model that excels in text generation when modified is the denoising autoencoder BART [51], which utilizes a sequence-to-sequence architecture. On the other hand, T5 [52] treats every natural language processing (NLP) problem as a \"text-totext\" problem, where it takes text as input and produces new text as output, making it well-suited for controlled text generation tasks. \n\nAnother approach for controlled language generation is the Plug and Play Language Model (PPLM) introduced by Dathathri et al. [53]. PPLM combines a pre-trained language model with one or more attribute classifiers, eliminating the need for training from scratch and allowing it to drive text generation based on specific attributes. \n\nLarge Language Models (LLMs) are trained on a large corpus of general text. These models have been performing extremely well on text-generation tasks. It gives a promising performance on downstream tasks such as complex reasoning, problem-solving, question-answering, etc. PaLM [54] is a 540 billion parameter, dense decoder-only transformer model. This, combined with fine-tuning techniques like Chain-of-Thought Prompting [55], shows remarkable performance on reasoning and understanding-based text generation tasks. LLAMA [56] is a collection of LLMs with parameters ranging from 7 billion to 65 billion. This has been fine-tuned by techniques to create Alpaca [57], which is fine-tuned on 52K instruction-following demonstrations generated in the technique called self-instruct [58].",
            "score": 0.41542470813776494,
            "section_title": "Pre-trained Language Model",
            "char_start_offset": 23792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "259836938"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.105224609375
        },
        {
            "corpus_id": "3543617",
            "title": "Syntax-Directed Variational Autoencoder for Structured Data",
            "text": "However, because of the lack of formalization of syntax and semantics serving as the restriction of the particular structured data, underfitted general-purpose string generative models will often lead to invalid outputs. Therefore, to obtain a reasonable model via such training procedure, we need to prepare large amount of valid combinations of the structures, which is time consuming or even not practical in domains like drug discovery. \n\nTo tackle such a challenge, one approach is to incorporate the structure restrictions explicitly into the generative model. For the considerations of computational cost and model generality, contextfree grammars (CFG) have been taken into account in the decoder parametrization. For instance, in molecule generation tasks, Kusner et al. (2017) proposes a grammar variational autoencoder (GVAE) in which the CFG of SMILES notation is incorporated into the decoder. The model generates the parse trees directly in a top-down direction, by repeatedly expanding any nonterminal with its production rules. Although the CFG provides a mechanism for generating syntactic valid objects, it is still incapable to regularize the model for generating semantic valid objects (Kusner et al., 2017). For example, in molecule generation, the semantic of the SMILES languages requires that the rings generated must be closed; in program generation, the referenced variable should be defined in advance and each variable can only be defined exactly once in each local context (illustrated in Fig 1b). \n\nAll the examples require cross-serial like dependencies which are not enforceable by CFG, implying that more constraints beyond CFG are needed to achieve semantic valid production in VAE. \n\nIn the theory of compiler, attribute grammars, or syntax-directed definition has been proposed for attaching semantics to a parse tree generated by context-free grammar. Thus one straightforward but not practical application of attribute grammars is, after generating a syntactic valid molecule candidate, to conduct offline semantic checking. This process needs to be repeated until a semantically valid one is discovered, which is at best computationally inefficient and at worst infeasible, due to extremely low rate of passing checking.",
            "score": 0.41541857433197804,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2259
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1966552734375
        },
        {
            "corpus_id": "276408784",
            "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
            "text": "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.",
            "score": 0.4153186160487977,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1689453125
        },
        {
            "corpus_id": "276107584",
            "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
            "text": "During inference, Classifier-Free Guidance (CFG) [Ho and Salimans 2022] is incorporated for both motion generation and motion editing to boost sampling quality and align conditions and target motion. Table 5. Evaluation of text-based and trajectory-based motion editing on MotionFix [Athanasiou et al. 2024] dataset. TMED * mean that we reimplement the models since original models are trained on the skeleton of SMPL format, while our models are trained on HumanML3D format. \n\nFor all motion generation tasks, we generate target motion   with the guidance of arbitrary conditions : \n\nwhere  is the timestep and   > 1 is a hyper-parameter to control the strength of corresponding conditional guidance. \n\nFor all motion editing tasks, which aim to modify the source motion based on the condition. Hence, we generate the target motion   with source motion   first and then condition : \n\nwhere   > 1 is a hyper-parameter to control the strength of source motion guidance.",
            "score": 0.41524526814222856,
            "section_title": "MotionLab Inference",
            "char_start_offset": 26837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 968
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 307,
                    "matchedPaperCorpusId": "271600825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "265157726",
            "title": "Testing LLMs on Code Generation with Varying Levels of Prompt Specificity",
            "text": "Emerging trends and challenges. The advent of large language models (LLMs) are transforming computational science and technology, offering unprecedented capabilities in processing and generating human-like text. LLMs hold promise in applications ranging from natural language understanding to automated code generation. Automated code generation is a field that explores the potential of LLMs to convert natural language prompts into executable computer code, potentially transforming software development by significantly reducing manual coding effort and minimizing human error. For example, 40% of the code generated by GitHub Copilot is being checked in with no modifications [3]. Evaluating the effectiveness and versatility of LLMs in code generation tasks is complicated, however, necessitating meticulous exploration and comprehensive testing methods. \n\nThe evolution of generative AI has been marked by the release of LLMs, such as Bard, ChatGPT-3.5, ChatGPT-4, and Claude-2, each with unique architectures and capabilities. These LLMs are driving innovations in natural language processing (NLP) and machine learning research in diverse domains. Applying these LLMs to generate code automatically is enabling more intuitive and efficient software development processes, allowing developers to focus on higher-level designs and logic. However, the inherent complexity and variability of natural language pose significant challenges in harnessing the full potential of LLMs in this domain. \n\nOverview of prompt engineering. A key element in leveraging LLMs for code generation is prompt engineering [2], which involves crafting prompts with varying degrees of specificity to elicit desired responses from LLMs. A prompt is typically a statement, question, or instruction given to an LLM to invoke a specific response or action. It guides the LLM in understanding the user's requirements and generating coherent and contextually appropriate responses. For instance, in code generation tasks, a prompt could range from a high-level problem description to a detailed input, output, and functional requirements specification. Understanding the intricacies of prompts and prompt engineering is crucial to enabling the customization of user interactions with LLMs, allowing them to optimize results in accordance with the requirements of a given task. \n\nPrompt specificity can significantly impact the accuracy and efficiency of the output generated by an LLM [10].",
            "score": 0.4150596839941268,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2353
                },
                {
                    "start": 2356,
                    "end": 2467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1348876953125
        },
        {
            "corpus_id": "277244782",
            "title": "AI-Generated Content in Landscape Architecture: A Survey",
            "text": "AI-generated content (AIGC) [165] is an important component of AI-related applications. Specifically, AIGC utilizes technologies such as generative adversarial networks (GANs) [60], pre-trained models [175], and literature indexing to generate new relevant content, such as news, articles, music, images, and more, through learning from existing data and pattern recognition. From another perspective, AIGC is a concept of general artificial intelligence, aiming to create AI capable of performing any human intelligence task. Within this framework, generative AI is one approach or pathway to achieve the goal of general AI, generating realistic data through adversarial training between a generator and a discriminator. Currently, common information carriers include text, images, audio, video, documents, web pages, and more [166]. AIGC refers to the use of AI technologies, particularly natural language processing and generative models, to automatically generate content/carriers in forms such as text, images, audio, and video. Some key technologies include natural language processing, generative models, text generation, image generation, audio generation, and video generation. The combination of these technologies enables AIGC to automatically generate various forms of content based on given conditions, expanding the application potential of AI in creative and media fields. \n\n1) Natural language processing (NLP) [182]: It is a core technology for text processing, involving the conversion of human language into a form that computers can understand and process. This technology includes tasks such as text tokenization, part-of-speech tagging, syntactic parsing, named entity recognition, sentiment analysis, and more. By applying NLP to text, AIGC can understand and process the input language information to generate corresponding rich content. \n\n2) Generative models: As an essential component of AIGC, generative models are a class of machine learning models that can generate text, images, audio, or video that adhere to specific rules based on given conditions. Representative algorithms for generative models include recurrent neural networks (RNNs) [25], variational autoencoders (VAEs) [88], and GANs [60]. These models can learn the underlying distribution of data and generate content with logical and semantic coherence.",
            "score": 0.4150060397409971,
            "section_title": "Key Technologies of AIGC",
            "char_start_offset": 21963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2347
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 201,
                    "end": 206,
                    "matchedPaperCorpusId": "268725737"
                },
                {
                    "start": 2172,
                    "end": 2176,
                    "matchedPaperCorpusId": "90263689"
                },
                {
                    "start": 2210,
                    "end": 2214,
                    "matchedPaperCorpusId": "211146177"
                },
                {
                    "start": 2225,
                    "end": 2229,
                    "matchedPaperCorpusId": "10319744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06512451171875
        },
        {
            "corpus_id": "271270499",
            "title": "Towards Zero-Shot Multimodal Machine Translation",
            "text": "We show that our method allows us to obtain an MMT system with a good trade-off between strong translation quality on unambiguous examples (i.e. where images are not necessary to translate correctly) and the capacity to exploit visual context for disambiguation. However, some applications could require stronger disambiguation capabilities and be less reliant on translation fi- delity on unambiguous cases or vice versa. Instead of retraining a model to control the trade-off between the two objectives, we instead propose to use classifier-free guidance (CFG) (Ho and Salimans, 2021;Sanchez et al., 2023) to control this trade-off at inference time. We define CFG in the context of MMT as follows: \n\nwhere f \u03b8 is the text-only MT system, f \u03b8,\u03b2 the adapted MMT system, x and y the source and generated sentence, i the visual input, j the token index and \u03b3 the CFG value controlling guidance. We analyse the evolution of BLEU and COMET scores on standard generation benchmarks (where text context is enough to translate correctly), and CoMMuTE scores when varying the \u03b3 parameter. Table 6 shows that ZeroMMT-3.3B can achieve a boost in CoMMuTE accuracy of up to 4.2 points for \u03b3 = 1.5, while facing only a moderate drop of BLEU and COMET scores on unambiguous generation benchmarks (which do not require images as additional context in theory). Higher \u03b3 values result in stronger disambiguation capabilities, as shown by CoMMuTE, but this comes at the ex-pense of a drop in generation quality on the unambiguous benchmarks. CFG can therefore allow us to control the trade-off between disambiguation capability and translation fidelity depending on the application. Importantly, we strongly outperform Multilingual OpenFlamingo on all metrics for different CFG values and we can obtain CoMMuTE scores up to 71.7 on average for \u03b3 = 3.0. Results for ZeroMMT-600M and 1.3B can be found in Table 11 in Appendix A.2.",
            "score": 0.4146222253185903,
            "section_title": "Controlling the disambiguation level",
            "char_start_offset": 22331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 563,
                    "end": 586,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "266149498",
            "title": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations",
            "text": "Both training objectives, L prior and L decoder , integrate Classifier-Free Guidance (CFG) [11], enhancing the model's generative capabilities. During training, conditions are omitted 10% of the time to foster unconditional generation, subsequently improving test performance as CFG works as implicit classifier guidance [11].",
            "score": 0.41450130713776434,
            "section_title": "Preliminaries",
            "char_start_offset": 10175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "10959945",
            "title": "Two decades of statistical language modeling: where do we go from here?",
            "text": "While all SLMs get some inspiration from an intuitive view of language, in most models actual linguistic content is quite negligible. Several SLM techniques, however, are directly derived from grammars commonly uses by linguists. \n\nContext free grammar (CFG) is a crude yet well understood model of natural language. A CFG is defined by a vocabulary, a set of non-terminal symbols and a set of production or transition rules. Sentences are generated, starting with an initial non-terminal, by repeated application of the transition rules, each transforming a non-terminal into a sequence of terminals (i.e. words) and non-terminals, until a terminals-only sequence is achieved. Specific CFGs have been created based on parsed and annotated corpora such as [39], with good, though still incomplete, coverage of new data. \n\nA probabilistic (or stochastic) context free grammar puts a probability distribution on the transitions emanating from each non-terminal, thereby inducing a distribution over the set of all sentences. These transition probabilities can be estimated from annotated corpora using the Inside-Outside algorithm [40], an Estimation-Maximization (EM) algorithm (see [41]). However, the likelihood surfaces of these models tend to contain many local maxima, and the locally maximal likelihood points found by the algorithm usually fall short of the global maximum. Furthermore, even if global ML estimation were feasible, it is generally believed that context sensitive transition probabilities are needed to adequately account for actual behavior of language. Unfortunately, no efficient training algorithm is known for this situation. \n\nIn spite of this, [42] successfully incorporated CFG knowledge sources into a SLM to achieve a 15% reduction in a speech recognition error rate in the ATIS domain. They did so by parsing the utterances with a CFG to produce a sequence of grammatical fragments of various types, then constructing a trigram of fragment types to supplant the standard ngram. \n\nLink grammar is a lexicalized grammar proposed by [43]. Each word is associated with one or more ordered sets of typed links; each such link must be connected to a similarly typed link of another word in the sentence. A legal parse consists of satisfying all links in the sentence via a planar graph.",
            "score": 0.41450130713776434,
            "section_title": "Linguistically motivated models",
            "char_start_offset": 16311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2009
                },
                {
                    "start": 2012,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2312
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "252796"
                },
                {
                    "start": 1129,
                    "end": 1133,
                    "matchedPaperCorpusId": "121084921"
                },
                {
                    "start": 1182,
                    "end": 1186,
                    "matchedPaperCorpusId": "62304080"
                },
                {
                    "start": 1672,
                    "end": 1676,
                    "matchedPaperCorpusId": "15437229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10992431640625
        },
        {
            "corpus_id": "266162752",
            "title": "4M: Massively Multimodal Masked Modeling",
            "text": "Drawing parallels to ControlNet [130], these allow for steering the generation using more than just text. However, 4M is able to perform these tasks with just a single network and condition on multiple (partial) modalitiesindividually or simultaneously. The conditions can either be hand-specified, or extracted from an image using 4M itself, thereby removing the need for specialist models to create the conditions. \n\nMultimodal weighted guidance. Classifier-free guidance [50] has been shown to improve image fidelity in token-based generative models [40,123,18]. Inspired by Liu et al. [68] that perform compositional generation on multiple text conditions, we can guide our generation by weighting different (parts of) modalities by different continuous amounts -even negatively. This unlocks further multimodal editing capabilities (see Figure 4 bottom), such as being able to weakly condition on certain modalities, or using negative weighing to avoid a certain concept in the generation. Multimodal guidance can be achieved by computing a weighted sum of the logits of an unconditional and each conditional case: logits guided = logits uncond + n i=1 w i logits cond, i \u2212 logits uncond . We provide additional visualizations covering 4M's wide range of generative capabilities on our website and in Appendix A.4.",
            "score": 0.4143097512686986,
            "section_title": "Generative Capabilities & Probing the Learned Representation",
            "char_start_offset": 20665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 37,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 561,
                    "end": 564,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "249375227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "271543899",
            "title": "Comparison of Large Language Models for Generating Contextually Relevant Questions",
            "text": "This research evaluated three LLMs in facilitating QG for educational applications. We aimed to gain deeper understanding of how Generative AI can be used to produce contextually relevant questions using slide text as context. The proposed pipeline to extract the concepts or \"answers\" resulted in a unique and effective method to process slides and obtain more granular information for the question generation task. This method can be applied for other text-based educational materials besides slides, such as textbooks and websites. The resulting questions may contribute to effective learning, inspire content creation, and allow for assessment and knowledge reinforcement. \n\nAll the evaluated LLMs demonstrated capability in QG, scoring high in clarity, relevance, and slide relation. The models performed well without fine-tuning, making them immediately applicable. Despite some limitations in answer alignment and occasional biases that complicate question interpretation, these models have potential for educational applications. Nevertheless, for tasks requiring high precision and QA alignment, fine-tuning and further improvements are necessary. \n\nMaterials are available at https://github.com/limu-research/2024-ectel-qg.",
            "score": 0.41405654826354443,
            "section_title": "Conclusion",
            "char_start_offset": 11446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0867919921875
        },
        {
            "corpus_id": "273403512",
            "title": "Unlocking the Capabilities of Masked Generative Models for Image Synthesis via Self-Guidance",
            "text": "In this paper, we define generalized guidance for discrete diffusion models, as a counterpart for guidance in continuous domain [22]. To generate guidance, we propose an auxiliary task to apply semantic smoothing in VQ tokens. Experimental results show that the proposed guidance effectively and efficiently improves the generative capabilities of MGMs on class conditional image generation. \n\nFuture work. Although the proposed guidance can improve the generative capabilities of MGMs on class conditional image generation, there is still room for improvement in various aspects: (1) experiments on large-scale text conditional generative models [5], (2) generalization to discrete diffusion models [16], and (3) generalization to various domains such as audio [63], and video [57]. \n\nSocietal impacts. While our research does not directly touch ethical issues, the rapid growth of generative models raises considerations in AI ethics (e.g., potential misuse in creating deepfakes [34], generating antisocial content [12], or vulnerabilities to adversarial attacks [61,17,30]). \n\nA Detailed Implementation of feature selection module (TOAST) \n\nThe architecture of the proposed sampling method consists of two parts: MaskGIT [4] and TOAST [49]. Since we have not changed the MaskGIT architecture for plug-and-play sampling guidance, we briefly explain the implementation details of TOAST below. The TOAST modules consist of three parts: token selection module, channel selection module, and linear feed-forward networks. \n\n(i) The token selection module selects the task or class-relevant tokens by measuring the similarity with the learnable anchor vector \u03be c . We generate class conditional anchors \u03be c with simple class conditional MLPs. \n\n(ii) The channel selection is applied with learnable linear transformation matrix P. Then, the output of the token and channel selection module is calculated via z i = P \u2022 sim(z i , \u03be c ), where z i denotes the i-th input token. \n\n(iii) After the feature selection, the output is processed with L layer MLP layers, where L is equal to the number of Transformer's layers.",
            "score": 0.4137874452294604,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 29281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1740
                },
                {
                    "start": 1743,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1971
                },
                {
                    "start": 1974,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 132,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "254563906"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1066,
                    "end": 1070,
                    "matchedPaperCorpusId": "208139345"
                },
                {
                    "start": 1070,
                    "end": 1073,
                    "matchedPaperCorpusId": "258048845"
                },
                {
                    "start": 1073,
                    "end": 1076,
                    "matchedPaperCorpusId": "261049753"
                },
                {
                    "start": 1225,
                    "end": 1228,
                    "matchedPaperCorpusId": "246680316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2333984375
        },
        {
            "corpus_id": "275920706",
            "title": "Visual Generation Without Guidance",
            "text": "In this work, we proposed Guidance-Free Training (GFT) as an alternative to guided sampling in visual generative models, achieving comparable performance to Classifier-Free Guidance (CFG). GFT reduces sampling computational costs by 50%. The method is simple to implement, requiring minimal modifications to existing codebases. Unlike previous distillation-based methods, GFT enables direct training from scratch. \n\nOur extensive evaluation across multiple types of visual models demonstrates GFT's effectiveness. The approach maintains high sample quality while offering flexible control over the diversity-fidelity trade-off through temperature adjustment. GFT represents an advancement in making high-quality visual generation more efficient and accessible.",
            "score": 0.4137254550319555,
            "section_title": "Conclusion",
            "char_start_offset": 20339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 760
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "270257839",
            "title": "Safeguarding Large Language Models: A Survey",
            "text": "Guidance AI2 , a programming paradigm, offers superior control and efficiency than conventional prompting and chaining.It allows users to constrain generation (e.g., with regex and CFGs) and interleave control (conditional, loops) and generation seamlessly.This guardrail tool integrates text generation, prompts, and logic control within a single, continuous flow in a Python environment, thereby refining the text processing approach in LLMs.This unified method allows more effective LLM control than traditional prompts or thought language chains.Its features include simple and intuitive syntax built on the Handlebars template language, assuring the variable insertion in any prompts.The Guidance program has a well-defined linear execution order directly corresponding to the token sequence processed by the language model.The illustration graph of Guidance AI working flow is demonstrated in Figure 6.At any timesteps during the program execution, the language model can be called for generation(via {{gen}} tag) or to make logical flow decisions, such as {{#select}}{{or}}{{/select}} commands.Guidance supports a variety of LLMs, and during dialogue, it can use role labels to map the current LLM to correct tokens or API calls, such as {{#assistant}}, {{#user}}, {{#system}} etc.It also can be integrated with HuggingFace models, including using Guidance acceleration to speed up standard prompts by reusing key-value caches to shorten prompt execution times and using token healing to optimize prompt boundaries.Regarding token healing, this concept is related to fixing the subtleties introduced by the language model's normal greedy tokenization method.Specifically, it involves advancing the model one step while simultaneously restricting the prefix of the generated token to be the same as the previous token.Regex patterns to enforce formatting 3.1.6LMQL (Language Model Query Language) LMQL 3 , a programming interface for LLMs focusing on controlled output and safety of generated content, is designed by SRI Lab at ETH Zurich.Building on the foundation of Guidance AI, the LMQL project further advances the concept of \"prompt templates\" into a new programming language paradigm.",
            "score": 0.41361443160232847,
            "section_title": "Guidance AI",
            "char_start_offset": 16844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1101
                },
                {
                    "start": 1101,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1522
                },
                {
                    "start": 1522,
                    "end": 1665
                },
                {
                    "start": 1665,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 2045
                },
                {
                    "start": 2045,
                    "end": 2197
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05224609375
        },
        {
            "corpus_id": "270560901",
            "title": "Improving Reward-Conditioned Policies for Multi-Armed Bandits using Normalized Weight Functions",
            "text": "Conditional generation for optimization and decisionmaking Conditional generative models have been explored in many recent works to solve optimization and decision-making problems.Jiang et al. (2019) uses CVAEs for slate optimization problems by learning the joint distribution of components on the slate conditioned on user responses.Chen et al. (2021) propose to formulate decisionmaking as sequence modeling tasks where the state, action and reward at each step are consumed by a transformer to output the action, which can be also seen as an instantiation of reward-conditoned policies.Follow-up works generalize this idea to other models such as diffusion models (Ajay et al., 2023) or with more complex setups with online components (Zheng et al., 2022).For generation tasks, Kanungo et al. (2022) explores conditioning on positive click-throughrates to improve headline generation by fintuning pretrained language models for reward conditioning.",
            "score": 0.4134975704041939,
            "section_title": "Reward-conditioned policies",
            "char_start_offset": 6679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 952
                }
            ],
            "ref_mentions": [
                {
                    "start": 668,
                    "end": 687,
                    "matchedPaperCorpusId": "254044710"
                },
                {
                    "start": 782,
                    "end": 803,
                    "matchedPaperCorpusId": "250343948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "277272356",
            "title": "Guidance Free Image Editing via Explicit Conditioning",
            "text": "Despite the success of diffusion models, faster training and more diverse and higher quality sampling was shown to depend on selecting data, noise or velocity parametrizations (Salimans & Ho, 2022). Conditional Flow Matching (CFM) (Lipman et al., 2023;Tong et al., 2024;Lipman et al., 2024;Esser et al., 2024) provides an alternative natural parametrization for diffusion, namely via optimal transport, that yields SOTA models. More broadly, general diffusion and flow models can be recast in a unified framework, viewed under Evidence Lower BOund (ELBO) objectives (Kingma & Gao, 2023). \n\nCFG and selected alternatives Classifier Free Guidance (CFG) (Ho & Salimans, 2022;Zheng et al., 2023) is a key pillar of SOTA diffusion and flow systems, defining a conditional framework to image generation. Great focus is on its theoretical underpinning (Bradley & Nakkiran, 2024) and candidate alternatives such as Independent Condition Guidance (ICG) and Time Step Guidance (TSG) (Sadat et al., 2024). Our work provides a new avenue of research for conditional image generation with a new Explicit Conditioning (EC) mechanism that is on par with CFG in terms of sample diversity and quality. \n\nSingle pass distillation. Previous work (Meng et al., 2023) can distill CFG from three passes to one pass, sensibly reducing latency, while matching CFG on diversity and quality generation. More recently (Ahn et al., 2024) proposed to extend the distillation-based line of methods to learn a mapping from normal Gaussian to a guidance-free Gaussian distribution. Their work shares similarities with us in terms of exploring the sampling distribution, however, they still rely on guidance-dependent teachers to learn such a mapping. Our approach achieves the same goal without the painstaking of distillation. \n\nImage editing.",
            "score": 0.4131492776912344,
            "section_title": "Related Work",
            "char_start_offset": 5845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 270,
                    "matchedPaperCorpusId": "259847293"
                },
                {
                    "start": 270,
                    "end": 290,
                    "matchedPaperCorpusId": "274598274"
                },
                {
                    "start": 290,
                    "end": 309,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 845,
                    "end": 871,
                    "matchedPaperCorpusId": "271903235"
                },
                {
                    "start": 1227,
                    "end": 1246,
                    "matchedPaperCorpusId": "252762155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "265607951",
            "title": "Evaluating ChatGPT and Bard AI on Arabic Sentiment Analysis",
            "text": "While BERT and BERT-like models are discriminative models for NLU tasks, the NLP community also witnessed a surge in the development and application of generative models designed to produce new text samples. Examples of generative models include, the GPT (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020), T5 (Raffel et al., 2020), andBLOOM (Scao et al., 2022). Similar to BERTlike models, there have been proposed multilingual and language-specific generative models and, more specifically for Arabic, such as AraT5 (Elmadany et al., 2022), andAraGPT-2 (Antoun et al., 2021). Generative models have shown promise in tasks like text completion, translation, summarization, and even sentiment analysis, where they can be used to generate sentiment-consistent text expansions, modifications, or new text examples. In particular, Elmadany et al. (2022) shows that the AraT5 model outperforms state-of-the-art models on several Arabic language generation tasks. AraT5 is pre-trained on a large Arabic text and code dataset and fine-tuned on diverse Arabic language generation tasks, including machine translation, summarization, question answering, and paraphrasing.",
            "score": 0.412963173538644,
            "section_title": "Generative Models for Arabic NLP",
            "char_start_offset": 7440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1178
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 301,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 301,
                    "end": 320,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 325,
                    "end": 351,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 533,
                    "end": 561,
                    "matchedPaperCorpusId": "247451025"
                },
                {
                    "start": 843,
                    "end": 865,
                    "matchedPaperCorpusId": "247451025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070556640625
        },
        {
            "corpus_id": "258352203",
            "title": "Controlled Text Generation with Natural Language Instructions",
            "text": "Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCTG is more flexible to different constraint types and has a much smaller impact on the generation quality and speed because it does not modify the decoding procedure. Additionally, InstructCTG allows the model to adapt to new constraints without re-training through the use of few-shot task generalization and in-context learning abilities of instruction-tuned language models.",
            "score": 0.4126585217342824,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30126953125
        },
        {
            "corpus_id": "245986550",
            "title": "A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models",
            "text": "The PLMs have mastered a remarkable level of linguistic knowledge (semantic, syntax, etc.) from large-scale corpus, naturally enabling the production of more fluent and diverse text. However, due to the black box characteristics of neural networks, the general PLMs are still not sufficiently controllable during the text generation process. How to fully exploit the powerful PLMs to generate the desired and controllable text has become a promising yet challenging field in both academia and industry. Based on the systematic review of the key concepts, methods, and findings in the latest developments in PLM-based controllable text generation, we think this promising and fastgrowing area is still facing a number of challenges. \n\nFirst, PLMs have learned rich knowledge from large-scale corpus used for pre-training. However, an NLG model needs to learn control constraints on its own training corpus. It is often difficult for the existing PLM-based models to ensure the domain diversity of the generated text while pursuing controllability. This is indeed the well-known catastrophic forgetting problem in PLM. In the field of text generation, it is still a challenge to overcome this problem and improve the ability of the PLM-based NLG model to generate multi-domain text that satisfies specific control conditions, with few or zero domain-specific samples. \n\nSecond, controlling the generation of text in the decoding stage of a generative model is a lowcost method of model training. It can maintain the characteristics of the original language model to the greatest extent. However, in most cases, the existing methods are relatively rudimentary, and only use the external decoupled attribute discriminator to control the attributes. There is a distribution gap between the discriminator and the generator, leading to a coarser granularity in the guidance process and decreased quality of the generated text. In addition, it is difficult to directly apply the decoding-time approachesto fine-grained control scenarios such as data-to-text or multi-attribute control tasks. \n\nThird, from the perspective of probability theory, a generative pre-trained language model (referring specifically to the GPT-like models) is essentially an enhanced version of dense conditional probability p (x n | x 1 , x 2 , . . .",
            "score": 0.41230775020776256,
            "section_title": "Challenges",
            "char_start_offset": 72116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2083
                },
                {
                    "start": 2086,
                    "end": 2319
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.174072265625
        },
        {
            "corpus_id": "265213217",
            "title": "Fusion-Eval: Integrating Assistant Evaluators with LLMs",
            "text": "Evaluating the performance of natural language generation models has significant challenges (Ouyang et al., 2022), particularly in terms of evaluation benchmarks and evaluation paradigms (Wang et al., 2023b). This study focuses on the latter one. Typically, the evaluation paradigms fall into three categories: human-based, automatic-metrics-based and model-based evaluations. Among these, human evaluations are regarded as the most reliable, yet they come with high costs and issues of scalability. \n\nAutomatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are prevalent in evaluations, relying on comparisons with a 'gold' standard reference. However, the creation of these gold references is a labor-intensive process. Furthermore, studies such as Fabbri et al. (2021) have demonstrated that these automatic metrics often do not correlate well with human judgment. \n\nModel-based evaluations aim to enhance the correlation with human judgment using neural networks fine-tuned on specific datasets. Neural evaluators like BLEURT (Sellam et al., 2020) and its variant SMART (Amplayo et al., 2022) show improved alignment with human assessments in various generative tasks. These models offer flexibility in evaluation methods. They can either compare the response to the source (reference-free), or to the gold standard (reference-dependent). \n\nRecent advancements have seen the use of Large Language Models (LLMs) as referencefree evaluators in Natural Language Generation (NLG) tasks. Notably, studies by Fu et al. (2023); Wang et al. (2023a) have leveraged LLMs to rate candidate outputs based on their generation probability alone, eliminating the need for reference text comparisons. Additionally, Liu et al. (2023) introduced a method called G-Eval, where LLMs, guided by human-crafted evaluation criteria, score responses. Meta-evaluations indicate that these LLM-based evaluators reach a level of human correlation on par with medium-sized neural evaluators (Zhong et al., 2022).",
            "score": 0.4116950719163058,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 113,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 533,
                    "end": 556,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 567,
                    "end": 578,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 772,
                    "end": 792,
                    "matchedPaperCorpusId": "220768873"
                },
                {
                    "start": 1051,
                    "end": 1072,
                    "matchedPaperCorpusId": "215548699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.127685546875
        },
        {
            "corpus_id": "252715883",
            "title": "Imagen Video: High Definition Video Generation with Diffusion Models",
            "text": "We found classifier free guidance (Ho & Salimans, 2021) to be critical for generating high fidelity samples which respect a given text prompt. This is consistent with earlier results on text-to-image models (Nichol et al., 2021;Ramesh et al., 2022;Saharia et al., 2022b;Yu et al., 2022). \n\nIn the conditional generation setting, the data x is generated conditional on a signal c, which here represents a contextualized embedding of the text prompt, and a conditional diffusion model can be trained by using this signal c as an additional input to the denoising model x\u03b8 (z t , c). After training, Ho & Salimans (2021) find that sample quality can be improved by adjusting the denoising prediction x\u03b8 (z t , c) using \n\nwhere w is the guidance strength, x\u03b8 (z t , c) is the conditional model, and x\u03b8 (z t ) = x\u03b8 (z t , c = \u2205) is an unconditional model. The unconditional model is jointly trained with the conditional model by dropping out the conditioning input c. The predictions of the adjusted denoising model x\u03b8 (z t , c) are clipped to respect the range of possible pixel values, which we discuss in more detail in the next section. Note that the linear transformation in Equation 5 can equivalently be performed in v-space \n\nFor w > 0 this adjustment has the effect of over-emphasizing the effect of conditioning on the signal c, which tends to produce samples of lower diversity but higher quality compared to sampling from the regular conditional model (Ho & Salimans, 2021). The method can be interpreted as a way to guide the samples towards areas where an implicit classifier p(c|z t ) has high likelihood; as such, it is an adaptation of the explicit classifier guidance method proposed by Dhariwal & Nichol (2022).",
            "score": 0.411381551079378,
            "section_title": "CLASSIFIER FREE GUIDANCE",
            "char_start_offset": 17387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1725
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "270560735",
            "title": "Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation",
            "text": "During inference, we use dopri5 [25], an off-the-shelf numerical ODE solver, to iteratively solve for z given the estimated vector field v \u03b8 .Specifically, at each iteration the solver determines the increment to the time parameter t, resulting in a dynamic scheduling for the inference process.The process halts when an acceptance criterion is met, defined by an error approximation of the solver and a tolerance parameter provided by the user.\n\nMulti-Source Classifier Free Guidance.We employ classifier-free guidance (CFG) [26] for the conditional vector field estimation v \u03b8 (z, t|Y).Since our set of conditioning signals combines both global and local concepts, we further experiment with multi source CFG.While prior work [27] suggest a separate evaluation for each condition, we evaluate the model considering all and partial conditions.During each inference step, we obtain an estimated vector field for each set of conditions Y \u2208 {{local}, {text}, {local, text}}.The resulting CFG formulation then follows:\n\n(4) When following the standard CFG setup (\u03b1 text = \u03b1 local = 0), we observe that the model adheres to the temporal condition while ignoring instrumentation information provided in the text prompt.To increase text influence on guidance, we set a positive weight to the text-only term \u03b1 text > 0. We found that \u03b1 text = 0.5, \u03b1 local = 0, \u03b1 local,text = 1.5 offer a good trade-off between audio quality, text alignment and temporal controls adherence.",
            "score": 0.4113085278615607,
            "section_title": "Inference",
            "char_start_offset": 9978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 445
                },
                {
                    "start": 447,
                    "end": 485
                },
                {
                    "start": 485,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1015
                },
                {
                    "start": 1017,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1466
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "122754533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "267406528",
            "title": "CodePori: Large-Scale System for Autonomous Software Development Using Multi-Agent Technology",
            "text": "Generative AI specifically generate novel content similar to human-made creations [34], [35]. There have been significant advancements in the field of Generative AI recently [36]. In recent years, generative AI has found applications across multiple domains, including computer vision, NLP, and the creation of videos and images [37], [38]. In the domain of NLP, generative AI is frequently employed for a range of activities, such as generating text, developing dialogue systems, translating languages, and generating code. According to Aydin et al. [39], researchers have utilized generative AI models to enhance different tasks in NLP. Techniques like Generative Adversarial Networks (GANs) and autoregressive language models, including GPT, which fall under the umbrella of generative AI, have been used for activities such as creating dialogue systems, translating languages, and generating text. The foundation of the GPT model can be traced back to the introduction of the transformer architecture proposed by Vaswani et al. [40]. This groundbreaking architecture transformed the NLP field with the introduction of the self-attention mechanism. The development of the self-attention mechanism has revolutionized the NLP field [41]. It enabled the model to understand the contextual relationships between words, regardless of their placement in a sequence [42], [43]. In 2018, OpenAI presented the GPT-1 model, which showed the remarkable capabilities of largescale language models in text-generation tasks [2]. LLMs consist of billions of parameters, are trained on large datasets, and have outstanding performance in processing tasks involving both natural and programming languages [44], [45]. LLMs designed for processing and generating human-like text, and GPT is a well-known example within this category [46]. GPT models are known for their capability to handle a wide range of language tasks, including translation, summarization, question-answering, and creative writing, all without requiring training specific to each task [2]. Advances in GPT models have increased their effectiveness for different tasks and their usability in various industries.",
            "score": 0.4110599176851999,
            "section_title": "Generative Artificial Intelligence",
            "char_start_offset": 49227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 333,
                    "matchedPaperCorpusId": "256615207"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1094970703125
        },
        {
            "corpus_id": "252917661",
            "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
            "text": "Among existing generative models, GAN (Goodfellow et al., 2014) suffers from the instability issue (Salimans et al., 2016), subjecting to mode collapse (Metz et al., 2017); VAE (Kingma & Welling, 2014) has to rely on surrogate objectives to approximate maximum likelihood training and Flow-based models (Dinh et al., 2017) has to use specialized architectures to construct reversible transform. Diffusion models (Ho et al., 2020;Nichol & Dhariwal, 2021) have circumvented several of these limitations and emerged as a new paradigm for generative models, theoretically underpinned by non-equilibrium thermodynamics (Sohl-Dickstein et al., 2015) and score-matching network (Song & Ermon, 2019). To date, the major breakthroughs are in domains using continuous signals, such as vision (Saharia et al., 2022a;b;Ramesh et al., 2022) and audio (Kong et al., 2020). However, extending continuous diffusion models to natural language remains an open challenge due to the inherently discrete nature of texts. \n\nOn the basis of unconditional generation in continuous space which is illustrated in Figure 1(a), existing efforts (Hoogeboom et al., 2021;Austin et al., 2021) start customizing diffusion models to text in discrete space on unconditional language modeling (i.e., free text generation). Diffusion-LM (Li et al., 2022), as in Figure 1(b), models texts in continuous space and proposes to use an extra-trained classifier as guidance (i.e., the condition signal x) to impose subtle changes (usually complex, finegrained constraints) on generated sentences. Nonetheless, these models do not naturally generalize to conditional language modeling (i.e., the model assigns probabilities p(w|x) to sequences of words w given x). In the more general sequence-to-sequence (SEQ2SEQ) setting where the condition x is also a sequence of words, applying Diffusion-LM can be difficult.",
            "score": 0.4107684329318646,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 63,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 99,
                    "end": 122,
                    "matchedPaperCorpusId": "1687220"
                },
                {
                    "start": 152,
                    "end": 171,
                    "matchedPaperCorpusId": "6610705"
                },
                {
                    "start": 177,
                    "end": 201,
                    "matchedPaperCorpusId": "211146177"
                },
                {
                    "start": 303,
                    "end": 322,
                    "matchedPaperCorpusId": "8768364"
                },
                {
                    "start": 412,
                    "end": 429,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 429,
                    "end": 453,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 614,
                    "end": 643,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 671,
                    "end": 691,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 782,
                    "end": 805,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 838,
                    "end": 857,
                    "matchedPaperCorpusId": "221818900"
                },
                {
                    "start": 1117,
                    "end": 1141,
                    "matchedPaperCorpusId": "235262511"
                },
                {
                    "start": 1141,
                    "end": 1161,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1494140625
        },
        {
            "corpus_id": "254636532",
            "title": "Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting",
            "text": "ized downsampling operation to be critical for high fidelity. Simple bicubic downsampling resulted in significant artifacts along the mask boundaries in the final output image, and switching to a parameterized downsampling convolution resulted in much higher fidelity. We also initialize the corresponding new input channel weights to zero (like [27]) -at initialization the model is identical to Imagen (it ignores the conditioning image & mask). Classifier-Free Guidance.\n\nClassifier-Free Guidance (CFG) [16] is a technique to bias samples to a particular conditioning (e.g., text prompt), at the cost of mode coverage. CFG has been found to be highly effective in boosting text-image alignment as well as image fidelity in text\u2192image models [9,27,36,50]. We found CFG continues to be critical for ensuring strong alignment between the generated image and the input text prompt for text-guided image inpainting. We follow [15] and use high guidance weights with guidance oscillation. In the base model, where ensuring strong alignment with text is most critical, we use a guidance weight schedule which oscillates between 1 and 30. We observe that high guidance weights combined with oscillating guidance [15] result in the best trade-off between sample fidelity and text-image alignment.",
            "score": 0.41075914744697783,
            "section_title": "Imagen Editor",
            "char_start_offset": 12520,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 510,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "253734769",
            "title": "EDGE: Editable Dance Generation From Music",
            "text": "Diffusion models [19,54] are a class of deep generative models which learn a data distribution by reversing a scheduled noising process. In the past few years, diffusion models have been shown to be a promising avenue for generative modeling, exceeding the state-of-the-art in generative tasks [18,24,50]. Much like previous generative approaches like VAE [28] and GAN [12], diffusion models are also capable of conditional generation. Dhariwal et al. [6] introduced classifier guidance for image generation, where the output of a diffusion model may be \"steered\" towards a target, such as a class label, using the gradients of a differentiable auxiliary model. Saharia et al. [49] proposed to use direct concatenation of conditions for Pix2Pix-like tasks, akin to Conditional GAN and CVAE [38,55], and Ho et al. [20] demonstrated that classifier-free guidance can achieve state-of-the-art results while allowing more explicit control over the diversity-fidelity tradeoff.\n\nMost recently, diffusion-based methods have demonstrated strong performance in generating motions conditioned on text [26,58,63]. While the tasks of text-to-motion and music-conditioned dance generation share high-level similarities, the dance generation task suffers more challenging computational scaling (see Sec. 3) and, due to its specialized nature, much lower data availability.",
            "score": 0.41068188939661143,
            "section_title": "Generative Diffusion Models",
            "char_start_offset": 5198,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 794,
                    "end": 797,
                    "matchedPaperCorpusId": "13936837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61669921875
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "Given the non-negligible domain shift between text and image domains, we observe that direct training on a limited interleaved text-and-image dataset can result in misaligning generated texts and images and diminished image quality. Consequently, we adopt a two-stage training strategy: an initial pretraining stage focusing on coarse feature alignment for unimodal generation, followed by a fine-tuning stage dedicated to intricate feature learning for multimodal generation. Furthermore, to amplify the effectiveness of the generative tokens throughout the diffusion process, we incorporate the idea of classifier-free guidance (Ho & Salimans, 2022) \n\nThe pretraining stage loss is expressed as \n\n1 to rescale the loss into a similar numerical range. \n\nAfter the pretraining stage, the model is capable of generating images for single text descriptions but struggles with interleaved vision-and-language generation, which includes multiple text-image pairs and requires complicated reasoning for both text and image generation. To address this, in the fine-tuning stage, we further fine-tune our model with PEFT parameters by interleaved visionand-language datasets, such as VIST, where the data sample has several steps with text-image and texts are sequentially relevant. During this stage, we construct three types of tasks from the dataset, encompassing Classifier-Free Guidance (CFG) To enhance the coherence between the generated text and images, we first leverage the idea of Classifier-free Guidance for multimodal generation. Classifierfree guidance is introduced in the text-to-image diffusion process. This method observes that the generation model P \u03b8 can achieve improved conditional results by training on both conditional and unconditional generation with conditioning dropout. In our context, we want the model to focus directly on the output features h voken from LLM. Instead of using original stable diffusion unconditional distributions (dropping \u0125voken ), the whole feature mapper also needs to be included during the unconditional process. Therefore, our objective is to accentuate the trainable condition h voken and the generation model is fixed.",
            "score": 0.41063146564140895,
            "section_title": "TRAINING STRATEGY",
            "char_start_offset": 15635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "267312219",
            "title": "Spatial-Aware Latent Initialization for Controllable Image Generation",
            "text": "Text-to-Image Generation. For the past few years, GANs [8] have dominated the text-to-image generation area [12,24,35,44]. More recently, diffusion models [11,29] have attracted increasing attention due to their impressive generation performance with respect to the stability of the training process and quality of synthetic images. The ablated diffusion model with guidance (ADM-G) [4] firstly introduces classifier guidance for label-conditional generation and modifies the U-Net architecture in diffusion models, achieving superior performance over state-of-the-art GANs. Afterward, to avoid the extra training of classifiers, classifier-free guidance is proposed [10]. Based on this guidance method, several text-to-image diffusion models [36,39] have been developed to create more realistic and human-style images from text prompts. For instance, LDM [26] proposes compressing images into the latent space to accelerate the training and sampling process. Notably, this approach utilizes the cross-attention mechanism to incorporate textual guidance in the image generation. Dall-E-2 [23] introduces a multimodal latent space to align the text inputs and synthetic images. Moreover, Imagen [28] adopts a pre-trained large language model [22] as the text encoder to enhance the understanding of text inputs. Despite these advancements, accurately synthesizing images based on position descriptions in text prompts remains a challenge for all text-to-image generative models. Layout Conditioned Image Generation. Recent works [1,7] have highlighted the challenges of guiding object layout generation solely using text inputs. Many existing approaches [19,30,31,37] address this challenge by incorporating additional spatial conditions such as bounding boxes or segmentation masks to guide the layout in image generation. These methods, however, do not consider the text inputs and are typically trained on large annotated datasets. \n\nInspired by the layout conditioned generation methods, several studies [13,18,34,41,42] have proposed combining text inputs with bounding boxes or segmentation masks to achieve better layout control. For instance, GLIGEN [13] introduces an additional attention layer to a pre-trained diffusion model to integrate both text and bounding box inputs.",
            "score": 0.4100164046149034,
            "section_title": "Related Work",
            "char_start_offset": 4968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1933
                },
                {
                    "start": 1936,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "202577442"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "91183909"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1194,
                    "end": 1198,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1528,
                    "end": 1531,
                    "matchedPaperCorpusId": "254018089"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "81981856"
                },
                {
                    "start": 1657,
                    "end": 1660,
                    "matchedPaperCorpusId": "201103722"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "249282298"
                },
                {
                    "start": 2007,
                    "end": 2011,
                    "matchedPaperCorpusId": "255942528"
                },
                {
                    "start": 2017,
                    "end": 2020,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 2020,
                    "end": 2023,
                    "matchedPaperCorpusId": "257834036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7763671875
        },
        {
            "corpus_id": "257079124",
            "title": "Guiding Large Language Models via Directional Stimulus Prompting",
            "text": "The control of language models (LMs) has been extensively studied. \n\nEarly approaches fine-tuned LMs on datasets containing desired attributes [17]. [24] proposed class-conditioned LMs, generating text with predefined control codes. However, direct LM training is costly. To address this, PPLM [12] trains an attribute model and passes gradients to control generation. GeDi [27] and DExperts [36] use class-conditional distributions as generative discriminators to guide generation, reducing computation complexity. These methods require either additional LM training or internal gradients and logistics, making them not applicable to black-box LLMs. Our approach proposes a solution to control black-box LLMs by inserting directional stimulus into the input query prompt and optimizing based on the return output. \n\nReinforcement learning for NLP Reinforcement learning has been successfully applied to various NLP tasks, such as syntactic parsing [44,29], machine translation [71,28], summarization [48,62], conversational systems [31], etc. Language models define probability distributions over tokens in their vocabulary, and the text generation problem can be naturally formulated as selecting an action in an RL setting. Therefore, there have been extensive research efforts on optimizing LMs with RL, usually by aligning them with human preferences [80,70,40,62]. For example, the LLM InstructGPT [46] is optimized with RL to better follow users' instructions and intent. In contrast with these works that directly update the LLMs to align with human preferences, our work optimizes a small policy model that generates text (stimulus) to guide LLMs to generate more human-preferred output instead of directly optimizing the LLMs, bypassing the inefficient LLM's optimization.",
            "score": 0.4098534335230105,
            "section_title": "Controllable text generation",
            "char_start_offset": 31658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 69,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1782
                }
            ],
            "ref_mentions": [
                {
                    "start": 1005,
                    "end": 1008,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 1366,
                    "end": 1369,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2381591796875
        },
        {
            "corpus_id": "221321491",
            "title": "CODIT: Code Editing With Tree-Based Neural Models",
            "text": "Modeling Code Changes. Generating source code using machine learning models has been explored in the past [31], [32], [33], [34]. These methods model a probability distribution p(c|\u03ba) where c is the generated code and \u03ba is any contextual information upon which the generated code is conditioned. In this work, we generate code edits. Thus, we are interested in models that predict code given its previous version. We achieve this using NMTstyle models, which are a special case of p(c|\u03ba), where c is the new and \u03ba is the previous version of the code. NMT allows us to represent code edits with a single end-to-end model, taking into consideration the original version of a code and defining a conditional probability distribution of the target version. Similar ideas have been explored in NLP for paraphrasing [35]. \n\nGrammar-based modeling. Context Free Grammars (CFG) have been used to describe the syntax of programming languages [36] and natural language [37], [38]. A CFG is a tuple G = (N, \u03a3, P, S) where N is a set of non-terminals, \u03a3 is a set of terminals, P is a set of production rules in the form of \u03b1 \u2192 \u03b2 and a \u2208 N , b \u2208 (N \u222a \u03a3) * , and S is the start symbol. A sentence (i.e. sequence of tokens) that belongs to the language defined by G can be parsed by applying the appropriate derivation rules from the start symbol S. A common technique for generation of utterances is to expand the left-most, bottom-most non-terminal until all nonterminals have been expanded. Probabilistic context-free grammar (PCFG) is an extension of CFG, where each production rule in associated with a probability, i.e. is defined as (N, \u03a3, P, \u03a0, S) where \u03a0 defines a probability distribution for each production rule in P conditioned on \u03b1. \n\nNeural Machine Translation Models. NMT models are usually a cascade of an encoder and a decoder.",
            "score": 0.40980812056083904,
            "section_title": "BACKGROUND",
            "char_start_offset": 6680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1830
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "2846066"
                },
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "12671905"
                },
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "21164835"
                },
                {
                    "start": 810,
                    "end": 814,
                    "matchedPaperCorpusId": "17246494"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "5182310"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "17432009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.149658203125
        }
    ],
    "quotes": {
        "cost": 0.2694750000000001,
        "quotes": [
            {
                "idx": 0,
                "key": "[233168627 | Franceschelli et al. | 2021 | Citations: 42]",
                "snippets": "A possibility is to use classifier guidance (Dhariwal et al., 2021): the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance (Ho, 2022): to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) (Nichol et al., 2021) demonstrates how classifier-free guidance can be effectively used to generate text-conditional images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
                    "[245335086 | Nichol et al. | 2021 | Citations: 3629]": "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im."
                },
                "metadata": [
                    {
                        "section_title": "Diffusion Models",
                        "pdf_hash": "",
                        "start": 150,
                        "end": 911,
                        "sentence_offsets": [
                            {
                                "start": 150,
                                "end": 321
                            },
                            {
                                "start": 322,
                                "end": 618
                            },
                            {
                                "start": 621,
                                "end": 732
                            },
                            {
                                "start": 733,
                                "end": 911
                            }
                        ],
                        "ref_mentions": [
                            "234357997",
                            "249145348",
                            "245335086"
                        ],
                        "quote": "A possibility is to use classifier guidance (Dhariwal et al., 2021): the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance (Ho, 2022): to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) (Nichol et al., 2021) demonstrates how classifier-free guidance can be effectively used to generate text-conditional images."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[249926846 | Yu et al. | 2022 | Citations: 1133]",
                "snippets": "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Classifier-Free Guidance and Reranking",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1535,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 174
                            },
                            {
                                "start": 175,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 549
                            },
                            {
                                "start": 550,
                                "end": 691
                            },
                            {
                                "start": 694,
                                "end": 774
                            },
                            {
                                "start": 775,
                                "end": 986
                            },
                            {
                                "start": 989,
                                "end": 1133
                            },
                            {
                                "start": 1134,
                                "end": 1230
                            },
                            {
                                "start": 1231,
                                "end": 1381
                            },
                            {
                                "start": 1382,
                                "end": 1535
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[255372955 | Chang et al. | 2023 | Citations: 556]",
                "snippets": "We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classifier Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1152,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 126
                            },
                            {
                                "start": 127,
                                "end": 262
                            },
                            {
                                "start": 263,
                                "end": 366
                            },
                            {
                                "start": 367,
                                "end": 479
                            },
                            {
                                "start": 482,
                                "end": 533
                            },
                            {
                                "start": 534,
                                "end": 676
                            },
                            {
                                "start": 677,
                                "end": 834
                            },
                            {
                                "start": 837,
                                "end": 1002
                            },
                            {
                                "start": 1003,
                                "end": 1153
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[257505012 | Zhang et al. | 2023 | Citations: 280]",
                "snippets": "Classifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Guidance in diffusion-based image synthesis",
                        "pdf_hash": "",
                        "start": 1293,
                        "end": 2121,
                        "sentence_offsets": [
                            {
                                "start": 1293,
                                "end": 1318
                            },
                            {
                                "start": 1319,
                                "end": 1553
                            },
                            {
                                "start": 1554,
                                "end": 1730
                            },
                            {
                                "start": 1731,
                                "end": 1825
                            },
                            {
                                "start": 1826,
                                "end": 1989
                            },
                            {
                                "start": 1990,
                                "end": 2121
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[257632090 | Jiang et al. | 2023 | Citations: 61]",
                "snippets": "Classifier-free guidance (Ho, 2022) uses a mixing weight (abbreviated as cfg) to control the weighted sum of noise predictions between a conditional and an unconditional diffusion model. With cf g = 1, it operates similarly to standard conditional generation. When cf g > 1, the model generates images that are more align to the text input, but comes with a cost of reduced image diversity. Practically speaking, an optimal cf g > 1 can yield images that not only better match the conditions but also exhibit enhanced quality. Since LSSD provides object slots as text tokens to the pre-trained DMs, classifier-free guidance can be applied directly to the input slot, akin to how it's applied to input text in standard DMs, without any model modifications. And our experiment shows that a cf g > 1 for slots can also significantly improve the image generation quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Real-World Image Generation Results",
                        "pdf_hash": "",
                        "start": 449,
                        "end": 1310,
                        "sentence_offsets": [
                            {
                                "start": 449,
                                "end": 629
                            },
                            {
                                "start": 630,
                                "end": 702
                            },
                            {
                                "start": 703,
                                "end": 833
                            },
                            {
                                "start": 834,
                                "end": 969
                            },
                            {
                                "start": 970,
                                "end": 1198
                            },
                            {
                                "start": 1199,
                                "end": 1310
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (Ho, 2022) uses a mixing weight (abbreviated as cfg) to control the weighted sum of noise predictions between a conditional and an unconditional diffusion model. With cf g = 1, it operates similarly to standard conditional generation. When cf g > 1, the model generates images that are more align to the text input, but comes with a cost of reduced image diversity. Practically speaking, an optimal cf g > 1 can yield images that not only better match the conditions but also exhibit enhanced quality. Since LSSD provides object slots as text tokens to the pre-trained DMs, classifier-free guidance can be applied directly to the input slot, akin to how it's applied to input text in standard DMs, without any model modifications. And our experiment shows that a cf g > 1 for slots can also significantly improve the image generation quality."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[259212148 | Meekeren et al. | 2023 | Citations: 1]",
                "snippets": "While prompt-based image generation can produce realistic images, using Classifier-free Guidance (CFG) [42] can provide even greater control over the generation process. CFG, in Stable Diffusion, amplifies the effect of the text prompt on the generated image. By default, Stable Diffusion applies a classifier to the text prompt to guide the generation of the image [12]. However, CFG allows for fine-tuning the influence of this classifier, resulting in more creative control over the generated image. Typically, CFG is defined to be in a range between 1 and 30, with lower values generating more creative images. \n\nMore specifically, CFG determines the trade-off between the coverage of modes and image fidelity [42]. When set to 1, the model generates samples based solely on the prior distribution, without any guidance. As the guidance scale increases, the model is instructed to produce samples that better match some given condition. The classifier-free aspect of this technique refers to the fact that it does not require training a classifier to incorporate guidance during generation [42]. Instead, the model is guided by adding a penalty term to the generation process, forcing the generated images to match the desired condition.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Prompt Engineering",
                        "pdf_hash": "",
                        "start": 987,
                        "end": 2228,
                        "sentence_offsets": [
                            {
                                "start": 987,
                                "end": 1156
                            },
                            {
                                "start": 1157,
                                "end": 1246
                            },
                            {
                                "start": 1247,
                                "end": 1358
                            },
                            {
                                "start": 1359,
                                "end": 1489
                            },
                            {
                                "start": 1490,
                                "end": 1601
                            },
                            {
                                "start": 1604,
                                "end": 1706
                            },
                            {
                                "start": 1707,
                                "end": 1811
                            },
                            {
                                "start": 1812,
                                "end": 1927
                            },
                            {
                                "start": 1928,
                                "end": 2086
                            },
                            {
                                "start": 2087,
                                "end": 2228
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While prompt-based image generation can produce realistic images, using Classifier-free Guidance (CFG) [42] can provide even greater control over the generation process. CFG, in Stable Diffusion, amplifies the effect of the text prompt on the generated image. By default, Stable Diffusion applies a classifier to the text prompt to guide the generation of the image [12]. However, CFG allows for fine-tuning the influence of this classifier, resulting in more creative control over the generated image. Typically, CFG is defined to be in a range between 1 and 30, with lower values generating more creative images. \n\nMore specifically, CFG determines the trade-off between the coverage of modes and image fidelity [42]. When set to 1, the model generates samples based solely on the prior distribution, without any guidance. As the guidance scale increases, the model is instructed to produce samples that better match some given condition. The classifier-free aspect of this technique refers to the fact that it does not require training a classifier to incorporate guidance during generation [42]. Instead, the model is guided by adding a penalty term to the generation process, forcing the generated images to match the desired condition."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[259308807 | Sanchez et al. | 2023 | Citations: 55]",
                "snippets": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 934,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[260899968 | O'Neill et al. | 2023 | Citations: 1]",
                "snippets": "In the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sajjadi et al., 2018). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;(Sahu et al., 2022)Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247958012 | Sahu et al. | 2022 | Citations: 64]": "Data augmentation is a widely employed technique to alleviate the problem of data scarcity. In this work, we propose a prompting-based approach to generate labelled training data for intent classification with off-the-shelf language models (LMs) such as GPT-3. An advantage of this method is that no task-specific LM-fine-tuning for data generation is required; hence the method requires no hyper parameter tuning and is applicable even when the available training data is very scarce. We evaluate the proposed method in a few-shot setting on four diverse intent classification tasks. We find that GPT-generated data significantly boosts the performance of intent classifiers when intents in consideration are sufficiently distinct from each other. In tasks with semantically close intents, we observe that the generated data is less helpful. Our analysis shows that this is because GPT often generates utterances that belong to a closely-related intent instead of the desired one. We present preliminary evidence that a prompting-based GPT classifier could be helpful in filtering the generated data to enhance its quality.",
                    "[44104089 | Sajjadi et al. | 2018 | Citations: 583]": "Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution."
                },
                "metadata": [
                    {
                        "section_title": "Related work Evolution of Generative Models and Control Over Conditional Generation",
                        "pdf_hash": "",
                        "start": 1102,
                        "end": 2160,
                        "sentence_offsets": [
                            {
                                "start": 1102,
                                "end": 1261
                            },
                            {
                                "start": 1262,
                                "end": 1447
                            },
                            {
                                "start": 1448,
                                "end": 1503
                            },
                            {
                                "start": 1506,
                                "end": 1509
                            },
                            {
                                "start": 1512,
                                "end": 1618
                            },
                            {
                                "start": 1619,
                                "end": 1736
                            },
                            {
                                "start": 1737,
                                "end": 1974
                            },
                            {
                                "start": 1975,
                                "end": 2160
                            }
                        ],
                        "ref_mentions": [
                            "44104089",
                            "247958012"
                        ],
                        "quote": "In the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sajjadi et al., 2018). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;(Sahu et al., 2022)Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022)."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[261242901 | Han et al. | 2023 | Citations: 16]",
                "snippets": "For diffusion models, there exist two primary strategies for achieving controllable generation. One of these is classifier guidance (CG) (Dhariwal and Nichol 2021; Liu et al. 2023b), which utilizes a classifier during the sampling process and mixes its input gradient of the log probability with the score estimate of diffusion model. It is flexible and controllable, but tends to suffer a performance degradation (Ho and Salimans 2022). Another approach, named classifierfree guidance (CFG) (Ho and Salimans 2022;Nichol et al. 2021;Ramesh et al. 2022;Saharia et al. 2022), achieves the same effect through training a conditional diffusion model directly without a guidance classifier. This method performs better but requires a large amount of data with diverse text descriptions, which is difficult for our InstructME trained with source-target paired data. In this work, to attain a tradeoff between quality and controllability, we adopt both classifier and classifier-free guidance to achieve the controllable editing of Remix operations.\n\nWe specify instrument and genre tags with CFG by incorporating these tags into text commands to train the conditional diffusion models. During the training, we discard our text condition y randomly with a certain probability p CFG following (Liu et al. 2023a;Wang et al. 2023). Then, in the sampling, we can estimate the noise \u03b5\u03b8 (t, T (y), p s , z s , z t ) with a linear combination of the conditional and unconditional score estimates:",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245117331 | Liu et al. | 2021 | Citations: 256]": "Controllable image synthesis models allow creation of diverse images based on text instructions or guidance from a reference image. Recently, denoising diffusion probabilistic models have been shown to generate more realistic imagery than prior methods, and have been successfully demonstrated in unconditional and class-conditional settings. We investigate fine-grained, continuous control of this model class, and introduce a novel unified framework for semantic diffusion guidance, which allows either language or image guidance, or both. Guidance is injected into a pretrained unconditional diffusion model using the gradient of image-text or image matching scores, without re-training the diffusion model. We explore CLIP-based language guidance as well as both content and style-based image guidance in a unified framework. Our text-guided synthesis approach can be applied to datasets without associated text annotations. We conduct experiments on FFHQ and LSUN datasets, and show results on fine-grained text-guided image synthesis, synthesis of images related to a style or content reference image, and examples with both textual and image guidance.1",
                    "[245335086 | Nichol et al. | 2021 | Citations: 3629]": "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.",
                    "[248097655 | Ramesh et al. | 2022 | Citations: 6915]": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
                    "[248986576 | Saharia et al. | 2022 | Citations: 6075]": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results."
                },
                "metadata": [
                    {
                        "section_title": "Towards Advanced Music Editing -Remix",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1482,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 95
                            },
                            {
                                "start": 96,
                                "end": 334
                            },
                            {
                                "start": 335,
                                "end": 437
                            },
                            {
                                "start": 438,
                                "end": 685
                            },
                            {
                                "start": 686,
                                "end": 859
                            },
                            {
                                "start": 860,
                                "end": 1042
                            },
                            {
                                "start": 1045,
                                "end": 1180
                            },
                            {
                                "start": 1181,
                                "end": 1322
                            },
                            {
                                "start": 1323,
                                "end": 1483
                            }
                        ],
                        "ref_mentions": [
                            "245117331",
                            "245335086",
                            "248097655",
                            "248986576"
                        ],
                        "quote": "For diffusion models, there exist two primary strategies for achieving controllable generation. One of these is classifier guidance (CG) (Dhariwal and Nichol 2021; Liu et al. 2023b), which utilizes a classifier during the sampling process and mixes its input gradient of the log probability with the score estimate of diffusion model. It is flexible and controllable, but tends to suffer a performance degradation (Ho and Salimans 2022). Another approach, named classifierfree guidance (CFG) (Ho and Salimans 2022;Nichol et al. 2021;Ramesh et al. 2022;Saharia et al. 2022), achieves the same effect through training a conditional diffusion model directly without a guidance classifier. This method performs better but requires a large amount of data with diverse text descriptions, which is difficult for our InstructME trained with source-target paired data. In this work, to attain a tradeoff between quality and controllability, we adopt both classifier and classifier-free guidance to achieve the controllable editing of Remix operations.\n\nWe specify instrument and genre tags with CFG by incorporating these tags into text commands to train the conditional diffusion models. During the training, we discard our text condition y randomly with a certain probability p CFG following (Liu et al. 2023a;Wang et al. 2023). Then, in the sampling, we can estimate the noise \u03b5\u03b8 (t, T (y), p s , z s , z t ) with a linear combination of the conditional and unconditional score estimates:"
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[265498527 | Augustin et al. | 2023 | Citations: 5]",
                "snippets": "Classifier-free guidance (CFG) [36] was introduced as an alternative to classifier guidance [9,45](Nichol et al., 2021)(Song et al., 2020). (Dhariwal et al., 2021) already used a class-conditional denoising model \u03f5 \u03b8 (x t , t, y) that was given the target class as additional input. The class label y was thereby integrated into the model via adaptive group normalization layers. They introduced classifier guidance to enforce the generation of the correct target class by strengthening the influence of y on the output of the generative process. Classifier-free guidance is an alternative that also strengthens the impact of the conditioning signal in combination with a conditional denoising model \u03f5 \u03b8 (x t , t, y) without the requirement of an external classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[227209335 | Song et al. | 2020 | Citations: 6585]": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
                    "[245335086 | Nichol et al. | 2021 | Citations: 3629]": "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im."
                },
                "metadata": [
                    {
                        "section_title": "A.3. Classifier-Free Guidance and Cross-Attention Conditioning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 714,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 105
                            },
                            {
                                "start": 106,
                                "end": 229
                            },
                            {
                                "start": 230,
                                "end": 326
                            },
                            {
                                "start": 327,
                                "end": 493
                            },
                            {
                                "start": 494,
                                "end": 714
                            }
                        ],
                        "ref_mentions": [
                            "245335086",
                            "227209335",
                            "234357997"
                        ],
                        "quote": "Classifier-free guidance (CFG) [36] was introduced as an alternative to classifier guidance [9,45](Nichol et al., 2021)(Song et al., 2020). (Dhariwal et al., 2021) already used a class-conditional denoising model \u03f5 \u03b8 (x t , t, y) that was given the target class as additional input. The class label y was thereby integrated into the model via adaptive group normalization layers. They introduced classifier guidance to enforce the generation of the correct target class by strengthening the influence of y on the output of the generative process. Classifier-free guidance is an alternative that also strengthens the impact of the conditioning signal in combination with a conditional denoising model \u03f5 \u03b8 (x t , t, y) without the requirement of an external classifier."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[266053531 | Zhang et al. | 2023 | Citations: 24]",
                "snippets": "Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Token-Level Highlight Guidance",
                        "pdf_hash": "",
                        "start": 379,
                        "end": 1521,
                        "sentence_offsets": [
                            {
                                "start": 379,
                                "end": 513
                            },
                            {
                                "start": 516,
                                "end": 619
                            },
                            {
                                "start": 620,
                                "end": 690
                            },
                            {
                                "start": 691,
                                "end": 736
                            },
                            {
                                "start": 737,
                                "end": 814
                            },
                            {
                                "start": 817,
                                "end": 903
                            },
                            {
                                "start": 904,
                                "end": 979
                            },
                            {
                                "start": 982,
                                "end": 1117
                            },
                            {
                                "start": 1118,
                                "end": 1152
                            },
                            {
                                "start": 1155,
                                "end": 1178
                            },
                            {
                                "start": 1179,
                                "end": 1320
                            },
                            {
                                "start": 1321,
                                "end": 1521
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[266149498 | Patel et al. | 2023 | Citations: 19]",
                "snippets": "During training, conditions are omitted 10% of the time to foster unconditional generation, subsequently improving test performance as CFG works as implicit classifier guidance [11].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 144,
                        "end": 326,
                        "sentence_offsets": [
                            {
                                "start": 144,
                                "end": 326
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "During training, conditions are omitted 10% of the time to foster unconditional generation, subsequently improving test performance as CFG works as implicit classifier guidance [11]."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[266162752 | Mizrahi et al. | 2023 | Citations: 74]",
                "snippets": "Classifier-free guidance is crucial for improving both image fidelity and how well the generation matches the conditioning. It is most commonly used in diffusion models [50], but can be applied in token-based models as well (Gafni et al., 2022)(Yu et al., 2022)(Chang et al., 2023). We perform classifier-free guidance by computing a weighted combinations of the logits of a forward pass with the conditioning and one without the conditioning: logits guided = logits uncond + w (logits cond \u2212 logits uncond ) . \n\nHere, w is the guidance scale. When performing chained generation, we add each fully generated modality to the set of guided modalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249926846 | Yu et al. | 2022 | Citations: 1133]": "We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",
                    "[255372955 | Chang et al. | 2023 | Citations: 556]": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
                    "[247628171 | Gafni et al. | 2022 | Citations: 524]": "Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote."
                },
                "metadata": [
                    {
                        "section_title": "A.3 Generation procedure details",
                        "pdf_hash": "",
                        "start": 618,
                        "end": 1221,
                        "sentence_offsets": [
                            {
                                "start": 618,
                                "end": 741
                            },
                            {
                                "start": 742,
                                "end": 854
                            },
                            {
                                "start": 855,
                                "end": 1082
                            },
                            {
                                "start": 1085,
                                "end": 1115
                            },
                            {
                                "start": 1116,
                                "end": 1221
                            }
                        ],
                        "ref_mentions": [
                            "247628171",
                            "249926846",
                            "255372955"
                        ],
                        "quote": "Classifier-free guidance is crucial for improving both image fidelity and how well the generation matches the conditioning. It is most commonly used in diffusion models [50], but can be applied in token-based models as well (Gafni et al., 2022)(Yu et al., 2022)(Chang et al., 2023). We perform classifier-free guidance by computing a weighted combinations of the logits of a forward pass with the conditioning and one without the conditioning: logits guided = logits uncond + w (logits cond \u2212 logits uncond ) . \n\nHere, w is the guidance scale. When performing chained generation, we add each fully generated modality to the set of guided modalities."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[266693789 | Lin et al. | 2023 | Citations: 17]",
                "snippets": "Classifier-Free Guidance (CFG) (Ho, 2022) uses Bayes' rule and finds that the diffusion model itself can be inverted as an implicit classifier. Specifically, the model is queried both conditionally and unconditionally at every inference step and the difference is amplified toward the conditional direction. Both methods only work for conditional generation and entangle sample quality with conditional alignment [24]. Self-Supervised Guidance [20] uses self-supervised networks to generate synthetic clustering labels for unconditional data. This allows unconditional data to use CFG for improving quality. Guidance-Free Training [4] shows that CFG can be applied at training. This bakes the guided flow into the model and saves computation during inference...Although these methods are effective in improving quality, they also present issues such as increased complexity and reduced diversity (Ho, 2022)[24], etc.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 530,
                        "end": 1282,
                        "sentence_offsets": [
                            {
                                "start": 530,
                                "end": 667
                            },
                            {
                                "start": 668,
                                "end": 831
                            },
                            {
                                "start": 832,
                                "end": 942
                            },
                            {
                                "start": 943,
                                "end": 1066
                            },
                            {
                                "start": 1067,
                                "end": 1131
                            },
                            {
                                "start": 1132,
                                "end": 1201
                            },
                            {
                                "start": 1202,
                                "end": 1283
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-Free Guidance (CFG) (Ho, 2022) uses Bayes' rule and finds that the diffusion model itself can be inverted as an implicit classifier. Specifically, the model is queried both conditionally and unconditionally at every inference step and the difference is amplified toward the conditional direction. Both methods only work for conditional generation and entangle sample quality with conditional alignment [24]. Self-Supervised Guidance [20] uses self-supervised networks to generate synthetic clustering labels for unconditional data. This allows unconditional data to use CFG for improving quality. Guidance-Free Training [4] shows that CFG can be applied at training. This bakes the guided flow into the model and saves computation during inference"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1692,
                        "end": 1841,
                        "sentence_offsets": [
                            {
                                "start": 1692,
                                "end": 1840
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Although these methods are effective in improving quality, they also present issues such as increased complexity and reduced diversity (Ho, 2022)[24], etc."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[267027965 | Zolna et al. | 2024 | Citations: 1]",
                "snippets": "Classifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classifier-free guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 999,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 146
                            },
                            {
                                "start": 147,
                                "end": 225
                            },
                            {
                                "start": 228,
                                "end": 381
                            },
                            {
                                "start": 382,
                                "end": 414
                            },
                            {
                                "start": 417,
                                "end": 466
                            },
                            {
                                "start": 467,
                                "end": 526
                            },
                            {
                                "start": 527,
                                "end": 674
                            },
                            {
                                "start": 675,
                                "end": 708
                            },
                            {
                                "start": 709,
                                "end": 791
                            },
                            {
                                "start": 792,
                                "end": 867
                            },
                            {
                                "start": 868,
                                "end": 999
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4)."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[268856926 | Hwang et al. | 2024 | Citations: 15]",
                "snippets": "Techniques have been proposed for conditionally sampling images corresponding to specific classes or text prompts by adding a guidance term to the predicted noise.Ho & Salimans (2022) added the gradient of the log probability predicted by a classifier to \u03f5(x t , t), enabling an unconditional diffusion model to generate class-conditioned images.Subsequently, classifier-free guidance (CFG) was proposed.Instead of using a classifier, the noise predictor's architecture was modified to accept condition c as an input.The following formula is then used as the predicted noise:",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Guidances for Diffusion Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 575,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 163
                            },
                            {
                                "start": 163,
                                "end": 346
                            },
                            {
                                "start": 346,
                                "end": 404
                            },
                            {
                                "start": 404,
                                "end": 517
                            },
                            {
                                "start": 517,
                                "end": 575
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Techniques have been proposed for conditionally sampling images corresponding to specific classes or text prompts by adding a guidance term to the predicted noise.Ho & Salimans (2022) added the gradient of the log probability predicted by a classifier to \u03f5(x t , t), enabling an unconditional diffusion model to generate class-conditioned images.Subsequently, classifier-free guidance (CFG) was proposed.Instead of using a classifier, the noise predictor's architecture was modified to accept condition c as an input.The following formula is then used as the predicted noise:"
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[270199289 | Gu et al. | 2024 | Citations: 10]",
                "snippets": "Classifier-free Guidance (CFG), which utilizes the diffusion model itself to perform guidance at test time. More specifically, we perform sampling using the following linear combination:\n\nwhere \u03b3 is the guidance weight, and x \u03b8 (x t ) = x \u03b8 (x t , c = \u2205) is the unconditional denoising output.\n\nDuring training, we drop the condition c with certain probability p uncond to facilitate unconditional prediction. When \u03b3 > 1, CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation....However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 177,
                        "end": 752,
                        "sentence_offsets": [
                            {
                                "start": 167,
                                "end": 322
                            },
                            {
                                "start": 322,
                                "end": 400
                            },
                            {
                                "start": 402,
                                "end": 507
                            },
                            {
                                "start": 509,
                                "end": 623
                            },
                            {
                                "start": 623,
                                "end": 786
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free Guidance (CFG), which utilizes the diffusion model itself to perform guidance at test time. More specifically, we perform sampling using the following linear combination:\n\nwhere \u03b3 is the guidance weight, and x \u03b8 (x t ) = x \u03b8 (x t , c = \u2205) is the unconditional denoising output.\n\nDuring training, we drop the condition c with certain probability p uncond to facilitate unconditional prediction. When \u03b3 > 1, CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation"
                    },
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 1104,
                        "end": 1275,
                        "sentence_offsets": [
                            {
                                "start": 1104,
                                "end": 1274
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[270869763 | Fuest et al. | 2024 | Citations: 24]",
                "snippets": "To address this limitation, Classifier-free guidance (CFG) (Ho, 2022) eliminates the need for a pre-trained classifier.CFG works by training an unconditional diffusion model parametrized by \u03f5 \u03b8 (x t , t, \u03d5) together with a conditional model parametrized by \u03f5 \u03b8 (x t , t, c).For the unconditional model, a null input token \u03d5 is used as a conditioning signal c.The network is trained by randomly dropping out the conditioning signal with probability p uncond .Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network.Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73]100].These methods do not need annotated data, allowing the use of larger unlabelled datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Diffusion Model Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 995,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 113
                            },
                            {
                                "start": 113,
                                "end": 268
                            },
                            {
                                "start": 268,
                                "end": 353
                            },
                            {
                                "start": 353,
                                "end": 452
                            },
                            {
                                "start": 452,
                                "end": 557
                            },
                            {
                                "start": 559,
                                "end": 722
                            },
                            {
                                "start": 722,
                                "end": 906
                            },
                            {
                                "start": 906,
                                "end": 995
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "To address this limitation, Classifier-free guidance (CFG) (Ho, 2022) eliminates the need for a pre-trained classifier.CFG works by training an unconditional diffusion model parametrized by \u03f5 \u03b8 (x t , t, \u03d5) together with a conditional model parametrized by \u03f5 \u03b8 (x t , t, c).For the unconditional model, a null input token \u03d5 is used as a conditioning signal c.The network is trained by randomly dropping out the conditioning signal with probability p uncond .Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network.Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73]100].These methods do not need annotated data, allowing the use of larger unlabelled datasets."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[270923987 | Sadat et al. | 2024 | Citations: 14]",
                "snippets": "In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score. This insight leads us to propose independent condition guidance (ICG), a method that replicates the behavior of CFG at inference time without requiring separate training of an unconditional model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score. This insight leads us to propose independent condition guidance (ICG), a method that replicates the behavior of CFG at inference time without requiring separate training of an unconditional model.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[271957385 | Yang et al. | 2024 | Citations: 15]",
                "snippets": "Classifier-free guidance (CFG) [24] has been demonstrated as an effective way to enhance the generation quality in both image and audio domains [35], [53]. CFG can be formulated as: \n\nwhere \u03bb denotes the guidance scale. CFC tries to model conditional distribution p(x|y) and unconditional distribution p(x) in the training. In the inference stage, \u03bb = 1 denotes that we do not use classifier-free guidance, when \u03bb > 1 the model decreases the unconditional likelihood of the sample while increasing the conditional likelihood. In other words, classifierfree guidance conducts this by decreasing the unconditional likelihood with a negative score term. During the training stage, previous works try to mask the condition information of some samples (e.g. set the training sample's text as empty with 10% probability), so that these samples can be used to optimize unconditional distribution p(x).\n\nMany prior studies [10], [22] demonstrate that text transcriptions generated by an ASR system can be utilized to train a TTS system. Inevitably, the text transcription from the ASR system is not flawless, referred to as noisy or weak labels. Despite this, existing literature does not thoroughly explain why TTS systems can be effectively trained using such dataset. In this study, we show that including a small part of the noisy label in the training set is equivalent to introducing the CFC training strategy, thus we do not need to deliberately construct masked samples during training stage.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256416291 | Yang et al. | 2023 | Citations: 101]": "Expressive text-to-speech (TTS) aims to synthesize speech with varying speaking styles to better reflect human speech patterns. In this study, we attempt to use natural language as a style prompt to control the styles in the synthetic speech, e.g., \u201cSigh tone in full of sad mood with some helpless feeling\u201d. Considering that there is no existing TTS corpus that is suitable to benchmark this novel task, we first construct a speech corpus whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named InstructTTS, which is novel in the sense of the following aspects: (1) We fully take advantage of self-supervised learning and cross-modal metric learning and propose a novel three-stage training procedure to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt. Extensive objective and subjective evaluation has been conducted to verify the effectiveness and expressiveness of InstructTTS. Experimental results show that InstructTTS can synthesize high-fidelity and natural speech with style prompts controlling the speaking style.",
                    "[268247980 | Esser et al. | 2024 | Citations: 1401]": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."
                },
                "metadata": [
                    {
                        "section_title": "E. Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1492,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 155
                            },
                            {
                                "start": 156,
                                "end": 181
                            },
                            {
                                "start": 184,
                                "end": 219
                            },
                            {
                                "start": 220,
                                "end": 323
                            },
                            {
                                "start": 324,
                                "end": 525
                            },
                            {
                                "start": 526,
                                "end": 650
                            },
                            {
                                "start": 651,
                                "end": 752
                            },
                            {
                                "start": 753,
                                "end": 894
                            },
                            {
                                "start": 897,
                                "end": 1029
                            },
                            {
                                "start": 1030,
                                "end": 1138
                            },
                            {
                                "start": 1139,
                                "end": 1263
                            },
                            {
                                "start": 1264,
                                "end": 1493
                            }
                        ],
                        "ref_mentions": [
                            "256416291",
                            "268247980"
                        ],
                        "quote": "Classifier-free guidance (CFG) [24] has been demonstrated as an effective way to enhance the generation quality in both image and audio domains [35], [53]. CFG can be formulated as: \n\nwhere \u03bb denotes the guidance scale. CFC tries to model conditional distribution p(x|y) and unconditional distribution p(x) in the training. In the inference stage, \u03bb = 1 denotes that we do not use classifier-free guidance, when \u03bb > 1 the model decreases the unconditional likelihood of the sample while increasing the conditional likelihood. In other words, classifierfree guidance conducts this by decreasing the unconditional likelihood with a negative score term. During the training stage, previous works try to mask the condition information of some samples (e.g. set the training sample's text as empty with 10% probability), so that these samples can be used to optimize unconditional distribution p(x).\n\nMany prior studies [10], [22] demonstrate that text transcriptions generated by an ASR system can be utilized to train a TTS system. Inevitably, the text transcription from the ASR system is not flawless, referred to as noisy or weak labels. Despite this, existing literature does not thoroughly explain why TTS systems can be effectively trained using such dataset. In this study, we show that including a small part of the noisy label in the training set is equivalent to introducing the CFC training strategy, thus we do not need to deliberately construct masked samples during training stage."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[273186941 | Kasymov et al. | 2024 | Citations: 1]",
                "snippets": "Classifier-Free Guidance offers a simpler and more robust alternative by eliminating the need for an external classifier. Instead, the diffusion model itself is trained in two modes -conditional and unconditional: \n\n\u2022 conditional mode -the model is trained to predict the denoised data x 0 given noisy data x t and conditioning information y, learning the conditional distribution p \u03b8 (x t\u22121 |x t , y); \u2022 unconditional mode -the same model is also trained without any conditioning, learning the unconditional distribution p \u03b8 (x t\u22121 |x t ). \n\nDuring inference, CFG uses a combination of the conditional and unconditional predictions to guide the generation (see Algorithm 1). Specifically, for a given noisy sample x t , the guidance is achieved by interpolating between the conditional and unconditional predictions as follows: \n\nwhere \u03f5 \u03b8 (x t , \u00f8) is the model's prediction of the noise in x t when no conditioning is provided (unconditional), \u03f5 \u03b8 (x t , y) is the prediction of the noise in x t when conditioned on y, and w is the guidance scale, which controls how strongly the conditional information influences the generation. \n\nBy adjusting w, one can control the balance between sample diversity and adherence to the conditioning y. When w = 1, the process is equivalent to standard conditional generation. When w > 1, the conditional prediction is amplified, guiding the model to produce samples that more closely match the conditioning information, potentially at the cost of diversity.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "PRELIMINARIES",
                        "pdf_hash": "",
                        "start": 456,
                        "end": 1953,
                        "sentence_offsets": [
                            {
                                "start": 456,
                                "end": 577
                            },
                            {
                                "start": 578,
                                "end": 669
                            },
                            {
                                "start": 672,
                                "end": 996
                            },
                            {
                                "start": 999,
                                "end": 1131
                            },
                            {
                                "start": 1132,
                                "end": 1284
                            },
                            {
                                "start": 1287,
                                "end": 1589
                            },
                            {
                                "start": 1592,
                                "end": 1697
                            },
                            {
                                "start": 1698,
                                "end": 1771
                            },
                            {
                                "start": 1772,
                                "end": 1953
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-Free Guidance offers a simpler and more robust alternative by eliminating the need for an external classifier. Instead, the diffusion model itself is trained in two modes -conditional and unconditional: \n\n\u2022 conditional mode -the model is trained to predict the denoised data x 0 given noisy data x t and conditioning information y, learning the conditional distribution p \u03b8 (x t\u22121 |x t , y); \u2022 unconditional mode -the same model is also trained without any conditioning, learning the unconditional distribution p \u03b8 (x t\u22121 |x t ). \n\nDuring inference, CFG uses a combination of the conditional and unconditional predictions to guide the generation (see Algorithm 1). Specifically, for a given noisy sample x t , the guidance is achieved by interpolating between the conditional and unconditional predictions as follows: \n\nwhere \u03f5 \u03b8 (x t , \u00f8) is the model's prediction of the noise in x t when no conditioning is provided (unconditional), \u03f5 \u03b8 (x t , y) is the prediction of the noise in x t when conditioned on y, and w is the guidance scale, which controls how strongly the conditional information influences the generation. \n\nBy adjusting w, one can control the balance between sample diversity and adherence to the conditioning y. When w = 1, the process is equivalent to standard conditional generation. When w > 1, the conditional prediction is amplified, guiding the model to produce samples that more closely match the conditioning information, potentially at the cost of diversity."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[273549320 | Nie et al. | 2024 | Citations: 30]",
                "snippets": "CFG (Ho & Salimans, 2022) is an effective and versatile technique widely used in both continuous and discrete diffusion models, with applications spanning image (Ho & Salimans, 2022;Chang et al., 2023) and text generation (Lovelace et al., 2024). Rooted in Bayes' rule, CFG simultaneously trains a conditional and an unconditional diffusion model, introducing a rescaled distribution for inference. Specifically, at a given timestep t \u2208 [0] 1], CFG (Chang et al., 2023) is defined as: \n\nwhere c is the condition, w is a hyperparameter that flexibly controls the strength of c, and p \u03b8 (x 0 |c, x t ) and p \u03b8 (x 0 |x t ) are the conditional and unconditional models respectively. \n\nNotably, it seems that the conditional model must be trained on paired data before applying CFG. Consequently, to the best of our knowledge, all existing work (Ho & Salimans, 2022;Chang et al., 2023;Lovelace et al., 2024) fall into supervised settings, where paired data are readily available. \n\nUnsupervised CFG. We extend CFG to an unsupervised setting by introducing a new formulation: \n\nwhere m is a mask sequence of the same length as c. Compared to Eq. ( 6), the dummy variable m translates the unconditional distribution to a conditional format without adding new information. For simplicity, we continue to refer to p \u03b8 (x 0 |m, x t ) as the unconditional distribution in unsupervised CFG throughout this paper.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "UNSUPERVISED CLASSIFIER-FREE GUIDANCE",
                        "pdf_hash": "",
                        "start": 193,
                        "end": 1593,
                        "sentence_offsets": [
                            {
                                "start": 188,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 591
                            },
                            {
                                "start": 592,
                                "end": 677
                            },
                            {
                                "start": 680,
                                "end": 871
                            },
                            {
                                "start": 874,
                                "end": 970
                            },
                            {
                                "start": 971,
                                "end": 1167
                            },
                            {
                                "start": 1170,
                                "end": 1187
                            },
                            {
                                "start": 1188,
                                "end": 1262
                            },
                            {
                                "start": 1265,
                                "end": 1316
                            },
                            {
                                "start": 1317,
                                "end": 1457
                            },
                            {
                                "start": 1458,
                                "end": 1593
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "CFG (Ho & Salimans, 2022) is an effective and versatile technique widely used in both continuous and discrete diffusion models, with applications spanning image (Ho & Salimans, 2022;Chang et al., 2023) and text generation (Lovelace et al., 2024). Rooted in Bayes' rule, CFG simultaneously trains a conditional and an unconditional diffusion model, introducing a rescaled distribution for inference. Specifically, at a given timestep t \u2208 [0] 1], CFG (Chang et al., 2023) is defined as: \n\nwhere c is the condition, w is a hyperparameter that flexibly controls the strength of c, and p \u03b8 (x 0 |c, x t ) and p \u03b8 (x 0 |x t ) are the conditional and unconditional models respectively. \n\nNotably, it seems that the conditional model must be trained on paired data before applying CFG. Consequently, to the best of our knowledge, all existing work (Ho & Salimans, 2022;Chang et al., 2023;Lovelace et al., 2024) fall into supervised settings, where paired data are readily available. \n\nUnsupervised CFG. We extend CFG to an unsupervised setting by introducing a new formulation: \n\nwhere m is a mask sequence of the same length as c. Compared to Eq. ( 6), the dummy variable m translates the unconditional distribution to a conditional format without adding new information. For simplicity, we continue to refer to p \u03b8 (x 0 |m, x t ) as the unconditional distribution in unsupervised CFG throughout this paper."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[274117064 | Kaiser et al. | 2024 | Citations: 1]",
                "snippets": "The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16].\n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42].\n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254854389 | Peebles et al. | 2022 | Citations: 2433]": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops\u2014through increased transformer depth/width or increased number of input tokens\u2014consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512\u00d7512 and 256\u00d7256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 388,
                        "end": 1795,
                        "sentence_offsets": [
                            {
                                "start": 333,
                                "end": 497
                            },
                            {
                                "start": 498,
                                "end": 668
                            },
                            {
                                "start": 669,
                                "end": 880
                            },
                            {
                                "start": 883,
                                "end": 1061
                            },
                            {
                                "start": 1062,
                                "end": 1287
                            },
                            {
                                "start": 1288,
                                "end": 1425
                            },
                            {
                                "start": 1428,
                                "end": 1527
                            },
                            {
                                "start": 1528,
                                "end": 1667
                            },
                            {
                                "start": 1668,
                                "end": 1907
                            }
                        ],
                        "ref_mentions": [
                            "254854389"
                        ],
                        "quote": "The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16].\n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42].\n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25]."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[274422874 | Hyung et al. | 2024 | Citations: 4]",
                "snippets": "Classifier-Free Guidance (CFG) [10] uses Bayes' rule to replace a classifier-guided score with a linear combination of conditional and unconditional score estimates: \n\nCFG jointly trains the unconditional model \u03f5 \u03b8 (x t |\u03d5) and the conditional model \u03f5 \u03b8 (x t |c) (= \u03f5 \u03b8 (x t )) within a single model by setting the condition c to a null token \u03d5. Using this guided denoising process, the model better captures the conditions and often generates high-fidelity outputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 466,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 165
                            },
                            {
                                "start": 168,
                                "end": 345
                            },
                            {
                                "start": 346,
                                "end": 466
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-Free Guidance (CFG) [10] uses Bayes' rule to replace a classifier-guided score with a linear combination of conditional and unconditional score estimates: \n\nCFG jointly trains the unconditional model \u03f5 \u03b8 (x t |\u03d5) and the conditional model \u03f5 \u03b8 (x t |c) (= \u03f5 \u03b8 (x t )) within a single model by setting the condition c to a null token \u03d5. Using this guided denoising process, the model better captures the conditions and often generates high-fidelity outputs."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[274514993 | Wu et al. | 2024 | Citations: 9]",
                "snippets": "Impact of Classifier-free Guidance. Classifier-Free Guidance (CFG) scale is hyperparameter that control the trade-off between sample quality and diversity in conditional generative models. The visual variations of generated images with different CFG scales t are illustrated in Fig. 10 As observed, higher CFG scales lead to better alignment between the generated images and the text prompts, but cause more chaotic object structures, stronger stylization, and worse photorealism. For example, when CFG=15, the structure of the book in the image becomes disordered. Conversely, lower CFG scales result in poorer consistency between the image content and the prompt, but improve the photorealism and fine-grained texture details.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Visual Comparative Analysis",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 728,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 35
                            },
                            {
                                "start": 36,
                                "end": 188
                            },
                            {
                                "start": 189,
                                "end": 480
                            },
                            {
                                "start": 481,
                                "end": 565
                            },
                            {
                                "start": 566,
                                "end": 728
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Impact of Classifier-free Guidance. Classifier-Free Guidance (CFG) scale is hyperparameter that control the trade-off between sample quality and diversity in conditional generative models. The visual variations of generated images with different CFG scales t are illustrated in Fig. 10 As observed, higher CFG scales lead to better alignment between the generated images and the text prompts, but cause more chaotic object structures, stronger stylization, and worse photorealism. For example, when CFG=15, the structure of the book in the image becomes disordered. Conversely, lower CFG scales result in poorer consistency between the image content and the prompt, but improve the photorealism and fine-grained texture details."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[276421312 | Tang et al. | 2025 | Citations: 5]",
                "snippets": "Classifier-free guidance (CFG) (Ho, 2022)) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality...CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 323,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 177
                            },
                            {
                                "start": 178,
                                "end": 324
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (CFG) (Ho, 2022)) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality"
                    },
                    {
                        "section_title": "Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 1919,
                        "end": 2047,
                        "sentence_offsets": [
                            {
                                "start": 1919,
                                "end": 2046
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[276741036 | Bian et al. | 2024 | Citations: 5]",
                "snippets": "Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(x|c) and unconditional generation p(x), and guidance during sampling is provided by the following equation: \n\nwhere xt (c) is the result conditioned on c, xt (\u2205) is the unconditioned result, and w is a weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A.4 CLASSIFIER-FREE GUIDANCE",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 639,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 158
                            },
                            {
                                "start": 159,
                                "end": 359
                            },
                            {
                                "start": 362,
                                "end": 524
                            },
                            {
                                "start": 525,
                                "end": 639
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(x|c) and unconditional generation p(x), and guidance during sampling is provided by the following equation: \n\nwhere xt (c) is the result conditioned on c, xt (\u2205) is the unconditioned result, and w is a weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[276741986 | Lu et al. | 2025 | Citations: 11]",
                "snippets": "Classifier-free guidance: To avoid training classifiers, classifier-free guidance (CFG) is proposed. The main idea of CFG is to train a diffusion model that can be used for both conditional noise predictor \u03f5 \u03b8 (x t , t, c) and unconditional noise predictor \u03f5 \u03b8 (x t , t): \n\nwhere \u03f5 \u03b8 (x t , t) = \u03f5 \u03b8 (x t , t, \u2205). Noise prediction of \u03f5 \u03b8 (x t , t, \u2205) and \u03f5 \u03b8 (x t , t, c) can be jointly learned by randomly discard conditioning with probability of p uncond . For decision making tasks, we can train diffusion models using condition of discounted returns, and using classifier-free guidance for better plan sampling. We can normalize the discounted return in the dataset for training, and use condition of 1 as target return for CFG sampling during inference (Ajay et al., 2022). However, experiments shows that fixing 1 as target may lead to unrealistic or unstable plans.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254044710 | Ajay et al. | 2022 | Citations: 408]": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making."
                },
                "metadata": [
                    {
                        "section_title": "A GUIDED SAMPLING ALGORITHMS",
                        "pdf_hash": "",
                        "start": 897,
                        "end": 1769,
                        "sentence_offsets": [
                            {
                                "start": 897,
                                "end": 997
                            },
                            {
                                "start": 998,
                                "end": 1168
                            },
                            {
                                "start": 1171,
                                "end": 1210
                            },
                            {
                                "start": 1211,
                                "end": 1355
                            },
                            {
                                "start": 1356,
                                "end": 1512
                            },
                            {
                                "start": 1513,
                                "end": 1675
                            },
                            {
                                "start": 1676,
                                "end": 1769
                            }
                        ],
                        "ref_mentions": [
                            "254044710"
                        ],
                        "quote": "Classifier-free guidance: To avoid training classifiers, classifier-free guidance (CFG) is proposed. The main idea of CFG is to train a diffusion model that can be used for both conditional noise predictor \u03f5 \u03b8 (x t , t, c) and unconditional noise predictor \u03f5 \u03b8 (x t , t): \n\nwhere \u03f5 \u03b8 (x t , t) = \u03f5 \u03b8 (x t , t, \u2205). Noise prediction of \u03f5 \u03b8 (x t , t, \u2205) and \u03f5 \u03b8 (x t , t, c) can be jointly learned by randomly discard conditioning with probability of p uncond . For decision making tasks, we can train diffusion models using condition of discounted returns, and using classifier-free guidance for better plan sampling. We can normalize the discounted return in the dataset for training, and use condition of 1 as target return for CFG sampling during inference (Ajay et al., 2022). However, experiments shows that fixing 1 as target may lead to unrealistic or unstable plans."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[276774646 | Jacobi et al. | 2025 | Citations: 0]",
                "snippets": "In diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)).\n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance.\n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[269043390 | Chen et al. | 2024 | Citations: 58]": "Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models."
                },
                "metadata": [
                    {
                        "section_title": "Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 862,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 162
                            },
                            {
                                "start": 163,
                                "end": 228
                            },
                            {
                                "start": 231,
                                "end": 425
                            },
                            {
                                "start": 426,
                                "end": 555
                            },
                            {
                                "start": 556,
                                "end": 678
                            },
                            {
                                "start": 681,
                                "end": 864
                            }
                        ],
                        "ref_mentions": [
                            "269043390"
                        ],
                        "quote": "In diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)).\n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance.\n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276813045 | Jeon | 2025 | Citations: 0]",
                "snippets": "In this report, I present an inpainting framework named \\textit{ControlFill}, which involves training two distinct prompts: one for generating plausible objects within a designated mask (\\textit{creation}) and another for filling the region by extending the background (\\textit{removal}). During the inference stage, these learned embeddings guide a diffusion network that operates without requiring heavy text encoders. By adjusting the relative significance of the two prompts and employing classifier-free guidance, users can control the intensity of removal or creation. Furthermore, I introduce a method to spatially vary the intensity of guidance by assigning different scales to individual pixels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this report, I present an inpainting framework named \\textit{ControlFill}, which involves training two distinct prompts: one for generating plausible objects within a designated mask (\\textit{creation}) and another for filling the region by extending the background (\\textit{removal}). During the inference stage, these learned embeddings guide a diffusion network that operates without requiring heavy text encoders. By adjusting the relative significance of the two prompts and employing classifier-free guidance, users can control the intensity of removal or creation. Furthermore, I introduce a method to spatially vary the intensity of guidance by assigning different scales to individual pixels.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[276961040 | Zhao et al. | 2025 | Citations: 1]",
                "snippets": "Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance...For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image (Rombach et al., 2021) and text-to-3D (Poole et al., 2022) generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing...It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training (Ho, 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[252596091 | Poole et al. | 2022 | Citations: 2438]": "Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors."
                },
                "metadata": [
                    {
                        "quote": "Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1101,
                        "end": 1364,
                        "sentence_offsets": [
                            {
                                "start": 1101,
                                "end": 1269
                            },
                            {
                                "start": 1270,
                                "end": 1364
                            }
                        ],
                        "ref_mentions": [
                            "245335280",
                            "252596091"
                        ],
                        "quote": "For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image (Rombach et al., 2021) and text-to-3D (Poole et al., 2022) generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1779,
                        "end": 2081,
                        "sentence_offsets": [
                            {
                                "start": 1779,
                                "end": 1926
                            },
                            {
                                "start": 1927,
                                "end": 2080
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training (Ho, 2022)."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[277043912 | Zhou et al. | 2025 | Citations: 1]",
                "snippets": "Inspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity.\n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale\n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Appendix A. Effect of CFG Scale in Text-to-Video Generation",
                        "pdf_hash": "",
                        "start": 964,
                        "end": 2289,
                        "sentence_offsets": [
                            {
                                "start": 896,
                                "end": 1144
                            },
                            {
                                "start": 1147,
                                "end": 1302
                            },
                            {
                                "start": 1303,
                                "end": 1457
                            },
                            {
                                "start": 1458,
                                "end": 1627
                            },
                            {
                                "start": 1628,
                                "end": 1825
                            },
                            {
                                "start": 1828,
                                "end": 1973
                            },
                            {
                                "start": 1974,
                                "end": 2074
                            },
                            {
                                "start": 2075,
                                "end": 2277
                            },
                            {
                                "start": 2280,
                                "end": 2470
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Inspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity.\n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale\n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[277043967 | Buzzard | 2025 | Citations: 0]",
                "snippets": "While classifiers have been used to guide the diffusion process in text generation [14], to the best of my knowledge classifierfree guidance has not yet been explored.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classifier-free Guidance",
                        "pdf_hash": "",
                        "start": 595,
                        "end": 762,
                        "sentence_offsets": [
                            {
                                "start": 595,
                                "end": 762
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While classifiers have been used to guide the diffusion process in text generation [14], to the best of my knowledge classifierfree guidance has not yet been explored."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[277321603 | Phunyaphibarn et al. | 2025 | Citations: 1]",
                "snippets": "Classifier-Free Guidance (CFG) [28] is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Classifier-Free Guidance (CFG) [28] is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[278171688 | Choi et al. | 2025 | Citations: 0]",
                "snippets": "In diffusion-based generative models, classifier-free guidance (Ho, 2022) is well-explored to strengthen the influence of conditioning signals during inference. This is achieved by using both conditional and unconditional predictions from the same model to guide the generation process as follows: \n\nwhere s is guidance scale. Since each modality exhibits different characteristics, we hypothesize that using a single guidance scale for all modalities may be sub-optimal. To allow better control over each modality during inference, we propose multimodal classifier-free guidance by assigning modalityspecific guidance scales: \n\nwhere s t is guidance scale for text modality and s v for remained modality, namely video. By adjusting s t and s v , we can adaptively control the focus between modalities. Higher s t encourages the model to follow the text more closely, improving intelligibility, while higher s v leads to better lip synchronizations. \n\nTo support CFG, we apply modality dropout during training by randomly dropping text, video, or both. This not only enables multimodal CFG but also improves robustness in cases where a modality may be missing.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Multimodal Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1154,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 154
                            },
                            {
                                "start": 155,
                                "end": 291
                            },
                            {
                                "start": 294,
                                "end": 320
                            },
                            {
                                "start": 321,
                                "end": 465
                            },
                            {
                                "start": 466,
                                "end": 620
                            },
                            {
                                "start": 623,
                                "end": 713
                            },
                            {
                                "start": 714,
                                "end": 796
                            },
                            {
                                "start": 797,
                                "end": 943
                            },
                            {
                                "start": 946,
                                "end": 1046
                            },
                            {
                                "start": 1047,
                                "end": 1154
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "In diffusion-based generative models, classifier-free guidance (Ho, 2022) is well-explored to strengthen the influence of conditioning signals during inference. This is achieved by using both conditional and unconditional predictions from the same model to guide the generation process as follows: \n\nwhere s is guidance scale. Since each modality exhibits different characteristics, we hypothesize that using a single guidance scale for all modalities may be sub-optimal. To allow better control over each modality during inference, we propose multimodal classifier-free guidance by assigning modalityspecific guidance scales: \n\nwhere s t is guidance scale for text modality and s v for remained modality, namely video. By adjusting s t and s v , we can adaptively control the focus between modalities. Higher s t encourages the model to follow the text more closely, improving intelligibility, while higher s v leads to better lip synchronizations. \n\nTo support CFG, we apply modality dropout during training by randomly dropping text, video, or both. This not only enables multimodal CFG but also improves robustness in cases where a modality may be missing."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[278171703 | Liang et al. | 2025 | Citations: 0]",
                "snippets": "Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation (Wang et al., 2024). The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse (Mokady et al., 2023)- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[270220558 | Wang et al. | 2024 | Citations: 13]": "Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io."
                },
                "metadata": [
                    {
                        "section_title": "B. Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1247,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 222
                            },
                            {
                                "start": 223,
                                "end": 348
                            },
                            {
                                "start": 349,
                                "end": 479
                            },
                            {
                                "start": 480,
                                "end": 539
                            },
                            {
                                "start": 542,
                                "end": 581
                            },
                            {
                                "start": 582,
                                "end": 816
                            },
                            {
                                "start": 817,
                                "end": 922
                            },
                            {
                                "start": 923,
                                "end": 1041
                            },
                            {
                                "start": 1042,
                                "end": 1119
                            },
                            {
                                "start": 1120,
                                "end": 1247
                            }
                        ],
                        "ref_mentions": [
                            "270220558",
                            "253581838"
                        ],
                        "quote": "Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation (Wang et al., 2024). The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse (Mokady et al., 2023)- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33]."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[278394371 | Pfaff et al. | 2025 | Citations: 0]",
                "snippets": "To enable both conditional and unconditional generation within a single model, we randomly mask the conditioning input with 10% probability during training. This allows classifier-free guidance (CFG) [46] at inference time, where predictions are computed using a weighted combination of conditional and unconditional outputs: \n\nwhere w is the guidance weight, and xcond and xuncond are the model predictions under conditional and unconditional contexts, respectively. A weight of w = \u22121 corresponds to unconditional sampling, w = 0 yields conditional sampling without guidance, and w > 0 applies classifier-free guidance during sampling. We apply CFG to both discrete and continuous components.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B.6.1 Text-Conditioned Generation",
                        "pdf_hash": "",
                        "start": 317,
                        "end": 1011,
                        "sentence_offsets": [
                            {
                                "start": 317,
                                "end": 473
                            },
                            {
                                "start": 474,
                                "end": 642
                            },
                            {
                                "start": 645,
                                "end": 784
                            },
                            {
                                "start": 785,
                                "end": 954
                            },
                            {
                                "start": 955,
                                "end": 1011
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To enable both conditional and unconditional generation within a single model, we randomly mask the conditioning input with 10% probability during training. This allows classifier-free guidance (CFG) [46] at inference time, where predictions are computed using a weighted combination of conditional and unconditional outputs: \n\nwhere w is the guidance weight, and xcond and xuncond are the model predictions under conditional and unconditional contexts, respectively. A weight of w = \u22121 corresponds to unconditional sampling, w = 0 yields conditional sampling without guidance, and w > 0 applies classifier-free guidance during sampling. We apply CFG to both discrete and continuous components."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.038961,
        "cot": "For this query about classifier-free guidance (CFG) in NLP-specific generative tasks, I need to organize the quotes into meaningful dimensions that will help create a comprehensive summary. \n\nFirst, I'll establish an \"Introduction\" dimension to provide background on what CFG is and its origins, as this is essential context for understanding the adaptations in NLP.\n\nFor the main dimensions, I need to cover:\n\n1. \"Background and Definition of CFG\" - This will be a synthesis section explaining what CFG is, its origins in diffusion models, and the fundamental mechanism. This will include explaining how CFG differs from classifier guidance and its basic mathematical formulation.\n\n2. \"Adaptations of CFG for NLP Tasks\" - This will be a synthesis section discussing how CFG has been specifically adapted for NLP applications like conditional text generation and language modeling. Many quotes discuss how the technique has been transferred from diffusion models to autoregressive language models.\n\n3. \"Implementation Techniques in NLP\" - This will be a synthesis section covering the technical details of how CFG is implemented in NLP contexts, including training procedures, inference strategies, and mathematical formulations specific to language models.\n\n4. \"Benefits of CFG in NLP Applications\" - This will be a list format section highlighting the specific advantages observed when applying CFG to NLP tasks, as mentioned across multiple papers.\n\n5. \"Trade-offs and Limitations\" - This will be a list format section detailing the challenges, constraints, and compromises involved in using CFG for NLP tasks.\n\n6. \"Applications and Use Cases\" - This will be a synthesis section covering the specific NLP applications where CFG has been successfully implemented, showing its practical utility.\n\nThis organization will provide a comprehensive overview that directly addresses the query about adaptations, applications, benefits, and trade-offs of CFG in NLP contexts. The introduction and background sections will ensure readers have the necessary context, while the specific sections on adaptations, implementations, benefits, and trade-offs will directly address the core of the query.",
        "plan": {
            "Introduction and Background of CFG (synthesis)": [
                0,
                1,
                3,
                5,
                9,
                15,
                17,
                20,
                22,
                25,
                30
            ],
            "Adaptations of CFG for NLP Tasks (synthesis)": [
                6,
                7,
                10,
                12,
                14,
                21,
                31,
                32,
                35
            ],
            "Implementation Techniques in NLP (synthesis)": [
                2,
                11,
                18,
                19,
                27,
                34,
                36
            ],
            "Benefits of CFG in NLP Applications (list)": [
                6,
                7,
                14,
                28,
                31,
                35
            ],
            "Trade-offs and Limitations (list)": [
                1,
                2,
                5,
                13,
                16,
                22,
                24,
                33,
                35
            ],
            "Applications and Case Studies (synthesis)": [
                4,
                8,
                23,
                26,
                29,
                31
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction and Background of CFG",
                "tldr": "Classifier-free guidance (CFG) emerged as an innovative technique to enhance conditional generation quality without requiring separate classifiers. It offers a powerful mechanism for balancing sample fidelity and diversity by combining conditional and unconditional generation during inference. (11 sources)",
                "text": "\nClassifier-free guidance (CFG), introduced by Ho in 2022, represents a significant advancement in the field of generative modeling, particularly for conditional generation tasks <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. CFG was developed as an alternative to classifier guidance, which was earlier proposed by Dhariwal et al. to improve sample quality in diffusion models <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>. While classifier guidance requires training a separate classifier model alongside the diffusion model, CFG eliminates this requirement by leveraging the generative model itself to provide guidance <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper> <Paper corpusId=\"257505012\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nThe core mechanism of CFG involves jointly training a model that can perform both conditional and unconditional generation <Paper corpusId=\"233168627\" paperTitle=\"(Franceschelli et al., 2021)\" isShortName></Paper>. During training, the conditioning information is randomly dropped out with some probability, effectively teaching the model to generate both with and without conditions <Paper corpusId=\"270869763\" paperTitle=\"(Fuest et al., 2024)\" isShortName></Paper>. This is typically implemented by using a null token or placeholder as the conditioning signal for the unconditional case <Paper corpusId=\"257505012\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. During inference, CFG employs a linear combination of the conditional and unconditional predictions to guide the generation process <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This combination can be represented as:\n\n\u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)]\n\nwhere \u03f5(x, t; c) is the conditional noise prediction, \u03f5(x, t) is the unconditional prediction, and w is the guidance weight that controls the balance between sample fidelity and diversity <Paper corpusId=\"274117064\" paperTitle=\"(Kaiser et al., 2024)\" isShortName></Paper>.\n\nThe guidance weight w serves as a critical hyperparameter that allows for explicit control over how strongly the conditional information influences the generation <Paper corpusId=\"273186941\" paperTitle=\"(Kasymov et al., 2024)\" isShortName></Paper>. When w = 1, the process is equivalent to standard conditional generation, while values of w > 1 amplify the conditional prediction, pushing the model to produce outputs that more closely match the conditioning information <Paper corpusId=\"273186941\" paperTitle=\"(Kasymov et al., 2024)\" isShortName></Paper>. Intuitively, this approach decreases the unconditional likelihood while increasing the conditional likelihood, thereby enhancing alignment between the generated output and the provided condition <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>.\n\nCFG gained significant popularity after its successful application in text-to-image generation systems like GLIDE (Guided Language to Image Diffusion for Generation and Editing), where it demonstrated superior performance compared to other guidance techniques <Paper corpusId=\"245335086\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper> <Paper corpusId=\"233168627\" paperTitle=\"(Franceschelli et al., 2021)\" isShortName></Paper>. Human evaluators preferred images generated with classifier-free guidance for both photorealism and caption similarity <Paper corpusId=\"245335086\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper>. Since then, CFG has become a staple technique in various conditional generation tasks, including image generation with Stable Diffusion, video generation, and increasingly in NLP applications <Paper corpusId=\"276421312\" paperTitle=\"(Tang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276961040\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\nDespite its widespread adoption, CFG does come with certain limitations. It increases the training computational budget, as the unconditional task can consume up to 20% of the resources <Paper corpusId=\"274117064\" paperTitle=\"(Kaiser et al., 2024)\" isShortName></Paper>. Additionally, while CFG enhances alignment with conditions, this improvement often comes at the cost of reduced sample diversity <Paper corpusId=\"274117064\" paperTitle=\"(Kaiser et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Classifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance."
                        ],
                        "paper": {
                            "corpus_id": 257505012,
                            "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
                            "authors": [
                                {
                                    "authorId": "48934876",
                                    "name": "Chenshuang Zhang"
                                },
                                {
                                    "authorId": "31044159",
                                    "name": "Chaoning Zhang"
                                },
                                {
                                    "authorId": "50495602",
                                    "name": "Mengchun Zhang"
                                },
                                {
                                    "authorId": "145017151",
                                    "name": "In-So Kweon"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 280
                        },
                        "score": 0.9716796875
                    },
                    {
                        "id": "(Franceschelli et al., 2021)",
                        "snippets": [
                            "A possibility is to use classifier guidance (Dhariwal et al., 2021): the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance (Ho, 2022): to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) (Nichol et al., 2021) demonstrates how classifier-free guidance can be effectively used to generate text-conditional images."
                        ],
                        "paper": {
                            "corpus_id": 233168627,
                            "title": "Creativity and Machine Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2067291198",
                                    "name": "Giorgio Franceschelli"
                                },
                                {
                                    "authorId": "1806767",
                                    "name": "Mirco Musolesi"
                                }
                            ],
                            "year": 2021,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 42
                        },
                        "score": 0.9443359375
                    },
                    {
                        "id": "(Fuest et al., 2024)",
                        "snippets": [
                            "To address this limitation, Classifier-free guidance (CFG) (Ho, 2022) eliminates the need for a pre-trained classifier.CFG works by training an unconditional diffusion model parametrized by \u03f5 \u03b8 (x t , t, \u03d5) together with a conditional model parametrized by \u03f5 \u03b8 (x t , t, c).For the unconditional model, a null input token \u03d5 is used as a conditioning signal c.The network is trained by randomly dropping out the conditioning signal with probability p uncond .Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network.Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73]100].These methods do not need annotated data, allowing the use of larger unlabelled datasets."
                        ],
                        "paper": {
                            "corpus_id": 270869763,
                            "title": "Diffusion Models and Representation Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2309173490",
                                    "name": "Michael Fuest"
                                },
                                {
                                    "authorId": "2273647740",
                                    "name": "Pingchuan Ma"
                                },
                                {
                                    "authorId": "2309172432",
                                    "name": "Ming Gui"
                                },
                                {
                                    "authorId": "2273655571",
                                    "name": "Johannes S. Fischer"
                                },
                                {
                                    "authorId": "2292259521",
                                    "name": "Vincent Tao Hu"
                                },
                                {
                                    "authorId": "2257038709",
                                    "name": "Bjorn Ommer"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 24
                        },
                        "score": 0.94580078125
                    },
                    {
                        "id": "(Yu et al., 2022)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                        ],
                        "paper": {
                            "corpus_id": 249926846,
                            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2338016295",
                                    "name": "Jiahui Yu"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "1821711",
                                    "name": "Thang Luong"
                                },
                                {
                                    "authorId": "1396954703",
                                    "name": "Gunjan Baid"
                                },
                                {
                                    "authorId": "2331539",
                                    "name": "Zirui Wang"
                                },
                                {
                                    "authorId": "2053781980",
                                    "name": "Vijay Vasudevan"
                                },
                                {
                                    "authorId": "31702389",
                                    "name": "Alexander Ku"
                                },
                                {
                                    "authorId": "2118771180",
                                    "name": "Yinfei Yang"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "143911112",
                                    "name": "Wei Han"
                                },
                                {
                                    "authorId": "27456119",
                                    "name": "Zarana Parekh"
                                },
                                {
                                    "authorId": "2158973314",
                                    "name": "Xin Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1387994164",
                                    "name": "Jason Baldridge"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1133
                        },
                        "score": 0.96484375
                    },
                    {
                        "id": "(Kaiser et al., 2024)",
                        "snippets": [
                            "The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16].\n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42].\n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25]."
                        ],
                        "paper": {
                            "corpus_id": 274117064,
                            "title": "The Unreasonable Effectiveness of Guidance for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2267395983",
                                    "name": "Tim Kaiser"
                                },
                                {
                                    "authorId": "1832165240",
                                    "name": "Nikolas Adaloglou"
                                },
                                {
                                    "authorId": "2065390431",
                                    "name": "M. Kollmann"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9482421875
                    },
                    {
                        "id": "(Kasymov et al., 2024)",
                        "snippets": [
                            "Classifier-Free Guidance offers a simpler and more robust alternative by eliminating the need for an external classifier. Instead, the diffusion model itself is trained in two modes -conditional and unconditional: \n\n\u2022 conditional mode -the model is trained to predict the denoised data x 0 given noisy data x t and conditioning information y, learning the conditional distribution p \u03b8 (x t\u22121 |x t , y); \u2022 unconditional mode -the same model is also trained without any conditioning, learning the unconditional distribution p \u03b8 (x t\u22121 |x t ). \n\nDuring inference, CFG uses a combination of the conditional and unconditional predictions to guide the generation (see Algorithm 1). Specifically, for a given noisy sample x t , the guidance is achieved by interpolating between the conditional and unconditional predictions as follows: \n\nwhere \u03f5 \u03b8 (x t , \u00f8) is the model's prediction of the noise in x t when no conditioning is provided (unconditional), \u03f5 \u03b8 (x t , y) is the prediction of the noise in x t when conditioned on y, and w is the guidance scale, which controls how strongly the conditional information influences the generation. \n\nBy adjusting w, one can control the balance between sample diversity and adherence to the conditioning y. When w = 1, the process is equivalent to standard conditional generation. When w > 1, the conditional prediction is amplified, guiding the model to produce samples that more closely match the conditioning information, potentially at the cost of diversity."
                        ],
                        "paper": {
                            "corpus_id": 273186941,
                            "title": "AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "145172609",
                                    "name": "A. Kasymov"
                                },
                                {
                                    "authorId": "46220915",
                                    "name": "Marcin Sendera"
                                },
                                {
                                    "authorId": "1379958688",
                                    "name": "Michal Stypulkowski"
                                },
                                {
                                    "authorId": "2310705964",
                                    "name": "Maciej Zieba"
                                },
                                {
                                    "authorId": "1790922",
                                    "name": "P. Spurek"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.958984375
                    },
                    {
                        "id": "(Nichol et al., 2021)",
                        "snippets": [
                            "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im."
                        ],
                        "paper": {
                            "corpus_id": 245335086,
                            "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "39593364",
                                    "name": "Bob McGrew"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 3629
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2025)",
                        "snippets": [
                            "Classifier-free guidance (CFG) (Ho, 2022)) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality",
                            "CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation."
                        ],
                        "paper": {
                            "corpus_id": 276421312,
                            "title": "Diffusion Models without Classifier-free Guidance",
                            "authors": [
                                {
                                    "authorId": "1387822270",
                                    "name": "Zhicong Tang"
                                },
                                {
                                    "authorId": "9324504",
                                    "name": "Jianmin Bao"
                                },
                                {
                                    "authorId": "2265723915",
                                    "name": "Dongdong Chen"
                                },
                                {
                                    "authorId": "2238211403",
                                    "name": "Baining Guo"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.953125
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance",
                            "For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image (Rombach et al., 2021) and text-to-3D (Poole et al., 2022) generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing",
                            "It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training (Ho, 2022)."
                        ],
                        "paper": {
                            "corpus_id": 276961040,
                            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
                            "authors": [
                                {
                                    "authorId": "2144306665",
                                    "name": "Xiaoming Zhao"
                                },
                                {
                                    "authorId": "2281750850",
                                    "name": "Alexander G. Schwing"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9306640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Adaptations of CFG for NLP Tasks",
                "tldr": "Classifier-free guidance has been successfully adapted from image generation to various NLP tasks by leveraging the autoregressive nature of language models and manipulating token prediction probabilities. These adaptations have enabled more controllable text generation while maintaining the core CFG principle of balancing conditional and unconditional outputs. (14 sources)",
                "text": "\nThe adaptation of classifier-free guidance (CFG) to natural language processing tasks represents a significant extension of the technique beyond its original image generation domain. Sanchez et al. demonstrated that CFG can be broadly applied as an inference-time technique in pure language modeling, showing improvements across a variety of tasks including question answering, reasoning, code generation, and machine translation <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>. Their work established that CFG could bring performance improvements equivalent to using a model with twice the parameter count, suggesting a cost-effective alternative to scaling model size <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\nFor autoregressive language models, which inherently excel at unconditional generation, CFG represents a natural evolution of existing techniques <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper> <Paper corpusId=\"44104089\" paperTitle=\"(Sajjadi et al., 2018)\" isShortName></Paper>. The adaptation of CFG to language models involves manipulating the generation of subsequent tokens to emphasize conditioning on the prompt, building upon established frameworks of logit guidance and log-probability adjustment <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper>.\n\nThe mathematical formulation for applying CFG to language models has been clearly established. Given a sequence of tokens, the likelihood of predicting the entire sequence can be expressed using conditional probabilities, with the CFG sampling denoted as a weighted combination of conditional and unconditional predictions <Paper corpusId=\"266053531\" paperTitle=\"(Zhang et al._1, 2023)\" isShortName></Paper>. The guided logits for token sampling can be computed as:\n\n```\nlogits_guided = logits_uncond + w(logits_cond - logits_uncond)\n```\n\nwhere `w` is the guidance scale that controls the strength of the conditioning <Paper corpusId=\"266162752\" paperTitle=\"(Mizrahi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247628171\" paperTitle=\"(Gafni et al., 2022)\" isShortName></Paper> <Paper corpusId=\"255372955\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This formulation has been successfully applied not only to text generation but also to multimodal tasks where text conditions other modalities <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\nAn interesting extension of CFG for NLP includes negative prompting, which guides generation away from specific aspects by adjusting the guidance parameter to be negative <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper>. This technique exploits the latent semantic information in token predictions to provide finer control over the generated content <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper>.\n\nRecent innovations have also introduced unsupervised CFG, extending the technique to scenarios where paired conditional data might not be readily available <Paper corpusId=\"273549320\" paperTitle=\"(Nie et al., 2024)\" isShortName></Paper>. This adaptation involves introducing a mask sequence as a dummy variable that translates the unconditional distribution to a conditional format without adding new information <Paper corpusId=\"273549320\" paperTitle=\"(Nie et al., 2024)\" isShortName></Paper>.\n\nThe application of CFG has expanded beyond pure text generation to include agent training <Paper corpusId=\"267027965\" paperTitle=\"(Zolna et al., 2024)\" isShortName></Paper>, text-to-video generation <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>, and video-to-audio synthesis <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270220558\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. In the context of agent training, the guided policy computes a weighted combination of conditional and unconditional action predictions, with experimental evidence showing consistent performance improvements when classifier-free guidance is enabled <Paper corpusId=\"267027965\" paperTitle=\"(Zolna et al., 2024)\" isShortName></Paper>.\n\nDespite its growing adoption across various NLP tasks, some researchers have noted that certain applications, such as text generation with diffusion models, have primarily used classifiers for guidance rather than the classifier-free approach <Paper corpusId=\"277043967\" paperTitle=\"(Buzzard, 2025)\" isShortName></Paper>. This suggests that there may still be untapped potential for applying CFG more broadly in specific NLP domains.",
                "citations": [
                    {
                        "id": "(Sanchez et al., 2023)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline."
                        ],
                        "paper": {
                            "corpus_id": 259308807,
                            "title": "Stay on topic with Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2056723344",
                                    "name": "Guillaume Sanchez"
                                },
                                {
                                    "authorId": "2072838294",
                                    "name": "Honglu Fan"
                                },
                                {
                                    "authorId": "51444076",
                                    "name": "Alexander Spangher"
                                },
                                {
                                    "authorId": "34490455",
                                    "name": "Elad Levi"
                                },
                                {
                                    "authorId": "1451644426",
                                    "name": "Pawan Sasanka Ammanamanchi"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 55
                        },
                        "score": 0.9560546875
                    },
                    {
                        "id": "(O'Neill et al., 2023)",
                        "snippets": [
                            "In the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sajjadi et al., 2018). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;(Sahu et al., 2022)Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022)."
                        ],
                        "paper": {
                            "corpus_id": 260899968,
                            "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation",
                            "authors": [
                                {
                                    "authorId": "2072014290",
                                    "name": "C. O'Neill"
                                },
                                {
                                    "authorId": "93622633",
                                    "name": "Y. Ting"
                                },
                                {
                                    "authorId": "50062876",
                                    "name": "I. Ciuc\u0103"
                                },
                                {
                                    "authorId": "2229023715",
                                    "name": "Jack William Miller"
                                },
                                {
                                    "authorId": "2229238231",
                                    "name": "Thang Bui"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Sajjadi et al., 2018)",
                        "snippets": [
                            "Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution."
                        ],
                        "paper": {
                            "corpus_id": 44104089,
                            "title": "Assessing Generative Models via Precision and Recall",
                            "authors": [
                                {
                                    "authorId": "2283034",
                                    "name": "Mehdi S. M. Sajjadi"
                                },
                                {
                                    "authorId": "1936951",
                                    "name": "Olivier Bachem"
                                },
                                {
                                    "authorId": "34302129",
                                    "name": "Mario Lucic"
                                },
                                {
                                    "authorId": "1698617",
                                    "name": "O. Bousquet"
                                },
                                {
                                    "authorId": "1802148",
                                    "name": "S. Gelly"
                                }
                            ],
                            "year": 2018,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 583
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al._1, 2023)",
                        "snippets": [
                            "Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning."
                        ],
                        "paper": {
                            "corpus_id": 266053531,
                            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
                            "authors": [
                                {
                                    "authorId": "2145915052",
                                    "name": "Yuechen Zhang"
                                },
                                {
                                    "authorId": "152230789",
                                    "name": "Shengju Qian"
                                },
                                {
                                    "authorId": "2272493196",
                                    "name": "Bohao Peng"
                                },
                                {
                                    "authorId": "25059098",
                                    "name": "Shu Liu"
                                },
                                {
                                    "authorId": "2273012826",
                                    "name": "Jiaya Jia"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 24
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Mizrahi et al., 2023)",
                        "snippets": [
                            "Classifier-free guidance is crucial for improving both image fidelity and how well the generation matches the conditioning. It is most commonly used in diffusion models [50], but can be applied in token-based models as well (Gafni et al., 2022)(Yu et al., 2022)(Chang et al., 2023). We perform classifier-free guidance by computing a weighted combinations of the logits of a forward pass with the conditioning and one without the conditioning: logits guided = logits uncond + w (logits cond \u2212 logits uncond ) . \n\nHere, w is the guidance scale. When performing chained generation, we add each fully generated modality to the set of guided modalities."
                        ],
                        "paper": {
                            "corpus_id": 266162752,
                            "title": "4M: Massively Multimodal Masked Modeling",
                            "authors": [
                                {
                                    "authorId": "2111623708",
                                    "name": "David Mizrahi"
                                },
                                {
                                    "authorId": "153825349",
                                    "name": "Roman Bachmann"
                                },
                                {
                                    "authorId": "2273474116",
                                    "name": "Ouguzhan Fatih Kar"
                                },
                                {
                                    "authorId": "143895090",
                                    "name": "Teresa Yeo"
                                },
                                {
                                    "authorId": "2273661239",
                                    "name": "Mingfei Gao"
                                },
                                {
                                    "authorId": "2273361790",
                                    "name": "Afshin Dehghan"
                                },
                                {
                                    "authorId": "40029556",
                                    "name": "Amir Zamir"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 74
                        },
                        "score": 0.93798828125
                    },
                    {
                        "id": "(Gafni et al., 2022)",
                        "snippets": [
                            "Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote."
                        ],
                        "paper": {
                            "corpus_id": 247628171,
                            "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors",
                            "authors": [
                                {
                                    "authorId": "90840812",
                                    "name": "Oran Gafni"
                                },
                                {
                                    "authorId": "33964593",
                                    "name": "Adam Polyak"
                                },
                                {
                                    "authorId": "1388005058",
                                    "name": "Oron Ashual"
                                },
                                {
                                    "authorId": "2086827528",
                                    "name": "Shelly Sheynin"
                                },
                                {
                                    "authorId": "153432684",
                                    "name": "Devi Parikh"
                                },
                                {
                                    "authorId": "2188620",
                                    "name": "Yaniv Taigman"
                                }
                            ],
                            "year": 2022,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 524
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u."
                        ],
                        "paper": {
                            "corpus_id": 255372955,
                            "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
                            "authors": [
                                {
                                    "authorId": "2914394",
                                    "name": "Huiwen Chang"
                                },
                                {
                                    "authorId": "2146204239",
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "152630175",
                                    "name": "Jarred Barber"
                                },
                                {
                                    "authorId": "2199119286",
                                    "name": "AJ Maschinot"
                                },
                                {
                                    "authorId": "143923528",
                                    "name": "Jos\u00e9 Lezama"
                                },
                                {
                                    "authorId": "39978626",
                                    "name": "Lu Jiang"
                                },
                                {
                                    "authorId": "152790163",
                                    "name": "Ming Yang"
                                },
                                {
                                    "authorId": "1702318",
                                    "name": "K. Murphy"
                                },
                                {
                                    "authorId": "1768236",
                                    "name": "W. Freeman"
                                },
                                {
                                    "authorId": "144544291",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "2167749913",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "1707347",
                                    "name": "Dilip Krishnan"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 556
                        },
                        "score": 0.94384765625
                    },
                    {
                        "id": "(Yu et al., 2022)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                        ],
                        "paper": {
                            "corpus_id": 249926846,
                            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2338016295",
                                    "name": "Jiahui Yu"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "1821711",
                                    "name": "Thang Luong"
                                },
                                {
                                    "authorId": "1396954703",
                                    "name": "Gunjan Baid"
                                },
                                {
                                    "authorId": "2331539",
                                    "name": "Zirui Wang"
                                },
                                {
                                    "authorId": "2053781980",
                                    "name": "Vijay Vasudevan"
                                },
                                {
                                    "authorId": "31702389",
                                    "name": "Alexander Ku"
                                },
                                {
                                    "authorId": "2118771180",
                                    "name": "Yinfei Yang"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "143911112",
                                    "name": "Wei Han"
                                },
                                {
                                    "authorId": "27456119",
                                    "name": "Zarana Parekh"
                                },
                                {
                                    "authorId": "2158973314",
                                    "name": "Xin Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1387994164",
                                    "name": "Jason Baldridge"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1133
                        },
                        "score": 0.96484375
                    },
                    {
                        "id": "(Zhou et al., 2025)",
                        "snippets": [
                            "Inspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity.\n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale\n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10."
                        ],
                        "paper": {
                            "corpus_id": 277043912,
                            "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2282670286",
                                    "name": "Ziqin Zhou"
                                },
                                {
                                    "authorId": "2331570564",
                                    "name": "Yifan Yang"
                                },
                                {
                                    "authorId": "2125051198",
                                    "name": "Yuqing Yang"
                                },
                                {
                                    "authorId": "2350871281",
                                    "name": "Tianyu He"
                                },
                                {
                                    "authorId": "2350821834",
                                    "name": "Houwen Peng"
                                },
                                {
                                    "authorId": "2268758860",
                                    "name": "Kai Qiu"
                                },
                                {
                                    "authorId": "2329560121",
                                    "name": "Qi Dai"
                                },
                                {
                                    "authorId": "2160727304",
                                    "name": "Lili Qiu"
                                },
                                {
                                    "authorId": "2294680622",
                                    "name": "Chong Luo"
                                },
                                {
                                    "authorId": "2320820962",
                                    "name": "Lingqiao Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Nie et al., 2024)",
                        "snippets": [
                            "CFG (Ho & Salimans, 2022) is an effective and versatile technique widely used in both continuous and discrete diffusion models, with applications spanning image (Ho & Salimans, 2022;Chang et al., 2023) and text generation (Lovelace et al., 2024). Rooted in Bayes' rule, CFG simultaneously trains a conditional and an unconditional diffusion model, introducing a rescaled distribution for inference. Specifically, at a given timestep t \u2208 [0] 1], CFG (Chang et al., 2023) is defined as: \n\nwhere c is the condition, w is a hyperparameter that flexibly controls the strength of c, and p \u03b8 (x 0 |c, x t ) and p \u03b8 (x 0 |x t ) are the conditional and unconditional models respectively. \n\nNotably, it seems that the conditional model must be trained on paired data before applying CFG. Consequently, to the best of our knowledge, all existing work (Ho & Salimans, 2022;Chang et al., 2023;Lovelace et al., 2024) fall into supervised settings, where paired data are readily available. \n\nUnsupervised CFG. We extend CFG to an unsupervised setting by introducing a new formulation: \n\nwhere m is a mask sequence of the same length as c. Compared to Eq. ( 6), the dummy variable m translates the unconditional distribution to a conditional format without adding new information. For simplicity, we continue to refer to p \u03b8 (x 0 |m, x t ) as the unconditional distribution in unsupervised CFG throughout this paper."
                        ],
                        "paper": {
                            "corpus_id": 273549320,
                            "title": "Scaling up Masked Diffusion Models on Text",
                            "authors": [
                                {
                                    "authorId": "2191077545",
                                    "name": "Shen Nie"
                                },
                                {
                                    "authorId": "2305318534",
                                    "name": "Fengqi Zhu"
                                },
                                {
                                    "authorId": "2325201427",
                                    "name": "Chao Du"
                                },
                                {
                                    "authorId": "19201674",
                                    "name": "Tianyu Pang"
                                },
                                {
                                    "authorId": "2284062049",
                                    "name": "Qian Liu"
                                },
                                {
                                    "authorId": "2309181383",
                                    "name": "Guangtao Zeng"
                                },
                                {
                                    "authorId": "2253977831",
                                    "name": "Min Lin"
                                },
                                {
                                    "authorId": "2253823025",
                                    "name": "Chongxuan Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 30
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Zolna et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4)."
                        ],
                        "paper": {
                            "corpus_id": 267027965,
                            "title": "GATS: Gather-Attend-Scatter",
                            "authors": [
                                {
                                    "authorId": "2065684527",
                                    "name": "K. Zolna"
                                },
                                {
                                    "authorId": "2066047403",
                                    "name": "Serkan Cabi"
                                },
                                {
                                    "authorId": "2279763460",
                                    "name": "Yutian Chen"
                                },
                                {
                                    "authorId": "2279718795",
                                    "name": "Eric Lau"
                                },
                                {
                                    "authorId": "2080928701",
                                    "name": "Claudio Fantacci"
                                },
                                {
                                    "authorId": "31143488",
                                    "name": "J. Pa\u0161ukonis"
                                },
                                {
                                    "authorId": "2060551",
                                    "name": "Jost Tobias Springenberg"
                                },
                                {
                                    "authorId": "2279752493",
                                    "name": "Sergio G\u00f3mez Colmenarejo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.93798828125
                    },
                    {
                        "id": "(Liang et al., 2025)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation (Wang et al., 2024). The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse (Mokady et al., 2023)- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33]."
                        ],
                        "paper": {
                            "corpus_id": 278171703,
                            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2278618341",
                                    "name": "Yuzhe Liang"
                                },
                                {
                                    "authorId": "2358111037",
                                    "name": "Wenzhe Liu"
                                },
                                {
                                    "authorId": "2358041541",
                                    "name": "Chunyu Qiang"
                                },
                                {
                                    "authorId": "2229877177",
                                    "name": "Zhikang Niu"
                                },
                                {
                                    "authorId": "2324996330",
                                    "name": "Yushen Chen"
                                },
                                {
                                    "authorId": "2116609277",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2278584538",
                                    "name": "Wenxi Chen"
                                },
                                {
                                    "authorId": "2358116915",
                                    "name": "Nan Li"
                                },
                                {
                                    "authorId": "2358098456",
                                    "name": "Chen Zhang"
                                },
                                {
                                    "authorId": "2321881822",
                                    "name": "Xie Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io."
                        ],
                        "paper": {
                            "corpus_id": 270220558,
                            "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching",
                            "authors": [
                                {
                                    "authorId": "2292208450",
                                    "name": "Yongqi Wang"
                                },
                                {
                                    "authorId": "2304739202",
                                    "name": "Wenxiang Guo"
                                },
                                {
                                    "authorId": "2048021099",
                                    "name": "Rongjie Huang"
                                },
                                {
                                    "authorId": "3068086",
                                    "name": "Jia-Bin Huang"
                                },
                                {
                                    "authorId": "2258561621",
                                    "name": "Zehan Wang"
                                },
                                {
                                    "authorId": "2292197957",
                                    "name": "Fuming You"
                                },
                                {
                                    "authorId": "2181010470",
                                    "name": "Ruiqi Li"
                                },
                                {
                                    "authorId": "2304453961",
                                    "name": "Zhou Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13
                        },
                        "score": 0
                    },
                    {
                        "id": "(Buzzard, 2025)",
                        "snippets": [
                            "While classifiers have been used to guide the diffusion process in text generation [14], to the best of my knowledge classifierfree guidance has not yet been explored."
                        ],
                        "paper": {
                            "corpus_id": 277043967,
                            "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
                            "authors": [
                                {
                                    "authorId": "2332536645",
                                    "name": "Zak Buzzard"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94287109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Implementation Techniques in NLP",
                "tldr": "Implementing classifier-free guidance in NLP requires specific training and inference strategies, such as condition dropout during training and specialized token prediction formulations during inference. Different applications have developed variations like multimodal CFG, independent condition guidance, and adaptive guidance scales to address modality-specific requirements. (8 sources)",
                "text": "\nThe implementation of classifier-free guidance (CFG) in NLP applications follows several established techniques that have been adapted from image generation while accommodating the unique characteristics of language models. A fundamental training technique involves randomly discarding or masking conditioning information for a portion of training samples, typically around 10%, to enable the model to learn both conditional and unconditional generation capabilities <Paper corpusId=\"266149498\" paperTitle=\"(Patel et al., 2023)\" isShortName></Paper> <Paper corpusId=\"278394371\" paperTitle=\"(Pfaff et al., 2025)\" isShortName></Paper>. This approach creates a joint model that can perform both tasks without requiring separate training of an unconditional model.\n\nDuring inference, the core implementation of CFG involves a weighted combination of conditional and unconditional predictions. The mathematical formulation typically follows:\n\n```\n\u03f5_guided = \u03f5_uncond + \u03bb(\u03f5_cond - \u03f5_uncond)\n```\n\nwhere \u03bb (lambda) represents the guidance scale that controls the strength of conditioning <Paper corpusId=\"271957385\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This formulation allows for flexible control over the balance between fidelity to the conditioning information and generation diversity.\n\nSeveral innovative variations of CFG implementation have emerged for NLP tasks. Chang et al. introduced a dynamic guidance approach where the scale increases linearly throughout the sampling process, allowing early tokens to be sampled more freely while later tokens are more strongly influenced by the conditioning prompt <Paper corpusId=\"255372955\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. This dynamic scaling helps mitigate the diversity loss that typically accompanies high guidance values.\n\nFor multimodal applications involving text and other modalities such as speech or video, researchers have developed modality-specific guidance scales. Choi et al. proposed multimodal classifier-free guidance where different guidance scales are applied to different modalities:\n\n```\n\u03f5_guided = \u03f5_uncond + s_t(\u03f5_t_cond - \u03f5_uncond) + s_v(\u03f5_v_cond - \u03f5_uncond)\n```\n\nwhere s_t and s_v are modality-specific guidance scales for text and video, respectively <Paper corpusId=\"278171688\" paperTitle=\"(Choi et al., 2025)\" isShortName></Paper>. This approach enables finer control over the influence of each modality, allowing developers to emphasize text intelligibility or other modal characteristics as needed.\n\nAnother significant implementation advancement is Independent Condition Guidance (ICG), which eliminates the need for separate training of an unconditional model. Sadat et al. demonstrated that by using a conditioning vector independent of the input data, the conditional score function can become equivalent to the unconditional score <Paper corpusId=\"270923987\" paperTitle=\"(Sadat et al., 2024)\" isShortName></Paper>. This insight reduces training complexity while maintaining the benefits of CFG.\n\nIn decision-making and planning applications, CFG has been implemented by training diffusion models using conditions based on discounted returns. During inference, normalized return values can be used as the target condition for CFG sampling <Paper corpusId=\"276741986\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"254044710\" paperTitle=\"(Ajay et al., 2022)\" isShortName></Paper>. However, researchers have noted that using fixed target values may lead to unstable plans, suggesting that adaptive approaches may be necessary for these applications.\n\nFor supporting more sophisticated control over generation, some implementations extend CFG to support negative prompting. This technique replaces the unconditional logit with a logit conditioned on a \"negative prompt,\" encouraging the model to generate content with features associated with the positive prompt while avoiding features associated with the negative prompt <Paper corpusId=\"255372955\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. This capability is particularly valuable in NLP applications where controlling tone, style, or content boundaries is important.",
                "citations": [
                    {
                        "id": "(Patel et al., 2023)",
                        "snippets": [
                            "During training, conditions are omitted 10% of the time to foster unconditional generation, subsequently improving test performance as CFG works as implicit classifier guidance [11]."
                        ],
                        "paper": {
                            "corpus_id": 266149498,
                            "title": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations",
                            "authors": [
                                {
                                    "authorId": "81331041",
                                    "name": "Maitreya Patel"
                                },
                                {
                                    "authorId": "2116705496",
                                    "name": "C. Kim"
                                },
                                {
                                    "authorId": "2116727866",
                                    "name": "Sheng Cheng"
                                },
                                {
                                    "authorId": "2064619864",
                                    "name": "Chitta Baral"
                                },
                                {
                                    "authorId": "1784500",
                                    "name": "Yezhou Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 19
                        },
                        "score": 0.9248046875
                    },
                    {
                        "id": "(Pfaff et al., 2025)",
                        "snippets": [
                            "To enable both conditional and unconditional generation within a single model, we randomly mask the conditioning input with 10% probability during training. This allows classifier-free guidance (CFG) [46] at inference time, where predictions are computed using a weighted combination of conditional and unconditional outputs: \n\nwhere w is the guidance weight, and xcond and xuncond are the model predictions under conditional and unconditional contexts, respectively. A weight of w = \u22121 corresponds to unconditional sampling, w = 0 yields conditional sampling without guidance, and w > 0 applies classifier-free guidance during sampling. We apply CFG to both discrete and continuous components."
                        ],
                        "paper": {
                            "corpus_id": 278394371,
                            "title": "Steerable Scene Generation with Post Training and Inference-Time Search",
                            "authors": [
                                {
                                    "authorId": "2348265706",
                                    "name": "Nicholas Pfaff"
                                },
                                {
                                    "authorId": "1758275",
                                    "name": "Hongkai Dai"
                                },
                                {
                                    "authorId": "2331626375",
                                    "name": "Sergey Zakharov"
                                },
                                {
                                    "authorId": "2359630378",
                                    "name": "Shun Iwase"
                                },
                                {
                                    "authorId": "2263905014",
                                    "name": "Russ Tedrake"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.966796875
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance (CFG) [24] has been demonstrated as an effective way to enhance the generation quality in both image and audio domains [35], [53]. CFG can be formulated as: \n\nwhere \u03bb denotes the guidance scale. CFC tries to model conditional distribution p(x|y) and unconditional distribution p(x) in the training. In the inference stage, \u03bb = 1 denotes that we do not use classifier-free guidance, when \u03bb > 1 the model decreases the unconditional likelihood of the sample while increasing the conditional likelihood. In other words, classifierfree guidance conducts this by decreasing the unconditional likelihood with a negative score term. During the training stage, previous works try to mask the condition information of some samples (e.g. set the training sample's text as empty with 10% probability), so that these samples can be used to optimize unconditional distribution p(x).\n\nMany prior studies [10], [22] demonstrate that text transcriptions generated by an ASR system can be utilized to train a TTS system. Inevitably, the text transcription from the ASR system is not flawless, referred to as noisy or weak labels. Despite this, existing literature does not thoroughly explain why TTS systems can be effectively trained using such dataset. In this study, we show that including a small part of the noisy label in the training set is equivalent to introducing the CFC training strategy, thus we do not need to deliberately construct masked samples during training stage."
                        ],
                        "paper": {
                            "corpus_id": 271957385,
                            "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2238138795",
                                    "name": "Dongchao Yang"
                                },
                                {
                                    "authorId": "2306974316",
                                    "name": "Rongjie Huang"
                                },
                                {
                                    "authorId": "2307148701",
                                    "name": "Yuanyuan Wang"
                                },
                                {
                                    "authorId": "66855276",
                                    "name": "Haohan Guo"
                                },
                                {
                                    "authorId": "2316948137",
                                    "name": "Dading Chong"
                                },
                                {
                                    "authorId": "51263928",
                                    "name": "Songxiang Liu"
                                },
                                {
                                    "authorId": "2107999711",
                                    "name": "Xixin Wu"
                                },
                                {
                                    "authorId": "2273659859",
                                    "name": "Helen M. Meng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 15
                        },
                        "score": 0.927734375
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u."
                        ],
                        "paper": {
                            "corpus_id": 255372955,
                            "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
                            "authors": [
                                {
                                    "authorId": "2914394",
                                    "name": "Huiwen Chang"
                                },
                                {
                                    "authorId": "2146204239",
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "152630175",
                                    "name": "Jarred Barber"
                                },
                                {
                                    "authorId": "2199119286",
                                    "name": "AJ Maschinot"
                                },
                                {
                                    "authorId": "143923528",
                                    "name": "Jos\u00e9 Lezama"
                                },
                                {
                                    "authorId": "39978626",
                                    "name": "Lu Jiang"
                                },
                                {
                                    "authorId": "152790163",
                                    "name": "Ming Yang"
                                },
                                {
                                    "authorId": "1702318",
                                    "name": "K. Murphy"
                                },
                                {
                                    "authorId": "1768236",
                                    "name": "W. Freeman"
                                },
                                {
                                    "authorId": "144544291",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "2167749913",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "1707347",
                                    "name": "Dilip Krishnan"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 556
                        },
                        "score": 0.94384765625
                    },
                    {
                        "id": "(Choi et al., 2025)",
                        "snippets": [
                            "In diffusion-based generative models, classifier-free guidance (Ho, 2022) is well-explored to strengthen the influence of conditioning signals during inference. This is achieved by using both conditional and unconditional predictions from the same model to guide the generation process as follows: \n\nwhere s is guidance scale. Since each modality exhibits different characteristics, we hypothesize that using a single guidance scale for all modalities may be sub-optimal. To allow better control over each modality during inference, we propose multimodal classifier-free guidance by assigning modalityspecific guidance scales: \n\nwhere s t is guidance scale for text modality and s v for remained modality, namely video. By adjusting s t and s v , we can adaptively control the focus between modalities. Higher s t encourages the model to follow the text more closely, improving intelligibility, while higher s v leads to better lip synchronizations. \n\nTo support CFG, we apply modality dropout during training by randomly dropping text, video, or both. This not only enables multimodal CFG but also improves robustness in cases where a modality may be missing."
                        ],
                        "paper": {
                            "corpus_id": 278171688,
                            "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
                            "authors": [
                                {
                                    "authorId": "2327997129",
                                    "name": "Jeongsoo Choi"
                                },
                                {
                                    "authorId": "2292215700",
                                    "name": "Ji-Hoon Kim"
                                },
                                {
                                    "authorId": "1395322099",
                                    "name": "Kim Sung-Bin"
                                },
                                {
                                    "authorId": "2243191148",
                                    "name": "Tae-Hyun Oh"
                                },
                                {
                                    "authorId": "2264317306",
                                    "name": "Joon Son Chung"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.96630859375
                    },
                    {
                        "id": "(Sadat et al., 2024)",
                        "snippets": [
                            "In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score. This insight leads us to propose independent condition guidance (ICG), a method that replicates the behavior of CFG at inference time without requiring separate training of an unconditional model."
                        ],
                        "paper": {
                            "corpus_id": 270923987,
                            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2261742393",
                                    "name": "Seyedmorteza Sadat"
                                },
                                {
                                    "authorId": "2204861903",
                                    "name": "Manuel Kansy"
                                },
                                {
                                    "authorId": "1466533438",
                                    "name": "Otmar Hilliges"
                                },
                                {
                                    "authorId": "145848224",
                                    "name": "Romann M. Weber"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 14
                        },
                        "score": 0.9560546875
                    },
                    {
                        "id": "(Lu et al., 2025)",
                        "snippets": [
                            "Classifier-free guidance: To avoid training classifiers, classifier-free guidance (CFG) is proposed. The main idea of CFG is to train a diffusion model that can be used for both conditional noise predictor \u03f5 \u03b8 (x t , t, c) and unconditional noise predictor \u03f5 \u03b8 (x t , t): \n\nwhere \u03f5 \u03b8 (x t , t) = \u03f5 \u03b8 (x t , t, \u2205). Noise prediction of \u03f5 \u03b8 (x t , t, \u2205) and \u03f5 \u03b8 (x t , t, c) can be jointly learned by randomly discard conditioning with probability of p uncond . For decision making tasks, we can train diffusion models using condition of discounted returns, and using classifier-free guidance for better plan sampling. We can normalize the discounted return in the dataset for training, and use condition of 1 as target return for CFG sampling during inference (Ajay et al., 2022). However, experiments shows that fixing 1 as target may lead to unrealistic or unstable plans."
                        ],
                        "paper": {
                            "corpus_id": 276741986,
                            "title": "What Makes a Good Diffusion Planner for Decision Making?",
                            "authors": [
                                {
                                    "authorId": "2315320844",
                                    "name": "Haofei Lu"
                                },
                                {
                                    "authorId": "2268676735",
                                    "name": "Dongqi Han"
                                },
                                {
                                    "authorId": "2152966656",
                                    "name": "Yifei Shen"
                                },
                                {
                                    "authorId": "2303586558",
                                    "name": "Dongsheng Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 11
                        },
                        "score": 0.947265625
                    },
                    {
                        "id": "(Ajay et al., 2022)",
                        "snippets": [
                            "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making."
                        ],
                        "paper": {
                            "corpus_id": 254044710,
                            "title": "Is Conditional Generative Modeling all you need for Decision-Making?",
                            "authors": [
                                {
                                    "authorId": "150004828",
                                    "name": "Anurag Ajay"
                                },
                                {
                                    "authorId": "15394275",
                                    "name": "Yilun Du"
                                },
                                {
                                    "authorId": null,
                                    "name": "Abhi Gupta"
                                },
                                {
                                    "authorId": "1763295",
                                    "name": "J. Tenenbaum"
                                },
                                {
                                    "authorId": "35132120",
                                    "name": "T. Jaakkola"
                                },
                                {
                                    "authorId": "33932184",
                                    "name": "Pulkit Agrawal"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 408
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Benefits of CFG in NLP Applications",
                "tldr": "Classifier-free guidance offers significant advantages for NLP applications including improved performance across diverse tasks, enhanced prompt adherence, and the ability to balance semantic coherence with diversity. These benefits make CFG a powerful technique for controlling text generation without requiring extensive model modifications. (8 sources)",
                "text": "\n- **Substantial performance improvements**: CFG has demonstrated significant enhancements across a wide range of NLP tasks including question answering, reasoning, code generation, and machine translation, achieving state-of-the-art results on benchmarks like LAMBADA with smaller models than previously required <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Parameter efficiency**: Applying CFG can bring improvements equivalent to using a model with twice the parameter count, offering a cost-effective alternative to scaling model size for better performance <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Compatibility with other techniques**: CFG can be stacked alongside other inference-time methods such as Chain-of-Thought and Self-Consistency, yielding additional improvements in difficult tasks <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Enhanced semantic alignment**: By amplifying the directional shift between conditional and unconditional predictions, CFG improves the adherence of generated text to provided prompts or conditions without requiring external classifiers <Paper corpusId=\"276774646\" paperTitle=\"(Jacobi et al., 2025)\" isShortName></Paper>.\n\n- **Controllable fidelity-diversity trade-off**: The guidance scale parameter offers precise control over the balance between semantic coherence with the prompt and creative diversity in the generated output <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\n- **Support for negative prompting**: CFG enables the avoidance of specific aspects in generation through negative prompting, allowing models to be guided away from undesired content or styles <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper> <Paper corpusId=\"44104089\" paperTitle=\"(Sajjadi et al., 2018)\" isShortName></Paper>.\n\n- **Cross-modal generation capabilities**: The technique has been successfully adapted for multimodal applications including text-to-video generation and video-to-audio synthesis, demonstrating its versatility across different media types <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper> <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270220558\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n- **Improved agent performance**: When applied to agent training, CFG consistently improves performance by enhancing the alignment between language conditioning and agent actions <Paper corpusId=\"267027965\" paperTitle=\"(Zolna et al., 2024)\" isShortName></Paper>.\n\n- **Flexibility in implementation**: The technique can be implemented with minimal modifications to existing models and training pipelines, making it an accessible approach for improving conditional generation <Paper corpusId=\"276774646\" paperTitle=\"(Jacobi et al., 2025)\" isShortName></Paper>.\n\n- **Enhanced user experience**: Human evaluations have shown a 75% preference for models using CFG over baseline approaches for assistant-style interactions, indicating significant improvements in faithfulness and coherence for challenging prompts <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Sanchez et al., 2023)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline."
                        ],
                        "paper": {
                            "corpus_id": 259308807,
                            "title": "Stay on topic with Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2056723344",
                                    "name": "Guillaume Sanchez"
                                },
                                {
                                    "authorId": "2072838294",
                                    "name": "Honglu Fan"
                                },
                                {
                                    "authorId": "51444076",
                                    "name": "Alexander Spangher"
                                },
                                {
                                    "authorId": "34490455",
                                    "name": "Elad Levi"
                                },
                                {
                                    "authorId": "1451644426",
                                    "name": "Pawan Sasanka Ammanamanchi"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 55
                        },
                        "score": 0.9560546875
                    },
                    {
                        "id": "(Jacobi et al., 2025)",
                        "snippets": [
                            "In diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)).\n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance.\n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier."
                        ],
                        "paper": {
                            "corpus_id": 276774646,
                            "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
                            "authors": [
                                {
                                    "authorId": "2348485713",
                                    "name": "Jonathan Jacobi"
                                },
                                {
                                    "authorId": "2333352",
                                    "name": "Gal Niv"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.96826171875
                    },
                    {
                        "id": "(Zhou et al., 2025)",
                        "snippets": [
                            "Inspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity.\n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale\n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10."
                        ],
                        "paper": {
                            "corpus_id": 277043912,
                            "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2282670286",
                                    "name": "Ziqin Zhou"
                                },
                                {
                                    "authorId": "2331570564",
                                    "name": "Yifan Yang"
                                },
                                {
                                    "authorId": "2125051198",
                                    "name": "Yuqing Yang"
                                },
                                {
                                    "authorId": "2350871281",
                                    "name": "Tianyu He"
                                },
                                {
                                    "authorId": "2350821834",
                                    "name": "Houwen Peng"
                                },
                                {
                                    "authorId": "2268758860",
                                    "name": "Kai Qiu"
                                },
                                {
                                    "authorId": "2329560121",
                                    "name": "Qi Dai"
                                },
                                {
                                    "authorId": "2160727304",
                                    "name": "Lili Qiu"
                                },
                                {
                                    "authorId": "2294680622",
                                    "name": "Chong Luo"
                                },
                                {
                                    "authorId": "2320820962",
                                    "name": "Lingqiao Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(O'Neill et al., 2023)",
                        "snippets": [
                            "In the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sajjadi et al., 2018). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;(Sahu et al., 2022)Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022)."
                        ],
                        "paper": {
                            "corpus_id": 260899968,
                            "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation",
                            "authors": [
                                {
                                    "authorId": "2072014290",
                                    "name": "C. O'Neill"
                                },
                                {
                                    "authorId": "93622633",
                                    "name": "Y. Ting"
                                },
                                {
                                    "authorId": "50062876",
                                    "name": "I. Ciuc\u0103"
                                },
                                {
                                    "authorId": "2229023715",
                                    "name": "Jack William Miller"
                                },
                                {
                                    "authorId": "2229238231",
                                    "name": "Thang Bui"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Sajjadi et al., 2018)",
                        "snippets": [
                            "Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution."
                        ],
                        "paper": {
                            "corpus_id": 44104089,
                            "title": "Assessing Generative Models via Precision and Recall",
                            "authors": [
                                {
                                    "authorId": "2283034",
                                    "name": "Mehdi S. M. Sajjadi"
                                },
                                {
                                    "authorId": "1936951",
                                    "name": "Olivier Bachem"
                                },
                                {
                                    "authorId": "34302129",
                                    "name": "Mario Lucic"
                                },
                                {
                                    "authorId": "1698617",
                                    "name": "O. Bousquet"
                                },
                                {
                                    "authorId": "1802148",
                                    "name": "S. Gelly"
                                }
                            ],
                            "year": 2018,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 583
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liang et al., 2025)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation (Wang et al., 2024). The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse (Mokady et al., 2023)- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33]."
                        ],
                        "paper": {
                            "corpus_id": 278171703,
                            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2278618341",
                                    "name": "Yuzhe Liang"
                                },
                                {
                                    "authorId": "2358111037",
                                    "name": "Wenzhe Liu"
                                },
                                {
                                    "authorId": "2358041541",
                                    "name": "Chunyu Qiang"
                                },
                                {
                                    "authorId": "2229877177",
                                    "name": "Zhikang Niu"
                                },
                                {
                                    "authorId": "2324996330",
                                    "name": "Yushen Chen"
                                },
                                {
                                    "authorId": "2116609277",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2278584538",
                                    "name": "Wenxi Chen"
                                },
                                {
                                    "authorId": "2358116915",
                                    "name": "Nan Li"
                                },
                                {
                                    "authorId": "2358098456",
                                    "name": "Chen Zhang"
                                },
                                {
                                    "authorId": "2321881822",
                                    "name": "Xie Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io."
                        ],
                        "paper": {
                            "corpus_id": 270220558,
                            "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching",
                            "authors": [
                                {
                                    "authorId": "2292208450",
                                    "name": "Yongqi Wang"
                                },
                                {
                                    "authorId": "2304739202",
                                    "name": "Wenxiang Guo"
                                },
                                {
                                    "authorId": "2048021099",
                                    "name": "Rongjie Huang"
                                },
                                {
                                    "authorId": "3068086",
                                    "name": "Jia-Bin Huang"
                                },
                                {
                                    "authorId": "2258561621",
                                    "name": "Zehan Wang"
                                },
                                {
                                    "authorId": "2292197957",
                                    "name": "Fuming You"
                                },
                                {
                                    "authorId": "2181010470",
                                    "name": "Ruiqi Li"
                                },
                                {
                                    "authorId": "2304453961",
                                    "name": "Zhou Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zolna et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4)."
                        ],
                        "paper": {
                            "corpus_id": 267027965,
                            "title": "GATS: Gather-Attend-Scatter",
                            "authors": [
                                {
                                    "authorId": "2065684527",
                                    "name": "K. Zolna"
                                },
                                {
                                    "authorId": "2066047403",
                                    "name": "Serkan Cabi"
                                },
                                {
                                    "authorId": "2279763460",
                                    "name": "Yutian Chen"
                                },
                                {
                                    "authorId": "2279718795",
                                    "name": "Eric Lau"
                                },
                                {
                                    "authorId": "2080928701",
                                    "name": "Claudio Fantacci"
                                },
                                {
                                    "authorId": "31143488",
                                    "name": "J. Pa\u0161ukonis"
                                },
                                {
                                    "authorId": "2060551",
                                    "name": "Jost Tobias Springenberg"
                                },
                                {
                                    "authorId": "2279752493",
                                    "name": "Sergio G\u00f3mez Colmenarejo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.93798828125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Trade-offs and Limitations",
                "tldr": "While classifier-free guidance offers significant benefits for NLP applications, it introduces several important trade-offs, primarily between sample quality and diversity. The technique also comes with computational overhead, implementation challenges, and potential for quality degradation in specific scenarios. (12 sources)",
                "text": "\n- **Reduced sample diversity**: One of the most significant limitations of CFG is that improvements in conditional alignment and sample quality come at the expense of reduced output diversity. Higher guidance scales lead to stronger conditioning but cause the model to focus on fewer modes of the distribution <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper> <Paper corpusId=\"266693789\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\n- **Computational overhead**: CFG increases the computational budget for both training and inference. During training, the unconditional task can consume up to 20% of the resources, while inference requires computing both conditional and unconditional predictions at each step <Paper corpusId=\"274117064\" paperTitle=\"(Kaiser et al., 2024)\" isShortName></Paper> <Paper corpusId=\"254854389\" paperTitle=\"(Peebles et al., 2022)\" isShortName></Paper>.\n\n- **Training complexity**: Joint learning of unconditional and conditional generation can result in poor priors for the unconditional case, which may in turn degrade the quality of the conditional generation <Paper corpusId=\"277321603\" paperTitle=\"(Phunyaphibarn et al., 2025)\" isShortName></Paper>.\n\n- **Sensitivity to guidance scale**: Finding the optimal guidance weight presents a challenge, as incorrect values can lead to either poor conditional alignment (when too low) or degraded sample quality (when too high) <Paper corpusId=\"274514993\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\n- **Risk of mode collapse**: At high guidance scales, CFG tends to trigger mode collapse, severely limiting the diversity of generated outputs and potentially producing stereotypical or repetitive content <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270220558\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n- **Task-specific tuning requirements**: Different NLP tasks may require different guidance scales and implementation strategies, making it difficult to establish universal best practices across applications <Paper corpusId=\"259212148\" paperTitle=\"(Meekeren et al., 2023)\" isShortName></Paper>.\n\n- **Structural degradation at high guidance scales**: Particularly in multimodal applications, excessive guidance can lead to chaotic object structures, over-stylization, and reduced overall quality, requiring careful balancing of the guidance parameter <Paper corpusId=\"274514993\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\n- **Implementation complexity**: Implementing dynamic guidance approaches, such as linearly increasing the scale throughout the sampling process, adds complexity to the system but may be necessary to mitigate diversity issues <Paper corpusId=\"255372955\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>.\n\n- **Incomplete theoretical understanding**: Despite its practical success, the working principles of CFG have not been fully explained theoretically, which can limit systematic improvements and adaptations to new domains <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\n- **Integration challenges with other techniques**: While CFG can be combined with other methods, such integration requires careful design to avoid conflicting effects between different approaches <Paper corpusId=\"270199289\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Yu et al., 2022)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                        ],
                        "paper": {
                            "corpus_id": 249926846,
                            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2338016295",
                                    "name": "Jiahui Yu"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "1821711",
                                    "name": "Thang Luong"
                                },
                                {
                                    "authorId": "1396954703",
                                    "name": "Gunjan Baid"
                                },
                                {
                                    "authorId": "2331539",
                                    "name": "Zirui Wang"
                                },
                                {
                                    "authorId": "2053781980",
                                    "name": "Vijay Vasudevan"
                                },
                                {
                                    "authorId": "31702389",
                                    "name": "Alexander Ku"
                                },
                                {
                                    "authorId": "2118771180",
                                    "name": "Yinfei Yang"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "143911112",
                                    "name": "Wei Han"
                                },
                                {
                                    "authorId": "27456119",
                                    "name": "Zarana Parekh"
                                },
                                {
                                    "authorId": "2158973314",
                                    "name": "Xin Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1387994164",
                                    "name": "Jason Baldridge"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1133
                        },
                        "score": 0.96484375
                    },
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) (Ho, 2022) uses Bayes' rule and finds that the diffusion model itself can be inverted as an implicit classifier. Specifically, the model is queried both conditionally and unconditionally at every inference step and the difference is amplified toward the conditional direction. Both methods only work for conditional generation and entangle sample quality with conditional alignment [24]. Self-Supervised Guidance [20] uses self-supervised networks to generate synthetic clustering labels for unconditional data. This allows unconditional data to use CFG for improving quality. Guidance-Free Training [4] shows that CFG can be applied at training. This bakes the guided flow into the model and saves computation during inference",
                            "Although these methods are effective in improving quality, they also present issues such as increased complexity and reduced diversity (Ho, 2022)[24], etc."
                        ],
                        "paper": {
                            "corpus_id": 266693789,
                            "title": "Diffusion Model with Perceptual Loss",
                            "authors": [
                                {
                                    "authorId": "32370203",
                                    "name": "Shanchuan Lin"
                                },
                                {
                                    "authorId": "2277412143",
                                    "name": "Xiao Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 17
                        },
                        "score": 0.9404296875
                    },
                    {
                        "id": "(Kaiser et al., 2024)",
                        "snippets": [
                            "The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16].\n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42].\n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25]."
                        ],
                        "paper": {
                            "corpus_id": 274117064,
                            "title": "The Unreasonable Effectiveness of Guidance for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2267395983",
                                    "name": "Tim Kaiser"
                                },
                                {
                                    "authorId": "1832165240",
                                    "name": "Nikolas Adaloglou"
                                },
                                {
                                    "authorId": "2065390431",
                                    "name": "M. Kollmann"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9482421875
                    },
                    {
                        "id": "(Peebles et al., 2022)",
                        "snippets": [
                            "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops\u2014through increased transformer depth/width or increased number of input tokens\u2014consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512\u00d7512 and 256\u00d7256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter."
                        ],
                        "paper": {
                            "corpus_id": 254854389,
                            "title": "Scalable Diffusion Models with Transformers",
                            "authors": [
                                {
                                    "authorId": "35235273",
                                    "name": "William S. Peebles"
                                },
                                {
                                    "authorId": "1817030",
                                    "name": "Saining Xie"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 2433
                        },
                        "score": 0
                    },
                    {
                        "id": "(Phunyaphibarn et al., 2025)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) [28] is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation."
                        ],
                        "paper": {
                            "corpus_id": 277321603,
                            "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2268399939",
                                    "name": "Prin Phunyaphibarn"
                                },
                                {
                                    "authorId": "2328609307",
                                    "name": "Phillip Y. Lee"
                                },
                                {
                                    "authorId": "2292419803",
                                    "name": "Jaihoon Kim"
                                },
                                {
                                    "authorId": "2292259803",
                                    "name": "Minhyuk Sung"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9638671875
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Impact of Classifier-free Guidance. Classifier-Free Guidance (CFG) scale is hyperparameter that control the trade-off between sample quality and diversity in conditional generative models. The visual variations of generated images with different CFG scales t are illustrated in Fig. 10 As observed, higher CFG scales lead to better alignment between the generated images and the text prompts, but cause more chaotic object structures, stronger stylization, and worse photorealism. For example, when CFG=15, the structure of the book in the image becomes disordered. Conversely, lower CFG scales result in poorer consistency between the image content and the prompt, but improve the photorealism and fine-grained texture details."
                        ],
                        "paper": {
                            "corpus_id": 274514993,
                            "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
                            "authors": [
                                {
                                    "authorId": "2146667089",
                                    "name": "Junfeng Wu"
                                },
                                {
                                    "authorId": "2294789302",
                                    "name": "Yi Jiang"
                                },
                                {
                                    "authorId": "2186151254",
                                    "name": "Chuofan Ma"
                                },
                                {
                                    "authorId": "2266428398",
                                    "name": "Yuliang Liu"
                                },
                                {
                                    "authorId": "2310758544",
                                    "name": "Hengshuang Zhao"
                                },
                                {
                                    "authorId": "2244754235",
                                    "name": "Zehuan Yuan"
                                },
                                {
                                    "authorId": "2257280698",
                                    "name": "Song Bai"
                                },
                                {
                                    "authorId": "2273663142",
                                    "name": "Xiang Bai"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 9
                        },
                        "score": 0.92529296875
                    },
                    {
                        "id": "(Liang et al., 2025)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation (Wang et al., 2024). The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse (Mokady et al., 2023)- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33]."
                        ],
                        "paper": {
                            "corpus_id": 278171703,
                            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2278618341",
                                    "name": "Yuzhe Liang"
                                },
                                {
                                    "authorId": "2358111037",
                                    "name": "Wenzhe Liu"
                                },
                                {
                                    "authorId": "2358041541",
                                    "name": "Chunyu Qiang"
                                },
                                {
                                    "authorId": "2229877177",
                                    "name": "Zhikang Niu"
                                },
                                {
                                    "authorId": "2324996330",
                                    "name": "Yushen Chen"
                                },
                                {
                                    "authorId": "2116609277",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2278584538",
                                    "name": "Wenxi Chen"
                                },
                                {
                                    "authorId": "2358116915",
                                    "name": "Nan Li"
                                },
                                {
                                    "authorId": "2358098456",
                                    "name": "Chen Zhang"
                                },
                                {
                                    "authorId": "2321881822",
                                    "name": "Xie Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io."
                        ],
                        "paper": {
                            "corpus_id": 270220558,
                            "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching",
                            "authors": [
                                {
                                    "authorId": "2292208450",
                                    "name": "Yongqi Wang"
                                },
                                {
                                    "authorId": "2304739202",
                                    "name": "Wenxiang Guo"
                                },
                                {
                                    "authorId": "2048021099",
                                    "name": "Rongjie Huang"
                                },
                                {
                                    "authorId": "3068086",
                                    "name": "Jia-Bin Huang"
                                },
                                {
                                    "authorId": "2258561621",
                                    "name": "Zehan Wang"
                                },
                                {
                                    "authorId": "2292197957",
                                    "name": "Fuming You"
                                },
                                {
                                    "authorId": "2181010470",
                                    "name": "Ruiqi Li"
                                },
                                {
                                    "authorId": "2304453961",
                                    "name": "Zhou Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13
                        },
                        "score": 0
                    },
                    {
                        "id": "(Meekeren et al., 2023)",
                        "snippets": [
                            "While prompt-based image generation can produce realistic images, using Classifier-free Guidance (CFG) [42] can provide even greater control over the generation process. CFG, in Stable Diffusion, amplifies the effect of the text prompt on the generated image. By default, Stable Diffusion applies a classifier to the text prompt to guide the generation of the image [12]. However, CFG allows for fine-tuning the influence of this classifier, resulting in more creative control over the generated image. Typically, CFG is defined to be in a range between 1 and 30, with lower values generating more creative images. \n\nMore specifically, CFG determines the trade-off between the coverage of modes and image fidelity [42]. When set to 1, the model generates samples based solely on the prior distribution, without any guidance. As the guidance scale increases, the model is instructed to produce samples that better match some given condition. The classifier-free aspect of this technique refers to the fact that it does not require training a classifier to incorporate guidance during generation [42]. Instead, the model is guided by adding a penalty term to the generation process, forcing the generated images to match the desired condition."
                        ],
                        "paper": {
                            "corpus_id": 259212148,
                            "title": "Exploring the Effectiveness of Dataset Synthesis: An application of Apple Detection in Orchards",
                            "authors": [
                                {
                                    "authorId": "104149480",
                                    "name": "A. V. Meekeren"
                                },
                                {
                                    "authorId": "51304755",
                                    "name": "Maya Aghaei"
                                },
                                {
                                    "authorId": "40109500",
                                    "name": "K. Dijkstra"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9453125
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u."
                        ],
                        "paper": {
                            "corpus_id": 255372955,
                            "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
                            "authors": [
                                {
                                    "authorId": "2914394",
                                    "name": "Huiwen Chang"
                                },
                                {
                                    "authorId": "2146204239",
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "152630175",
                                    "name": "Jarred Barber"
                                },
                                {
                                    "authorId": "2199119286",
                                    "name": "AJ Maschinot"
                                },
                                {
                                    "authorId": "143923528",
                                    "name": "Jos\u00e9 Lezama"
                                },
                                {
                                    "authorId": "39978626",
                                    "name": "Lu Jiang"
                                },
                                {
                                    "authorId": "152790163",
                                    "name": "Ming Yang"
                                },
                                {
                                    "authorId": "1702318",
                                    "name": "K. Murphy"
                                },
                                {
                                    "authorId": "1768236",
                                    "name": "W. Freeman"
                                },
                                {
                                    "authorId": "144544291",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "2167749913",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "1707347",
                                    "name": "Dilip Krishnan"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 556
                        },
                        "score": 0.94384765625
                    },
                    {
                        "id": "(Gu et al., 2024)",
                        "snippets": [
                            "Classifier-free Guidance (CFG), which utilizes the diffusion model itself to perform guidance at test time. More specifically, we perform sampling using the following linear combination:\n\nwhere \u03b3 is the guidance weight, and x \u03b8 (x t ) = x \u03b8 (x t , c = \u2205) is the unconditional denoising output.\n\nDuring training, we drop the condition c with certain probability p uncond to facilitate unconditional prediction. When \u03b3 > 1, CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation",
                            ".However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both."
                        ],
                        "paper": {
                            "corpus_id": 270199289,
                            "title": "Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling",
                            "authors": [
                                {
                                    "authorId": "2287733778",
                                    "name": "Jiatao Gu"
                                },
                                {
                                    "authorId": "2304371756",
                                    "name": "Ying Shen"
                                },
                                {
                                    "authorId": "2443456",
                                    "name": "Shuangfei Zhai"
                                },
                                {
                                    "authorId": "2254045488",
                                    "name": "Yizhe Zhang"
                                },
                                {
                                    "authorId": "3111912",
                                    "name": "N. Jaitly"
                                },
                                {
                                    "authorId": "49158771",
                                    "name": "J. Susskind"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 10
                        },
                        "score": 0.94580078125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Case Studies",
                "tldr": "Classifier-free guidance has been successfully applied across diverse NLP and multimodal applications, demonstrating particular value in controllable text generation, image editing, and text-to-video generation. Real-world implementations show how CFG enables precise control over generated content while balancing fidelity and diversity requirements. (6 sources)",
                "text": "\nClassifier-free guidance has been implemented in a variety of practical applications, demonstrating its versatility across both pure NLP and multimodal systems. In text-to-image generation, CFG has become a standard technique for improving alignment between textual prompts and generated visual content. When applied to slot-based image generation systems like LSSD (Latent Slot-based Scene Decomposition), CFG can be applied directly to input slots without requiring model modifications, significantly improving image quality while maintaining the semantic structure dictated by the text input <Paper corpusId=\"257632090\" paperTitle=\"(Jiang et al., 2023)\" isShortName></Paper>.\n\nFor music generation and editing, CFG has been combined with diffusion models to enable controllable manipulation of audio characteristics. The InstructME system employed both classifier guidance and classifier-free guidance approaches to achieve a balance between quality and controllability, particularly for specialized tasks like instrument and genre manipulation in music remixing <Paper corpusId=\"261242901\" paperTitle=\"(Han et al., 2023)\" isShortName></Paper>. This hybrid approach demonstrates how CFG can be adapted to domain-specific requirements where training data might have unique characteristics that make pure CFG challenging to implement.\n\nIn image editing applications, CFG has been leveraged to create sophisticated inpainting systems. The ControlFill framework uses learned embeddings to guide diffusion networks, allowing users to control the intensity of object removal or creation by adjusting the relative significance of different prompts through classifier-free guidance <Paper corpusId=\"276813045\" paperTitle=\"(Jeon, 2025)\" isShortName></Paper>. This application demonstrates how CFG can enable user-controlled content generation without requiring text encoders during inference, making the approach more computationally efficient.\n\nText-to-video generation represents another important case study where CFG has been successfully implemented. By extending the CFG mechanism from diffusion models to autoregressive language model-based video generation frameworks, developers have achieved a balance between semantic coherence and visual diversity <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>. Empirical investigations across various CFG configurations have revealed that scales within the range of 5.0 to 7.5 typically produce high-quality videos with optimal adherence to input prompts, highlighting the importance of parameter tuning in practical applications.\n\nThe joint training approach of unconditional and conditional models within a single architecture has proven particularly valuable for resource-constrained applications. By setting the condition to a null token during training, models can learn both conditional and unconditional generation capabilities simultaneously <Paper corpusId=\"274422874\" paperTitle=\"(Hyung et al., 2024)\" isShortName></Paper>. This approach has enabled high-fidelity outputs in various generative tasks while maintaining computational efficiency.\n\nIn controlled scene generation tasks, CFG provides an effective mechanism for balancing accuracy and diversity. By adjusting the guidance weight parameter, developers can achieve the appropriate balance for specific use cases, making the technique highly adaptable to different application requirements <Paper corpusId=\"276741036\" paperTitle=\"(Bian et al., 2024)\" isShortName></Paper>. This adaptability has contributed to CFG's widespread adoption across diverse generative applications.",
                "citations": [
                    {
                        "id": "(Jiang et al., 2023)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) uses a mixing weight (abbreviated as cfg) to control the weighted sum of noise predictions between a conditional and an unconditional diffusion model. With cf g = 1, it operates similarly to standard conditional generation. When cf g > 1, the model generates images that are more align to the text input, but comes with a cost of reduced image diversity. Practically speaking, an optimal cf g > 1 can yield images that not only better match the conditions but also exhibit enhanced quality. Since LSSD provides object slots as text tokens to the pre-trained DMs, classifier-free guidance can be applied directly to the input slot, akin to how it's applied to input text in standard DMs, without any model modifications. And our experiment shows that a cf g > 1 for slots can also significantly improve the image generation quality."
                        ],
                        "paper": {
                            "corpus_id": 257632090,
                            "title": "Object-Centric Slot Diffusion",
                            "authors": [
                                {
                                    "authorId": "2211907956",
                                    "name": "Jindong Jiang"
                                },
                                {
                                    "authorId": "2212038042",
                                    "name": "Fei Deng"
                                },
                                {
                                    "authorId": "2212051383",
                                    "name": "Gautam Singh"
                                },
                                {
                                    "authorId": "2112164991",
                                    "name": "S. Ahn"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 61
                        },
                        "score": 0.931640625
                    },
                    {
                        "id": "(Han et al., 2023)",
                        "snippets": [
                            "For diffusion models, there exist two primary strategies for achieving controllable generation. One of these is classifier guidance (CG) (Dhariwal and Nichol 2021; Liu et al. 2023b), which utilizes a classifier during the sampling process and mixes its input gradient of the log probability with the score estimate of diffusion model. It is flexible and controllable, but tends to suffer a performance degradation (Ho and Salimans 2022). Another approach, named classifierfree guidance (CFG) (Ho and Salimans 2022;Nichol et al. 2021;Ramesh et al. 2022;Saharia et al. 2022), achieves the same effect through training a conditional diffusion model directly without a guidance classifier. This method performs better but requires a large amount of data with diverse text descriptions, which is difficult for our InstructME trained with source-target paired data. In this work, to attain a tradeoff between quality and controllability, we adopt both classifier and classifier-free guidance to achieve the controllable editing of Remix operations.\n\nWe specify instrument and genre tags with CFG by incorporating these tags into text commands to train the conditional diffusion models. During the training, we discard our text condition y randomly with a certain probability p CFG following (Liu et al. 2023a;Wang et al. 2023). Then, in the sampling, we can estimate the noise \u03b5\u03b8 (t, T (y), p s , z s , z t ) with a linear combination of the conditional and unconditional score estimates:"
                        ],
                        "paper": {
                            "corpus_id": 261242901,
                            "title": "InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2118914313",
                                    "name": "Bing Han"
                                },
                                {
                                    "authorId": "2147243559",
                                    "name": "Junyu Dai"
                                },
                                {
                                    "authorId": "84556222",
                                    "name": "Xuchen Song"
                                },
                                {
                                    "authorId": "3314779",
                                    "name": "Weituo Hao"
                                },
                                {
                                    "authorId": "2234532553",
                                    "name": "Xinyan He"
                                },
                                {
                                    "authorId": "2234359627",
                                    "name": "Dong Guo"
                                },
                                {
                                    "authorId": "2855690",
                                    "name": "Jitong Chen"
                                },
                                {
                                    "authorId": "2234520458",
                                    "name": "Yuxuan Wang"
                                },
                                {
                                    "authorId": "2480051",
                                    "name": "Y. Qian"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 16
                        },
                        "score": 0.953125
                    },
                    {
                        "id": "(Jeon, 2025)",
                        "snippets": [
                            "In this report, I present an inpainting framework named \\textit{ControlFill}, which involves training two distinct prompts: one for generating plausible objects within a designated mask (\\textit{creation}) and another for filling the region by extending the background (\\textit{removal}). During the inference stage, these learned embeddings guide a diffusion network that operates without requiring heavy text encoders. By adjusting the relative significance of the two prompts and employing classifier-free guidance, users can control the intensity of removal or creation. Furthermore, I introduce a method to spatially vary the intensity of guidance by assigning different scales to individual pixels."
                        ],
                        "paper": {
                            "corpus_id": 276813045,
                            "title": "ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning",
                            "authors": [
                                {
                                    "authorId": "2348738665",
                                    "name": "Boseong Jeon"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Zhou et al., 2025)",
                        "snippets": [
                            "Inspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity.\n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale\n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10."
                        ],
                        "paper": {
                            "corpus_id": 277043912,
                            "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2282670286",
                                    "name": "Ziqin Zhou"
                                },
                                {
                                    "authorId": "2331570564",
                                    "name": "Yifan Yang"
                                },
                                {
                                    "authorId": "2125051198",
                                    "name": "Yuqing Yang"
                                },
                                {
                                    "authorId": "2350871281",
                                    "name": "Tianyu He"
                                },
                                {
                                    "authorId": "2350821834",
                                    "name": "Houwen Peng"
                                },
                                {
                                    "authorId": "2268758860",
                                    "name": "Kai Qiu"
                                },
                                {
                                    "authorId": "2329560121",
                                    "name": "Qi Dai"
                                },
                                {
                                    "authorId": "2160727304",
                                    "name": "Lili Qiu"
                                },
                                {
                                    "authorId": "2294680622",
                                    "name": "Chong Luo"
                                },
                                {
                                    "authorId": "2320820962",
                                    "name": "Lingqiao Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Hyung et al., 2024)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) [10] uses Bayes' rule to replace a classifier-guided score with a linear combination of conditional and unconditional score estimates: \n\nCFG jointly trains the unconditional model \u03f5 \u03b8 (x t |\u03d5) and the conditional model \u03f5 \u03b8 (x t |c) (= \u03f5 \u03b8 (x t )) within a single model by setting the condition c to a null token \u03d5. Using this guided denoising process, the model better captures the conditions and often generates high-fidelity outputs."
                        ],
                        "paper": {
                            "corpus_id": 274422874,
                            "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
                            "authors": [
                                {
                                    "authorId": "2071816370",
                                    "name": "J. Hyung"
                                },
                                {
                                    "authorId": "2333250087",
                                    "name": "Kinam Kim"
                                },
                                {
                                    "authorId": "2186865215",
                                    "name": "Susung Hong"
                                },
                                {
                                    "authorId": "2117955517",
                                    "name": "Minjeong Kim"
                                },
                                {
                                    "authorId": "1795455",
                                    "name": "J. Choo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.9228515625
                    },
                    {
                        "id": "(Bian et al., 2024)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(x|c) and unconditional generation p(x), and guidance during sampling is provided by the following equation: \n\nwhere xt (c) is the result conditioned on c, xt (\u2205) is the unconditioned result, and w is a weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved."
                        ],
                        "paper": {
                            "corpus_id": 276741036,
                            "title": "DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes",
                            "authors": [
                                {
                                    "authorId": "2327218388",
                                    "name": "Hengwei Bian"
                                },
                                {
                                    "authorId": "2152007435",
                                    "name": "Lingdong Kong"
                                },
                                {
                                    "authorId": "2237483104",
                                    "name": "Haozhe Xie"
                                },
                                {
                                    "authorId": "2272233402",
                                    "name": "Liang Pan"
                                },
                                {
                                    "authorId": "2327935778",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "2277717448",
                                    "name": "Ziwei Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 5
                        },
                        "score": 0.9580078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.19859100000000002
    }
}